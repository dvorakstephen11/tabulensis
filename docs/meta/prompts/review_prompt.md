# Codebase Context for Review

## Directory Structure

```text
/
  .cursorignore
  .cursorindexingignore
  .gitattributes.example
  .github/
    workflows/
      ci.yml
      fuzz.yml
      pages.yml
      perf.yml
      perf_e2e.yml
      perf_fullscale.yml
      release.yml
      wasm.yml
      web_ui_tests.yml
  .gitignore
  benchmarks/
    baselines/
      e2e.json
      full-scale.json
      gate.json
      quick.json
    latest_e2e.csv
    latest_e2e.json
    latest_fullscale.csv
    latest_fullscale.json
    latest_gate.csv
    latest_gate.json
    latest_quick.csv
    latest_quick.json
    README.md
    results/
      .gitkeep
      2025-12-12_163759.json
      2025-12-12_175341.json
      2025-12-12_203400.json
      2025-12-12_203454.json
      2025-12-12_223643.json
      2025-12-13_000346.json
      2025-12-13_000410.json
      2025-12-13_155200_fullscale.json
      2025-12-13_165236_fullscale.json
      2025-12-13_174735_fullscale.json
      2025-12-13_174822_fullscale.json
      2025-12-13_175318_fullscale.json
      2025-12-13_202028.json
      2025-12-13_202327_fullscale.json
      2025-12-14_003645.json
      2025-12-14_004611.json
      2025-12-14_004643_fullscale.json
      2025-12-14_005407_fullscale.json
      2025-12-14_202417_fullscale.json
      2025-12-15_183914.json
      2025-12-15_191921_fullscale.json
      2025-12-17_164231_fullscale.json
      2025-12-17_164437_fullscale.json
      2025-12-18_221709_fullscale.json
      2025-12-22_191531_fullscale.json
      2025-12-23_132920_fullscale.json
      2025-12-23_173949_fullscale.json
      2025-12-23_190426_fullscale_parallel.json
      2025-12-23_190454_fullscale.json
      2025-12-23_214522_fullscale_parallel.json
      2025-12-23_214545_fullscale_parallel.json
      2025-12-23_215619.json
      2025-12-23_215710_fullscale_parallel.json
      2025-12-23_220012_fullscale_parallel.json
      2025-12-23_220029_fullscale_parallel.json
      combined_results.csv
      plots/
        commit_comparison.png
        latest_comparison.png
        metric_breakdown_fullscale.png
        metric_breakdown_quick.png
        speedup_heatmap.png
        time_trends.png
        trend_summary.md
    results_e2e/
      .gitkeep
      2025-12-31_150320.json
      2025-12-31_150514.json
      2025-12-31_150704.json
      2025-12-31_150906.json
      2025-12-31_164706.json
      2025-12-31_164838.json
      2025-12-31_165012.json
      2025-12-31_165858.json
      2025-12-31_170022.json
      2025-12-31_194404.json
      2026-01-01_213949.json
      2026-01-01_214159.json
      2026-01-02_132622.json
      2026-01-02_145448.json
      2026-01-02_160100.json
      2026-01-02_175352.json
    wasm_memory_budgets.json
  Cargo.lock
  Cargo.toml
  cli/
    Cargo.toml
    src/
      commands/
        diff.rs
        host.rs
        info.rs
        mod.rs
      main.rs
      output/
        git_diff.rs
        json.rs
        mod.rs
        text.rs
    tests/
      determinism_cli_json.rs
      git_textconv.rs
      integration_tests.rs
  core/
    benches/
      diff_benchmarks.rs
    Cargo.lock
    Cargo.toml
    examples/
      basic_diff.rs
      custom_config.rs
      database_mode.rs
      streaming.rs
    fuzz/
      Cargo.toml
      corpus/
        fuzz_datamashup_parse/
          base_query.bin
          duplicate_datamashup_parts_a09562b6.bin
          mashup_base64_whitespace_a09562b6.bin
          mashup_utf16_le_a09562b6.bin
          multi_query_with_embedded_84afdf77.bin
          one_query_a09562b6.bin
          pbix_embedded_queries_81c5ac5b.bin
          pbix_legacy_multi_query_a_e527a09a.bin
          pbix_legacy_one_query_a_e527a09a.bin
          permissions_defaults_7dd163ff.bin
        fuzz_diff_grids/
          seed.bin
        fuzz_m_section_and_ast/
          if_then_else.txt
          lambda.txt
          let_in.txt
          literal.txt
          record_list.txt
          try_otherwise.txt
        fuzz_open_pbix/
          pbit_model_a.pbit
          pbix_embedded_queries.pbix
          pbix_legacy_multi_query_a.pbix
          pbix_legacy_one_query_a.pbix
          pbix_no_datamashup_no_schema.pbix
        fuzz_open_workbook/
          corrupt_base64.xlsx
          duplicate_datamashup_parts.xlsx
          mashup_base64_whitespace.xlsx
          mashup_utf16_le.xlsx
          minimal.xlsx
          multi_query_with_embedded.xlsx
          no_content_types.xlsx
          not_a_zip.txt
          one_query.xlsx
          permissions_defaults.xlsx
          random_zip.zip
          vba_base.xlsm
      fuzz_targets/
        fuzz_datamashup_parse.rs
        fuzz_diff_grids.rs
        fuzz_m_section_and_ast.rs
        fuzz_open_pbix.rs
        fuzz_open_workbook.rs
      seed_fixtures.yaml
    src/
      addressing.rs
      alignment/
        anchor_chain.rs
        anchor_discovery.rs
        assembly.rs
        gap_strategy.rs
        lap.rs
        mod.rs
        move_extraction.rs
        runs.rs
      alignment_types.rs
      architecture.md
      bin/
        wasm_smoke.rs
      capabilities.rs
      column_alignment.rs
      config.rs
      container.rs
      database_alignment.rs
      datamashup.rs
      datamashup_framing.rs
      datamashup_package.rs
      dax.rs
      diff.rs
      diffable.rs
      engine/
        amr.rs
        context.rs
        grid_diff.rs
        grid_primitives.rs
        hardening.rs
        mod.rs
        move_mask.rs
        sheet_diff.rs
        workbook_diff.rs
      error_codes.rs
      excel_open_xml.rs
      formula.rs
      formula_diff.rs
      grid_metadata.rs
      grid_parser.rs
      grid_view.rs
      hashing.rs
      lib.rs
      m_ast/
        step_model.rs
      m_ast.rs
      m_ast_diff/
        apted.rs
        gumtree.rs
        mod.rs
      m_diff.rs
      m_section.rs
      m_semantic_detail.rs
      matching/
        hungarian.rs
        mod.rs
      memory_estimate.rs
      memory_metrics.rs
      model.rs
      model_diff.rs
      object_diff.rs
      output/
        json.rs
        json_lines.rs
        mod.rs
      package.rs
      perf.rs
      permission_bindings.rs
      policy.rs
      progress.rs
      rect_block_move.rs
      region_mask.rs
      row_alignment.rs
      session.rs
      sink.rs
      string_pool.rs
      tabular_schema.rs
      vba.rs
      workbook.rs
    tests/
      addressing_pg2_tests.rs
      amr_multi_gap_tests.rs
      branch4_object_diff_tests.rs
      common/
        mod.rs
      d1_database_mode_tests.rs
      d2_d4_database_mode_workbook_tests.rs
      data_mashup_tests.rs
      database_mode_wrapper_tests.rs
      e2e_perf_workbook_open.rs
      engine_tests.rs
      excel_open_xml_tests.rs
      f7_formula_canonicalization_tests.rs
      f7_formula_diff_integration_tests.rs
      f7_formula_parser_tests.rs
      f7_formula_shift_tests.rs
      g10_row_block_alignment_grid_workbook_tests.rs
      g11_row_block_move_grid_workbook_tests.rs
      g12_column_block_move_grid_workbook_tests.rs
      g12_rect_block_move_grid_workbook_tests.rs
      g13_fuzzy_row_move_grid_workbook_tests.rs
      g14_move_combination_tests.rs
      g15_column_structure_row_alignment_tests.rs
      g1_g2_grid_workbook_tests.rs
      g5_g7_grid_workbook_tests.rs
      g8_row_alignment_grid_workbook_tests.rs
      g9_column_alignment_grid_workbook_tests.rs
      grid_view_hashstats_tests.rs
      grid_view_tests.rs
      hardening_tests.rs
      integration_test.rs
      leaf_diff_equivalence_tests.rs
      limit_behavior_tests.rs
      m10_embedded_m_diff_tests.rs
      m10_m_parser_tier2_tests.rs
      m4_package_parts_tests.rs
      m4_permissions_metadata_tests.rs
      m5_query_domain_tests.rs
      m6_textual_m_diff_tests.rs
      m7_ast_canonicalization_tests.rs
      m7_semantic_m_diff_tests.rs
      m8_m_canonicalize_tokens_tests.rs
      m8_m_parser_coverage_audit_tests.rs
      m8_m_parser_expansion_tests.rs
      m8_semantic_m_diff_nonlet_tests.rs
      m9_composed_end_to_end_tests.rs
      m9_m_parser_tier1_tests.rs
      m_section_splitting_tests.rs
      metrics_unit_tests.rs
      output_tests.rs
      package_streaming_tests.rs
      parallel_determinism_tests.rs
      pbix_host_support_tests.rs
      perf_large_grid_tests.rs
      permission_bindings_tests.rs
      pg1_ir_tests.rs
      pg3_snapshot_tests.rs
      pg4_diffop_tests.rs
      pg5_grid_diff_tests.rs
      pg6_object_vs_grid_tests.rs
      robustness_regressions.yaml
      robustness_regressions_tests.rs
      schema_guard_tests.rs
      signature_tests.rs
      sparse_grid_tests.rs
      streaming_contract_tests.rs
      streaming_determinism_tests.rs
      streaming_sink_tests.rs
      string_pool_tests.rs
  desktop/
    src-tauri/
      build.rs
      Cargo.toml
      gen/
        schemas/
          acl-manifests.json
          capabilities.json
          desktop-schema.json
          windows-schema.json
      icons/
        icon.ico
      src/
        batch.rs
        diff_runner.rs
        export/
          audit_xlsx.rs
          mod.rs
        main.rs
        search.rs
        store/
          mod.rs
          op_sink.rs
          op_store.rs
          schema.sql
          types.rs
      tauri.conf.json
  fixtures/
    manifest.yaml
    manifest_cli_tests.lock.json
    manifest_cli_tests.yaml
    manifest_perf_e2e.yaml
    manifest_release_smoke.yaml
    pyproject.toml
    README.md
    requirements.txt
    src/
      __init__.py
      generate.py
      generators/
        __init__.py
        base.py
        corrupt.py
        database.py
        grid.py
        mashup.py
        objects.py
        pbix.py
        perf.py
        xlsb.py
  ideas.md
  logs/
    2025-11-28b-diffop-pg4/
      activity_log.txt
  packaging/
    homebrew/
      excel-diff.rb.template
    scoop/
      excel-diff.json.template
  plan_review.md
  README.md
  related_files.txt.md
  scripts/
    add_regression_fixture.py
    arch_guard.py
    check_fixture_references.py
    check_perf_thresholds.py
    combine_results_to_csv.py
    compare_perf_results.py
    dev_test.py
    export_e2e_metrics.py
    export_perf_metrics.py
    fuzz_corpus_maint.py
    fuzz_triage.py
    generate_web_cli_fixtures.py
    ingest_private_corpus.py
    seed_fuzz_corpus.py
    update_baselines.py
    verify_release_versions.py
    visualize_benchmarks.py
    wasm_memory_harness.cjs
  ui_payload/
    Cargo.toml
    src/
      alignment.rs
      capabilities.rs
      lib.rs
      options.rs
      outcome.rs
  wasm/
    Cargo.toml
    src/
      lib.rs
  web/
    diff_worker.js
    diff_worker_client.js
    export.js
    grid_metrics.js
    grid_painter.js
    grid_theme.js
    grid_viewer.js
    index.html
    main.js
    native_diff_client.js
    package.json
    platform.js
    render.js
    test_outcome_payload.js
    test_render.js
    test_view_model.js
    testdata/
      sample_outcome.json
      sample_payload.json
      sample_report.json
    view_model.js
```

## File Contents

### File: `.github\workflows\ci.yml`

```yaml
name: CI

on:
  push:
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install fixture generator deps
        run: python -m pip install -r fixtures/requirements.txt

      - name: Install fixture generator
        run: python -m pip install -e fixtures --no-deps

      - name: Fixture reference guard
        run: python scripts/check_fixture_references.py

      - name: Generate test fixtures
        run: generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --force --clean

      - name: Verify fixture checksums
        run: generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --verify-lock fixtures/manifest_cli_tests.lock.json

      - name: Run tests
        run: cargo test --workspace

      - name: Architecture guard
        run: python scripts/arch_guard.py

      - name: Run parallel feature tests
        run: cargo test -p excel_diff --features parallel

      - name: Build examples
        run: cargo build --workspace --examples

      - name: Run clippy (deny unwrap/expect)
        run: cargo clippy --workspace -- -D clippy::unwrap_used -D clippy::expect_used

  test-smoke-matrix:
    name: Smoke Tests (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install fixture generator deps
        run: python -m pip install -r fixtures/requirements.txt

      - name: Install fixture generator
        run: python -m pip install -e fixtures --no-deps

      - name: Generate test fixtures
        run: generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --force --clean

      - name: Run core smoke tests
        run: cargo test -p excel_diff

      - name: Run CLI smoke tests
        run: cargo test -p excel_diff_cli

  build-matrix:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Check core (default features)
        run: cargo check -p excel_diff

      - name: Check core (engine-wasm)
        run: cargo check -p excel_diff --no-default-features --features "excel-open-xml,model-diff"

      - name: Feature audit (default)
        run: cargo tree -p excel_diff -e features

      - name: Feature audit (engine-wasm)
        run: cargo tree -p excel_diff -e features --no-default-features --features "excel-open-xml,model-diff"

      - name: Feature audit (excel-open-xml only)
        run: cargo tree -p excel_diff -e features --no-default-features --features "excel-open-xml"

      - name: Feature audit (model-diff only)
        run: cargo tree -p excel_diff -e features --no-default-features --features "model-diff"

      - name: Check ui_payload
        run: cargo check -p ui_payload

      - name: Check wasm bindings
        run: cargo check -p excel_diff_wasm --target wasm32-unknown-unknown

      - name: Check desktop crate
        run: cargo check -p excel_diff_desktop


```

---

### File: `.github\workflows\fuzz.yml`

```yaml
name: Fuzzing

on:
  schedule:
    - cron: "0 4 * * 0"
  workflow_dispatch: {}

jobs:
  fuzz:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust (nightly)
        uses: dtolnay/rust-action@stable
        with:
          toolchain: nightly

      - name: Install cargo-fuzz
        run: cargo install cargo-fuzz --locked

      - name: Fuzz core targets
        working-directory: core/fuzz
        run: |
          set -e
          for target in fuzz_datamashup_parse fuzz_diff_grids fuzz_m_section_and_ast fuzz_open_workbook fuzz_open_pbix; do
            echo "Running $target"
            cargo fuzz run "$target" -max_total_time=60
          done

      - name: Upload fuzz artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fuzz-artifacts
          path: |
            core/fuzz/artifacts
          if-no-files-found: ignore

```

---

### File: `.github\workflows\pages.yml`

```yaml
name: Deploy Web Demo

on:
  push:
    branches: [main, master]
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

      - name: Build WASM package
        run: wasm-pack build wasm --release --target web --out-dir ../web/wasm

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'web'

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4


```

---

### File: `.github\workflows\perf.yml`

```yaml
name: Performance Regression

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  perf-regression:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-action@stable
        
      - name: Build with perf metrics
        run: cargo build --release --features perf-metrics
        working-directory: core

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Check perf thresholds (quick suite)
        run: python scripts/check_perf_thresholds.py --suite quick --baseline benchmarks/baselines/quick.json --export-csv benchmarks/latest_quick.csv --export-json benchmarks/latest_quick.json

      - name: Check perf thresholds (gate suite)
        env:
          EXCEL_DIFF_PERF_SLACK_FACTOR: "1.3"
        run: python scripts/check_perf_thresholds.py --suite gate --baseline benchmarks/baselines/gate.json --test-target perf_large_grid_tests --export-csv benchmarks/latest_gate.csv --export-json benchmarks/latest_gate.json

      - name: Upload perf artifacts
        uses: actions/upload-artifact@v4
        with:
          name: perf-quick
          path: |
            benchmarks/latest_quick.csv
            benchmarks/latest_quick.json

      - name: Upload gate perf artifacts
        uses: actions/upload-artifact@v4
        with:
          name: perf-gate
          path: |
            benchmarks/latest_gate.csv
            benchmarks/latest_gate.json


```

---

### File: `.github\workflows\perf_e2e.yml`

```yaml
name: Performance E2E

on:
  schedule:
    - cron: "0 4 * * *"
  workflow_dispatch: {}

jobs:
  perf-e2e:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-action@stable

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install fixture generator deps
        run: python -m pip install -r fixtures/requirements.txt

      - name: Install fixture generator
        run: python -m pip install -e fixtures --no-deps

      - name: Generate e2e fixtures
        run: generate-fixtures --manifest fixtures/manifest_perf_e2e.yaml --force --clean

      - name: Export + enforce e2e metrics
        run: python scripts/export_e2e_metrics.py --skip-fixtures --baseline benchmarks/baselines/e2e.json --export-csv benchmarks/latest_e2e.csv

      - name: Upload perf artifacts
        uses: actions/upload-artifact@v4
        with:
          name: perf-e2e
          path: |
            benchmarks/latest_e2e.json
            benchmarks/latest_e2e.csv
            benchmarks/results_e2e/*.json

```

---

### File: `.github\workflows\perf_fullscale.yml`

```yaml
name: Performance Full Scale

on:
  schedule:
    - cron: "0 3 * * *"
  workflow_dispatch: {}

jobs:
  perf-fullscale:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-action@stable

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Run full-scale perf suite + gates
        run: python scripts/check_perf_thresholds.py --suite full-scale --baseline benchmarks/baselines/full-scale.json --export-csv benchmarks/latest_fullscale.csv --export-json benchmarks/latest_fullscale.json

      - name: Upload perf artifacts
        uses: actions/upload-artifact@v4
        with:
          name: perf-fullscale
          path: |
            benchmarks/latest_fullscale.csv
            benchmarks/latest_fullscale.json

```

---

### File: `.github\workflows\release.yml`

```yaml
name: Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (do not create release)'
        required: false
        default: 'true'
        type: boolean

env:
  CARGO_TERM_COLOR: always

jobs:
  validate-release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Verify tag matches crate versions
        run: python3 scripts/verify_release_versions.py

  build-windows:
    needs: validate-release
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build release binary
        run: cargo build --release --locked -p excel_diff_cli

      - name: Get version
        id: version
        shell: bash
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          else
            echo "version=v0.0.0-dev" >> $GITHUB_OUTPUT
          fi

      - name: Create standalone EXE
        shell: bash
        run: |
          cp target/release/excel-diff.exe excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.exe

      - name: Create package directory
        shell: bash
        run: |
          mkdir -p staging/excel-diff-${{ steps.version.outputs.version }}-windows-x86_64
          cp target/release/excel-diff.exe staging/excel-diff-${{ steps.version.outputs.version }}-windows-x86_64/
          cp README.md staging/excel-diff-${{ steps.version.outputs.version }}-windows-x86_64/

      - name: Create ZIP archive
        shell: pwsh
        run: |
          Compress-Archive -Path staging/excel-diff-${{ steps.version.outputs.version }}-windows-x86_64 -DestinationPath excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip

      - name: Compute SHA256
        shell: bash
        run: |
          sha256sum excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip > excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip.sha256
          sha256sum excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.exe > excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.exe.sha256

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: windows-x86_64
          path: |
            excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.exe
            excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.exe.sha256
            excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip
            excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip.sha256

  build-macos-x64:
    needs: validate-release
    runs-on: macos-13
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build release binary
        run: cargo build --release --locked -p excel_diff_cli

      - name: Get version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          else
            echo "version=v0.0.0-dev" >> $GITHUB_OUTPUT
          fi

      - name: Create tarball
        run: |
          mkdir -p staging
          cp target/release/excel-diff staging/
          cp README.md staging/
          tar -czvf excel-diff-${{ steps.version.outputs.version }}-macos-x86_64.tar.gz -C staging .

      - name: Compute SHA256
        run: |
          shasum -a 256 excel-diff-${{ steps.version.outputs.version }}-macos-x86_64.tar.gz > excel-diff-${{ steps.version.outputs.version }}-macos-x86_64.tar.gz.sha256

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: macos-x86_64
          path: |
            excel-diff-${{ steps.version.outputs.version }}-macos-x86_64.tar.gz
            excel-diff-${{ steps.version.outputs.version }}-macos-x86_64.tar.gz.sha256

      - name: Upload raw binary for universal
        uses: actions/upload-artifact@v4
        with:
          name: macos-x86_64-binary
          path: target/release/excel-diff

  build-macos-arm64:
    needs: validate-release
    runs-on: macos-14
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build release binary
        run: cargo build --release --locked -p excel_diff_cli

      - name: Get version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          else
            echo "version=v0.0.0-dev" >> $GITHUB_OUTPUT
          fi

      - name: Create tarball
        run: |
          mkdir -p staging
          cp target/release/excel-diff staging/
          cp README.md staging/
          tar -czvf excel-diff-${{ steps.version.outputs.version }}-macos-arm64.tar.gz -C staging .

      - name: Compute SHA256
        run: |
          shasum -a 256 excel-diff-${{ steps.version.outputs.version }}-macos-arm64.tar.gz > excel-diff-${{ steps.version.outputs.version }}-macos-arm64.tar.gz.sha256

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: macos-arm64
          path: |
            excel-diff-${{ steps.version.outputs.version }}-macos-arm64.tar.gz
            excel-diff-${{ steps.version.outputs.version }}-macos-arm64.tar.gz.sha256

      - name: Upload raw binary for universal
        uses: actions/upload-artifact@v4
        with:
          name: macos-arm64-binary
          path: target/release/excel-diff

  build-macos-universal:
    runs-on: macos-14
    needs: [build-macos-x64, build-macos-arm64]
    steps:
      - uses: actions/checkout@v4

      - name: Get version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
          else
            echo "version=v0.0.0-dev" >> $GITHUB_OUTPUT
          fi

      - name: Download x86_64 binary
        uses: actions/download-artifact@v4
        with:
          name: macos-x86_64-binary
          path: x86_64

      - name: Download arm64 binary
        uses: actions/download-artifact@v4
        with:
          name: macos-arm64-binary
          path: arm64

      - name: Create universal binary
        run: |
          lipo -create x86_64/excel-diff arm64/excel-diff -output excel-diff
          file excel-diff
          codesign -s - excel-diff

      - name: Create tarball
        run: |
          mkdir -p staging
          cp excel-diff staging/
          cp README.md staging/
          tar -czvf excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz -C staging .

      - name: Compute SHA256
        run: |
          shasum -a 256 excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz > excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz.sha256
          cat excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz.sha256

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: macos-universal
          path: |
            excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz
            excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz.sha256

  smoke-macos-sequoia:
    name: Smoke Test (macOS Sequoia)
    runs-on: macos-15
    needs: [build-macos-universal]
    steps:
      - uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install fixture generator deps
        run: python -m pip install -r fixtures/requirements.txt

      - name: Install fixture generator
        run: python -m pip install -e fixtures --no-deps

      - name: Generate smoke fixtures
        run: generate-fixtures --manifest fixtures/manifest_release_smoke.yaml --force --clean

      - name: Download macOS universal artifact
        uses: actions/download-artifact@v4
        with:
          name: macos-universal
          path: artifacts/macos

      - name: Run smoke checks
        run: |
          set -euo pipefail

          TARBALL=$(ls artifacts/macos/excel-diff-*-macos-universal.tar.gz | head -n 1)
          mkdir -p staging
          tar -xzf "$TARBALL" -C staging
          chmod +x staging/excel-diff

          staging/excel-diff --version
          staging/excel-diff --help >/dev/null
          staging/excel-diff diff fixtures/generated/minimal.xlsx fixtures/generated/minimal.xlsx --format json >/dev/null

          set +e
          staging/excel-diff diff fixtures/generated/col_append_right_a.xlsx fixtures/generated/col_append_right_b.xlsx >/dev/null
          STATUS=$?
          set -e
          if [ "$STATUS" -ne 1 ]; then
            echo "Expected exit code 1 for differing files, got $STATUS"
            exit 1
          fi

  generate-manifests:
    runs-on: ubuntu-latest
    needs: [build-windows, build-macos-universal]
    steps:
      - uses: actions/checkout@v4

      - name: Get version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
            echo "version=$VERSION" >> $GITHUB_OUTPUT
            echo "version_num=${VERSION#v}" >> $GITHUB_OUTPUT
          else
            echo "version=v0.0.0-dev" >> $GITHUB_OUTPUT
            echo "version_num=0.0.0-dev" >> $GITHUB_OUTPUT
          fi

      - name: Download Windows artifact
        uses: actions/download-artifact@v4
        with:
          name: windows-x86_64
          path: artifacts/windows

      - name: Download macOS universal artifact
        uses: actions/download-artifact@v4
        with:
          name: macos-universal
          path: artifacts/macos

      - name: Extract checksums
        id: checksums
        run: |
          WINDOWS_SHA=$(cat artifacts/windows/*.zip.sha256 | awk '{print $1}')
          MACOS_SHA=$(cat artifacts/macos/*.tar.gz.sha256 | awk '{print $1}')
          echo "windows_sha=$WINDOWS_SHA" >> $GITHUB_OUTPUT
          echo "macos_sha=$MACOS_SHA" >> $GITHUB_OUTPUT

      - name: Generate Scoop manifest
        run: |
          cat > excel-diff.json << EOF
          {
            "version": "${{ steps.version.outputs.version_num }}",
            "description": "A tool for comparing Excel workbooks",
            "homepage": "https://github.com/dvora/excel_diff",
            "license": "MIT",
            "architecture": {
              "64bit": {
                "url": "https://github.com/dvora/excel_diff/releases/download/${{ steps.version.outputs.version }}/excel-diff-${{ steps.version.outputs.version }}-windows-x86_64.zip",
                "hash": "${{ steps.checksums.outputs.windows_sha }}"
              }
            },
            "bin": "excel-diff-${{ steps.version.outputs.version }}-windows-x86_64/excel-diff.exe",
            "checkver": "github",
            "autoupdate": {
              "architecture": {
                "64bit": {
                  "url": "https://github.com/dvora/excel_diff/releases/download/v\$version/excel-diff-v\$version-windows-x86_64.zip"
                }
              }
            }
          }
          EOF

      - name: Generate Homebrew formula
        run: |
          cat > excel-diff.rb << 'EOF'
          class ExcelDiff < Formula
            desc "A tool for comparing Excel workbooks"
            homepage "https://github.com/dvora/excel_diff"
            version "${{ steps.version.outputs.version_num }}"
            license "MIT"

            on_macos do
              url "https://github.com/dvora/excel_diff/releases/download/${{ steps.version.outputs.version }}/excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz"
              sha256 "${{ steps.checksums.outputs.macos_sha }}"
            end

            def install
              bin.install "excel-diff"
            end

            test do
              system "#{bin}/excel-diff", "--version"
            end
          end
          EOF

      - name: Upload manifests
        uses: actions/upload-artifact@v4
        with:
          name: manifests
          path: |
            excel-diff.json
            excel-diff.rb

  publish-release:
    runs-on: ubuntu-latest
    needs:
      - build-windows
      - build-macos-x64
      - build-macos-arm64
      - build-macos-universal
      - smoke-macos-sequoia
      - generate-manifests
    if: startsWith(github.ref, 'refs/tags/v') && github.event.inputs.dry_run != 'true'
    permissions:
      contents: write
    steps:
      - name: Get version
        id: version
        run: echo "version=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Create checksums file
        run: |
          cd artifacts
          cat windows-x86_64/*.sha256 >> checksums.txt
          cat macos-x86_64/*.sha256 >> checksums.txt
          cat macos-arm64/*.sha256 >> checksums.txt
          cat macos-universal/*.sha256 >> checksums.txt

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          name: ${{ steps.version.outputs.version }}
          draft: false
          prerelease: ${{ contains(steps.version.outputs.version, '-') }}
          files: |
            artifacts/windows-x86_64/excel-diff-*.exe
            artifacts/windows-x86_64/excel-diff-*.zip
            artifacts/macos-x86_64/excel-diff-*.tar.gz
            artifacts/macos-arm64/excel-diff-*.tar.gz
            artifacts/macos-universal/excel-diff-*.tar.gz
            artifacts/manifests/excel-diff.json
            artifacts/manifests/excel-diff.rb
            artifacts/checksums.txt
          body: |
            ## Installation

            ### Windows
            Download the standalone `.exe` (or the ZIP) and add it to your PATH.

            Or use Scoop (manifest from this Release):
            ```powershell
            # 1) Download `excel-diff.json` from the Release assets
            # 2) Install:
            scoop install .\excel-diff.json

            # Or, if you publish a Scoop bucket:
            # scoop bucket add excel-diff https://github.com/dvora/scoop-excel-diff
            # scoop install excel-diff
            ```

            ### macOS
            Using Homebrew (formula from this Release):
            ```bash
            curl -LO https://github.com/dvora/excel_diff/releases/download/${{ steps.version.outputs.version }}/excel-diff.rb
            brew install --formula ./excel-diff.rb

            # Or, if you publish a Homebrew tap:
            # brew tap dvora/excel-diff
            # brew install excel-diff
            ```

            Or download the universal binary:
            ```bash
            curl -LO https://github.com/dvora/excel_diff/releases/download/${{ steps.version.outputs.version }}/excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz
            tar -xzf excel-diff-${{ steps.version.outputs.version }}-macos-universal.tar.gz
            sudo mv excel-diff /usr/local/bin/
            ```

            ### Web Demo
            Try it in your browser at https://dvora.github.io/excel_diff

            ## Format Support
            - Supported: `.xlsx`, `.xlsm`, `.xltx`, `.xltm`, `.pbix`, `.pbit`
            - Unsupported (detected): `.xlsb` returns `EXDIFF_PKG_009` with a convert hint

            ## Permission bindings
            - DPAPI-encrypted permission bindings that cannot be validated default permissions and emit warning `EXDIFF_DM_009` (results may be marked incomplete).

            ## Checksums
            See `checksums.txt` for SHA256 hashes of all artifacts.


```

---

### File: `.github\workflows\wasm.yml`

```yaml
name: WASM Smoke

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  wasm-smoke:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Build wasm smoke binary
        run: cargo build --release --target wasm32-unknown-unknown --no-default-features -p excel_diff --bin wasm_smoke
        working-directory: core

      - name: Enforce wasm size budget
        run: |
          SIZE=$(stat -c%s "core/target/wasm32-unknown-unknown/release/wasm_smoke.wasm")
          echo "wasm_smoke.wasm size: $SIZE bytes"
          if [ "$SIZE" -gt 5000000 ]; then
            echo "WASM size $SIZE exceeds 5MB limit"
            exit 1
          fi

  wasm-web-demo:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh

      - name: Build WASM package
        run: wasm-pack build wasm --release --target web --out-dir ../web/wasm

      - name: Enforce web demo size budget
        run: |
          SIZE=$(stat -c%s "web/wasm/excel_diff_wasm_bg.wasm")
          echo "excel_diff_wasm_bg.wasm size: $SIZE bytes"
          if [ "$SIZE" -gt 10000000 ]; then
            echo "Web demo WASM size $SIZE exceeds 10MB limit"
            exit 1
          fi

      - name: Build WASM package (nodejs)
        run: wasm-pack build wasm --release --target nodejs --out-dir pkg-node

      - name: Enforce WASM memory budgets
        run: node scripts/wasm_memory_harness.cjs --pkg wasm/pkg-node --budgets benchmarks/wasm_memory_budgets.json

```

---

### File: `.github\workflows\web_ui_tests.yml`

```yaml
name: Web UI Tests

on:
  pull_request:
  push:
    branches: [main, master]

jobs:
  web-ui:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install Python deps
        run: python -m pip install openpyxl==3.1.5
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      - name: Build CLI for fixtures
        run: cargo build -p excel_diff_cli
      - name: Generate CLI payload/outcome fixtures
        run: python scripts/generate_web_cli_fixtures.py --output-dir target/web-testdata --bin target/debug/excel-diff
      - name: Render test
        run: node web/test_render.js
      - name: View model test
        run: node web/test_view_model.js
      - name: Payload/outcome compatibility test
        run: node web/test_outcome_payload.js
        env:
          WEB_PAYLOAD_PATH: target/web-testdata/payload.json
          WEB_OUTCOME_PATH: target/web-testdata/outcome.json

```

---

### File: `.gitignore`

```
# Rust
target/
**/target/
**/*.rs.bk

# Python
__pycache__/
**/__pycache__/
.venv/
*.pyc
*.egg-info/

# Local scratch
tmp/
corpus_private/

# Shared Generated Data
fixtures/generated/*.xlsx
fixtures/generated/*.pbix
fixtures/generated/*.zip
fixtures/generated/*.csv
fixtures/generated/*.xlsm
fixtures/generated/*.xlsb


# Docs
docs/meta/completion_estimates/

# WASM build output
web/wasm/

```

---

### File: `Cargo.toml`

```toml
[workspace]
members = ["core", "cli", "wasm", "ui_payload", "desktop/src-tauri"]
resolver = "2"

```

---

### File: `cli\Cargo.toml`

```toml
[package]
name = "excel_diff_cli"
version = "0.1.0"
edition = "2024"
description = "Command-line interface for comparing Excel workbooks"
license = "MIT"
repository = "https://github.com/dvora/excel_diff"
homepage = "https://github.com/dvora/excel_diff"

[[bin]]
name = "excel-diff"
path = "src/main.rs"
doc = false

[dependencies]
excel_diff = { path = "../core", features = ["model-diff", "parallel"] }
clap = { version = "4.4", features = ["derive"] }
serde_json = "1.0"
serde = { version = "1.0", features = ["derive"] }
anyhow = "1.0"
ui_payload = { path = "../ui_payload" }

[features]
default = []
perf-metrics = ["excel_diff/perf-metrics"]

[dev-dependencies]
serde_json = "1.0"
tempfile = "3"


```

---

### File: `cli\src\commands\diff.rs`

```rust
use crate::commands::host::{host_kind_from_path, open_host, Host, HostKind};
use crate::output::{git_diff, json, text};
use crate::{DiffPresetArg, OutputFormat};
use anyhow::{Context, Result, bail};
use excel_diff::{
    DiffConfig, DiffReport, DiffSummary, Grid, JsonLinesSink, ProgressCallback, SheetKind,
    Workbook, WorkbookPackage, index_to_address, suggest_key_columns, with_default_session,
};
use std::collections::HashMap;
#[cfg(feature = "perf-metrics")]
use std::fs::File;
use std::io::{self, BufWriter, IsTerminal, Write};
use std::path::Path;
use std::process::ExitCode;
use std::sync::Mutex;
use ui_payload::{
    DiffOutcome, DiffOutcomeConfig, DiffOutcomeMode, DiffPreset, SummaryMeta, SummarySink,
    limits_from_config, summarize_report,
};

#[derive(Clone, Copy, PartialEq, Eq)]
pub enum Verbosity {
    Quiet,
    Normal,
    Verbose,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct SheetKey {
    name_lower: String,
    kind: SheetKind,
}

 

#[allow(clippy::too_many_arguments)]
pub fn run(
    old_path: &str,
    new_path: &str,
    format: OutputFormat,
    force_json: bool,
    git_diff_mode: bool,
    fast: bool,
    precise: bool,
    preset: Option<DiffPresetArg>,
    quiet: bool,
    verbose: bool,
    database: bool,
    sheet: Option<String>,
    keys: Option<String>,
    auto_keys: bool,
    progress: bool,
    max_memory: Option<u32>,
    timeout: Option<u32>,
    max_ops: Option<usize>,
    metrics_json: Option<String>,
) -> Result<ExitCode> {
    if fast && precise {
        bail!("Cannot use both --fast and --precise flags together");
    }
    if preset.is_some() && (fast || precise) {
        bail!("Cannot combine --preset with --fast or --precise");
    }

    if git_diff_mode
        && matches!(
            format,
            OutputFormat::Json | OutputFormat::Jsonl | OutputFormat::Payload | OutputFormat::Outcome
        )
    {
        bail!("Cannot use --git-diff with --format json/jsonl/payload/outcome");
    }

    let mut format = format;

    let old_path_str = old_path;
    let new_path_str = new_path;
    let old_path = Path::new(old_path_str);
    let new_path = Path::new(new_path_str);

    let old_kind = host_kind_from_path(old_path)
        .ok_or_else(|| anyhow::anyhow!("unsupported input extension: {}", old_path.display()))?;
    let new_kind = host_kind_from_path(new_path)
        .ok_or_else(|| anyhow::anyhow!("unsupported input extension: {}", new_path.display()))?;

    if old_kind != new_kind {
        bail!("input host types must match");
    }

    if old_kind == HostKind::Pbix {
        if database || sheet.is_some() || keys.is_some() || auto_keys {
            bail!("database mode and sheet/key options are not supported for PBIX/PBIT");
        }
    } else {
        if !database && (sheet.is_some() || keys.is_some() || auto_keys) {
            bail!("--sheet, --keys, and --auto-keys require --database flag");
        }

        if database && keys.is_none() && !auto_keys {
            bail!("Database mode requires either --keys or --auto-keys");
        }

        if database && keys.is_some() && auto_keys {
            bail!("Cannot use both --keys and --auto-keys together");
        }
    }

    let verbosity = if quiet {
        Verbosity::Quiet
    } else if verbose {
        Verbosity::Verbose
    } else {
        Verbosity::Normal
    };

    let preset = resolve_preset(preset, fast, precise)?;
    let mut config = build_config(preset);
    config.hardening.max_memory_mb = max_memory;
    config.hardening.timeout_seconds = timeout;
    config.hardening.max_ops = max_ops;

    let old_host = open_host(old_path, old_kind, "old")?;
    let new_host = open_host(new_path, new_kind, "new")?;

    let mut estimated_cells: Option<u64> = None;
    if !database {
        estimated_cells = match (&old_host, &new_host) {
            (Host::Workbook(old_pkg), Host::Workbook(new_pkg)) => {
                Some(estimate_diff_cell_volume(&old_pkg.workbook, &new_pkg.workbook))
            }
            _ => None,
        };
        let (new_format, switched_cells) =
            maybe_auto_switch_jsonl(format, force_json, git_diff_mode, estimated_cells, &config);
        if let Some(cells) = switched_cells {
            eprintln!(
                "Warning: estimated {} cells; switching to JSONL output. Use --force-json to keep JSON.",
                cells
            );
        }
        format = new_format;
    }

    if old_kind == HostKind::Workbook && database {
        let (Host::Workbook(old_pkg), Host::Workbook(new_pkg)) = (&old_host, &new_host) else {
            unreachable!();
        };
        return run_database_mode(
            old_pkg,
            new_pkg,
            old_path_str,
            new_path_str,
            format,
            git_diff_mode,
            force_json,
            &config,
            preset,
            verbosity,
            sheet,
            keys,
            auto_keys,
            metrics_json,
        );
    }

    let progress = progress.then(CliProgress::new);

    if format == OutputFormat::Payload {
        let payload = match (&old_host, &new_host) {
            (Host::Workbook(old_pkg), Host::Workbook(new_pkg)) => match progress.as_ref() {
                Some(p) => ui_payload::build_payload_from_workbooks_with_progress(
                    old_pkg, new_pkg, &config, p,
                ),
                None => ui_payload::build_payload_from_workbooks(old_pkg, new_pkg, &config),
            },
            (Host::Pbix(old_pkg), Host::Pbix(new_pkg)) => {
                ui_payload::build_payload_from_pbix(old_pkg, new_pkg, &config)
            }
            _ => unreachable!(),
        };

        if let Some(p) = progress.as_ref() {
            p.finish();
        }

        print_warnings_to_stderr(&payload.report);

        if let Some(path) = metrics_json.as_deref() {
            write_metrics_json_report(Path::new(path), &payload.report)?;
        }

        let stdout = io::stdout();
        let mut handle = stdout.lock();
        json::write_json_value(&mut handle, &payload)?;
        return Ok(exit_code_from_report(&payload.report));
    }

    if format == OutputFormat::Outcome {
        let meta = summary_meta_from_paths(old_path_str, new_path_str);
        let outcome_config = DiffOutcomeConfig {
            preset: Some(preset),
            limits: Some(limits_from_config(&config)),
        };

        let outcome = match (&old_host, &new_host) {
            (Host::Workbook(old_pkg), Host::Workbook(new_pkg)) => {
                let use_large_mode = estimated_cells
                    .map(|cells| excel_diff::should_use_large_mode(cells, &config))
                    .unwrap_or(false);

                if use_large_mode {
                    let mut sink = SummarySink::new();
                    let summary = match progress.as_ref() {
                        Some(p) => old_pkg
                            .diff_streaming_with_progress(new_pkg, &config, &mut sink, p)
                            .context("Streaming diff failed")?,
                        None => old_pkg
                            .diff_streaming(new_pkg, &config, &mut sink)
                            .context("Streaming diff failed")?,
                    };

                    if let Some(p) = progress.as_ref() {
                        p.finish();
                    }

                    let summary = sink.into_summary(summary, meta.clone());
                    for warning in &summary.warnings {
                        eprintln!("Warning: {}", warning);
                    }

                    DiffOutcome {
                        diff_id: None,
                        mode: DiffOutcomeMode::Large,
                        payload: None,
                        summary: Some(summary),
                        config: Some(outcome_config),
                    }
                } else {
                    let payload = match progress.as_ref() {
                        Some(p) => ui_payload::build_payload_from_workbooks_with_progress(
                            old_pkg, new_pkg, &config, p,
                        ),
                        None => ui_payload::build_payload_from_workbooks(old_pkg, new_pkg, &config),
                    };

                    if let Some(p) = progress.as_ref() {
                        p.finish();
                    }

                    let summary = summarize_report(&payload.report, meta.clone());
                    print_warnings_to_stderr(&payload.report);

                    DiffOutcome {
                        diff_id: None,
                        mode: DiffOutcomeMode::Payload,
                        payload: Some(payload),
                        summary: Some(summary),
                        config: Some(outcome_config),
                    }
                }
            }
            (Host::Pbix(old_pkg), Host::Pbix(new_pkg)) => {
                let payload = ui_payload::build_payload_from_pbix(old_pkg, new_pkg, &config);
                let summary = summarize_report(&payload.report, meta);
                print_warnings_to_stderr(&payload.report);
                DiffOutcome {
                    diff_id: None,
                    mode: DiffOutcomeMode::Payload,
                    payload: Some(payload),
                    summary: Some(summary),
                    config: Some(outcome_config),
                }
            }
            _ => unreachable!(),
        };

        let stdout = io::stdout();
        let mut handle = stdout.lock();
        json::write_json_value(&mut handle, &outcome)?;

        return Ok(match outcome.mode {
            DiffOutcomeMode::Payload => exit_code_from_report(
                outcome
                    .payload
                    .as_ref()
                    .map(|p| &p.report)
                    .expect("payload report exists"),
            ),
            DiffOutcomeMode::Large => {
                let summary = outcome.summary.as_ref().expect("summary exists");
                if summary.op_count == 0 && summary.complete {
                    ExitCode::from(0)
                } else {
                    ExitCode::from(1)
                }
            }
        });
    }

    if format == OutputFormat::Jsonl && !git_diff_mode {
        return run_streaming_host(&old_host, &new_host, &config, progress.as_ref(), metrics_json.as_deref());
    }

    let report = match (&old_host, &new_host) {
        (Host::Workbook(old_pkg), Host::Workbook(new_pkg)) => match progress.as_ref() {
            Some(p) => old_pkg.diff_with_progress(new_pkg, &config, p),
            None => old_pkg.diff(new_pkg, &config),
        },
        (Host::Pbix(old_pkg), Host::Pbix(new_pkg)) => old_pkg.diff(new_pkg, &config),
        _ => unreachable!(),
    };

    if let Some(p) = progress.as_ref() {
        p.finish();
    }

    print_warnings_to_stderr(&report);

    if let Some(path) = metrics_json.as_deref() {
        write_metrics_json_report(Path::new(path), &report)?;
    }

    let stdout = io::stdout();
    let mut handle = stdout.lock();

    if git_diff_mode {
        git_diff::write_git_diff(&mut handle, &report, old_path_str, new_path_str)?;
    } else {
        match format {
            OutputFormat::Text => {
                text::write_text_report(
                    &mut handle,
                    &report,
                    old_path_str,
                    new_path_str,
                    verbosity,
                )?;
            }
            OutputFormat::Json => {
                json::write_json_report(&mut handle, &report)?;
            }
            OutputFormat::Jsonl => {
                bail!("Internal error: JSONL format should be handled by the streaming path");
            }
            OutputFormat::Payload | OutputFormat::Outcome => {
                bail!("Internal error: payload/outcome format should be handled earlier");
            }
        }
    }

    Ok(exit_code_from_report(&report))
}

fn run_streaming_host(
    old_host: &Host,
    new_host: &Host,
    config: &DiffConfig,
    progress: Option<&CliProgress>,
    metrics_json: Option<&str>,
) -> Result<ExitCode> {
    let stdout = io::stdout();
    let handle = stdout.lock();
    let mut writer = BufWriter::new(handle);
    let mut sink = JsonLinesSink::new(&mut writer);

    let summary = match (old_host, new_host, progress) {
        (Host::Workbook(old_pkg), Host::Workbook(new_pkg), Some(p)) => old_pkg
            .diff_streaming_with_progress(new_pkg, config, &mut sink, p)
            .context("Streaming diff failed")?,
        (Host::Workbook(old_pkg), Host::Workbook(new_pkg), None) => old_pkg
            .diff_streaming(new_pkg, config, &mut sink)
            .context("Streaming diff failed")?,
        (Host::Pbix(old_pkg), Host::Pbix(new_pkg), Some(p)) => old_pkg
            .diff_streaming_with_progress(new_pkg, config, &mut sink, p)
            .context("Streaming diff failed")?,
        (Host::Pbix(old_pkg), Host::Pbix(new_pkg), None) => old_pkg
            .diff_streaming(new_pkg, config, &mut sink)
            .context("Streaming diff failed")?,
        _ => unreachable!(),
    };

    writer.flush()?;

    if let Some(p) = progress {
        p.finish();
    }

    if let Some(path) = metrics_json {
        write_metrics_json_summary(Path::new(path), &summary)?;
    }

    for warning in &summary.warnings {
        eprintln!("Warning: {}", warning);
    }

    if summary.op_count == 0 && summary.complete {
        Ok(ExitCode::from(0))
    } else {
        Ok(ExitCode::from(1))
    }
}

struct CliProgress {
    state: Mutex<CliProgressState>,
}

struct CliProgressState {
    is_tty: bool,
    last_phase: String,
    last_percent: f32,
}

impl CliProgress {
    fn new() -> Self {
        Self {
            state: Mutex::new(CliProgressState {
                is_tty: io::stderr().is_terminal(),
                last_phase: String::new(),
                last_percent: -1.0,
            }),
        }
    }

    fn finish(&self) {
        let is_tty = self.lock_state().is_tty;
        if is_tty {
            let mut stderr = io::stderr().lock();
            let _ = writeln!(stderr);
        }
    }

    fn lock_state(&self) -> std::sync::MutexGuard<'_, CliProgressState> {
        match self.state.lock() {
            Ok(guard) => guard,
            Err(poisoned) => poisoned.into_inner(),
        }
    }

    fn render_bar(phase: &str, percent: f32) -> String {
        let pct = (percent.clamp(0.0, 1.0) * 100.0).clamp(0.0, 100.0);
        let width = 24usize;
        let filled = ((pct / 100.0) * width as f32).round() as usize;
        let filled = filled.min(width);
        let empty = width - filled;
        format!(
            "{:>14} [{}{}] {:>6.1}%",
            phase,
            "#".repeat(filled),
            "-".repeat(empty),
            pct
        )
    }
}

impl ProgressCallback for CliProgress {
    fn on_progress(&self, phase: &str, percent: f32) {
        let mut state = self.lock_state();

        if state.is_tty {
            let line = Self::render_bar(phase, percent);
            let mut stderr = io::stderr().lock();
            let _ = write!(stderr, "\r{}", line);
            let _ = stderr.flush();
        } else {
            let phase_changed = state.last_phase != phase;
            let pct = percent.clamp(0.0, 1.0);
            let emit = phase_changed || pct == 0.0 || pct == 1.0 || (pct - state.last_percent) >= 0.25;
            if emit {
                eprintln!("Progress: {} {:.0}%", phase, pct * 100.0);
                state.last_phase = phase.to_string();
                state.last_percent = pct;
            }
        }
    }
}

#[allow(clippy::too_many_arguments)]
fn run_database_mode(
    old_pkg: &WorkbookPackage,
    new_pkg: &WorkbookPackage,
    old_path: &str,
    new_path: &str,
    format: OutputFormat,
    git_diff_mode: bool,
    force_json: bool,
    config: &DiffConfig,
    preset: DiffPreset,
    verbosity: Verbosity,
    sheet: Option<String>,
    keys: Option<String>,
    auto_keys: bool,
    metrics_json: Option<String>,
) -> Result<ExitCode> {
    let sheet_name = determine_sheet_name(&old_pkg.workbook, &new_pkg.workbook, sheet)?;

    let mut format = format;
    let estimated_cells = estimate_sheet_cell_volume(old_pkg, new_pkg, &sheet_name)?;
    let (new_format, switched_cells) =
        maybe_auto_switch_jsonl(format, force_json, git_diff_mode, Some(estimated_cells), config);
    if let Some(cells) = switched_cells {
        eprintln!(
            "Warning: estimated {} cells in sheet '{}'; switching to JSONL output. Use --force-json to keep JSON.",
            cells,
            sheet_name
        );
    }
    format = new_format;
    
    let key_columns = if let Some(keys_str) = keys {
        parse_key_columns(&keys_str)?
    } else if auto_keys {
        let grid = find_sheet_grid(&old_pkg.workbook, &sheet_name)?;
        let suggested = with_default_session(|session| {
            suggest_key_columns(grid, &session.strings)
        });
        if suggested.is_empty() {
            eprintln!(
                "Warning: Could not auto-detect key columns for sheet '{}'; falling back to spreadsheet mode.",
                sheet_name
            );
            Vec::new()
        }
        else {
            let col_letters: Vec<String> = suggested.iter().map(|&c| col_index_to_letters(c)).collect();
            eprintln!("Auto-detected key columns: {}", col_letters.join(","));
            suggested
        }
    } else {
        bail!("Database mode requires either --keys or --auto-keys");
    };

    if format == OutputFormat::Outcome {
        let meta = summary_meta_from_paths(old_path, new_path);
        let outcome_config = DiffOutcomeConfig {
            preset: Some(preset),
            limits: Some(limits_from_config(config)),
        };
        let use_large_mode = excel_diff::should_use_large_mode(estimated_cells, config);

        let outcome = if use_large_mode {
            let mut sink = SummarySink::new();
            let summary = old_pkg
                .diff_database_mode_streaming(new_pkg, &sheet_name, &key_columns, config, &mut sink)
                .context("Database mode streaming diff failed")?;

            if let Some(path) = metrics_json.as_deref() {
                write_metrics_json_summary(Path::new(path), &summary)?;
            }

            let summary = sink.into_summary(summary, meta);
            for warning in &summary.warnings {
                eprintln!("Warning: {}", warning);
            }

            DiffOutcome {
                diff_id: None,
                mode: DiffOutcomeMode::Large,
                payload: None,
                summary: Some(summary),
                config: Some(outcome_config),
            }
        } else {
            let report = old_pkg
                .diff_database_mode(new_pkg, &sheet_name, &key_columns, config)
                .context("Database mode diff failed")?;

            print_warnings_to_stderr(&report);
            print_fallback_suggestions(&report, auto_keys, &sheet_name, old_pkg);

            if let Some(path) = metrics_json.as_deref() {
                write_metrics_json_report(Path::new(path), &report)?;
            }

            let payload = ui_payload::build_payload_from_workbook_report(report, old_pkg, new_pkg);
            let summary = summarize_report(&payload.report, meta);
            DiffOutcome {
                diff_id: None,
                mode: DiffOutcomeMode::Payload,
                payload: Some(payload),
                summary: Some(summary),
                config: Some(outcome_config),
            }
        };

        let stdout = io::stdout();
        let mut handle = stdout.lock();
        json::write_json_value(&mut handle, &outcome)?;

        return Ok(match outcome.mode {
            DiffOutcomeMode::Payload => exit_code_from_report(
                outcome
                    .payload
                    .as_ref()
                    .map(|p| &p.report)
                    .expect("payload report exists"),
            ),
            DiffOutcomeMode::Large => {
                let summary = outcome.summary.as_ref().expect("summary exists");
                if summary.op_count == 0 && summary.complete {
                    ExitCode::from(0)
                } else {
                    ExitCode::from(1)
                }
            }
        });
    }

    if format == OutputFormat::Payload {
        let report = old_pkg
            .diff_database_mode(new_pkg, &sheet_name, &key_columns, config)
            .context("Database mode diff failed")?;

        print_warnings_to_stderr(&report);
        print_fallback_suggestions(&report, auto_keys, &sheet_name, old_pkg);

        if let Some(path) = metrics_json.as_deref() {
            write_metrics_json_report(Path::new(path), &report)?;
        }

        let payload = ui_payload::build_payload_from_workbook_report(report, old_pkg, new_pkg);
        let stdout = io::stdout();
        let mut handle = stdout.lock();
        json::write_json_value(&mut handle, &payload)?;
        return Ok(exit_code_from_report(&payload.report));
    }

    if format == OutputFormat::Jsonl && !git_diff_mode {
        return run_database_streaming(
            old_pkg,
            new_pkg,
            &sheet_name,
            &key_columns,
            config,
            metrics_json.as_deref(),
        );
    }

    let report = old_pkg
        .diff_database_mode(new_pkg, &sheet_name, &key_columns, config)
        .context("Database mode diff failed")?;

    print_warnings_to_stderr(&report);
    print_fallback_suggestions(&report, auto_keys, &sheet_name, old_pkg);

    if let Some(path) = metrics_json.as_deref() {
        write_metrics_json_report(Path::new(path), &report)?;
    }

    let stdout = io::stdout();
    let mut handle = stdout.lock();

    if git_diff_mode {
        git_diff::write_git_diff(&mut handle, &report, old_path, new_path)?;
    } else {
        match format {
            OutputFormat::Text => {
                text::write_text_report(&mut handle, &report, old_path, new_path, verbosity)?;
            }
            OutputFormat::Json => {
                json::write_json_report(&mut handle, &report)?;
            }
            OutputFormat::Jsonl => {
                bail!("Internal error: JSONL format should be handled by the streaming path");
            }
            OutputFormat::Payload | OutputFormat::Outcome => {
                bail!("Internal error: payload/outcome format should be handled earlier");
            }
        }
    }

    Ok(exit_code_from_report(&report))
}

fn run_database_streaming(
    old_pkg: &WorkbookPackage,
    new_pkg: &WorkbookPackage,
    sheet_name: &str,
    key_columns: &[u32],
    config: &DiffConfig,
    metrics_json: Option<&str>,
) -> Result<ExitCode> {
    let stdout = io::stdout();
    let handle = stdout.lock();
    let mut writer = BufWriter::new(handle);
    let mut sink = JsonLinesSink::new(&mut writer);

    let summary = old_pkg
        .diff_database_mode_streaming(new_pkg, sheet_name, key_columns, config, &mut sink)
        .context("Database mode streaming diff failed")?;

    writer.flush()?;

    if let Some(path) = metrics_json {
        write_metrics_json_summary(Path::new(path), &summary)?;
    }

    for warning in &summary.warnings {
        eprintln!("Warning: {}", warning);
    }

    if summary.op_count == 0 && summary.complete {
        Ok(ExitCode::from(0))
    } else {
        Ok(ExitCode::from(1))
    }
}

fn determine_sheet_name(
    old_wb: &excel_diff::Workbook,
    new_wb: &excel_diff::Workbook,
    sheet: Option<String>,
) -> Result<String> {
    if let Some(name) = sheet {
        return Ok(name);
    }

    let has_data_sheet = |wb: &excel_diff::Workbook| -> bool {
        with_default_session(|session| {
            wb.sheets.iter().any(|s| {
                session.strings.resolve(s.name).to_lowercase() == "data"
            })
        })
    };

    if has_data_sheet(old_wb) || has_data_sheet(new_wb) {
        return Ok("Data".to_string());
    }

    if old_wb.sheets.len() == 1 && new_wb.sheets.len() == 1 {
        let old_name = with_default_session(|session| {
            session.strings.resolve(old_wb.sheets[0].name).to_string()
        });
        return Ok(old_name);
    }

    bail!("Multiple sheets found; please specify --sheet")
}

fn find_sheet_grid<'a>(wb: &'a excel_diff::Workbook, sheet_name: &str) -> Result<&'a Grid> {
    let sheet_name_lower = sheet_name.to_lowercase();
    with_default_session(|session| {
        wb.sheets
            .iter()
            .find(|s| session.strings.resolve(s.name).to_lowercase() == sheet_name_lower)
            .map(|s| &s.grid)
            .ok_or_else(|| {
                let available: Vec<String> = wb
                    .sheets
                    .iter()
                    .map(|s| session.strings.resolve(s.name).to_string())
                    .collect();
                anyhow::anyhow!(
                    "Sheet '{}' not found. Available sheets: {}",
                    sheet_name,
                    available.join(", ")
                )
            })
    })
}

fn parse_key_columns(keys_str: &str) -> Result<Vec<u32>> {
    let mut result = Vec::new();
    let mut seen = std::collections::HashSet::new();

    for token in keys_str.split(',') {
        let token = token.trim();
        if token.is_empty() {
            bail!("Invalid --keys: empty column token in '{}'", keys_str);
        }
        if !token.chars().all(|c| c.is_ascii_alphabetic()) {
            bail!(
                "Invalid --keys: '{}' is not a valid column letter (must be letters only, e.g. A, B, AA)",
                token
            );
        }
        let col_idx = col_letters_to_index(token)?;
        if !seen.insert(col_idx) {
            bail!("Invalid --keys: duplicate column '{}'", token);
        }
        result.push(col_idx);
    }

    if result.is_empty() {
        bail!("Invalid --keys: no columns specified");
    }

    Ok(result)
}

fn col_letters_to_index(letters: &str) -> Result<u32> {
    let mut col: u32 = 0;
    for ch in letters.chars() {
        let upper = ch.to_ascii_uppercase();
        if !upper.is_ascii_uppercase() {
            bail!("Invalid column letter: '{}'", ch);
        }
        col = col
            .checked_mul(26)
            .and_then(|c| c.checked_add((upper as u8 - b'A' + 1) as u32))
            .ok_or_else(|| anyhow::anyhow!("Column '{}' is out of range", letters))?;
    }
    Ok(col.saturating_sub(1))
}

fn col_index_to_letters(col: u32) -> String {
    let addr = index_to_address(0, col);
    addr.trim_end_matches(|c: char| c.is_ascii_digit()).to_string()
}

fn print_fallback_suggestions(
    report: &DiffReport,
    auto_keys: bool,
    sheet_name: &str,
    old_pkg: &WorkbookPackage,
) {
    let has_fallback_warning = report
        .warnings
        .iter()
        .any(|w| w.contains("database-mode:") && w.contains("falling back"));

    if has_fallback_warning && !auto_keys {
        if let Ok(grid) = find_sheet_grid(&old_pkg.workbook, sheet_name) {
            let suggested = with_default_session(|session| {
                suggest_key_columns(grid, &session.strings)
            });
            if !suggested.is_empty() {
                let col_letters: Vec<String> = suggested.iter().map(|&c| col_index_to_letters(c)).collect();
                eprintln!("Hint: try --keys={} for unique key columns", col_letters.join(","));
            }
        }
    }
    if has_fallback_warning && auto_keys {
        eprintln!("Hint: specify --keys to force database mode when auto-detection is ambiguous.");
    }
}

fn resolve_preset(
    preset: Option<DiffPresetArg>,
    fast: bool,
    precise: bool,
) -> Result<DiffPreset> {
    if fast {
        return Ok(DiffPreset::Fastest);
    }
    if precise {
        return Ok(DiffPreset::MostPrecise);
    }
    if let Some(preset) = preset {
        return Ok(match preset {
            DiffPresetArg::Fastest => DiffPreset::Fastest,
            DiffPresetArg::Balanced => DiffPreset::Balanced,
            DiffPresetArg::MostPrecise => DiffPreset::MostPrecise,
        });
    }
    Ok(DiffPreset::Balanced)
}

fn build_config(preset: DiffPreset) -> DiffConfig {
    preset.to_config()
}

fn estimate_diff_cell_volume(old: &Workbook, new: &Workbook) -> u64 {
    with_default_session(|session| {
        let mut max_counts: HashMap<SheetKey, u64> = HashMap::new();
        for sheet in old.sheets.iter().chain(new.sheets.iter()) {
            let name_lower = session.strings.resolve(sheet.name).to_lowercase();
            let key = SheetKey {
                name_lower,
                kind: sheet.kind.clone(),
            };
            let cell_count = sheet.grid.cell_count() as u64;
            max_counts
                .entry(key)
                .and_modify(|v| {
                    if cell_count > *v {
                        *v = cell_count;
                    }
                })
                .or_insert(cell_count);
        }

        max_counts.values().copied().sum()
    })
}

fn estimate_sheet_cell_volume(
    old_pkg: &WorkbookPackage,
    new_pkg: &WorkbookPackage,
    sheet_name: &str,
) -> Result<u64> {
    let old_cells = find_sheet_grid(&old_pkg.workbook, sheet_name)?.cell_count() as u64;
    let new_cells = find_sheet_grid(&new_pkg.workbook, sheet_name)?.cell_count() as u64;
    Ok(old_cells.max(new_cells))
}

fn maybe_auto_switch_jsonl(
    format: OutputFormat,
    force_json: bool,
    git_diff_mode: bool,
    estimated_cells: Option<u64>,
    config: &DiffConfig,
) -> (OutputFormat, Option<u64>) {
    if format == OutputFormat::Json && !force_json && !git_diff_mode {
        if let Some(cells) = estimated_cells {
            if excel_diff::should_use_large_mode(cells, config) {
                return (OutputFormat::Jsonl, Some(cells));
            }
        }
    }
    (format, None)
}

fn summary_meta_from_paths(old_path: &str, new_path: &str) -> SummaryMeta {
    let old_name = Path::new(old_path)
        .file_name()
        .and_then(|s| s.to_str())
        .map(|s| s.to_string());
    let new_name = Path::new(new_path)
        .file_name()
        .and_then(|s| s.to_str())
        .map(|s| s.to_string());

    SummaryMeta {
        old_path: Some(old_path.to_string()),
        new_path: Some(new_path.to_string()),
        old_name,
        new_name,
    }
}

fn print_warnings_to_stderr(report: &DiffReport) {
    for warning in &report.warnings {
        eprintln!("Warning: {}", warning);
    }
}

#[cfg(feature = "perf-metrics")]
fn write_metrics_json_report(path: &Path, report: &DiffReport) -> Result<()> {
    let metrics = report
        .metrics
        .as_ref()
        .context("Perf metrics not available; build with --features perf-metrics")?;
    write_metrics_json(path, metrics)
}

#[cfg(feature = "perf-metrics")]
fn write_metrics_json_summary(path: &Path, summary: &DiffSummary) -> Result<()> {
    let metrics = summary
        .metrics
        .as_ref()
        .context("Perf metrics not available; build with --features perf-metrics")?;
    write_metrics_json(path, metrics)
}

#[cfg(feature = "perf-metrics")]
fn write_metrics_json(
    path: &Path,
    metrics: &excel_diff::perf::DiffMetrics,
) -> Result<()> {
    if let Some(parent) = path.parent() {
        std::fs::create_dir_all(parent)
            .with_context(|| format!("Failed to create metrics directory: {}", parent.display()))?;
    }
    let mut file = File::create(path)
        .with_context(|| format!("Failed to create metrics file: {}", path.display()))?;
    serde_json::to_writer_pretty(&mut file, metrics)?;
    writeln!(file)?;
    Ok(())
}

#[cfg(not(feature = "perf-metrics"))]
fn write_metrics_json_report(_path: &Path, _report: &DiffReport) -> Result<()> {
    bail!("--metrics-json requires excel-diff to be built with --features perf-metrics")
}

#[cfg(not(feature = "perf-metrics"))]
fn write_metrics_json_summary(_path: &Path, _summary: &DiffSummary) -> Result<()> {
    bail!("--metrics-json requires excel-diff to be built with --features perf-metrics")
}

fn exit_code_from_report(report: &DiffReport) -> ExitCode {
    if report.ops.is_empty() && report.complete {
        ExitCode::from(0)
    } else {
        ExitCode::from(1)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn auto_switches_to_jsonl_for_large_estimate() {
        let config = DiffConfig::balanced();
        let cells = excel_diff::AUTO_STREAM_CELL_THRESHOLD + 1;
        let (format, switched) =
            maybe_auto_switch_jsonl(OutputFormat::Json, false, false, Some(cells), &config);
        assert_eq!(format, OutputFormat::Jsonl);
        assert_eq!(switched, Some(cells));
    }
}


```

---

### File: `cli\src\commands\host.rs`

```rust
use anyhow::{Context, Result};
use excel_diff::{PbixPackage, WorkbookPackage};
use std::fs::File;
use std::path::Path;

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub(crate) enum HostKind {
    Workbook,
    Pbix,
}

pub(crate) enum Host {
    Workbook(WorkbookPackage),
    Pbix(PbixPackage),
}

pub(crate) fn host_kind_from_path(path: &Path) -> Option<HostKind> {
    let ext = path.extension()?.to_string_lossy().to_ascii_lowercase();
    match ext.as_str() {
        "xlsx" | "xlsm" | "xltx" | "xltm" | "xlsb" => Some(HostKind::Workbook),
        "pbix" | "pbit" => Some(HostKind::Pbix),
        _ => None,
    }
}

pub(crate) fn open_host(path: &Path, kind: HostKind, label: &str) -> Result<Host> {
    let file = File::open(path)
        .with_context(|| format!("Failed to open {} file: {}", label, path.display()))?;

    let host = match kind {
        HostKind::Workbook => Host::Workbook(
            WorkbookPackage::open(file)
                .with_context(|| format!("Failed to parse {} workbook: {}", label, path.display()))?,
        ),
        HostKind::Pbix => Host::Pbix(
            PbixPackage::open(file)
                .with_context(|| format!("Failed to parse {} PBIX/PBIT: {}", label, path.display()))?,
        ),
    };

    Ok(host)
}

```

---

### File: `cli\src\commands\info.rs`

```rust
use anyhow::{Context, Result};
use excel_diff::{build_embedded_queries, build_queries, DataMashup, SheetKind};
use std::io::{self, Write};
use std::path::Path;
use std::process::ExitCode;

use crate::commands::host::{host_kind_from_path, open_host, Host};

pub fn run(path: &str, show_queries: bool) -> Result<ExitCode> {
    let path = Path::new(path);
    let kind = host_kind_from_path(path)
        .with_context(|| format!("Unsupported input extension: {}", path.display()))?;

    let host = open_host(path, kind, "input")?;

    let stdout = io::stdout();
    let mut handle = stdout.lock();

    let filename = path
        .file_name()
        .map(|s| s.to_string_lossy().to_string())
        .unwrap_or_else(|| path.display().to_string());

    match host {
        Host::Workbook(pkg) => {
            writeln!(handle, "Workbook: {}", filename)?;
            writeln!(handle, "Sheets: {}", pkg.workbook.sheets.len())?;
            for sheet in &pkg.workbook.sheets {
                let sheet_name =
                    excel_diff::with_default_session(|session| session.strings.resolve(sheet.name).to_string());
                let kind_str = match sheet.kind {
                    SheetKind::Worksheet => "worksheet",
                    SheetKind::Chart => "chart",
                    SheetKind::Macro => "macro",
                    SheetKind::Other => "other",
                };
                writeln!(handle, "- {} ({})", sheet_name, kind_str)?;
            }

            if show_queries {
                write_power_query_section(&mut handle, pkg.data_mashup.as_ref())?;
            }
        }
        Host::Pbix(pkg) => {
            writeln!(handle, "PBIX/PBIT: {}", filename)?;
            if show_queries {
                write_power_query_section(&mut handle, pkg.data_mashup())?;
            }
        }
    }

    Ok(ExitCode::from(0))
}

fn write_power_query_section<W: Write>(w: &mut W, dm_opt: Option<&DataMashup>) -> Result<()> {
    writeln!(w)?;

    let Some(dm) = dm_opt else {
        writeln!(w, "Power Query: none")?;
        return Ok(());
    };

    writeln!(w, "Power Query:")?;

    match build_queries(dm) {
        Ok(mut top) => {
            top.sort_by(|a, b| a.name.cmp(&b.name));
            writeln!(w, "  Top-level: {}", top.len())?;
            for q in top {
                write_query_line(w, &q)?;
            }
        }
        Err(e) => {
            writeln!(w, "  Top-level: error parsing queries: {}", e)?;
        }
    }

    let mut embedded = build_embedded_queries(dm);
    embedded.sort_by(|a, b| a.name.cmp(&b.name));
    writeln!(w, "  Embedded: {}", embedded.len())?;
    for q in embedded {
        write_query_line(w, &q)?;
    }

    Ok(())
}

fn write_query_line<W: Write>(w: &mut W, q: &excel_diff::Query) -> Result<()> {
    let load_flags = format_load_flags(&q.metadata);
    let group_path = q
        .metadata
        .group_path
        .as_ref()
        .map(|v| v.to_string())
        .unwrap_or_else(|| "(none)".to_string());

    if load_flags.is_empty() {
        writeln!(w, "  - {}  [group={}]", q.name, group_path)?;
    } else {
        writeln!(w, "  - {}  [{}]  [group={}]", q.name, load_flags, group_path)?;
    }

    Ok(())
}

fn format_load_flags(meta: &excel_diff::QueryMetadata) -> String {
    let mut flags = Vec::new();
    if meta.load_to_sheet {
        flags.push("sheet");
    }
    if meta.load_to_model {
        flags.push("model");
    }
    if meta.is_connection_only {
        flags.push("connection-only");
    }
    flags.join(",")
}

```

---

### File: `cli\src\commands\mod.rs`

```rust
pub mod diff;
pub mod host;
pub mod info;


```

---

### File: `cli\src\main.rs`

```rust
mod commands;
mod output;

use clap::{CommandFactory, Parser, Subcommand, ValueEnum};
use excel_diff::{
    ContainerError, DataMashupError, DiffError, GridParseError, PackageError, SectionParseError,
};
use std::process::ExitCode;

#[derive(Parser)]
#[command(name = "excel-diff", disable_version_flag = true, arg_required_else_help = true)]
#[command(about = "Compare Excel workbooks and show differences")]
pub struct Cli {
    #[arg(long, action = clap::ArgAction::SetTrue, help = "Show version and exit")]
    pub version: bool,
    #[arg(long, short, global = true, help = "Verbose output")]
    pub verbose: bool,
    #[command(subcommand)]
    pub command: Option<Commands>,
}

#[derive(Subcommand)]
pub enum Commands {
    #[command(about = "Compare two Excel workbooks or PBIX/PBIT packages")]
    Diff {
        #[arg(help = "Path to the old/base file (.xlsx, .xlsm, .xltx, .xltm, .xlsb, .pbix, .pbit)")]
        old: String,
        #[arg(help = "Path to the new/changed file (.xlsx, .xlsm, .xltx, .xltm, .xlsb, .pbix, .pbit)")]
        new: String,
        #[arg(long, short, value_enum, default_value = "text", help = "Output format")]
        format: OutputFormat,
        #[arg(long, help = "Force JSON output even for large diffs (disable auto-switch to JSONL)")]
        force_json: bool,
        #[arg(long, help = "Produce unified diff-style output for Git")]
        git_diff: bool,
        #[arg(long, help = "Use fastest diff preset (less precise move detection)")]
        fast: bool,
        #[arg(long, help = "Use most precise diff preset (slower, more accurate)")]
        precise: bool,
        #[arg(long, value_enum, help = "Diff preset")]
        preset: Option<DiffPresetArg>,
        #[arg(long, short, help = "Quiet mode: only show summary")]
        quiet: bool,
        #[arg(long, help = "Use database mode: align rows by key columns")]
        database: bool,
        #[arg(long, help = "Sheet name to diff in database mode")]
        sheet: Option<String>,
        #[arg(long, help = "Key columns for database mode (comma-separated column letters, e.g. A,B,C)")]
        keys: Option<String>,
        #[arg(long, help = "Auto-detect key columns for database mode")]
        auto_keys: bool,
        #[arg(long, help = "Show a progress bar on stderr")]
        progress: bool,
        #[arg(long, value_name = "MB", help = "Soft memory budget (MB) for advanced strategies")]
        max_memory: Option<u32>,
        #[arg(long, value_name = "SECONDS", help = "Abort diff after this many seconds")]
        timeout: Option<u32>,
        #[arg(long, value_name = "COUNT", help = "Maximum number of ops to emit before stopping")]
        max_ops: Option<usize>,
        #[arg(long, value_name = "PATH", help = "Write perf metrics JSON to this path")]
        metrics_json: Option<String>,
    },
    #[command(about = "Show information about a workbook or PBIX/PBIT package")]
    Info {
        #[arg(help = "Path to the file (.xlsx, .xlsm, .xltx, .xltm, .xlsb, .pbix, .pbit)")]
        path: String,
        #[arg(long, help = "Include Power Query information")]
        queries: bool,
    },
}

#[derive(Clone, Copy, Debug, ValueEnum, PartialEq, Eq)]
pub enum OutputFormat {
    Text,
    Json,
    Jsonl,
    Payload,
    Outcome,
}

#[derive(Clone, Copy, ValueEnum, PartialEq, Eq)]
pub enum DiffPresetArg {
    Fastest,
    Balanced,
    MostPrecise,
}

fn main() -> ExitCode {
    let cli = Cli::parse();

    if cli.version {
        print_version(cli.verbose);
        return ExitCode::from(0);
    }

    let result = match cli.command {
        Some(Commands::Diff {
            old,
            new,
            format,
            force_json,
            git_diff,
            fast,
            precise,
            preset,
            quiet,
            database,
            sheet,
            keys,
            auto_keys,
            progress,
            max_memory,
            timeout,
            max_ops,
            metrics_json,
        }) => commands::diff::run(
            &old,
            &new,
            format,
            force_json,
            git_diff,
            fast,
            precise,
            preset,
            quiet,
            cli.verbose,
            database,
            sheet,
            keys,
            auto_keys,
            progress,
            max_memory,
            timeout,
            max_ops,
            metrics_json,
        ),
        Some(Commands::Info { path, queries }) => commands::info::run(&path, queries),
        None => {
            let mut cmd = Cli::command();
            let _ = cmd.print_help();
            return ExitCode::from(2);
        }
    };

    match result {
        Ok(code) => code,
        Err(e) => {
            eprintln!("Error: {:#}", e);
            exit_code_for_error(&e)
        }
    }
}

fn print_version(verbose: bool) {
    println!("excel-diff {}", env!("CARGO_PKG_VERSION"));
    if !verbose {
        return;
    }

    let features = excel_diff::engine_features();
    println!(
        "features: vba={}, model-diff={}, parallel={}, std-fs={}",
        features.vba, features.model_diff, features.parallel, features.std_fs
    );
    println!("presets: fastest, balanced, most_precise");
    println!(
        "large_mode_threshold: {}",
        excel_diff::AUTO_STREAM_CELL_THRESHOLD
    );
}

fn exit_code_for_error(err: &anyhow::Error) -> ExitCode {
    if is_internal_error(err) {
        ExitCode::from(3)
    } else {
        ExitCode::from(2)
    }
}

fn is_internal_error(err: &anyhow::Error) -> bool {
    err.chain().any(|cause| {
        if let Some(diff_err) = cause.downcast_ref::<DiffError>() {
            return !matches!(diff_err, DiffError::SheetNotFound { .. });
        }
        cause.is::<PackageError>()
            || cause.is::<ContainerError>()
            || cause.is::<GridParseError>()
            || cause.is::<DataMashupError>()
            || cause.is::<SectionParseError>()
    })
}


```

---

### File: `cli\src\output\git_diff.rs`

```rust
use anyhow::Result;
use excel_diff::{
    CellValue, DiffOp, DiffReport, ExpressionChangeKind, ModelColumnProperty, QueryChangeKind,
    QueryMetadataField, RelationshipProperty, StepChange, StepDiff, StepType, StringId,
    index_to_address,
};
use std::collections::BTreeMap;
use std::io::Write;

pub fn write_git_diff<W: Write>(
    w: &mut W,
    report: &DiffReport,
    old_path: &str,
    new_path: &str,
) -> Result<()> {
    writeln!(w, "diff --git a/{} b/{}", old_path, new_path)?;
    writeln!(w, "--- a/{}", old_path)?;
    writeln!(w, "+++ b/{}", new_path)?;

    if report.ops.is_empty() {
        writeln!(w, "@@ No differences @@")?;
        return Ok(());
    }

    let (workbook_ops, sheet_ops, query_ops, model_ops) = partition_ops(report);

    if !workbook_ops.is_empty() {
        writeln!(w, "@@ Workbook @@")?;
        for op in &workbook_ops {
            write_op_diff_lines(w, report, op)?;
        }
    }

    for (sheet_name, ops) in &sheet_ops {
        writeln!(w, "@@ Sheet \"{}\" @@", sheet_name)?;
        for op in ops {
            write_op_diff_lines(w, report, op)?;
        }
    }

    if !query_ops.is_empty() {
        writeln!(w, "@@ Power Query @@")?;
        for op in &query_ops {
            write_op_diff_lines(w, report, op)?;
        }
    }

    if !model_ops.is_empty() {
        writeln!(w, "@@ Model @@")?;
        for op in &model_ops {
            write_op_diff_lines(w, report, op)?;
        }
    }

    Ok(())
}

fn partition_ops(
    report: &DiffReport,
) -> (
    Vec<&DiffOp>,
    BTreeMap<String, Vec<&DiffOp>>,
    Vec<&DiffOp>,
    Vec<&DiffOp>,
) {
    let mut workbook_ops: Vec<&DiffOp> = Vec::new();
    let mut sheet_ops: BTreeMap<String, Vec<&DiffOp>> = BTreeMap::new();
    let mut query_ops: Vec<&DiffOp> = Vec::new();
    let mut model_ops: Vec<&DiffOp> = Vec::new();

    for op in &report.ops {
        if op.is_m_op() {
            query_ops.push(op);
        } else if op.is_model_op() {
            model_ops.push(op);
        } else if let Some(sheet_id) = get_sheet_id(op) {
            let sheet_name = report
                .resolve(sheet_id)
                .unwrap_or("<unknown>")
                .to_string();
            sheet_ops.entry(sheet_name).or_default().push(op);
        } else {
            workbook_ops.push(op);
        }
    }

    (workbook_ops, sheet_ops, query_ops, model_ops)
}

fn get_sheet_id(op: &DiffOp) -> Option<excel_diff::StringId> {
    match op {
        DiffOp::SheetAdded { sheet } => Some(*sheet),
        DiffOp::SheetRemoved { sheet } => Some(*sheet),
        DiffOp::SheetRenamed { sheet, .. } => Some(*sheet),
        DiffOp::RowAdded { sheet, .. } => Some(*sheet),
        DiffOp::RowRemoved { sheet, .. } => Some(*sheet),
        DiffOp::RowReplaced { sheet, .. } => Some(*sheet),
        DiffOp::DuplicateKeyCluster { sheet, .. } => Some(*sheet),
        DiffOp::ColumnAdded { sheet, .. } => Some(*sheet),
        DiffOp::ColumnRemoved { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedRows { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedColumns { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedRect { sheet, .. } => Some(*sheet),
        DiffOp::RectReplaced { sheet, .. } => Some(*sheet),
        DiffOp::CellEdited { sheet, .. } => Some(*sheet),
        DiffOp::ChartAdded { sheet, .. } => Some(*sheet),
        DiffOp::ChartRemoved { sheet, .. } => Some(*sheet),
        DiffOp::ChartChanged { sheet, .. } => Some(*sheet),
        _ => None,
    }
}

fn write_op_diff_lines<W: Write>(w: &mut W, report: &DiffReport, op: &DiffOp) -> Result<()> {
    match op {
        DiffOp::SheetAdded { sheet } => {
            writeln!(
                w,
                "+ Sheet \"{}\": ADDED",
                report.resolve(*sheet).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::SheetRemoved { sheet } => {
            writeln!(
                w,
                "- Sheet \"{}\": REMOVED",
                report.resolve(*sheet).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::SheetRenamed { from, to, .. } => {
            writeln!(
                w,
                "~ Sheet renamed: \"{}\" -> \"{}\"",
                report.resolve(*from).unwrap_or("<unknown>"),
                report.resolve(*to).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::RowAdded { row_idx, .. } => {
            writeln!(w, "+ Row {}: ADDED", row_idx + 1)?;
        }
        DiffOp::RowRemoved { row_idx, .. } => {
            writeln!(w, "- Row {}: REMOVED", row_idx + 1)?;
        }
        DiffOp::RowReplaced { row_idx, .. } => {
            writeln!(w, "~ Row {}: REPLACED", row_idx + 1)?;
        }
        DiffOp::DuplicateKeyCluster {
            key,
            left_rows,
            right_rows,
            ..
        } => {
            writeln!(
                w,
                "~ Duplicate key cluster: key=[{}] left_rows={} right_rows={}",
                format_key_values(key, report),
                left_rows.len(),
                right_rows.len()
            )?;
        }
        DiffOp::ColumnAdded { col_idx, .. } => {
            writeln!(w, "+ Column {}: ADDED", col_letter(*col_idx))?;
        }
        DiffOp::ColumnRemoved { col_idx, .. } => {
            writeln!(w, "- Column {}: REMOVED", col_letter(*col_idx))?;
        }
        DiffOp::BlockMovedRows {
            src_start_row,
            row_count,
            dst_start_row,
            ..
        } => {
            let src_end = src_start_row + row_count - 1;
            let dst_end = dst_start_row + row_count - 1;
            writeln!(
                w,
                "- Block: rows {}-{} (moved)",
                src_start_row + 1,
                src_end + 1
            )?;
            writeln!(
                w,
                "+ Block: rows {}-{} (moved)",
                dst_start_row + 1,
                dst_end + 1
            )?;
        }
        DiffOp::BlockMovedColumns {
            src_start_col,
            col_count,
            dst_start_col,
            ..
        } => {
            let src_end = src_start_col + col_count - 1;
            let dst_end = dst_start_col + col_count - 1;
            writeln!(
                w,
                "- Block: columns {}-{} (moved)",
                col_letter(*src_start_col),
                col_letter(src_end)
            )?;
            writeln!(
                w,
                "+ Block: columns {}-{} (moved)",
                col_letter(*dst_start_col),
                col_letter(dst_end)
            )?;
        }
        DiffOp::BlockMovedRect {
            src_start_row,
            src_row_count,
            src_start_col,
            src_col_count,
            dst_start_row,
            dst_start_col,
            ..
        } => {
            let src_range = format_range(
                *src_start_row,
                *src_start_col,
                *src_row_count,
                *src_col_count,
            );
            let dst_range = format_range(
                *dst_start_row,
                *dst_start_col,
                *src_row_count,
                *src_col_count,
            );
            writeln!(w, "- Block: {} (moved)", src_range)?;
            writeln!(w, "+ Block: {} (moved)", dst_range)?;
        }
        DiffOp::RectReplaced {
            start_row,
            row_count,
            start_col,
            col_count,
            ..
        } => {
            let range = format_range(*start_row, *start_col, *row_count, *col_count);
            writeln!(w, "~ Rect replaced: {}", range)?;
        }
        DiffOp::CellEdited {
            addr, from, to, ..
        } => {
            let old_str = format_cell_value(&from.value, report);
            let new_str = format_cell_value(&to.value, report);
            writeln!(w, "- Cell {}: {}", addr, old_str)?;
            writeln!(w, "+ Cell {}: {}", addr, new_str)?;
        }
        DiffOp::QueryAdded { name } => {
            writeln!(
                w,
                "+ Query \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::QueryRemoved { name } => {
            writeln!(
                w,
                "- Query \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::QueryRenamed { from, to } => {
            writeln!(
                w,
                "- Query \"{}\"",
                report.resolve(*from).unwrap_or("<unknown>")
            )?;
            writeln!(
                w,
                "+ Query \"{}\" (renamed)",
                report.resolve(*to).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::QueryDefinitionChanged {
            name,
            change_kind,
            semantic_detail,
            ..
        } => {
            let kind_str = match change_kind {
                QueryChangeKind::Semantic => "semantic change",
                QueryChangeKind::FormattingOnly => "formatting only",
                QueryChangeKind::Renamed => "renamed",
            };
            writeln!(
                w,
                "~ Query \"{}\": definition changed ({})",
                report.resolve(*name).unwrap_or("<unknown>"),
                kind_str
            )?;

            if let Some(detail) = semantic_detail {
                if !detail.step_diffs.is_empty() {
                    let mut added = 0usize;
                    let mut removed = 0usize;
                    let mut modified = 0usize;
                    let mut reordered = 0usize;

                    for d in &detail.step_diffs {
                        match d {
                            StepDiff::StepAdded { .. } => added += 1,
                            StepDiff::StepRemoved { .. } => removed += 1,
                            StepDiff::StepModified { .. } => modified += 1,
                            StepDiff::StepReordered { .. } => reordered += 1,
                        }
                    }

                    writeln!(w, "~   steps: +{} -{} ~{} r{}", added, removed, modified, reordered)?;

                    let max_lines = 3;
                    for d in detail.step_diffs.iter().take(max_lines) {
                        writeln!(w, "~   {}", format_step_diff(report, d))?;
                    }
                    if detail.step_diffs.len() > max_lines {
                        writeln!(
                            w,
                            "~   ... ({} more)",
                            detail.step_diffs.len() - max_lines
                        )?;
                    }
                } else if let Some(ast) = &detail.ast_summary {
                    writeln!(
                        w,
                        "~   ast: moved={} inserted={} deleted={} updated={}",
                        ast.moved, ast.inserted, ast.deleted, ast.updated
                    )?;
                }
            }
        }
        DiffOp::QueryMetadataChanged {
            name,
            field,
            old,
            new,
        } => {
            let field_name = match field {
                QueryMetadataField::LoadToSheet => "load_to_sheet",
                QueryMetadataField::LoadToModel => "load_to_model",
                QueryMetadataField::GroupPath => "group_path",
                QueryMetadataField::ConnectionOnly => "connection_only",
            };
            let old_str = old
                .map(|id| report.resolve(id).unwrap_or("<unknown>").to_string())
                .unwrap_or_else(|| "<none>".to_string());
            let new_str = new
                .map(|id| report.resolve(id).unwrap_or("<unknown>").to_string())
                .unwrap_or_else(|| "<none>".to_string());
            writeln!(
                w,
                "- Query \"{}\".{}: {}",
                report.resolve(*name).unwrap_or("<unknown>"),
                field_name,
                old_str
            )?;
            writeln!(
                w,
                "+ Query \"{}\".{}: {}",
                report.resolve(*name).unwrap_or("<unknown>"),
                field_name,
                new_str
            )?;
        }
        DiffOp::VbaModuleAdded { name } => {
            writeln!(
                w,
                "+ VBA module \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::VbaModuleRemoved { name } => {
            writeln!(
                w,
                "- VBA module \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::VbaModuleChanged { name } => {
            writeln!(
                w,
                "~ VBA module \"{}\": CHANGED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::NamedRangeAdded { name } => {
            writeln!(
                w,
                "+ Named range \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::NamedRangeRemoved { name } => {
            writeln!(
                w,
                "- Named range \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::NamedRangeChanged {
            name,
            old_ref,
            new_ref,
        } => {
            writeln!(
                w,
                "- Named range \"{}\": {}",
                report.resolve(*name).unwrap_or("<unknown>"),
                report.resolve(*old_ref).unwrap_or("<unknown>")
            )?;
            writeln!(
                w,
                "+ Named range \"{}\": {}",
                report.resolve(*name).unwrap_or("<unknown>"),
                report.resolve(*new_ref).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::ChartAdded { name, .. } => {
            writeln!(
                w,
                "+ Chart \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::ChartRemoved { name, .. } => {
            writeln!(
                w,
                "- Chart \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::ChartChanged { name, .. } => {
            writeln!(
                w,
                "~ Chart \"{}\": CHANGED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::TableAdded { name } => {
            writeln!(
                w,
                "+ Table \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::TableRemoved { name } => {
            writeln!(
                w,
                "- Table \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::ModelColumnAdded {
            table,
            name,
            data_type,
        } => {
            let label = format_column_ref(report, *table, *name);
            if let Some(ty) = data_type.and_then(|id| report.resolve(id)) {
                writeln!(w, "+ Column \"{}\": ADDED (type={})", label, ty)?;
            } else {
                writeln!(w, "+ Column \"{}\": ADDED", label)?;
            }
        }
        DiffOp::ModelColumnRemoved { table, name } => {
            writeln!(
                w,
                "- Column \"{}\": REMOVED",
                format_column_ref(report, *table, *name)
            )?;
        }
        DiffOp::ModelColumnTypeChanged {
            table,
            name,
            old_type,
            new_type,
        } => {
            let label = format_column_ref(report, *table, *name);
            let old_str = old_type
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new_type
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            writeln!(w, "- Column \"{}\": type: {}", label, old_str)?;
            writeln!(w, "+ Column \"{}\": type: {}", label, new_str)?;
        }
        DiffOp::ModelColumnPropertyChanged {
            table,
            name,
            field,
            old,
            new,
        } => {
            let label = format_column_ref(report, *table, *name);
            let old_str = old
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let field_name = column_field_name(*field);
            writeln!(w, "- Column \"{}\": {}: {}", label, field_name, old_str)?;
            writeln!(w, "+ Column \"{}\": {}: {}", label, field_name, new_str)?;
        }
        DiffOp::CalculatedColumnDefinitionChanged {
            table,
            name,
            change_kind,
            ..
        } => {
            writeln!(
                w,
                "~ Calculated column \"{}\": definition changed ({})",
                format_column_ref(report, *table, *name),
                expression_change_label(*change_kind)
            )?;
        }
        DiffOp::RelationshipAdded {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            writeln!(
                w,
                "+ Relationship {}: ADDED",
                format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column)
            )?;
        }
        DiffOp::RelationshipRemoved {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            writeln!(
                w,
                "- Relationship {}: REMOVED",
                format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column)
            )?;
        }
        DiffOp::RelationshipPropertyChanged {
            from_table,
            from_column,
            to_table,
            to_column,
            field,
            old,
            new,
        } => {
            let label =
                format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column);
            let old_str = old
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let field_name = relationship_field_name(*field);
            writeln!(w, "- Relationship {}: {}: {}", label, field_name, old_str)?;
            writeln!(w, "+ Relationship {}: {}: {}", label, field_name, new_str)?;
        }
        DiffOp::MeasureAdded { name } => {
            writeln!(
                w,
                "+ Measure \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::MeasureRemoved { name } => {
            writeln!(
                w,
                "- Measure \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )?;
        }
        DiffOp::MeasureDefinitionChanged { name, change_kind, .. } => {
            writeln!(
                w,
                "~ Measure \"{}\": definition changed ({})",
                report.resolve(*name).unwrap_or("<unknown>"),
                expression_change_label(*change_kind)
            )?;
        }
        _ => {
            writeln!(w, "~ {:?}", op)?;
        }
    }
    Ok(())
}

fn col_letter(col: u32) -> String {
    index_to_address(0, col)
        .chars()
        .take_while(|c| c.is_ascii_alphabetic())
        .collect()
}

fn format_range(start_row: u32, start_col: u32, row_count: u32, col_count: u32) -> String {
    let tl = index_to_address(start_row, start_col);
    let br = index_to_address(start_row + row_count - 1, start_col + col_count - 1);
    format!("{}:{}", tl, br)
}

fn format_cell_value(value: &Option<CellValue>, report: &DiffReport) -> String {
    match value {
        None => "<empty>".to_string(),
        Some(CellValue::Blank) => "<blank>".to_string(),
        Some(CellValue::Number(n)) => format_number(*n),
        Some(CellValue::Text(id)) => {
            let text = report.resolve(*id).unwrap_or("<unknown>");
            format!("\"{}\"", escape_string(text))
        }
        Some(CellValue::Bool(b)) => {
            if *b {
                "TRUE".to_string()
            } else {
                "FALSE".to_string()
            }
        }
        Some(CellValue::Error(id)) => report.resolve(*id).unwrap_or("#ERROR").to_string(),
    }
}

fn format_key_values(values: &[Option<CellValue>], report: &DiffReport) -> String {
    let parts: Vec<String> = values
        .iter()
        .map(|value| format_cell_value(value, report))
        .collect();
    parts.join(", ")
}

fn format_number(n: f64) -> String {
    if n.fract() == 0.0 && n.abs() < 1e15 {
        format!("{:.0}", n)
    } else {
        let s = format!("{:.10}", n);
        s.trim_end_matches('0').trim_end_matches('.').to_string()
    }
}

fn escape_string(s: &str) -> String {
    s.replace('\\', "\\\\")
        .replace('\n', "\\n")
        .replace('\r', "\\r")
        .replace('\t', "\\t")
        .replace('"', "\\\"")
}

fn format_step_diff(report: &DiffReport, d: &StepDiff) -> String {
    match d {
        StepDiff::StepAdded { step } => format!(
            "+ {} ({})",
            report.resolve(step.name).unwrap_or("<unknown>"),
            format_step_type(step.step_type)
        ),
        StepDiff::StepRemoved { step } => format!(
            "- {} ({})",
            report.resolve(step.name).unwrap_or("<unknown>"),
            format_step_type(step.step_type)
        ),
        StepDiff::StepReordered {
            name,
            from_index,
            to_index,
        } => format!(
            "r {} {} -> {}",
            report.resolve(*name).unwrap_or("<unknown>"),
            from_index,
            to_index
        ),
        StepDiff::StepModified {
            before: _,
            after,
            changes,
        } => {
            let mut line = format!(
                "~ {} ({})",
                report.resolve(after.name).unwrap_or("<unknown>"),
                format_step_type(after.step_type)
            );
            if !changes.is_empty() {
                let mut parts = Vec::new();
                for change in changes {
                    match change {
                        StepChange::Renamed { from, to } => parts.push(format!(
                            "renamed {} -> {}",
                            report.resolve(*from).unwrap_or("<unknown>"),
                            report.resolve(*to).unwrap_or("<unknown>")
                        )),
                        StepChange::SourceRefsChanged { removed, added } => parts.push(format!(
                            "refs -{} +{}",
                            removed.len(),
                            added.len()
                        )),
                        StepChange::ParamsChanged => parts.push("params".to_string()),
                    }
                }
                line.push_str(&format!(" [{}]", parts.join(", ")));
            }
            line
        }
    }
}

fn format_step_type(t: StepType) -> &'static str {
    match t {
        StepType::TableSelectRows => "Table.SelectRows",
        StepType::TableRemoveColumns => "Table.RemoveColumns",
        StepType::TableRenameColumns => "Table.RenameColumns",
        StepType::TableTransformColumnTypes => "Table.TransformColumnTypes",
        StepType::TableNestedJoin => "Table.NestedJoin",
        StepType::TableJoin => "Table.Join",
        StepType::Other => "Other",
    }
}

fn format_column_ref(report: &DiffReport, table: StringId, column: StringId) -> String {
    let table_name = report.resolve(table).unwrap_or("<unknown>");
    let column_name = report.resolve(column).unwrap_or("<unknown>");
    format!("{}.{}", table_name, column_name)
}

fn format_relationship_ref(
    report: &DiffReport,
    from_table: StringId,
    from_column: StringId,
    to_table: StringId,
    to_column: StringId,
) -> String {
    let from_table = report.resolve(from_table).unwrap_or("<unknown>");
    let from_column = report.resolve(from_column).unwrap_or("<unknown>");
    let to_table = report.resolve(to_table).unwrap_or("<unknown>");
    let to_column = report.resolve(to_column).unwrap_or("<unknown>");
    format!("{}[{}] -> {}[{}]", from_table, from_column, to_table, to_column)
}

fn column_field_name(field: ModelColumnProperty) -> &'static str {
    match field {
        ModelColumnProperty::Hidden => "hidden",
        ModelColumnProperty::FormatString => "format_string",
        ModelColumnProperty::SortBy => "sort_by",
        ModelColumnProperty::SummarizeBy => "summarize_by",
    }
}

fn relationship_field_name(field: RelationshipProperty) -> &'static str {
    match field {
        RelationshipProperty::CrossFilteringBehavior => "cross_filtering_behavior",
        RelationshipProperty::Cardinality => "cardinality",
        RelationshipProperty::IsActive => "is_active",
    }
}

fn expression_change_label(kind: ExpressionChangeKind) -> &'static str {
    match kind {
        ExpressionChangeKind::Semantic => "semantic change",
        ExpressionChangeKind::FormattingOnly => "formatting only",
        ExpressionChangeKind::Unknown => "unknown",
    }
}


```

---

### File: `cli\src\output\json.rs`

```rust
use anyhow::Result;
use excel_diff::DiffReport;
use serde::Serialize;
use std::io::Write;

pub fn write_json_report<W: Write>(w: &mut W, report: &DiffReport) -> Result<()> {
    write_json_value(w, report)
}

pub fn write_json_value<W: Write, T: Serialize>(w: &mut W, value: &T) -> Result<()> {
    serde_json::to_writer_pretty(&mut *w, value)?;
    writeln!(w)?;
    Ok(())
}


```

---

### File: `cli\src\output\mod.rs`

```rust
pub mod git_diff;
pub mod json;
pub mod text;


```

---

### File: `cli\src\output\text.rs`

```rust
use crate::commands::diff::Verbosity;
use anyhow::Result;
use excel_diff::{
    CellValue, DiffOp, DiffReport, ExpressionChangeKind, QueryChangeKind, QueryMetadataField,
    StepChange, StepDiff, StepType, StringId, index_to_address,
};
use std::collections::BTreeMap;
use std::io::Write;

pub fn write_text_report<W: Write>(
    w: &mut W,
    report: &DiffReport,
    old_path: &str,
    new_path: &str,
    verbosity: Verbosity,
) -> Result<()> {
    if verbosity != Verbosity::Quiet {
        let old_name = std::path::Path::new(old_path)
            .file_name()
            .map(|s| s.to_string_lossy())
            .unwrap_or_else(|| old_path.into());
        let new_name = std::path::Path::new(new_path)
            .file_name()
            .map(|s| s.to_string_lossy())
            .unwrap_or_else(|| new_path.into());
        writeln!(w, "Comparing: {} -> {}", old_name, new_name)?;
        writeln!(w)?;
    }

    if verbosity == Verbosity::Quiet {
        write_summary(w, report)?;
        return Ok(());
    }

    if report.ops.is_empty() {
        writeln!(w, "No differences found.")?;
        write_summary(w, report)?;
        return Ok(());
    }

    let (workbook_ops, sheet_ops, query_ops, model_ops) = partition_ops(report);

    if !workbook_ops.is_empty() {
        writeln!(w, "Workbook:")?;
        for op in &workbook_ops {
            let lines = render_op(report, op, verbosity);
            for line in lines {
                writeln!(w, "  {}", line)?;
            }
        }
        writeln!(w)?;
    }

    for (sheet_name, ops) in &sheet_ops {
        writeln!(w, "Sheet \"{}\":", sheet_name)?;
        for op in ops {
            let lines = render_op(report, op, verbosity);
            for line in lines {
                writeln!(w, "  {}", line)?;
            }
        }
        writeln!(w)?;
    }

    if !query_ops.is_empty() {
        writeln!(w, "Power Query:")?;
        for op in &query_ops {
            let lines = render_op(report, op, verbosity);
            for line in lines {
                writeln!(w, "  {}", line)?;
            }
        }
        writeln!(w)?;
    }

    if !model_ops.is_empty() {
        writeln!(w, "Model:")?;
        for op in &model_ops {
            let lines = render_op(report, op, verbosity);
            for line in lines {
                writeln!(w, "  {}", line)?;
            }
        }
        writeln!(w)?;
    }

    write_summary(w, report)?;
    Ok(())
}

fn partition_ops(
    report: &DiffReport,
) -> (
    Vec<&DiffOp>,
    BTreeMap<String, Vec<&DiffOp>>,
    Vec<&DiffOp>,
    Vec<&DiffOp>,
) {
    let mut workbook_ops: Vec<&DiffOp> = Vec::new();
    let mut sheet_ops: BTreeMap<String, Vec<&DiffOp>> = BTreeMap::new();
    let mut query_ops: Vec<&DiffOp> = Vec::new();
    let mut model_ops: Vec<&DiffOp> = Vec::new();

    for op in &report.ops {
        if op.is_m_op() {
            query_ops.push(op);
        } else if op.is_model_op() {
            model_ops.push(op);
        } else if let Some(sheet_id) = get_sheet_id(op) {
            let sheet_name = report
                .resolve(sheet_id)
                .unwrap_or("<unknown>")
                .to_string();
            sheet_ops.entry(sheet_name).or_default().push(op);
        } else {
            workbook_ops.push(op);
        }
    }

    (workbook_ops, sheet_ops, query_ops, model_ops)
}

fn get_sheet_id(op: &DiffOp) -> Option<StringId> {
    match op {
        DiffOp::SheetAdded { sheet } => Some(*sheet),
        DiffOp::SheetRemoved { sheet } => Some(*sheet),
        DiffOp::SheetRenamed { sheet, .. } => Some(*sheet),
        DiffOp::RowAdded { sheet, .. } => Some(*sheet),
        DiffOp::RowRemoved { sheet, .. } => Some(*sheet),
        DiffOp::RowReplaced { sheet, .. } => Some(*sheet),
        DiffOp::DuplicateKeyCluster { sheet, .. } => Some(*sheet),
        DiffOp::ColumnAdded { sheet, .. } => Some(*sheet),
        DiffOp::ColumnRemoved { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedRows { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedColumns { sheet, .. } => Some(*sheet),
        DiffOp::BlockMovedRect { sheet, .. } => Some(*sheet),
        DiffOp::RectReplaced { sheet, .. } => Some(*sheet),
        DiffOp::CellEdited { sheet, .. } => Some(*sheet),
        DiffOp::ChartAdded { sheet, .. } => Some(*sheet),
        DiffOp::ChartRemoved { sheet, .. } => Some(*sheet),
        DiffOp::ChartChanged { sheet, .. } => Some(*sheet),
        _ => None,
    }
}

fn render_op(report: &DiffReport, op: &DiffOp, verbosity: Verbosity) -> Vec<String> {
    match op {
        DiffOp::SheetAdded { sheet } => {
            vec![format!(
                "Sheet \"{}\": ADDED",
                report.resolve(*sheet).unwrap_or("<unknown>")
            )]
        }
        DiffOp::SheetRemoved { sheet } => {
            vec![format!(
                "Sheet \"{}\": REMOVED",
                report.resolve(*sheet).unwrap_or("<unknown>")
            )]
        }
        DiffOp::SheetRenamed { from, to, .. } => {
            vec![format!(
                "Sheet renamed: \"{}\" -> \"{}\"",
                report.resolve(*from).unwrap_or("<unknown>"),
                report.resolve(*to).unwrap_or("<unknown>")
            )]
        }
        DiffOp::RowAdded { row_idx, .. } => {
            vec![format!("Row {}: ADDED", row_idx + 1)]
        }
        DiffOp::RowRemoved { row_idx, .. } => {
            vec![format!("Row {}: REMOVED", row_idx + 1)]
        }
        DiffOp::RowReplaced { row_idx, .. } => {
            vec![format!("Row {}: REPLACED", row_idx + 1)]
        }
        DiffOp::DuplicateKeyCluster {
            key,
            left_rows,
            right_rows,
            ..
        } => {
            let key_str = format_key_values(key, report);
            if verbosity == Verbosity::Verbose {
                vec![format!(
                    "Duplicate key cluster: key=[{}] left_rows={} right_rows={}",
                    key_str,
                    format_row_list(left_rows),
                    format_row_list(right_rows)
                )]
            } else {
                vec![format!(
                    "Duplicate key cluster: key=[{}] left_rows={} right_rows={}",
                    key_str,
                    left_rows.len(),
                    right_rows.len()
                )]
            }
        }
        DiffOp::ColumnAdded { col_idx, .. } => {
            vec![format!("Column {}: ADDED", col_letter(*col_idx))]
        }
        DiffOp::ColumnRemoved { col_idx, .. } => {
            vec![format!("Column {}: REMOVED", col_letter(*col_idx))]
        }
        DiffOp::BlockMovedRows {
            src_start_row,
            row_count,
            dst_start_row,
            block_hash,
            ..
        } => {
            let src_end = src_start_row + row_count - 1;
            let dst_end = dst_start_row + row_count - 1;
            let mut result = vec![format!(
                "Block moved: rows {}-{}  rows {}-{}",
                src_start_row + 1,
                src_end + 1,
                dst_start_row + 1,
                dst_end + 1
            )];
            if verbosity == Verbosity::Verbose {
                if let Some(hash) = block_hash {
                    result.push(format!("  (hash: {:016x})", hash));
                }
            }
            result
        }
        DiffOp::BlockMovedColumns {
            src_start_col,
            col_count,
            dst_start_col,
            block_hash,
            ..
        } => {
            let src_end = src_start_col + col_count - 1;
            let dst_end = dst_start_col + col_count - 1;
            let mut result = vec![format!(
                "Block moved: columns {}-{}  columns {}-{}",
                col_letter(*src_start_col),
                col_letter(src_end),
                col_letter(*dst_start_col),
                col_letter(dst_end)
            )];
            if verbosity == Verbosity::Verbose {
                if let Some(hash) = block_hash {
                    result.push(format!("  (hash: {:016x})", hash));
                }
            }
            result
        }
        DiffOp::BlockMovedRect {
            src_start_row,
            src_row_count,
            src_start_col,
            src_col_count,
            dst_start_row,
            dst_start_col,
            block_hash,
            ..
        } => {
            let src_range = format_range(
                *src_start_row,
                *src_start_col,
                *src_row_count,
                *src_col_count,
            );
            let dst_range = format_range(
                *dst_start_row,
                *dst_start_col,
                *src_row_count,
                *src_col_count,
            );
            let mut result = vec![format!("Block moved: {}  {}", src_range, dst_range)];
            if verbosity == Verbosity::Verbose {
                if let Some(hash) = block_hash {
                    result.push(format!("  (hash: {:016x})", hash));
                }
            }
            result
        }
        DiffOp::RectReplaced {
            start_row,
            row_count,
            start_col,
            col_count,
            ..
        } => {
            let range = format_range(*start_row, *start_col, *row_count, *col_count);
            vec![format!("Rect replaced: {}", range)]
        }
        DiffOp::CellEdited {
            addr, from, to, ..
        } => {
            let old_str = format_cell_value(&from.value, report);
            let new_str = format_cell_value(&to.value, report);
            let mut result = vec![format!("Cell {}: {}  {}", addr, old_str, new_str)];
            if verbosity == Verbosity::Verbose {
                if let Some(formula_id) = from.formula {
                    if let Some(formula) = report.resolve(formula_id) {
                        result.push(format!("  old formula: ={}", formula));
                    }
                }
                if let Some(formula_id) = to.formula {
                    if let Some(formula) = report.resolve(formula_id) {
                        result.push(format!("  new formula: ={}", formula));
                    }
                }
            }
            result
        }
        DiffOp::QueryAdded { name } => {
            vec![format!(
                "Query \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::QueryRemoved { name } => {
            vec![format!(
                "Query \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::QueryRenamed { from, to } => {
            vec![format!(
                "Query renamed: \"{}\"  \"{}\"",
                report.resolve(*from).unwrap_or("<unknown>"),
                report.resolve(*to).unwrap_or("<unknown>")
            )]
        }
        DiffOp::QueryDefinitionChanged {
            name,
            change_kind,
            semantic_detail,
            ..
        } => {
            let kind_str = match change_kind {
                QueryChangeKind::Semantic => "semantic change",
                QueryChangeKind::FormattingOnly => "formatting only",
                QueryChangeKind::Renamed => "renamed",
            };

            let mut lines = vec![format!(
                "Query \"{}\": definition changed ({})",
                report.resolve(*name).unwrap_or("<unknown>"),
                kind_str
            )];

            let Some(detail) = semantic_detail else {
                return lines;
            };

            if !detail.step_diffs.is_empty() {
                let mut added = 0usize;
                let mut removed = 0usize;
                let mut modified = 0usize;
                let mut reordered = 0usize;

                for d in &detail.step_diffs {
                    match d {
                        StepDiff::StepAdded { .. } => added += 1,
                        StepDiff::StepRemoved { .. } => removed += 1,
                        StepDiff::StepModified { .. } => modified += 1,
                        StepDiff::StepReordered { .. } => reordered += 1,
                    }
                }

                lines.push(format!(
                    "  steps: +{} -{} ~{} r{}",
                    added, removed, modified, reordered
                ));

                let max_lines = if verbosity == Verbosity::Verbose { 50 } else { 5 };
                for d in detail.step_diffs.iter().take(max_lines) {
                    lines.push(format!("  {}", format_step_diff(report, d)));
                }
                if detail.step_diffs.len() > max_lines {
                    lines.push(format!(
                        "  ... ({} more)",
                        detail.step_diffs.len() - max_lines
                    ));
                }

                return lines;
            }

            if let Some(ast) = &detail.ast_summary {
                lines.push(format!(
                    "  ast: mode={:?} moved={} inserted={} deleted={} updated={}",
                    ast.mode, ast.moved, ast.inserted, ast.deleted, ast.updated
                ));
                if verbosity == Verbosity::Verbose && !ast.move_hints.is_empty() {
                    for mh in ast.move_hints.iter().take(8) {
                        lines.push(format!(
                            "  ast_move: hash={} size={} from={} to={}",
                            mh.subtree_hash, mh.subtree_size, mh.from_preorder, mh.to_preorder
                        ));
                    }
                }
            }

            lines
        }
        DiffOp::QueryMetadataChanged {
            name,
            field,
            old,
            new,
        } => {
            let field_name = match field {
                QueryMetadataField::LoadToSheet => "load_to_sheet",
                QueryMetadataField::LoadToModel => "load_to_model",
                QueryMetadataField::GroupPath => "group_path",
                QueryMetadataField::ConnectionOnly => "connection_only",
            };
            let old_str = old
                .map(|id| report.resolve(id).unwrap_or("<unknown>").to_string())
                .unwrap_or_else(|| "<none>".to_string());
            let new_str = new
                .map(|id| report.resolve(id).unwrap_or("<unknown>").to_string())
                .unwrap_or_else(|| "<none>".to_string());
            vec![format!(
                "Query \"{}\": {} changed: {}  {}",
                report.resolve(*name).unwrap_or("<unknown>"),
                field_name,
                old_str,
                new_str
            )]
        }
        DiffOp::VbaModuleAdded { name } => {
            vec![format!(
                "VBA module \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::VbaModuleRemoved { name } => {
            vec![format!(
                "VBA module \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::VbaModuleChanged { name } => {
            vec![format!(
                "VBA module \"{}\": CHANGED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::NamedRangeAdded { name } => {
            vec![format!(
                "Named range \"{}\": ADDED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::NamedRangeRemoved { name } => {
            vec![format!(
                "Named range \"{}\": REMOVED",
                report.resolve(*name).unwrap_or("<unknown>")
            )]
        }
        DiffOp::NamedRangeChanged {
            name,
            old_ref,
            new_ref,
        } => {
            let mut lines = vec![format!(
                "Named range \"{}\": CHANGED",
                report.resolve(*name).unwrap_or("<unknown>")
            )];
            if verbosity == Verbosity::Verbose {
                let old_str = report.resolve(*old_ref).unwrap_or("<unknown>");
                let new_str = report.resolve(*new_ref).unwrap_or("<unknown>");
                lines.push(format!("  refers_to: {} -> {}", old_str, new_str));
            }
            lines
        }
        DiffOp::ChartAdded { name, .. } => vec![format!(
            "Chart \"{}\": ADDED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::ChartRemoved { name, .. } => vec![format!(
            "Chart \"{}\": REMOVED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::ChartChanged { name, .. } => vec![format!(
            "Chart \"{}\": CHANGED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::MeasureAdded { name } => vec![format!(
            "Measure \"{}\": ADDED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::MeasureRemoved { name } => vec![format!(
            "Measure \"{}\": REMOVED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::MeasureDefinitionChanged { name, change_kind, .. } => vec![format!(
            "Measure \"{}\": definition changed ({})",
            report.resolve(*name).unwrap_or("<unknown>"),
            expression_change_label(*change_kind)
        )],
        DiffOp::TableAdded { name } => vec![format!(
            "Table \"{}\": ADDED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::TableRemoved { name } => vec![format!(
            "Table \"{}\": REMOVED",
            report.resolve(*name).unwrap_or("<unknown>")
        )],
        DiffOp::ModelColumnAdded {
            table,
            name,
            data_type,
        } => {
            let label = format_column_ref(report, *table, *name);
            let mut lines = vec![format!("Column \"{}\": ADDED", label)];
            if verbosity == Verbosity::Verbose {
                if let Some(ty) = data_type.and_then(|id| report.resolve(id)) {
                    lines.push(format!("  type: {}", ty));
                }
            }
            lines
        }
        DiffOp::ModelColumnRemoved { table, name } => vec![format!(
            "Column \"{}\": REMOVED",
            format_column_ref(report, *table, *name)
        )],
        DiffOp::ModelColumnTypeChanged {
            table,
            name,
            old_type,
            new_type,
        } => {
            let old_str = old_type
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new_type
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            vec![format!(
                "Column \"{}\": type changed: {} -> {}",
                format_column_ref(report, *table, *name),
                old_str,
                new_str
            )]
        }
        DiffOp::ModelColumnPropertyChanged {
            table,
            name,
            field,
            old,
            new,
        } => {
            let old_str = old
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            vec![format!(
                "Column \"{}\": {} changed: {} -> {}",
                format_column_ref(report, *table, *name),
                column_field_name(*field),
                old_str,
                new_str
            )]
        }
        DiffOp::CalculatedColumnDefinitionChanged {
            table,
            name,
            change_kind,
            ..
        } => vec![format!(
            "Calculated column \"{}\": definition changed ({})",
            format_column_ref(report, *table, *name),
            expression_change_label(*change_kind)
        )],
        DiffOp::RelationshipAdded {
            from_table,
            from_column,
            to_table,
            to_column,
        } => vec![format!(
            "Relationship {}: ADDED",
            format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column)
        )],
        DiffOp::RelationshipRemoved {
            from_table,
            from_column,
            to_table,
            to_column,
        } => vec![format!(
            "Relationship {}: REMOVED",
            format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column)
        )],
        DiffOp::RelationshipPropertyChanged {
            from_table,
            from_column,
            to_table,
            to_column,
            field,
            old,
            new,
        } => {
            let old_str = old
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            let new_str = new
                .and_then(|id| report.resolve(id))
                .unwrap_or("<none>");
            vec![format!(
                "Relationship {}: {} changed: {} -> {}",
                format_relationship_ref(report, *from_table, *from_column, *to_table, *to_column),
                relationship_field_name(*field),
                old_str,
                new_str
            )]
        }
        _ => vec![format!("{:?}", op)],
    }
}

fn col_letter(col: u32) -> String {
    index_to_address(0, col)
        .chars()
        .take_while(|c| c.is_ascii_alphabetic())
        .collect()
}

fn format_step_diff(report: &DiffReport, d: &StepDiff) -> String {
    match d {
        StepDiff::StepAdded { step } => format!(
            "+ {} ({})",
            report.resolve(step.name).unwrap_or("<unknown>"),
            format_step_type(step.step_type)
        ),
        StepDiff::StepRemoved { step } => format!(
            "- {} ({})",
            report.resolve(step.name).unwrap_or("<unknown>"),
            format_step_type(step.step_type)
        ),
        StepDiff::StepReordered {
            name,
            from_index,
            to_index,
        } => format!(
            "r {} {} -> {}",
            report.resolve(*name).unwrap_or("<unknown>"),
            from_index,
            to_index
        ),
        StepDiff::StepModified {
            before: _,
            after,
            changes,
        } => {
            let mut line = format!(
                "~ {} ({})",
                report.resolve(after.name).unwrap_or("<unknown>"),
                format_step_type(after.step_type)
            );
            if !changes.is_empty() {
                let mut parts = Vec::new();
                for change in changes {
                    match change {
                        StepChange::Renamed { from, to } => parts.push(format!(
                            "renamed {} -> {}",
                            report.resolve(*from).unwrap_or("<unknown>"),
                            report.resolve(*to).unwrap_or("<unknown>")
                        )),
                        StepChange::SourceRefsChanged { removed, added } => parts.push(format!(
                            "refs -{} +{}",
                            removed.len(),
                            added.len()
                        )),
                        StepChange::ParamsChanged => parts.push("params".to_string()),
                    }
                }
                line.push_str(&format!(" [{}]", parts.join(", ")));
            }
            line
        }
    }
}

fn format_step_type(t: StepType) -> &'static str {
    match t {
        StepType::TableSelectRows => "Table.SelectRows",
        StepType::TableRemoveColumns => "Table.RemoveColumns",
        StepType::TableRenameColumns => "Table.RenameColumns",
        StepType::TableTransformColumnTypes => "Table.TransformColumnTypes",
        StepType::TableNestedJoin => "Table.NestedJoin",
        StepType::TableJoin => "Table.Join",
        StepType::Other => "Other",
    }
}

fn format_range(start_row: u32, start_col: u32, row_count: u32, col_count: u32) -> String {
    let tl = index_to_address(start_row, start_col);
    let br = index_to_address(start_row + row_count - 1, start_col + col_count - 1);
    format!("{}:{}", tl, br)
}

fn format_cell_value(value: &Option<CellValue>, report: &DiffReport) -> String {
    match value {
        None => "<empty>".to_string(),
        Some(CellValue::Blank) => "<blank>".to_string(),
        Some(CellValue::Number(n)) => format_number(*n),
        Some(CellValue::Text(id)) => {
            let text = report.resolve(*id).unwrap_or("<unknown>");
            format!("\"{}\"", escape_string(text))
        }
        Some(CellValue::Bool(b)) => {
            if *b {
                "TRUE".to_string()
            } else {
                "FALSE".to_string()
            }
        }
        Some(CellValue::Error(id)) => report.resolve(*id).unwrap_or("#ERROR").to_string(),
    }
}

fn format_key_values(values: &[Option<CellValue>], report: &DiffReport) -> String {
    let parts: Vec<String> = values
        .iter()
        .map(|value| format_cell_value(value, report))
        .collect();
    parts.join(", ")
}

fn format_row_list(rows: &[u32]) -> String {
    let parts: Vec<String> = rows.iter().map(|row| (row + 1).to_string()).collect();
    parts.join(", ")
}

fn format_number(n: f64) -> String {
    if n.fract() == 0.0 && n.abs() < 1e15 {
        format!("{:.0}", n)
    } else {
        let s = format!("{:.10}", n);
        s.trim_end_matches('0').trim_end_matches('.').to_string()
    }
}

fn escape_string(s: &str) -> String {
    s.replace('\\', "\\\\")
        .replace('\n', "\\n")
        .replace('\r', "\\r")
        .replace('\t', "\\t")
        .replace('"', "\\\"")
}

fn write_summary<W: Write>(w: &mut W, report: &DiffReport) -> Result<()> {
    writeln!(w, "---")?;
    writeln!(w, "Summary:")?;
    writeln!(w, "  Total changes: {}", report.ops.len())?;

    let counts = count_ops(report);
    if counts.sheets > 0 {
        writeln!(w, "  Sheet changes: {}", counts.sheets)?;
    }
    if counts.rows > 0 {
        writeln!(w, "  Row changes: {}", counts.rows)?;
    }
    if counts.cols > 0 {
        writeln!(w, "  Column changes: {}", counts.cols)?;
    }
    if counts.blocks > 0 {
        writeln!(w, "  Block moves: {}", counts.blocks)?;
    }
    if counts.cells > 0 {
        writeln!(w, "  Cell edits: {}", counts.cells)?;
    }
    if counts.queries > 0 {
        writeln!(w, "  Query changes: {}", counts.queries)?;
    }
    if counts.model > 0 {
        writeln!(w, "  Model changes: {}", counts.model)?;
    }

    if !report.complete {
        writeln!(w, "  Status: INCOMPLETE (some changes may be missing)")?;
    } else {
        writeln!(w, "  Status: complete")?;
    }

    Ok(())
}

struct OpCounts {
    sheets: usize,
    rows: usize,
    cols: usize,
    blocks: usize,
    cells: usize,
    queries: usize,
    model: usize,
}

fn count_ops(report: &DiffReport) -> OpCounts {
    let mut counts = OpCounts {
        sheets: 0,
        rows: 0,
        cols: 0,
        blocks: 0,
        cells: 0,
        queries: 0,
        model: 0,
    };

    for op in &report.ops {
        match op {
            DiffOp::SheetAdded { .. }
            | DiffOp::SheetRemoved { .. }
            | DiffOp::SheetRenamed { .. } => counts.sheets += 1,
            DiffOp::RowAdded { .. }
            | DiffOp::RowRemoved { .. }
            | DiffOp::RowReplaced { .. }
            | DiffOp::DuplicateKeyCluster { .. } => {
                counts.rows += 1
            }
            DiffOp::ColumnAdded { .. } | DiffOp::ColumnRemoved { .. } => counts.cols += 1,
            DiffOp::BlockMovedRows { .. }
            | DiffOp::BlockMovedColumns { .. }
            | DiffOp::BlockMovedRect { .. }
            | DiffOp::RectReplaced { .. } => counts.blocks += 1,
            DiffOp::CellEdited { .. } => counts.cells += 1,
            DiffOp::QueryAdded { .. }
            | DiffOp::QueryRemoved { .. }
            | DiffOp::QueryRenamed { .. }
            | DiffOp::QueryDefinitionChanged { .. }
            | DiffOp::QueryMetadataChanged { .. } => counts.queries += 1,
            _ if op.is_model_op() => counts.model += 1,
            _ => {}
        }
    }

    counts
}

fn format_column_ref(report: &DiffReport, table: StringId, column: StringId) -> String {
    let table_name = report.resolve(table).unwrap_or("<unknown>");
    let column_name = report.resolve(column).unwrap_or("<unknown>");
    format!("{}.{}", table_name, column_name)
}

fn format_relationship_ref(
    report: &DiffReport,
    from_table: StringId,
    from_column: StringId,
    to_table: StringId,
    to_column: StringId,
) -> String {
    let from_table = report.resolve(from_table).unwrap_or("<unknown>");
    let from_column = report.resolve(from_column).unwrap_or("<unknown>");
    let to_table = report.resolve(to_table).unwrap_or("<unknown>");
    let to_column = report.resolve(to_column).unwrap_or("<unknown>");
    format!("{}[{}] -> {}[{}]", from_table, from_column, to_table, to_column)
}

fn column_field_name(field: excel_diff::ModelColumnProperty) -> &'static str {
    match field {
        excel_diff::ModelColumnProperty::Hidden => "hidden",
        excel_diff::ModelColumnProperty::FormatString => "format_string",
        excel_diff::ModelColumnProperty::SortBy => "sort_by",
        excel_diff::ModelColumnProperty::SummarizeBy => "summarize_by",
    }
}

fn relationship_field_name(field: excel_diff::RelationshipProperty) -> &'static str {
    match field {
        excel_diff::RelationshipProperty::CrossFilteringBehavior => "cross_filtering_behavior",
        excel_diff::RelationshipProperty::Cardinality => "cardinality",
        excel_diff::RelationshipProperty::IsActive => "is_active",
    }
}

fn expression_change_label(kind: ExpressionChangeKind) -> &'static str {
    match kind {
        ExpressionChangeKind::Semantic => "semantic change",
        ExpressionChangeKind::FormattingOnly => "formatting only",
        ExpressionChangeKind::Unknown => "unknown",
    }
}


```

---

### File: `cli\tests\determinism_cli_json.rs`

```rust
use std::process::Command;

fn excel_diff_cmd() -> Command {
    Command::new(env!("CARGO_BIN_EXE_excel-diff"))
}

fn fixture_path(name: &str) -> String {
    let p = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .parent()
        .unwrap()
        .join("fixtures")
        .join("generated")
        .join(name);

    p.to_string_lossy().into_owned()
}

fn run_json_diff_with_threads(threads: &str) -> serde_json::Value {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "json",
            &fixture_path("composed_grid_mashup_a.xlsx"),
            &fixture_path("composed_grid_mashup_b.xlsx"),
        ])
        .env("RAYON_NUM_THREADS", threads)
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    serde_json::from_str(&stdout).expect("output should be valid JSON")
}

#[test]
fn json_output_is_deterministic_across_thread_counts() {
    let one = run_json_diff_with_threads("1");
    let two = run_json_diff_with_threads("2");
    let eight = run_json_diff_with_threads("8");

    assert_eq!(one, two, "json output should be stable across threads");
    assert_eq!(one, eight, "json output should be stable across threads");
}

```

---

### File: `cli\tests\git_textconv.rs`

```rust
use std::fs;
use std::path::PathBuf;
use std::process::Command;

fn run_git(repo: &PathBuf, args: &[&str]) -> String {
    let out = Command::new("git")
        .args(args)
        .current_dir(repo)
        .output()
        .expect("git should run");
    assert!(out.status.success(), "git failed: {:?}", out);
    String::from_utf8_lossy(&out.stdout).to_string()
}

#[test]
fn git_textconv_uses_excel_diff_info() {
    if Command::new("git").arg("--version").output().is_err() {
        return;
    }

    let tmp = tempfile::tempdir().expect("tempdir");
    let repo = tmp.path().to_path_buf();

    run_git(&repo, &["init"]);
    run_git(&repo, &["config", "user.email", "test@example.com"]);
    run_git(&repo, &["config", "user.name", "Test"]);

    fs::write(repo.join(".gitattributes"), "*.xlsx diff=xlsx\n").expect("write gitattributes");

    let exe = PathBuf::from(env!("CARGO_BIN_EXE_excel-diff"));
    let exe_str = exe.to_string_lossy().replace('\\', "/");
    let textconv = format!("\"{}\" info", exe_str);
    run_git(&repo, &["config", "diff.xlsx.binary", "true"]);
    run_git(&repo, &["config", "diff.xlsx.textconv", &textconv]);

    let fixture_dir = PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .parent()
        .unwrap()
        .join("fixtures")
        .join("generated");

    let a = fixture_dir.join("pg1_basic_two_sheets.xlsx");
    let b = fixture_dir.join("one_query.xlsx");

    let target = repo.join("book.xlsx");
    fs::copy(&a, &target).expect("copy fixture a");
    run_git(&repo, &["add", "book.xlsx"]);
    run_git(&repo, &["commit", "-m", "add book"]);

    fs::copy(&b, &target).expect("copy fixture b");

    let diff = run_git(&repo, &["diff", "--textconv"]);

    assert!(diff.contains("Workbook:"), "expected textconv output");
    assert!(diff.contains("Sheets:"), "expected workbook structure");
}


```

---

### File: `cli\tests\integration_tests.rs`

```rust
use std::process::Command;

fn excel_diff_cmd() -> Command {
    Command::new(env!("CARGO_BIN_EXE_excel-diff"))
}

fn fixture_path(name: &str) -> String {
    let p = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .parent()
        .unwrap()
        .join("fixtures")
        .join("generated")
        .join(name);

    p.to_string_lossy().into_owned()
}

fn run_jsonl_diff(old_path: &str, new_path: &str) -> String {
    let output = excel_diff_cmd()
        .args(["diff", "--format", "jsonl", old_path, new_path])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "jsonl diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    String::from_utf8_lossy(&output.stdout).into_owned()
}

#[test]
fn identical_files_exit_0() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            &fixture_path("equal_sheet_a.xlsx"),
            &fixture_path("equal_sheet_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert!(
        output.status.success(),
        "identical files should exit 0: {:?}",
        String::from_utf8_lossy(&output.stderr)
    );
}

#[test]
fn different_files_exit_1() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "different files should exit 1: stdout={}, stderr={}",
        String::from_utf8_lossy(&output.stdout),
        String::from_utf8_lossy(&output.stderr)
    );
}

#[test]
fn max_memory_zero_exits_1_and_warns() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--max-memory",
            "0",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "memory-capped diff should exit 1: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("Warning:"), "should print a warning");
    assert!(
        stderr.to_lowercase().contains("memory"),
        "warning should mention memory: {}",
        stderr
    );
}

#[test]
fn timeout_zero_exits_1_and_warns() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--timeout",
            "0",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "timeout diff should exit 1: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("Warning:"), "should print a warning");
    assert!(
        stderr.to_lowercase().contains("timeout"),
        "warning should mention timeout: {}",
        stderr
    );
}

#[test]
fn nonexistent_file_exit_2() {
    let output = excel_diff_cmd()
        .args(["diff", "nonexistent_a.xlsx", "nonexistent_b.xlsx"])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "nonexistent file should exit 2: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );
}

#[test]
fn json_output_is_valid_json() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "json",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");

    assert!(parsed.get("version").is_some(), "should have version field");
    assert!(parsed.get("ops").is_some(), "should have ops field");
    assert!(parsed.get("strings").is_some(), "should have strings field");
}

#[test]
fn payload_output_contains_report_and_sheets() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "payload",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("payload output should be valid JSON");

    assert!(parsed.get("report").is_some(), "payload should include report");
    assert!(parsed.get("sheets").is_some(), "payload should include sheets");
    assert!(
        parsed.get("alignments").is_some(),
        "payload should include alignments"
    );
}

#[test]
fn outcome_output_contains_mode_and_payload() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "outcome",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("outcome output should be valid JSON");

    assert_eq!(
        parsed.get("mode").and_then(|v| v.as_str()),
        Some("payload")
    );
    let payload = parsed.get("payload").expect("payload should exist");
    assert!(payload.get("report").is_some(), "payload should include report");
}

#[test]
fn jsonl_first_line_is_header() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "jsonl",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let first_line = stdout.lines().next().expect("should have at least one line");
    let header: serde_json::Value =
        serde_json::from_str(first_line).expect("first line should be valid JSON");

    assert_eq!(header.get("kind").and_then(|v| v.as_str()), Some("Header"));
    assert!(header.get("version").is_some());
    assert!(header.get("strings").is_some());
}

#[test]
fn jsonl_progress_keeps_stdout_jsonl_and_writes_to_stderr() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "jsonl",
            "--progress",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "diff with progress should exit 1: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    for (idx, line) in stdout.lines().enumerate() {
        serde_json::from_str::<serde_json::Value>(line).unwrap_or_else(|e| {
            panic!("stdout line {idx} should be valid JSON: {e}; line={line}");
        });
    }

    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(
        !stderr.is_empty(),
        "progress should write to stderr (even in tests): stdout_len={}, stderr_len={}",
        stdout.len(),
        stderr.len()
    );
}

#[test]
fn info_shows_sheets() {
    let output = excel_diff_cmd()
        .args(["info", &fixture_path("pg1_basic_two_sheets.xlsx")])
        .output()
        .expect("failed to run excel-diff");

    assert!(output.status.success());
    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Sheets:"));
}

#[test]
fn info_with_queries_shows_power_query() {
    let output = excel_diff_cmd()
        .args(["info", "--queries", &fixture_path("one_query.xlsx")])
        .output()
        .expect("failed to run excel-diff");

    assert!(output.status.success());
    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Power Query:"));
}

#[test]
fn fast_and_precise_are_mutually_exclusive() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--fast",
            "--precise",
            &fixture_path("equal_sheet_a.xlsx"),
            &fixture_path("equal_sheet_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "conflicting flags should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("Cannot use both"));
}

#[test]
fn preset_conflicts_with_fast() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--preset",
            "balanced",
            "--fast",
            &fixture_path("equal_sheet_a.xlsx"),
            &fixture_path("equal_sheet_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "conflicting flags should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("--preset"));
}

#[test]
fn database_mode_requires_keys_or_auto_keys() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "database without keys should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("--keys") || stderr.contains("--auto-keys"));
}

#[test]
fn database_flags_require_database_mode() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--keys",
            "A",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "keys without database flag should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(stderr.contains("--database"));
}

#[test]
fn git_diff_produces_unified_style() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--git-diff",
            &fixture_path("single_cell_value_a.xlsx"),
            &fixture_path("single_cell_value_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("diff --git"));
    assert!(stdout.contains("---"));
    assert!(stdout.contains("+++"));
    assert!(stdout.contains("@@"));
}

#[test]
fn git_diff_conflicts_with_json_format() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--git-diff",
            "--format",
            "json",
            &fixture_path("equal_sheet_a.xlsx"),
            &fixture_path("equal_sheet_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "git-diff with json format should exit 2"
    );
}

#[test]
fn row_changes_detected() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            &fixture_path("row_insert_middle_a.xlsx"),
            &fixture_path("row_insert_middle_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Row") && stdout.contains("ADDED"));
}

#[test]
fn column_changes_detected() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            &fixture_path("col_insert_middle_a.xlsx"),
            &fixture_path("col_insert_middle_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Column") && stdout.contains("ADDED"));
}

#[test]
fn power_query_changes_detected() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            &fixture_path("m_add_query_a.xlsx"),
            &fixture_path("m_add_query_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(stdout.contains("Power Query") || stdout.contains("Query"));
}

#[test]
fn diff_pbix_power_query_changes_detected() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "json",
            &fixture_path("pbix_legacy_multi_query_a.pbix"),
            &fixture_path("pbix_legacy_multi_query_b.pbix"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "pbix diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array()).unwrap();
    let has_query_op = ops.iter().any(|op| {
        op.get("kind")
            .and_then(|k| k.as_str())
            .map(|k| k.starts_with("Query"))
            .unwrap_or(false)
    });
    assert!(has_query_op, "expected at least one Query op in pbix diff");
}

#[test]
fn diff_pbix_jsonl_writes_header_and_ops() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "jsonl",
            &fixture_path("pbix_legacy_multi_query_a.pbix"),
            &fixture_path("pbix_legacy_multi_query_b.pbix"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "pbix jsonl diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let mut lines = stdout.lines();
    let first_line = lines.next().expect("jsonl should have a header line");
    let header: serde_json::Value =
        serde_json::from_str(first_line).expect("header line should be valid JSON");
    assert_eq!(header.get("kind").and_then(|v| v.as_str()), Some("Header"));
    let strings = header
        .get("strings")
        .and_then(|v| v.as_array())
        .expect("header should include string table");
    assert!(!strings.is_empty(), "header string table should be non-empty");

    let mut has_query_op = false;
    for line in lines {
        let op: excel_diff::DiffOp =
            serde_json::from_str(line).expect("jsonl op line should be valid JSON");
        for id in collect_string_ids(&op) {
            assert!(
                (id.0 as usize) < strings.len(),
                "StringId {} out of range for header string table (len={})",
                id.0,
                strings.len()
            );
        }

        if op.is_m_op() {
            has_query_op = true;
        }
    }

    assert!(has_query_op, "expected at least one Query op in jsonl output");
}

#[test]
fn diff_pbix_composed_reports_query_and_metadata_changes() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "json",
            &fixture_path("pbix_composed_a.pbix"),
            &fixture_path("pbix_composed_b.pbix"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "pbix composed diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed
        .get("ops")
        .and_then(|v| v.as_array())
        .expect("ops array should exist");

    let has_def_change = ops.iter().any(|op| {
        op.get("kind")
            .and_then(|k| k.as_str())
            .is_some_and(|k| k == "QueryDefinitionChanged")
    });
    let has_metadata_change = ops.iter().any(|op| {
        op.get("kind")
            .and_then(|k| k.as_str())
            .is_some_and(|k| k == "QueryMetadataChanged")
    });

    assert!(has_def_change, "expected QueryDefinitionChanged in pbix composed diff");
    assert!(
        has_metadata_change,
        "expected QueryMetadataChanged in pbix composed diff"
    );
}

#[test]
fn jsonl_output_is_deterministic_for_xlsx() {
    let first = run_jsonl_diff(
        &fixture_path("single_cell_value_a.xlsx"),
        &fixture_path("single_cell_value_b.xlsx"),
    );
    let second = run_jsonl_diff(
        &fixture_path("single_cell_value_a.xlsx"),
        &fixture_path("single_cell_value_b.xlsx"),
    );

    assert_eq!(first, second, "jsonl output should be deterministic");
}

#[test]
fn jsonl_output_is_deterministic_for_pbix() {
    let first = run_jsonl_diff(
        &fixture_path("pbix_legacy_multi_query_a.pbix"),
        &fixture_path("pbix_legacy_multi_query_b.pbix"),
    );
    let second = run_jsonl_diff(
        &fixture_path("pbix_legacy_multi_query_a.pbix"),
        &fixture_path("pbix_legacy_multi_query_b.pbix"),
    );

    assert_eq!(first, second, "jsonl output should be deterministic");
}

fn collect_string_ids(op: &excel_diff::DiffOp) -> Vec<excel_diff::StringId> {
    fn collect_cell_value(ids: &mut Vec<excel_diff::StringId>, value: &excel_diff::CellValue) {
        match value {
            excel_diff::CellValue::Text(id) | excel_diff::CellValue::Error(id) => ids.push(*id),
            excel_diff::CellValue::Number(_) | excel_diff::CellValue::Bool(_) | excel_diff::CellValue::Blank => {}
        }
    }

    fn collect_snapshot(ids: &mut Vec<excel_diff::StringId>, snap: &excel_diff::CellSnapshot) {
        if let Some(value) = &snap.value {
            collect_cell_value(ids, value);
        }
        if let Some(formula) = snap.formula {
            ids.push(formula);
        }
    }

    fn collect_extracted_string(ids: &mut Vec<excel_diff::StringId>, value: &excel_diff::ExtractedString) {
        if let excel_diff::ExtractedString::Known { value } = value {
            ids.push(*value);
        }
    }

    fn collect_extracted_string_list(
        ids: &mut Vec<excel_diff::StringId>,
        value: &excel_diff::ExtractedStringList,
    ) {
        if let excel_diff::ExtractedStringList::Known { values } = value {
            ids.extend(values.iter().copied());
        }
    }

    fn collect_rename_pairs(
        ids: &mut Vec<excel_diff::StringId>,
        value: &excel_diff::ExtractedRenamePairs,
    ) {
        if let excel_diff::ExtractedRenamePairs::Known { pairs } = value {
            for excel_diff::RenamePair { from, to } in pairs {
                ids.push(*from);
                ids.push(*to);
            }
        }
    }

    fn collect_column_type_changes(
        ids: &mut Vec<excel_diff::StringId>,
        value: &excel_diff::ExtractedColumnTypeChanges,
    ) {
        if let excel_diff::ExtractedColumnTypeChanges::Known { changes } = value {
            for change in changes {
                ids.push(change.column);
            }
        }
    }

    fn collect_step_params(ids: &mut Vec<excel_diff::StringId>, params: &excel_diff::StepParams) {
        match params {
            excel_diff::StepParams::TableSelectRows { .. } => {}
            excel_diff::StepParams::TableRemoveColumns { columns } => collect_extracted_string_list(ids, columns),
            excel_diff::StepParams::TableRenameColumns { renames } => collect_rename_pairs(ids, renames),
            excel_diff::StepParams::TableTransformColumnTypes { transforms } => {
                collect_column_type_changes(ids, transforms);
            }
            excel_diff::StepParams::TableNestedJoin {
                left_keys,
                right_keys,
                new_column,
                ..
            } => {
                collect_extracted_string_list(ids, left_keys);
                collect_extracted_string_list(ids, right_keys);
                collect_extracted_string(ids, new_column);
            }
            excel_diff::StepParams::TableJoin {
                left_keys,
                right_keys,
                ..
            } => {
                collect_extracted_string_list(ids, left_keys);
                collect_extracted_string_list(ids, right_keys);
            }
            excel_diff::StepParams::Other { .. } => {}
        }
    }

    fn collect_step_snapshot(ids: &mut Vec<excel_diff::StringId>, snapshot: &excel_diff::StepSnapshot) {
        ids.push(snapshot.name);
        ids.extend(snapshot.source_refs.iter().copied());
        if let Some(params) = &snapshot.params {
            collect_step_params(ids, params);
        }
    }

    fn collect_step_diff(ids: &mut Vec<excel_diff::StringId>, diff: &excel_diff::StepDiff) {
        match diff {
            excel_diff::StepDiff::StepAdded { step } | excel_diff::StepDiff::StepRemoved { step } => {
                collect_step_snapshot(ids, step);
            }
            excel_diff::StepDiff::StepReordered { name, .. } => ids.push(*name),
            excel_diff::StepDiff::StepModified { before, after, changes } => {
                collect_step_snapshot(ids, before);
                collect_step_snapshot(ids, after);
                for change in changes {
                    if let excel_diff::StepChange::Renamed { from, to } = change {
                        ids.push(*from);
                        ids.push(*to);
                    }
                    if let excel_diff::StepChange::SourceRefsChanged { removed, added } = change {
                        ids.extend(removed.iter().copied());
                        ids.extend(added.iter().copied());
                    }
                }
            }
        }
    }

    fn collect_semantic_detail(ids: &mut Vec<excel_diff::StringId>, detail: &excel_diff::QuerySemanticDetail) {
        for diff in &detail.step_diffs {
            collect_step_diff(ids, diff);
        }
    }

    let mut ids = Vec::new();
    match op {
        excel_diff::DiffOp::SheetAdded { sheet } | excel_diff::DiffOp::SheetRemoved { sheet } => ids.push(*sheet),
        excel_diff::DiffOp::SheetRenamed { sheet, from, to } => {
            ids.push(*sheet);
            ids.push(*from);
            ids.push(*to);
        }
        excel_diff::DiffOp::RowAdded { sheet, .. }
        | excel_diff::DiffOp::RowRemoved { sheet, .. }
        | excel_diff::DiffOp::RowReplaced { sheet, .. } => ids.push(*sheet),
        excel_diff::DiffOp::DuplicateKeyCluster { sheet, key, .. } => {
            ids.push(*sheet);
            for value in key.iter().flatten() {
                collect_cell_value(&mut ids, value);
            }
        }
        excel_diff::DiffOp::ColumnAdded { sheet, .. } | excel_diff::DiffOp::ColumnRemoved { sheet, .. } => ids.push(*sheet),
        excel_diff::DiffOp::BlockMovedRows { sheet, .. }
        | excel_diff::DiffOp::BlockMovedColumns { sheet, .. }
        | excel_diff::DiffOp::BlockMovedRect { sheet, .. }
        | excel_diff::DiffOp::RectReplaced { sheet, .. } => ids.push(*sheet),
        excel_diff::DiffOp::CellEdited { sheet, from, to, .. } => {
            ids.push(*sheet);
            collect_snapshot(&mut ids, from);
            collect_snapshot(&mut ids, to);
        }
        excel_diff::DiffOp::VbaModuleAdded { name }
        | excel_diff::DiffOp::VbaModuleRemoved { name }
        | excel_diff::DiffOp::VbaModuleChanged { name } => ids.push(*name),
        excel_diff::DiffOp::NamedRangeAdded { name } | excel_diff::DiffOp::NamedRangeRemoved { name } => ids.push(*name),
        excel_diff::DiffOp::NamedRangeChanged { name, old_ref, new_ref } => {
            ids.push(*name);
            ids.push(*old_ref);
            ids.push(*new_ref);
        }
        excel_diff::DiffOp::ChartAdded { sheet, name }
        | excel_diff::DiffOp::ChartRemoved { sheet, name }
        | excel_diff::DiffOp::ChartChanged { sheet, name } => {
            ids.push(*sheet);
            ids.push(*name);
        }
        excel_diff::DiffOp::QueryAdded { name }
        | excel_diff::DiffOp::QueryRemoved { name }
        | excel_diff::DiffOp::QueryDefinitionChanged { name, .. } => ids.push(*name),
        excel_diff::DiffOp::QueryRenamed { from, to } => {
            ids.push(*from);
            ids.push(*to);
        }
        excel_diff::DiffOp::QueryMetadataChanged { name, old, new, .. } => {
            ids.push(*name);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        excel_diff::DiffOp::TableAdded { name } | excel_diff::DiffOp::TableRemoved { name } => {
            ids.push(*name);
        }
        excel_diff::DiffOp::ModelColumnAdded {
            table,
            name,
            data_type,
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(data_type) = data_type {
                ids.push(*data_type);
            }
        }
        excel_diff::DiffOp::ModelColumnRemoved { table, name } => {
            ids.push(*table);
            ids.push(*name);
        }
        excel_diff::DiffOp::ModelColumnTypeChanged {
            table,
            name,
            old_type,
            new_type,
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(old_type) = old_type {
                ids.push(*old_type);
            }
            if let Some(new_type) = new_type {
                ids.push(*new_type);
            }
        }
        excel_diff::DiffOp::ModelColumnPropertyChanged {
            table,
            name,
            old,
            new,
            ..
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        excel_diff::DiffOp::CalculatedColumnDefinitionChanged { table, name, .. } => {
            ids.push(*table);
            ids.push(*name);
        }
        excel_diff::DiffOp::RelationshipAdded {
            from_table,
            from_column,
            to_table,
            to_column,
        }
        | excel_diff::DiffOp::RelationshipRemoved {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            ids.push(*from_table);
            ids.push(*from_column);
            ids.push(*to_table);
            ids.push(*to_column);
        }
        excel_diff::DiffOp::RelationshipPropertyChanged {
            from_table,
            from_column,
            to_table,
            to_column,
            old,
            new,
            ..
        } => {
            ids.push(*from_table);
            ids.push(*from_column);
            ids.push(*to_table);
            ids.push(*to_column);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        excel_diff::DiffOp::MeasureAdded { name }
        | excel_diff::DiffOp::MeasureRemoved { name }
        | excel_diff::DiffOp::MeasureDefinitionChanged { name, .. } => ids.push(*name),
        _ => {}
    }

    if let excel_diff::DiffOp::QueryDefinitionChanged { semantic_detail, .. } = op {
        if let Some(detail) = semantic_detail {
            collect_semantic_detail(&mut ids, detail);
        }
    }

    ids
}

#[test]
fn diff_pbit_model_changes_detected() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--format",
            "json",
            &fixture_path("pbit_model_a.pbit"),
            &fixture_path("pbit_model_b.pbit"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "pbit diff should detect changes: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array()).unwrap();
    let mut has_table_op = false;
    let mut has_column_op = false;
    let mut has_relationship_op = false;
    let mut has_calc_column_op = false;
    let mut has_measure_op = false;

    for op in ops {
        let Some(kind) = op.get("kind").and_then(|k| k.as_str()) else {
            continue;
        };
        if kind.starts_with("Table") {
            has_table_op = true;
        } else if kind.starts_with("ModelColumn") {
            has_column_op = true;
        } else if kind.starts_with("Relationship") {
            has_relationship_op = true;
        } else if kind == "CalculatedColumnDefinitionChanged" {
            has_calc_column_op = true;
        } else if kind.starts_with("Measure") {
            has_measure_op = true;
        }
    }

    assert!(has_table_op, "expected at least one Table op in pbit diff");
    assert!(has_column_op, "expected at least one ModelColumn op in pbit diff");
    assert!(has_relationship_op, "expected at least one Relationship op in pbit diff");
    assert!(has_calc_column_op, "expected at least one CalculatedColumn op in pbit diff");
    assert!(has_measure_op, "expected at least one Measure op in pbit diff");
}

#[test]
fn d1_database_reorder_no_diff() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert!(
        output.status.success(),
        "D1 reorder should exit 0 (no changes): stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );
}

#[test]
fn d1_database_reorder_json_empty_ops() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A",
            "--format",
            "json",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array());
    assert!(
        ops.map(|o| o.is_empty()).unwrap_or(false),
        "D1 reorder should have empty ops array"
    );
}

#[test]
fn d2_database_row_added() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A",
            "--format",
            "json",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_row_added_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "D2 row added should exit 1"
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array()).unwrap();
    let has_row_added = ops.iter().any(|op| {
        op.get("kind").and_then(|k| k.as_str()) == Some("RowAdded")
    });
    assert!(has_row_added, "D2 should contain RowAdded op");
}

#[test]
fn d3_database_row_updated() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A",
            "--format",
            "json",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_row_update_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "D3 row update should exit 1"
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array()).unwrap();
    let has_cell_edited = ops.iter().any(|op| {
        op.get("kind").and_then(|k| k.as_str()) == Some("CellEdited")
    });
    assert!(has_cell_edited, "D3 should contain CellEdited op");
}

#[test]
fn d4_database_reorder_and_change() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A",
            "--format",
            "json",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_reorder_and_change_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(1),
        "D4 reorder+change should exit 1"
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    let parsed: serde_json::Value =
        serde_json::from_str(&stdout).expect("output should be valid JSON");
    let ops = parsed.get("ops").and_then(|v| v.as_array()).unwrap();
    
    let has_cell_edited = ops.iter().any(|op| {
        op.get("kind").and_then(|k| k.as_str()) == Some("CellEdited")
    });
    assert!(has_cell_edited, "D4 should contain CellEdited op");
    
    assert!(
        ops.len() < 10,
        "D4 should have few ops (reorder ignored, only changes): got {} ops",
        ops.len()
    );
}

#[test]
fn database_multi_column_keys() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "A,C",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert!(
        output.status.success(),
        "Multi-column keys should work: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );
}

#[test]
fn database_invalid_column_error() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--keys",
            "1",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "Invalid column should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(
        stderr.contains("not a valid column") || stderr.contains("Invalid"),
        "Should mention invalid column: {}",
        stderr
    );
}

#[test]
fn database_sheet_not_found_error() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "NoSuchSheet",
            "--keys",
            "A",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert_eq!(
        output.status.code(),
        Some(2),
        "Sheet not found should exit 2"
    );
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(
        stderr.contains("not found") || stderr.contains("Available"),
        "Should mention sheet not found: {}",
        stderr
    );
}

#[test]
fn database_auto_keys() {
    let output = excel_diff_cmd()
        .args([
            "diff",
            "--database",
            "--sheet",
            "Data",
            "--auto-keys",
            &fixture_path("db_equal_ordered_a.xlsx"),
            &fixture_path("db_equal_ordered_b.xlsx"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert!(
        output.status.success(),
        "Auto-keys should work: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );
    
    let stderr = String::from_utf8_lossy(&output.stderr);
    assert!(
        stderr.contains("Auto-detected") || stderr.is_empty(),
        "Should print auto-detected message or be silent"
    );
}

#[test]
fn info_pbix_includes_embedded_queries() {
    let output = excel_diff_cmd()
        .args([
            "info",
            "--queries",
            &fixture_path("pbix_embedded_queries.pbix"),
        ])
        .output()
        .expect("failed to run excel-diff");

    assert!(
        output.status.success(),
        "info should succeed for pbix: stderr={}",
        String::from_utf8_lossy(&output.stderr)
    );

    let stdout = String::from_utf8_lossy(&output.stdout);
    assert!(
        stdout.contains("PBIX/PBIT:"),
        "expected pbix header, got: {}",
        stdout
    );
    assert!(
        stdout.contains("Embedded/"),
        "expected embedded queries to be listed, got: {}",
        stdout
    );
}


```

---

### File: `core\benches\diff_benchmarks.rs`

```rust
use criterion::{BenchmarkId, Criterion, Throughput, criterion_group, criterion_main};
use excel_diff::{
    CellValue, DiffConfig, DiffSession, Grid, Sheet, SheetKind, Workbook,
    try_diff_workbooks_with_pool,
};
use std::time::Duration;

const MAX_BENCH_TIME_SECS: u64 = 30;
const WARMUP_SECS: u64 = 3;
const SAMPLE_SIZE: usize = 10;

fn create_large_grid(nrows: u32, ncols: u32, base_value: i32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number(
                    (base_value as i64 + row as i64 * 1000 + col as i64) as f64,
                )),
                None,
            );
        }
    }
    grid
}

fn create_repetitive_grid(nrows: u32, ncols: u32, pattern_length: u32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        let pattern_idx = row % pattern_length;
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number((pattern_idx * 1000 + col) as f64)),
                None,
            );
        }
    }
    grid
}

fn create_sparse_grid(nrows: u32, ncols: u32, fill_percent: u32, seed: u64) -> Grid {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            let mut hasher = DefaultHasher::new();
            (row, col, seed).hash(&mut hasher);
            let hash = hasher.finish();
            if (hash % 100) < fill_percent as u64 {
                grid.insert_cell(
                    row,
                    col,
                    Some(CellValue::Number((row * 1000 + col) as f64)),
                    None,
                );
            }
        }
    }
    grid
}

fn single_sheet_workbook(session: &mut DiffSession, name: &str, grid: Grid) -> Workbook {
    let sheet_name = session.strings.intern(name);
    Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn bench_identical_grids(c: &mut Criterion) {
    let mut group = c.benchmark_group("identical_grids");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000, 5000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_large_grid(*size, 50, 0);
        let grid_b = create_large_grid(*size, 50, 0);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 50));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

fn bench_single_cell_edit(c: &mut Criterion) {
    let mut group = c.benchmark_group("single_cell_edit");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000, 5000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_large_grid(*size, 50, 0);
        let mut grid_b = create_large_grid(*size, 50, 0);
        grid_b.insert_cell(size / 2, 25, Some(CellValue::Number(999999.0)), None);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 50));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

fn bench_all_rows_different(c: &mut Criterion) {
    let mut group = c.benchmark_group("all_rows_different");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_large_grid(*size, 50, 0);
        let grid_b = create_large_grid(*size, 50, 1);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 50));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

fn bench_adversarial_repetitive(c: &mut Criterion) {
    let mut group = c.benchmark_group("adversarial_repetitive");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_repetitive_grid(*size, 50, 100);
        let mut grid_b = create_repetitive_grid(*size, 50, 100);
        grid_b.insert_cell(size / 2, 25, Some(CellValue::Number(999999.0)), None);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 50));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

fn bench_sparse_grid(c: &mut Criterion) {
    let mut group = c.benchmark_group("sparse_grid_1pct");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000, 5000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_sparse_grid(*size, 100, 1, 12345);
        let mut grid_b = create_sparse_grid(*size, 100, 1, 12345);
        grid_b.insert_cell(size / 2, 50, Some(CellValue::Number(999999.0)), None);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 100));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

fn bench_row_insertion(c: &mut Criterion) {
    let mut group = c.benchmark_group("row_insertion");
    group.measurement_time(Duration::from_secs(MAX_BENCH_TIME_SECS));
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    for size in [500u32, 1000, 2000].iter() {
        let mut session = DiffSession::new();
        let grid_a = create_large_grid(*size, 50, 0);
        let mut grid_b = Grid::new(size + 100, 50);
        for row in 0..(size / 2) {
            for col in 0..50 {
                grid_b.insert_cell(
                    row,
                    col,
                    Some(CellValue::Number((row as i64 * 1000 + col as i64) as f64)),
                    None,
                );
            }
        }
        for col in 0..50 {
            for i in 0..100 {
                let row = size / 2 + i;
                let marker = 1_000_000.0 + i as f64 * 10.0 + col as f64;
                grid_b.insert_cell(row, col, Some(CellValue::Number(marker)), None);
            }
        }
        for row in (size / 2)..*size {
            for col in 0..50 {
                let new_row = row + 100;
                grid_b.insert_cell(
                    new_row,
                    col,
                    Some(CellValue::Number((row as i64 * 1000 + col as i64) as f64)),
                    None,
                );
            }
        }
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);
        let config = DiffConfig::default();

        group.throughput(Throughput::Elements(*size as u64 * 50));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

#[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
fn bench_pbit_model_diff(c: &mut Criterion) {
    let mut group = c.benchmark_group("pbit_model_diff");
    group.warm_up_time(Duration::from_secs(WARMUP_SECS));
    group.sample_size(SAMPLE_SIZE);

    let base = std::path::Path::new(env!("CARGO_MANIFEST_DIR"))
        .join("../fixtures/generated");
    let path_a = base.join("pbit_model_a.pbit");
    let path_b = base.join("pbit_model_b.pbit");

    let bytes_a = std::fs::read(&path_a).expect("read pbit_model_a.pbit");
    let bytes_b = std::fs::read(&path_b).expect("read pbit_model_b.pbit");
    let config = DiffConfig::default();

    group.bench_function("open_parse_diff", |b| {
        b.iter(|| {
            let cursor_a = std::io::Cursor::new(bytes_a.clone());
            let cursor_b = std::io::Cursor::new(bytes_b.clone());
            let pkg_a = excel_diff::PbixPackage::open(cursor_a).expect("open pbit a");
            let pkg_b = excel_diff::PbixPackage::open(cursor_b).expect("open pbit b");
            let report = pkg_a.diff(&pkg_b, &config);
            criterion::black_box(report);
        });
    });

    group.finish();
}

#[cfg(not(all(feature = "model-diff", feature = "excel-open-xml")))]
fn bench_pbit_model_diff(_c: &mut Criterion) {}

fn create_grid_with_block_move(nrows: u32, ncols: u32, move_start: u32, move_size: u32) -> (Grid, Grid) {
    let mut grid_a = Grid::new(nrows, ncols);
    let mut grid_b = Grid::new(nrows, ncols);

    for row in 0..nrows {
        for col in 0..ncols {
            let value = (row * 1000 + col) as f64;
            grid_a.insert_cell(row, col, Some(CellValue::Number(value)), None);
        }
    }

    let move_end = move_start + move_size;
    let dest_start = nrows - move_size - 100;

    for row in 0..move_start {
        for col in 0..ncols {
            let value = (row * 1000 + col) as f64;
            grid_b.insert_cell(row, col, Some(CellValue::Number(value)), None);
        }
    }

    for row in move_end..nrows {
        for col in 0..ncols {
            let value = (row * 1000 + col) as f64;
            let new_row = row - move_size + (dest_start - move_start + move_size);
            if new_row < nrows && new_row != dest_start && (new_row < dest_start || new_row >= dest_start + move_size) {
                grid_b.insert_cell(row - move_size, col, Some(CellValue::Number(value)), None);
            }
        }
    }

    for i in 0..move_size {
        for col in 0..ncols {
            let value = ((move_start + i) * 1000 + col) as f64;
            grid_b.insert_cell(dest_start + i, col, Some(CellValue::Number(value)), None);
        }
    }

    (grid_a, grid_b)
}

fn bench_block_move_alignment(c: &mut Criterion) {
    let mut group = c.benchmark_group("block_move_alignment");
    group.measurement_time(Duration::from_secs(60));
    group.warm_up_time(Duration::from_secs(5));
    group.sample_size(10);

    for size in [5000u32, 10000].iter() {
        let mut session = DiffSession::new();
        let (grid_a, grid_b) = create_grid_with_block_move(*size, 20, 100, 50);
        let wb_a = single_sheet_workbook(&mut session, "Bench", grid_a);
        let wb_b = single_sheet_workbook(&mut session, "Bench", grid_b);

        let config = DiffConfig::builder()
            .preflight_min_rows(u32::MAX)
            .max_move_detection_rows(20000)
            .build()
            .expect("valid config");

        group.throughput(Throughput::Elements(*size as u64 * 20));
        group.bench_with_input(BenchmarkId::new("rows", size), size, move |b, _| {
            b.iter(|| {
                let _ = try_diff_workbooks_with_pool(&wb_a, &wb_b, &mut session.strings, &config)
                    .expect("diff should succeed");
            });
        });
    }
    group.finish();
}

criterion_group!(
    benches,
    bench_identical_grids,
    bench_single_cell_edit,
    bench_all_rows_different,
    bench_adversarial_repetitive,
    bench_sparse_grid,
    bench_row_insertion,
    bench_pbit_model_diff,
);

criterion_group!(
    name = alignment_benches;
    config = Criterion::default().sample_size(10);
    targets = bench_block_move_alignment,
);

criterion_main!(benches, alignment_benches);

```

---

### File: `core\Cargo.toml`

```toml
[package]
name = "excel_diff"
version = "0.1.0"
edition = "2024"
description = "A library for comparing Excel workbooks"
license = "MIT"
repository = "https://github.com/dvora/excel_diff"
homepage = "https://github.com/dvora/excel_diff"

[lib]
name = "excel_diff"
path = "src/lib.rs"

[features]
default = ["excel-open-xml", "std-fs", "vba", "dpapi"]
excel-open-xml = []
vba = ["dep:ovba"]
std-fs = []
perf-metrics = []
dev-apis = []
model-diff = []
legacy-api = []
parallel = ["dep:rayon"]
dpapi = []

[dependencies]
quick-xml = "0.32"
thiserror = "1.0"
rayon = { version = "1.10.0", optional = true }
zip = { version = "0.6", default-features = false, features = ["deflate"] }
base64 = "0.22"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
xxhash-rust = { version = "0.8", features = ["xxh64", "xxh3"] }
rustc-hash = "1.1"
ovba = { version = "0.7.1", optional = true }
sha2 = "0.10"

[target.'cfg(windows)'.dependencies]
windows-sys = { version = "0.52", features = ["Win32_Foundation", "Win32_Security_Cryptography"] }

[dev-dependencies]
pretty_assertions = "1.4"
tempfile = "3.10"
criterion = { version = "0.5", features = ["html_reports"] }
serde_yaml = "0.9"

[[bench]]
name = "diff_benchmarks"
harness = false

```

---

### File: `core\examples\basic_diff.rs`

```rust
use excel_diff::{DiffConfig, WorkbookPackage};
use std::fs::File;

fn usage() -> ! {
    eprintln!("Usage: basic_diff <OLD.xlsx> <NEW.xlsx> [N]");
    eprintln!("  N: optionally print the first N ops (debug)");
    std::process::exit(2);
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut args = std::env::args().skip(1);
    let old_path = args.next().unwrap_or_else(|| usage());
    let new_path = args.next().unwrap_or_else(|| usage());
    let show_n: Option<usize> = args.next().map(|s| s.parse()).transpose()?;

    let old_pkg = WorkbookPackage::open(File::open(&old_path)?)?;
    let new_pkg = WorkbookPackage::open(File::open(&new_path)?)?;

    let report = old_pkg.diff(&new_pkg, &DiffConfig::default());

    println!("complete: {}", report.complete);
    println!("warnings: {}", report.warnings.len());
    println!("ops: {}", report.ops.len());

    if let Some(n) = show_n {
        for (i, op) in report.ops.iter().take(n).enumerate() {
            println!("{:>4}: {:?}", i, op);
        }
    }

    Ok(())
}


```

---

### File: `core\examples\custom_config.rs`

```rust
use excel_diff::{DiffConfig, WorkbookPackage};
use std::fs::File;

fn usage() -> ! {
    eprintln!("Usage: custom_config <OLD.xlsx> <NEW.xlsx>");
    std::process::exit(2);
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut args = std::env::args().skip(1);
    let old_path = args.next().unwrap_or_else(|| usage());
    let new_path = args.next().unwrap_or_else(|| usage());

    let old_pkg = WorkbookPackage::open(File::open(&old_path)?)?;
    let new_pkg = WorkbookPackage::open(File::open(&new_path)?)?;

    let mut cfg = DiffConfig::fastest();
    cfg.hardening.max_memory_mb = Some(256);
    cfg.hardening.timeout_seconds = Some(10);

    let report = old_pkg.diff(&new_pkg, &cfg);

    for warning in &report.warnings {
        eprintln!("warning: {}", warning);
    }

    println!("complete: {}", report.complete);
    println!("ops: {}", report.ops.len());
    Ok(())
}


```

---

### File: `core\examples\database_mode.rs`

```rust
use excel_diff::{DiffConfig, WorkbookPackage};
use std::fs::File;
use std::io;

fn usage() -> ! {
    eprintln!("Usage: database_mode <OLD.xlsx> <NEW.xlsx> <SHEET_NAME> <KEYS>");
    eprintln!("  KEYS: comma-separated column letters (e.g. A,C,AA)");
    eprintln!("  Note: key columns are 0-based indices internally (A=0, B=1, ...).");
    std::process::exit(2);
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut args = std::env::args().skip(1);
    let old_path = args.next().unwrap_or_else(|| usage());
    let new_path = args.next().unwrap_or_else(|| usage());
    let sheet_name = args.next().unwrap_or_else(|| usage());
    let keys = args.next().unwrap_or_else(|| usage());

    let key_columns = parse_key_columns(&keys)?;

    let old_pkg = WorkbookPackage::open(File::open(&old_path)?)?;
    let new_pkg = WorkbookPackage::open(File::open(&new_path)?)?;

    let report = old_pkg.diff_database_mode(&new_pkg, &sheet_name, &key_columns, &DiffConfig::default())?;

    for warning in &report.warnings {
        eprintln!("warning: {}", warning);
    }

    println!("complete: {}", report.complete);
    println!("ops: {}", report.ops.len());

    for (i, op) in report.ops.iter().take(25).enumerate() {
        println!("{:>4}: {:?}", i, op);
    }

    Ok(())
}

fn parse_key_columns(keys: &str) -> io::Result<Vec<u32>> {
    let mut out = Vec::new();
    for token in keys.split(',') {
        let token = token.trim();
        if token.is_empty() {
            continue;
        }
        out.push(col_letters_to_index(token)?);
    }

    if out.is_empty() {
        return Err(io::Error::new(
            io::ErrorKind::InvalidInput,
            "no key columns specified",
        ));
    }

    Ok(out)
}

fn col_letters_to_index(letters: &str) -> io::Result<u32> {
    let mut col: u32 = 0;
    for ch in letters.chars() {
        let upper = ch.to_ascii_uppercase();
        if !upper.is_ascii_uppercase() {
            return Err(io::Error::new(
                io::ErrorKind::InvalidInput,
                format!("invalid column token: '{letters}'"),
            ));
        }
        col = col
            .checked_mul(26)
            .and_then(|c| c.checked_add((upper as u8 - b'A' + 1) as u32))
            .ok_or_else(|| {
                io::Error::new(
                    io::ErrorKind::InvalidInput,
                    format!("column '{letters}' is out of range"),
                )
            })?;
    }
    Ok(col - 1)
}

```

---

### File: `core\examples\streaming.rs`

```rust
use excel_diff::{DiffConfig, JsonLinesSink, WorkbookPackage};
use std::fs::File;

fn usage() -> ! {
    eprintln!("Usage: streaming <OLD.xlsx> <NEW.xlsx> > out.jsonl");
    std::process::exit(2);
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut args = std::env::args().skip(1);
    let old_path = args.next().unwrap_or_else(|| usage());
    let new_path = args.next().unwrap_or_else(|| usage());

    let old_pkg = WorkbookPackage::open(File::open(&old_path)?)?;
    let new_pkg = WorkbookPackage::open(File::open(&new_path)?)?;

    let stdout = std::io::stdout();
    let handle = stdout.lock();
    let mut sink = JsonLinesSink::new(handle);

    let summary = old_pkg.diff_streaming(&new_pkg, &DiffConfig::default(), &mut sink)?;

    eprintln!(
        "complete={} ops={} warnings={}",
        summary.complete,
        summary.op_count,
        summary.warnings.len()
    );
    for warning in &summary.warnings {
        eprintln!("warning: {}", warning);
    }

    Ok(())
}


```

---

### File: `core\fuzz\Cargo.toml`

```toml
[package]
name = "excel_diff-fuzz"
version = "0.0.0"
authors = ["Automatically generated"]
publish = false
edition = "2021"

[package.metadata]
cargo-fuzz = true

[dependencies]
libfuzzer-sys = "0.4"
arbitrary = { version = "1", features = ["derive"] }

[dependencies.excel_diff]
path = ".."
features = ["excel-open-xml"]

[[bin]]
name = "fuzz_open_workbook"
path = "fuzz_targets/fuzz_open_workbook.rs"
test = false
doc = false
bench = false

[[bin]]
name = "fuzz_open_pbix"
path = "fuzz_targets/fuzz_open_pbix.rs"
test = false
doc = false
bench = false

[[bin]]
name = "fuzz_datamashup_parse"
path = "fuzz_targets/fuzz_datamashup_parse.rs"
test = false
doc = false
bench = false

[[bin]]
name = "fuzz_diff_grids"
path = "fuzz_targets/fuzz_diff_grids.rs"
test = false
doc = false
bench = false

[[bin]]
name = "fuzz_m_section_and_ast"
path = "fuzz_targets/fuzz_m_section_and_ast.rs"
test = false
doc = false
bench = false

[profile.release]
debug = 1


```

---

### File: `core\fuzz\fuzz_targets\fuzz_datamashup_parse.rs`

```rust
#![no_main]

use libfuzzer_sys::fuzz_target;

use excel_diff::{parse_data_mashup, build_data_mashup};

fuzz_target!(|data: &[u8]| {
    if let Ok(raw) = parse_data_mashup(data) {
        let _ = build_data_mashup(&raw);
    }
});


```

---

### File: `core\fuzz\fuzz_targets\fuzz_diff_grids.rs`

```rust
#![no_main]

use libfuzzer_sys::fuzz_target;
use arbitrary::Arbitrary;

use excel_diff::{
    Cell, CellValue, DiffConfig, Grid, Sheet, SheetKind, StringPool, Workbook,
    advanced::try_diff_workbooks_with_pool,
};

#[derive(Arbitrary, Debug)]
struct FuzzInput {
    old_rows: u8,
    old_cols: u8,
    new_rows: u8,
    new_cols: u8,
    old_cells: Vec<FuzzCell>,
    new_cells: Vec<FuzzCell>,
}

#[derive(Arbitrary, Debug)]
struct FuzzCell {
    row: u8,
    col: u8,
    value_type: u8,
    number_value: f64,
    text_idx: u8,
}

fn build_grid(rows: u8, cols: u8, cells: &[FuzzCell], pool: &mut StringPool) -> Grid {
    let nrows = (rows as u32).min(100).max(1);
    let ncols = (cols as u32).min(100).max(1);
    let mut grid = Grid::new(nrows, ncols);

    for cell in cells.iter().take(200) {
        let row = (cell.row as u32) % nrows;
        let col = (cell.col as u32) % ncols;

        let value = match cell.value_type % 4 {
            0 => None,
            1 => Some(CellValue::Number(if cell.number_value.is_finite() {
                cell.number_value
            } else {
                0.0
            })),
            2 => Some(CellValue::Bool(cell.number_value > 0.5)),
            _ => {
                let texts = ["A", "B", "C", "test", "value", ""];
                let idx = (cell.text_idx as usize) % texts.len();
                Some(CellValue::Text(pool.intern(texts[idx])))
            }
        };

        grid.insert_cell(row, col, value, None);
    }

    grid
}

fuzz_target!(|input: FuzzInput| {
    let mut pool = StringPool::new();

    let old_grid = build_grid(input.old_rows, input.old_cols, &input.old_cells, &mut pool);
    let new_grid = build_grid(input.new_rows, input.new_cols, &input.new_cells, &mut pool);

    let sheet_name = pool.intern("Sheet1");
    let old_wb = Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: old_grid,
        }],
        ..Default::default()
    };
    let new_wb = Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: new_grid,
        }],
        ..Default::default()
    };

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.alignment.max_align_cols = 50;

    let _ = try_diff_workbooks_with_pool(&old_wb, &new_wb, &mut pool, &config);
});


```

---

### File: `core\fuzz\fuzz_targets\fuzz_m_section_and_ast.rs`

```rust
#![no_main]

use excel_diff::{parse_m_expression, parse_section_members};
use libfuzzer_sys::fuzz_target;

fuzz_target!(|data: &[u8]| {
    let s = String::from_utf8_lossy(data);
    let expr = s.as_ref().get(..4096).unwrap_or(s.as_ref());
    let section = format!("section Section1;\nshared Foo = {};\n", expr);

    if let Ok(members) = parse_section_members(&section) {
        for m in members {
            let _ = parse_m_expression(&m.expression_m);
        }
    }
});

```

---

### File: `core\fuzz\fuzz_targets\fuzz_open_pbix.rs`

```rust
#![no_main]

use libfuzzer_sys::fuzz_target;
use std::io::Cursor;

use excel_diff::{ContainerLimits, PbixPackage};

fuzz_target!(|data: &[u8]| {
    let limits = ContainerLimits {
        max_entries: 2000,
        max_part_uncompressed_bytes: 5 * 1024 * 1024,
        max_total_uncompressed_bytes: 50 * 1024 * 1024,
    };

    let cursor = Cursor::new(data);
    let _ = PbixPackage::open_with_limits(cursor, limits);
});

```

---

### File: `core\fuzz\fuzz_targets\fuzz_open_workbook.rs`

```rust
#![no_main]

use libfuzzer_sys::fuzz_target;
use std::io::Cursor;

use excel_diff::{ContainerLimits, DiffSession, OpcContainer, WorkbookPackage, with_default_session};

fuzz_target!(|data: &[u8]| {
    with_default_session(|session| *session = DiffSession::new());
    let limits = ContainerLimits {
        max_entries: 100,
        max_part_uncompressed_bytes: 1024 * 1024,
        max_total_uncompressed_bytes: 10 * 1024 * 1024,
    };

    let cursor = Cursor::new(data);
    let _ = OpcContainer::open_from_reader_with_limits(cursor, limits);

    let cursor = Cursor::new(data);
    let _ = WorkbookPackage::open_with_limits(cursor, limits);
});


```

---

### File: `core\fuzz\seed_fixtures.yaml`

```yaml
fixtures:
  - file: "minimal.xlsx"
    targets: ["fuzz_open_workbook"]
    smoke: true

  - file: "one_query.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: true

  - file: "multi_query_with_embedded.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: true

  - file: "duplicate_datamashup_parts.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: false

  - file: "mashup_utf16_le.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: true

  - file: "mashup_base64_whitespace.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: true

  - file: "corrupt_base64.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: false

  - file: "permissions_defaults.xlsx"
    targets: ["fuzz_open_workbook", "fuzz_datamashup_parse"]
    smoke: false

  - file: "vba_base.xlsm"
    targets: ["fuzz_open_workbook"]
    smoke: true

  - file: "random_zip.zip"
    targets: ["fuzz_open_workbook"]
    smoke: true

  - file: "no_content_types.xlsx"
    targets: ["fuzz_open_workbook"]
    smoke: true

  - file: "not_a_zip.txt"
    targets: ["fuzz_open_workbook"]
    smoke: true

  - file: "pbix_legacy_one_query_a.pbix"
    targets: ["fuzz_open_pbix", "fuzz_datamashup_parse"]
    smoke: true

  - file: "pbix_legacy_multi_query_a.pbix"
    targets: ["fuzz_open_pbix", "fuzz_datamashup_parse"]
    smoke: true

  - file: "pbix_embedded_queries.pbix"
    targets: ["fuzz_open_pbix", "fuzz_datamashup_parse"]
    smoke: false

  - file: "pbix_no_datamashup_no_schema.pbix"
    targets: ["fuzz_open_pbix"]
    smoke: true

  - file: "pbit_model_a.pbit"
    targets: ["fuzz_open_pbix"]
    smoke: false

```

---

### File: `core\src\addressing.rs`

```rust
//! Excel cell addressing utilities.
//!
//! Provides conversion between A1-style cell addresses (e.g., "B2", "AA10") and
//! zero-based (row, column) index pairs.

use std::fmt;

/// Error returned when parsing an invalid A1-style cell address.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct AddressParseError {
    pub input: String,
}

impl fmt::Display for AddressParseError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "invalid cell address: '{}'", self.input)
    }
}

impl std::error::Error for AddressParseError {}

/// Convert zero-based (row, col) indices to an Excel A1 address string.
pub fn index_to_address(row: u32, col: u32) -> String {
    let mut col_index = col;
    let mut col_label = String::new();

    loop {
        let rem = (col_index % 26) as u8;
        col_label.push((b'A' + rem) as char);
        if col_index < 26 {
            break;
        }
        col_index = col_index / 26 - 1;
    }

    col_label.chars().rev().collect::<String>() + &(row + 1).to_string()
}

/// Parse an A1 address into zero-based (row, col) indices.
/// Returns `None` for malformed addresses.
pub fn address_to_index(a1: &str) -> Option<(u32, u32)> {
    if a1.is_empty() {
        return None;
    }

    let mut col: u32 = 0;
    let mut row: u32 = 0;
    let mut saw_letter = false;
    let mut saw_digit = false;

    for ch in a1.chars() {
        if ch.is_ascii_alphabetic() {
            saw_letter = true;
            if saw_digit {
                // Letters after digits are not allowed.
                return None;
            }
            let upper = ch.to_ascii_uppercase() as u8;
            if !upper.is_ascii_uppercase() {
                return None;
            }
            col = col
                .checked_mul(26)?
                .checked_add((upper - b'A' + 1) as u32)?;
        } else if ch.is_ascii_digit() {
            saw_digit = true;
            row = row.checked_mul(10)?.checked_add((ch as u8 - b'0') as u32)?;
        } else {
            return None;
        }
    }

    if !saw_letter || !saw_digit || row == 0 || col == 0 {
        return None;
    }

    Some((row - 1, col - 1))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn index_to_address_examples() {
        assert_eq!(index_to_address(0, 0), "A1");
        assert_eq!(index_to_address(0, 25), "Z1");
        assert_eq!(index_to_address(0, 26), "AA1");
        assert_eq!(index_to_address(0, 27), "AB1");
        assert_eq!(index_to_address(0, 51), "AZ1");
        assert_eq!(index_to_address(0, 52), "BA1");
    }

    #[test]
    fn round_trip_addresses() {
        let addresses = [
            "A1", "B2", "Z10", "AA1", "AA10", "AB7", "AZ5", "BA1", "ZZ10", "AAA1",
        ];
        for addr in addresses {
            let (r, c) = address_to_index(addr).expect("address should parse");
            assert_eq!(index_to_address(r, c), addr);
        }
    }

    #[test]
    fn invalid_addresses_rejected() {
        let invalid = ["", "1A", "A0", "A", "AA0", "A-1", "A1A"];
        for addr in invalid {
            assert!(address_to_index(addr).is_none(), "{addr} should be invalid");
        }
    }
}

```

---

### File: `core\src\alignment\anchor_chain.rs`

```rust
//! Anchor chain construction using Longest Increasing Subsequence (LIS).
//!
//! Implements anchor chain building as described in the unified grid diff
//! specification Section 10. Given a set of discovered anchors, this module
//! selects the maximal subset that preserves relative order in both grids.
//!
//! For example, if anchors show:
//! - Row A: old=0, new=0
//! - Row B: old=2, new=1  (B moved up)
//! - Row C: old=1, new=2
//!
//! The LIS algorithm selects {A, C} because their old_row indices (0, 1) are
//! increasing, making them a valid ordering chain. Row B is excluded because
//! including it would create a crossing (B is at old=2 but new=1, while C is
//! at old=1 but new=2).

use crate::alignment::anchor_discovery::Anchor;

pub fn build_anchor_chain(mut anchors: Vec<Anchor>) -> Vec<Anchor> {
    // Sort by new_row to preserve destination order before LIS on old_row.
    anchors.sort_by_key(|a| a.new_row);
    let indices = lis_indices(&anchors, |a| a.old_row);
    indices.into_iter().map(|idx| anchors[idx]).collect()
}

fn lis_indices<T, F>(items: &[T], key: F) -> Vec<usize>
where
    F: Fn(&T) -> u32,
{
    let mut piles: Vec<usize> = Vec::new();
    let mut predecessors: Vec<Option<usize>> = vec![None; items.len()];

    for (idx, item) in items.iter().enumerate() {
        let k = key(item);
        let pos = piles
            .binary_search_by_key(&k, |&pile_idx| key(&items[pile_idx]))
            .unwrap_or_else(|insert_pos| insert_pos);

        if pos > 0 {
            predecessors[idx] = Some(piles[pos - 1]);
        }

        if pos == piles.len() {
            piles.push(idx);
        } else {
            piles[pos] = idx;
        }
    }

    let Some(&last) = piles.last() else {
        return Vec::new();
    };

    let mut result: Vec<usize> = Vec::new();
    let mut current = last;
    loop {
        result.push(current);
        if let Some(prev) = predecessors[current] {
            current = prev;
        } else {
            break;
        }
    }
    result.reverse();
    result
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::alignment::anchor_discovery::Anchor;
    use crate::workbook::RowSignature;

    #[test]
    fn builds_increasing_chain() {
        let anchors = vec![
            Anchor {
                old_row: 0,
                new_row: 0,
                signature: RowSignature { hash: 1 },
            },
            Anchor {
                old_row: 2,
                new_row: 1,
                signature: RowSignature { hash: 2 },
            },
            Anchor {
                old_row: 1,
                new_row: 2,
                signature: RowSignature { hash: 3 },
            },
        ];

        let chain = build_anchor_chain(anchors);
        assert_eq!(chain.len(), 2);
        assert_eq!(chain[0].old_row, 0);
        assert_eq!(chain[1].old_row, 1);
    }
}

```

---

### File: `core\src\alignment\anchor_discovery.rs`

```rust
//! Anchor discovery for AMR alignment.
//!
//! Implements anchor discovery as described in the unified grid diff specification
//! Section 10. Anchors are rows that:
//!
//! 1. Are unique (appear exactly once) in BOTH grids
//! 2. Have matching signatures (content hash)
//!
//! These rows serve as fixed points around which the alignment is built.
//! Rows that are unique in one grid but not the other cannot be anchors
//! since their position cannot be reliably determined.

use std::collections::HashMap;

use crate::grid_metadata::{FrequencyClass, RowMeta};
#[cfg(test)]
use crate::grid_view::GridView;
use crate::workbook::RowSignature;

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub struct Anchor {
    pub old_row: u32,
    pub new_row: u32,
    pub signature: RowSignature,
}

#[cfg(test)]
#[allow(dead_code)]
pub fn discover_anchors(old: &GridView<'_>, new: &GridView<'_>) -> Vec<Anchor> {
    discover_anchors_from_meta(&old.row_meta, &new.row_meta)
}

pub fn discover_anchors_from_meta(old: &[RowMeta], new: &[RowMeta]) -> Vec<Anchor> {
    let mut old_unique: HashMap<RowSignature, u32> = HashMap::new();
    for meta in old.iter() {
        if meta.frequency_class == FrequencyClass::Unique {
            old_unique.insert(meta.signature, meta.row_idx);
        }
    }

    new.iter()
        .filter(|meta| meta.frequency_class == FrequencyClass::Unique)
        .filter_map(|meta| {
            old_unique.get(&meta.signature).map(|old_idx| Anchor {
                old_row: *old_idx,
                new_row: meta.row_idx,
                signature: meta.signature,
            })
        })
        .collect()
}

pub fn discover_context_anchors(old: &[RowMeta], new: &[RowMeta], k: usize) -> Vec<Anchor> {
    if k == 0 || old.len() < k || new.len() < k {
        return Vec::new();
    }

    fn window_signature(window: &[RowMeta]) -> Option<RowSignature> {
        if window.iter().any(|m| m.is_low_info()) {
            return None;
        }
        let mut acc: u128 = 0x9e37_79b1_85eb_ca87;
        for (idx, meta) in window.iter().enumerate() {
            let mul = 0x1000_0000_01b3u128;
            acc = acc
                .wrapping_mul(mul)
                .wrapping_add(meta.signature.hash ^ ((idx as u128) << 1) ^ 0x517c_c1b7_2722_0a95);
            acc ^= acc >> 33;
            acc = acc.rotate_left(7);
        }
        Some(RowSignature { hash: acc })
    }

    let mut count_old: HashMap<RowSignature, u32> = HashMap::new();
    let mut pos_old: HashMap<RowSignature, u32> = HashMap::new();
    for i in 0..=old.len() - k {
        if let Some(sig) = window_signature(&old[i..i + k]) {
            *count_old.entry(sig).or_insert(0) += 1;
            pos_old.entry(sig).or_insert(old[i].row_idx);
        }
    }

    let mut count_new: HashMap<RowSignature, u32> = HashMap::new();
    let mut pos_new: HashMap<RowSignature, u32> = HashMap::new();
    for i in 0..=new.len() - k {
        if let Some(sig) = window_signature(&new[i..i + k]) {
            *count_new.entry(sig).or_insert(0) += 1;
            pos_new.entry(sig).or_insert(new[i].row_idx);
        }
    }

    let mut anchors = Vec::new();
    for (sig, &new_row) in pos_new.iter() {
        if count_new.get(sig).copied().unwrap_or(0) != 1 {
            continue;
        }
        if count_old.get(sig).copied().unwrap_or(0) != 1 {
            continue;
        }
        if let Some(old_row) = pos_old.get(sig) {
            anchors.push(Anchor {
                old_row: *old_row,
                new_row,
                signature: *sig,
            });
        }
    }

    anchors
}

pub fn discover_local_anchors(old: &[RowMeta], new: &[RowMeta]) -> Vec<Anchor> {
    let mut count_old: HashMap<RowSignature, u32> = HashMap::new();
    for m in old.iter() {
        if !m.is_low_info() {
            *count_old.entry(m.signature).or_insert(0) += 1;
        }
    }

    let mut count_new: HashMap<RowSignature, u32> = HashMap::new();
    for m in new.iter() {
        if !m.is_low_info() {
            *count_new.entry(m.signature).or_insert(0) += 1;
        }
    }

    let mut pos_old: HashMap<RowSignature, u32> = HashMap::new();
    for m in old.iter() {
        if !m.is_low_info() && count_old.get(&m.signature).copied().unwrap_or(0) == 1 {
            pos_old.insert(m.signature, m.row_idx);
        }
    }

    let mut out = Vec::new();
    for m in new.iter() {
        if m.is_low_info() {
            continue;
        }
        if count_new.get(&m.signature).copied().unwrap_or(0) != 1 {
            continue;
        }
        if let Some(old_row) = pos_old.get(&m.signature) {
            out.push(Anchor {
                old_row: *old_row,
                new_row: m.row_idx,
                signature: m.signature,
            });
        }
    }
    out
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::grid_metadata::{FrequencyClass, RowMeta};

    fn meta_from_hashes(hashes: &[u128]) -> Vec<RowMeta> {
        hashes
            .iter()
            .enumerate()
            .map(|(idx, &hash)| {
                let sig = RowSignature { hash };
                RowMeta {
                    row_idx: idx as u32,
                    signature: sig,
                    non_blank_count: 1,
                    first_non_blank_col: 0,
                    frequency_class: FrequencyClass::Common,
                    is_low_info: false,
                }
            })
            .collect()
    }

    #[test]
    fn discovers_context_anchors_when_no_uniques() {
        let old = meta_from_hashes(&[1, 2, 3, 4, 5, 6, 1, 2]);
        let new = meta_from_hashes(&[7, 1, 2, 3, 4, 5, 6, 8]);

        let anchors = discover_context_anchors(&old, &new, 4);
        assert!(!anchors.is_empty(), "should find context anchors");
        let mut rows: Vec<(u32, u32)> = anchors.iter().map(|a| (a.old_row, a.new_row)).collect();
        rows.sort();
        assert!(rows.contains(&(0, 1)));
        assert!(rows.contains(&(1, 2)));
        assert!(rows.contains(&(2, 3)));
    }
}

```

---

### File: `core\src\alignment\assembly.rs`

```rust
//! Final alignment assembly for AMR algorithm.
//!
//! Implements the final assembly phase as described in the unified grid diff
//! specification Section 12. This module:
//!
//! 1. Orchestrates the full AMR pipeline (metadata  anchors  chain  gaps)
//! 2. Assembles matched pairs, insertions, deletions, and moves into final alignment
//! 3. Provides fast paths for special cases (RLE compression, single-run grids)
//!
//! The main entry point is `align_rows_amr` which returns an `Option<RowAlignment>`.
//! Returns `None` when alignment cannot be determined (falls back to positional diff).

use std::collections::HashSet;
use std::ops::Range;

use crate::alignment::anchor_chain::build_anchor_chain;
use crate::alignment::anchor_discovery::{
    Anchor, discover_anchors_from_meta, discover_context_anchors, discover_local_anchors,
};
use crate::alignment::gap_strategy::{GapStrategy, select_gap_strategy};
use crate::alignment::move_extraction::{
    extract_global_moves, find_block_move, moves_from_matched_pairs,
};
use crate::alignment::runs::{RowRun, compress_to_runs};
use crate::alignment_types::{RowAlignment, RowBlockMove};
use crate::config::DiffConfig;
use crate::grid_metadata::RowMeta;
use crate::grid_view::GridView;
#[cfg(any(test, feature = "dev-apis"))]
use crate::workbook::Grid;
use crate::workbook::RowSignature;
#[cfg(feature = "perf-metrics")]
use crate::perf::{DiffMetrics, Phase};

#[derive(Default)]
struct GapAlignmentResult {
    matched: Vec<(u32, u32)>,
    inserted: Vec<u32>,
    deleted: Vec<u32>,
    moves: Vec<RowBlockMove>,
}

struct GapCtx<'a> {
    old_range: Range<u32>,
    new_range: Range<u32>,
    old_slice: &'a [RowMeta],
    new_slice: &'a [RowMeta],
}

impl<'a> GapCtx<'a> {
    fn new(
        old_meta: &'a [RowMeta],
        new_meta: &'a [RowMeta],
        old_range: Range<u32>,
        new_range: Range<u32>,
    ) -> Self {
        let old_slice = slice_by_range(old_meta, &old_range);
        let new_slice = slice_by_range(new_meta, &new_range);
        Self {
            old_range,
            new_range,
            old_slice,
            new_slice,
        }
    }

    fn insert_all(&self) -> GapAlignmentResult {
        GapAlignmentResult {
            inserted: (self.new_range.start..self.new_range.end).collect(),
            ..Default::default()
        }
    }

    fn delete_all(&self) -> GapAlignmentResult {
        GapAlignmentResult {
            deleted: (self.old_range.start..self.old_range.end).collect(),
            ..Default::default()
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct RowAlignmentWithSignatures {
    pub alignment: RowAlignment,
    pub row_signatures_a: Vec<RowSignature>,
    pub row_signatures_b: Vec<RowSignature>,
}

#[cfg(test)]
pub fn align_rows_amr(old: &Grid, new: &Grid, config: &DiffConfig) -> Option<RowAlignment> {
    align_rows_amr_with_signatures(old, new, config).map(|result| result.alignment)
}

#[allow(dead_code)]
#[cfg(any(test, feature = "dev-apis"))]
pub fn align_rows_amr_with_signatures(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RowAlignmentWithSignatures> {
    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);
    align_rows_amr_with_signatures_from_views(&view_a, &view_b, config)
}

pub fn align_rows_amr_with_signatures_from_views(
    view_a: &GridView,
    view_b: &GridView,
    config: &DiffConfig,
) -> Option<RowAlignmentWithSignatures> {
    align_rows_amr_with_signatures_from_views_internal(
        view_a,
        view_b,
        config,
        #[cfg(feature = "perf-metrics")]
        None,
    )
}

pub(crate) fn align_meta_with_amr(
    rows_a: &[RowMeta],
    rows_b: &[RowMeta],
    config: &DiffConfig,
) -> Option<RowAlignment> {
    align_rows_from_meta(
        rows_a,
        rows_b,
        config,
        #[cfg(feature = "perf-metrics")]
        None,
    )
}

#[cfg(feature = "perf-metrics")]
pub fn align_rows_amr_with_signatures_from_views_with_metrics(
    view_a: &GridView,
    view_b: &GridView,
    config: &DiffConfig,
    metrics: &mut DiffMetrics,
) -> Option<RowAlignmentWithSignatures> {
    align_rows_amr_with_signatures_from_views_internal(view_a, view_b, config, Some(metrics))
}

fn align_rows_amr_with_signatures_from_views_internal(
    view_a: &GridView,
    view_b: &GridView,
    config: &DiffConfig,
    #[cfg(feature = "perf-metrics")] mut metrics: Option<&mut DiffMetrics>,
) -> Option<RowAlignmentWithSignatures> {
    let alignment = align_rows_from_meta(
        &view_a.row_meta,
        &view_b.row_meta,
        config,
        #[cfg(feature = "perf-metrics")]
        metrics.as_deref_mut(),
    )?;
    let row_signatures_a: Vec<RowSignature> =
        view_a.row_meta.iter().map(|meta| meta.signature).collect();
    let row_signatures_b: Vec<RowSignature> =
        view_b.row_meta.iter().map(|meta| meta.signature).collect();

    Some(RowAlignmentWithSignatures {
        alignment,
        row_signatures_a,
        row_signatures_b,
    })
}

fn align_rows_from_meta(
    rows_a: &[RowMeta],
    rows_b: &[RowMeta],
    config: &DiffConfig,
    #[cfg(feature = "perf-metrics")] mut metrics: Option<&mut DiffMetrics>,
) -> Option<RowAlignment> {
    if rows_a.len() == rows_b.len()
        && rows_a
            .iter()
            .zip(rows_b.iter())
            .all(|(a, b)| a.signature == b.signature)
    {
        let mut matched = Vec::with_capacity(rows_a.len());
        for (a, b) in rows_a.iter().zip(rows_b.iter()) {
            matched.push((a.row_idx, b.row_idx));
        }
        return Some(RowAlignment {
            matched,
            inserted: Vec::new(),
            deleted: Vec::new(),
            moves: Vec::new(),
        });
    }

    let runs_a = compress_to_runs(rows_a);
    let runs_b = compress_to_runs(rows_b);
    if runs_a.len() == 1 && runs_b.len() == 1 && runs_a[0].signature == runs_b[0].signature {
        let shared = runs_a[0].count.min(runs_b[0].count);
        let mut matched = Vec::new();
        for offset in 0..shared {
            matched.push((runs_a[0].start_row + offset, runs_b[0].start_row + offset));
        }
        let mut inserted = Vec::new();
        if runs_b[0].count > shared {
            inserted
                .extend((runs_b[0].start_row + shared)..(runs_b[0].start_row + runs_b[0].count));
        }
        let mut deleted = Vec::new();
        if runs_a[0].count > shared {
            deleted.extend((runs_a[0].start_row + shared)..(runs_a[0].start_row + runs_a[0].count));
        }
        return Some(RowAlignment {
            matched,
            inserted,
            deleted,
            moves: Vec::new(),
        });
    }

    let compressed_a = runs_a.len() * 2 <= rows_a.len();
    let compressed_b = runs_b.len() * 2 <= rows_b.len();
    if (compressed_a || compressed_b)
        && !runs_a.is_empty()
        && !runs_b.is_empty()
        && let Some(alignment) = align_runs_stable(&runs_a, &runs_b)
    {
        return Some(alignment);
    }

    let anchors = build_anchor_chain(discover_anchors_from_meta(rows_a, rows_b));
    let global_moves = if config.moves.max_move_iterations > 0 {
        #[cfg(feature = "perf-metrics")]
        let _guard = metrics
            .as_deref_mut()
            .map(|m| m.phase_guard(Phase::MoveDetection));
        extract_global_moves(rows_a, rows_b, &anchors, config)
    } else {
        Vec::new()
    };
    Some(assemble_from_meta(
        rows_a,
        rows_b,
        anchors,
        config,
        0,
        &global_moves,
    ))
}

fn assemble_from_meta(
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    anchors: Vec<Anchor>,
    config: &DiffConfig,
    depth: u32,
    global_moves: &[RowBlockMove],
) -> RowAlignment {
    if old_meta.is_empty() && new_meta.is_empty() {
        return RowAlignment::default();
    }

    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();
    let mut moves = Vec::new();

    let mut prev_old = old_meta.first().map(|m| m.row_idx).unwrap_or(0);
    let mut prev_new = new_meta.first().map(|m| m.row_idx).unwrap_or(0);

    for anchor in anchors.iter() {
        let gap_old = prev_old..anchor.old_row;
        let gap_new = prev_new..anchor.new_row;
        let gap_result = fill_gap(
            gap_old,
            gap_new,
            old_meta,
            new_meta,
            config,
            depth,
            global_moves,
        );
        matched.extend(gap_result.matched);
        inserted.extend(gap_result.inserted);
        deleted.extend(gap_result.deleted);
        moves.extend(gap_result.moves);

        matched.push((anchor.old_row, anchor.new_row));
        prev_old = anchor.old_row + 1;
        prev_new = anchor.new_row + 1;
    }

    let old_end = old_meta.last().map(|m| m.row_idx + 1).unwrap_or(prev_old);
    let new_end = new_meta.last().map(|m| m.row_idx + 1).unwrap_or(prev_new);
    let tail_result = fill_gap(
        prev_old..old_end,
        prev_new..new_end,
        old_meta,
        new_meta,
        config,
        depth,
        global_moves,
    );
    matched.extend(tail_result.matched);
    inserted.extend(tail_result.inserted);
    deleted.extend(tail_result.deleted);
    moves.extend(tail_result.moves);

    matched.sort_by_key(|(a, b)| (*a, *b));
    inserted.sort_unstable();
    deleted.sort_unstable();
    moves.sort_by_key(|m| (m.src_start_row, m.dst_start_row, m.row_count));

    RowAlignment {
        matched,
        inserted,
        deleted,
        moves,
    }
}

fn fill_gap(
    old_gap: Range<u32>,
    new_gap: Range<u32>,
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    config: &DiffConfig,
    depth: u32,
    global_moves: &[RowBlockMove],
) -> GapAlignmentResult {
    let ctx = GapCtx::new(old_meta, new_meta, old_gap, new_gap);
    let moves_in_gap = moves_within_gap(&ctx.old_range, &ctx.new_range, global_moves);
    let has_recursed = depth >= config.alignment.max_recursion_depth;
    let strategy = select_gap_strategy(
        ctx.old_slice,
        ctx.new_slice,
        config,
        has_recursed,
        !moves_in_gap.is_empty(),
    );

    match strategy {
        GapStrategy::Empty => GapAlignmentResult::default(),
        GapStrategy::InsertAll => ctx.insert_all(),
        GapStrategy::DeleteAll => ctx.delete_all(),
        GapStrategy::SmallEdit => align_gap_default(ctx.old_slice, ctx.new_slice, config),
        GapStrategy::HashFallback => align_gap_hash(ctx.old_slice, ctx.new_slice),
        GapStrategy::MoveCandidate => align_gap_with_moves(ctx.old_slice, ctx.new_slice, config),
        GapStrategy::ConsumeGlobalMoves => align_gap_with_global_moves(
            ctx.old_slice,
            ctx.new_slice,
            &moves_in_gap,
            config,
            depth,
        ),
        GapStrategy::RecursiveAlign => {
            align_gap_recursive(ctx.old_slice, ctx.new_slice, config, depth, global_moves)
        }
    }
}

fn moves_within_gap(
    old_range: &Range<u32>,
    new_range: &Range<u32>,
    moves: &[RowBlockMove],
) -> Vec<RowBlockMove> {
    moves
        .iter()
        .copied()
        .filter(|mv| {
            range_contains(old_range, mv.src_start_row, mv.row_count)
                && range_contains(new_range, mv.dst_start_row, mv.row_count)
        })
        .collect()
}

fn range_contains(range: &Range<u32>, start: u32, len: u32) -> bool {
    if len == 0 {
        return false;
    }
    let end = start.saturating_add(len);
    range.start <= start && end <= range.end
}

fn align_gap_without_global(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
    depth: u32,
) -> GapAlignmentResult {
    let has_recursed = depth >= config.alignment.max_recursion_depth;
    let strategy = select_gap_strategy(old_slice, new_slice, config, has_recursed, false);

    match strategy {
        GapStrategy::Empty => GapAlignmentResult::default(),
        GapStrategy::InsertAll => GapAlignmentResult {
            inserted: new_slice.iter().map(|m| m.row_idx).collect(),
            ..Default::default()
        },
        GapStrategy::DeleteAll => GapAlignmentResult {
            deleted: old_slice.iter().map(|m| m.row_idx).collect(),
            ..Default::default()
        },
        GapStrategy::SmallEdit => align_gap_default(old_slice, new_slice, config),
        GapStrategy::HashFallback => align_gap_hash(old_slice, new_slice),
        GapStrategy::MoveCandidate => align_gap_with_moves(old_slice, new_slice, config),
        GapStrategy::ConsumeGlobalMoves => align_gap_default(old_slice, new_slice, config),
        GapStrategy::RecursiveAlign => align_gap_recursive(old_slice, new_slice, config, depth, &[]),
    }
}

fn align_gap_with_global_moves(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    moves_in_gap: &[RowBlockMove],
    config: &DiffConfig,
    depth: u32,
) -> GapAlignmentResult {
    let mut result = GapAlignmentResult::default();
    if moves_in_gap.is_empty() {
        return align_gap_without_global(old_slice, new_slice, config, depth);
    }

    let mut skip_old: HashSet<u32> = HashSet::new();
    let mut skip_new: HashSet<u32> = HashSet::new();

    for mv in moves_in_gap {
        result.moves.push(*mv);
        for offset in 0..mv.row_count {
            let src = mv.src_start_row.saturating_add(offset);
            let dst = mv.dst_start_row.saturating_add(offset);
            result.matched.push((src, dst));
            skip_old.insert(src);
            skip_new.insert(dst);
        }
    }

    let filtered_old: Vec<RowMeta> = old_slice
        .iter()
        .copied()
        .filter(|m| !skip_old.contains(&m.row_idx))
        .collect();
    let filtered_new: Vec<RowMeta> = new_slice
        .iter()
        .copied()
        .filter(|m| !skip_new.contains(&m.row_idx))
        .collect();

    let mut remainder = align_gap_without_global(&filtered_old, &filtered_new, config, depth);
    result.matched.append(&mut remainder.matched);
    result.inserted.append(&mut remainder.inserted);
    result.deleted.append(&mut remainder.deleted);
    result.moves.append(&mut remainder.moves);
    result
}

fn align_gap_default(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
) -> GapAlignmentResult {
    if old_slice.len() as u32 > config.alignment.max_lcs_gap_size
        || new_slice.len() as u32 > config.alignment.max_lcs_gap_size
    {
        return align_gap_via_hash(old_slice, new_slice);
    }
    align_small_gap(old_slice, new_slice, config)
}

fn align_gap_hash(old_slice: &[RowMeta], new_slice: &[RowMeta]) -> GapAlignmentResult {
    let mut result = align_gap_via_hash(old_slice, new_slice);
    result
        .moves
        .extend(moves_from_matched_pairs(&result.matched));
    result
}

fn align_gap_with_moves(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
) -> GapAlignmentResult {
    let mut result = if old_slice.len() as u32 > config.alignment.max_lcs_gap_size
        || new_slice.len() as u32 > config.alignment.max_lcs_gap_size
    {
        align_gap_via_hash(old_slice, new_slice)
    } else {
        align_small_gap(old_slice, new_slice, config)
    };

    let mut detected_moves = moves_from_matched_pairs(&result.matched);

    if detected_moves.is_empty() {
        let has_nonzero_offset = result
            .matched
            .iter()
            .any(|(a, b)| (*b as i64 - *a as i64) != 0);

        if has_nonzero_offset
            && let Some(mv) =
                find_block_move(
                    old_slice,
                    new_slice,
                    config.moves.min_block_size_for_move,
                    config,
                )
        {
            detected_moves.push(mv);
        }
    }

    result.moves.extend(detected_moves);
    result
}

fn recursive_anchor_candidates(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    depth: u32,
    config: &DiffConfig,
) -> Vec<Anchor> {
    if depth == 0 {
        return discover_anchors_from_meta(old_slice, new_slice);
    }

    let k1 = config.alignment.context_anchor_k1 as usize;
    let k2 = config.alignment.context_anchor_k2 as usize;

    let mut anchors = discover_local_anchors(old_slice, new_slice);
    if anchors.is_empty() {
        anchors = discover_context_anchors(old_slice, new_slice, k1);
        if anchors.is_empty() {
            anchors = discover_context_anchors(old_slice, new_slice, k2);
        }
        return anchors;
    }

    if anchors.len() < k1 {
        let mut ctx_anchors = discover_context_anchors(old_slice, new_slice, k1);
        anchors.append(&mut ctx_anchors);
    }

    anchors
}

fn align_gap_recursive(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
    depth: u32,
    global_moves: &[RowBlockMove],
) -> GapAlignmentResult {
    let at_limit = depth >= config.alignment.max_recursion_depth;
    if at_limit {
        return align_gap_default(old_slice, new_slice, config);
    }

    let anchors = build_anchor_chain(recursive_anchor_candidates(
        old_slice, new_slice, depth, config,
    ));

    if anchors.is_empty() {
        return align_gap_default(old_slice, new_slice, config);
    }

    let alignment = assemble_from_meta(
        old_slice,
        new_slice,
        anchors,
        config,
        depth + 1,
        global_moves,
    );
    GapAlignmentResult {
        matched: alignment.matched,
        inserted: alignment.inserted,
        deleted: alignment.deleted,
        moves: alignment.moves,
    }
}

fn align_runs_stable(runs_a: &[RowRun], runs_b: &[RowRun]) -> Option<RowAlignment> {
    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();

    let mut idx_a = 0usize;
    let mut idx_b = 0usize;

    while idx_a < runs_a.len() && idx_b < runs_b.len() {
        let run_a = &runs_a[idx_a];
        let run_b = &runs_b[idx_b];

        if run_a.signature != run_b.signature {
            return None;
        }

        let shared = run_a.count.min(run_b.count);
        for offset in 0..shared {
            matched.push((run_a.start_row + offset, run_b.start_row + offset));
        }

        if run_a.count > shared {
            for offset in shared..run_a.count {
                deleted.push(run_a.start_row + offset);
            }
        }

        if run_b.count > shared {
            for offset in shared..run_b.count {
                inserted.push(run_b.start_row + offset);
            }
        }

        idx_a += 1;
        idx_b += 1;
    }

    for run in runs_a.iter().skip(idx_a) {
        for offset in 0..run.count {
            deleted.push(run.start_row + offset);
        }
    }

    for run in runs_b.iter().skip(idx_b) {
        for offset in 0..run.count {
            inserted.push(run.start_row + offset);
        }
    }

    matched.sort_by_key(|(a, b)| (*a, *b));
    inserted.sort_unstable();
    deleted.sort_unstable();

    Some(RowAlignment {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    })
}

fn slice_by_range<'a>(meta: &'a [RowMeta], range: &Range<u32>) -> &'a [RowMeta] {
    if meta.is_empty() || range.start >= range.end {
        return &[];
    }
    let base = meta.first().map(|m| m.row_idx).unwrap_or(0);
    if range.start < base {
        return &[];
    }
    let start = (range.start - base) as usize;
    if start >= meta.len() {
        return &[];
    }
    let end = (start + (range.end - range.start) as usize).min(meta.len());
    &meta[start..end]
}

fn align_small_gap(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
) -> GapAlignmentResult {
    let m = old_slice.len();
    let n = new_slice.len();
    if m == 0 && n == 0 {
        return GapAlignmentResult::default();
    }

    if m as u32 > config.alignment.max_lcs_gap_size
        || n as u32 > config.alignment.max_lcs_gap_size
    {
        return align_gap_via_hash(old_slice, new_slice);
    }

    if m.saturating_mul(n) > config.alignment.lcs_dp_work_limit {
        return align_gap_via_myers(old_slice, new_slice);
    }

    let mut dp = vec![vec![0u32; n + 1]; m + 1];
    for i in (0..m).rev() {
        for j in (0..n).rev() {
            if old_slice[i].signature == new_slice[j].signature {
                dp[i][j] = dp[i + 1][j + 1] + 1;
            } else {
                dp[i][j] = dp[i + 1][j].max(dp[i][j + 1]);
            }
        }
    }

    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();

    let mut i = 0usize;
    let mut j = 0usize;
    while i < m && j < n {
        if old_slice[i].signature == new_slice[j].signature {
            matched.push((old_slice[i].row_idx, new_slice[j].row_idx));
            i += 1;
            j += 1;
        } else if dp[i + 1][j] >= dp[i][j + 1] {
            deleted.push(old_slice[i].row_idx);
            i += 1;
        } else {
            inserted.push(new_slice[j].row_idx);
            j += 1;
        }
    }

    while i < m {
        deleted.push(old_slice[i].row_idx);
        i += 1;
    }
    while j < n {
        inserted.push(new_slice[j].row_idx);
        j += 1;
    }

    if matched.is_empty() && m == n {
        matched = old_slice
            .iter()
            .zip(new_slice.iter())
            .map(|(a, b)| (a.row_idx, b.row_idx))
            .collect();
        inserted.clear();
        deleted.clear();
    }

    GapAlignmentResult {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    }
}

fn align_gap_via_myers(old_slice: &[RowMeta], new_slice: &[RowMeta]) -> GapAlignmentResult {
    let m = old_slice.len();
    let n = new_slice.len();
    if m == 0 && n == 0 {
        return GapAlignmentResult::default();
    }

    let edits = myers_edit_script(old_slice, new_slice);

    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();

    for edit in edits {
        match edit {
            Edit::Match(i, j) => matched.push((old_slice[i].row_idx, new_slice[j].row_idx)),
            Edit::Insert(j) => inserted.push(new_slice[j].row_idx),
            Edit::Delete(i) => deleted.push(old_slice[i].row_idx),
        }
    }

    if matched.is_empty() && m == n {
        matched = old_slice
            .iter()
            .zip(new_slice.iter())
            .map(|(a, b)| (a.row_idx, b.row_idx))
            .collect();
        inserted.clear();
        deleted.clear();
    }

    matched.sort_by_key(|(a, b)| (*a, *b));
    inserted.sort_unstable();
    deleted.sort_unstable();

    GapAlignmentResult {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    }
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
enum Edit {
    Match(usize, usize),
    Insert(usize),
    Delete(usize),
}

fn myers_edit_script(old_slice: &[RowMeta], new_slice: &[RowMeta]) -> Vec<Edit> {
    let n = old_slice.len() as isize;
    let m = new_slice.len() as isize;
    if n == 0 {
        return (0..m as usize).map(Edit::Insert).collect();
    }
    if m == 0 {
        return (0..n as usize).map(Edit::Delete).collect();
    }

    let max = (n + m) as usize;
    let offset = max as isize;
    let mut v = vec![0isize; 2 * max + 1];
    let mut trace: Vec<Vec<isize>> = Vec::new();

    for d in 0..=max {
        let mut v_next = v.clone();
        for k in (-(d as isize)..=d as isize).step_by(2) {
            let idx = (k + offset) as usize;
            let x_start = if k == -(d as isize) || (k != d as isize && v[idx - 1] < v[idx + 1]) {
                v[idx + 1]
            } else {
                v[idx - 1] + 1
            };

            let mut x = x_start;
            let mut y = x - k;
            while x < n
                && y < m
                && old_slice[x as usize].signature == new_slice[y as usize].signature
            {
                x += 1;
                y += 1;
            }
            v_next[idx] = x;
            if x >= n && y >= m {
                trace.push(v_next);
                return reconstruct_myers(trace, old_slice.len(), new_slice.len(), offset);
            }
        }
        trace.push(v_next.clone());
        v = v_next;
    }

    Vec::new()
}

fn reconstruct_myers(
    trace: Vec<Vec<isize>>,
    old_len: usize,
    new_len: usize,
    offset: isize,
) -> Vec<Edit> {
    let mut edits = Vec::new();
    let mut x = old_len as isize;
    let mut y = new_len as isize;

    for d_rev in (0..trace.len()).rev() {
        let v = &trace[d_rev];
        let k = x - y;
        let idx = (k + offset) as usize;

        let (prev_x, prev_y, from_down);
        if d_rev == 0 {
            prev_x = 0;
            prev_y = 0;
            from_down = false;
        } else {
            let use_down =
                k == -(d_rev as isize) || (k != d_rev as isize && v[idx - 1] < v[idx + 1]);
            let prev_k = if use_down { k + 1 } else { k - 1 };
            let prev_idx = (prev_k + offset) as usize;
            let prev_v = &trace[d_rev - 1];
            prev_x = prev_v[prev_idx].max(0);
            prev_y = (prev_x - prev_k).max(0);
            from_down = use_down;
        }

        let mut cur_x = x;
        let mut cur_y = y;
        while cur_x > prev_x && cur_y > prev_y {
            cur_x -= 1;
            cur_y -= 1;
            edits.push(Edit::Match(cur_x as usize, cur_y as usize));
        }

        if d_rev > 0 {
            if from_down {
                edits.push(Edit::Insert(prev_y as usize));
            } else {
                edits.push(Edit::Delete(prev_x as usize));
            }
        }

        x = prev_x;
        y = prev_y;
    }

    edits.reverse();
    edits
}

fn align_gap_via_hash(old_slice: &[RowMeta], new_slice: &[RowMeta]) -> GapAlignmentResult {
    use std::collections::{HashMap, VecDeque};

    let m = old_slice.len();
    let n = new_slice.len();
    if m == 0 && n == 0 {
        return GapAlignmentResult::default();
    }

    let mut sig_to_new: HashMap<crate::workbook::RowSignature, VecDeque<u32>> = HashMap::new();
    for (j, meta) in new_slice.iter().enumerate() {
        sig_to_new
            .entry(meta.signature)
            .or_default()
            .push_back(j as u32);
    }

    let mut candidate_pairs: Vec<(u32, u32)> = Vec::new();
    for (i, meta) in old_slice.iter().enumerate() {
        if let Some(q) = sig_to_new.get_mut(&meta.signature)
            && let Some(j) = q.pop_front()
        {
            candidate_pairs.push((i as u32, j));
        }
    }

    if candidate_pairs.is_empty() && m == n {
        let matched = old_slice
            .iter()
            .zip(new_slice.iter())
            .map(|(a, b)| (a.row_idx, b.row_idx))
            .collect();

        return GapAlignmentResult {
            matched,
            inserted: Vec::new(),
            deleted: Vec::new(),
            moves: Vec::new(),
        };
    }

    let lis = lis_indices_u32(&candidate_pairs, |&(_, new_j)| new_j);

    let mut keep = vec![false; candidate_pairs.len()];
    for idx in lis {
        keep[idx] = true;
    }

    let mut used_old = vec![false; m];
    let mut used_new = vec![false; n];
    let mut matched: Vec<(u32, u32)> = Vec::new();

    for (k, (old_i, new_j)) in candidate_pairs.iter().copied().enumerate() {
        if keep[k] {
            used_old[old_i as usize] = true;
            used_new[new_j as usize] = true;
            matched.push((
                old_slice[old_i as usize].row_idx,
                new_slice[new_j as usize].row_idx,
            ));
        }
    }

    let mut deleted: Vec<u32> = Vec::new();
    for i in 0..m {
        if !used_old[i] {
            deleted.push(old_slice[i].row_idx);
        }
    }

    let mut inserted: Vec<u32> = Vec::new();
    for j in 0..n {
        if !used_new[j] {
            inserted.push(new_slice[j].row_idx);
        }
    }

    matched.sort_by_key(|(a, b)| (*a, *b));
    inserted.sort_unstable();
    deleted.sort_unstable();

    GapAlignmentResult {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    }
}

fn lis_indices_u32<T, F>(items: &[T], key: F) -> Vec<usize>
where
    F: Fn(&T) -> u32,
{
    let mut piles: Vec<usize> = Vec::new();
    let mut predecessors: Vec<Option<usize>> = vec![None; items.len()];

    for (idx, item) in items.iter().enumerate() {
        let k = key(item);
        let pos = piles
            .binary_search_by_key(&k, |&pile_idx| key(&items[pile_idx]))
            .unwrap_or_else(|insert_pos| insert_pos);

        if pos > 0 {
            predecessors[idx] = Some(piles[pos - 1]);
        }

        if pos == piles.len() {
            piles.push(idx);
        } else {
            piles[pos] = idx;
        }
    }

    let Some(&last) = piles.last() else {
        return Vec::new();
    };

    let mut result: Vec<usize> = Vec::new();
    let mut current = last;
    loop {
        result.push(current);
        if let Some(prev) = predecessors[current] {
            current = prev;
        } else {
            break;
        }
    }
    result.reverse();
    result
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::grid_metadata::{FrequencyClass, RowMeta};
    use crate::workbook::CellValue;

    fn grid_from_run_lengths(pattern: &[(i32, u32)]) -> Grid {
        let total_rows: u32 = pattern.iter().map(|(_, count)| *count).sum();
        let mut grid = Grid::new(total_rows, 1);
        let mut row_idx = 0u32;
        for (val, count) in pattern {
            for _ in 0..*count {
                grid.insert_cell(row_idx, 0, Some(CellValue::Number(*val as f64)), None);
                row_idx = row_idx.saturating_add(1);
            }
        }
        grid
    }

    fn grid_with_unique_rows(rows: &[i32]) -> Grid {
        let nrows = rows.len() as u32;
        let mut grid = Grid::new(nrows, 1);
        for (r, &val) in rows.iter().enumerate() {
            grid.insert_cell(r as u32, 0, Some(CellValue::Number(val as f64)), None);
        }
        grid
    }

    fn row_meta_from_hashes(start_row: u32, hashes: &[u128]) -> Vec<RowMeta> {
        hashes
            .iter()
            .enumerate()
            .map(|(idx, &hash)| {
                let signature = crate::workbook::RowSignature { hash };
                RowMeta {
                    row_idx: start_row + idx as u32,
                    signature,
                    non_blank_count: 1,
                    first_non_blank_col: 0,
                    frequency_class: FrequencyClass::Common,
                    is_low_info: false,
                }
            })
            .collect()
    }

    #[test]
    fn aligns_compressed_runs_with_insert_and_delete() {
        let grid_a = grid_from_run_lengths(&[(1, 50), (2, 5), (1, 50)]);
        let grid_b = grid_from_run_lengths(&[(1, 52), (2, 3), (1, 50)]);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed for repetitive runs");
        assert!(alignment.moves.is_empty());
        assert_eq!(alignment.inserted.len(), 2);
        assert_eq!(alignment.deleted.len(), 2);
        assert_eq!(alignment.matched.len(), 103);
        assert_eq!(alignment.matched[0], (0, 0));
    }

    #[test]
    fn run_alignment_falls_back_on_mismatch() {
        let grid_a = grid_from_run_lengths(&[(1, 3), (2, 3), (1, 3)]);
        let grid_b = grid_from_run_lengths(&[(1, 3), (3, 3), (1, 3)]);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should still produce result via full AMR");
        assert!(!alignment.matched.is_empty());
    }

    #[test]
    fn amr_disjoint_gaps_with_insertions_and_deletions() {
        let grid_a = grid_with_unique_rows(&[1, 2, 3, 100, 4, 5, 6, 200, 7, 8, 9]);
        let grid_b = grid_with_unique_rows(&[1, 2, 10, 3, 4, 5, 6, 7, 20, 8, 9]);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed with disjoint gaps");

        assert!(!alignment.matched.is_empty(), "should have matched pairs");

        let matched_is_monotonic = alignment
            .matched
            .windows(2)
            .all(|w| w[0].0 <= w[1].0 && w[0].1 <= w[1].1);
        assert!(
            matched_is_monotonic,
            "matched pairs should be monotonically increasing"
        );

        assert!(
            !alignment.inserted.is_empty() || !alignment.deleted.is_empty(),
            "should have insertions and/or deletions"
        );
    }

    #[test]
    fn amr_recursive_gap_alignment_returns_monotonic_alignment() {
        let grid_a = grid_with_unique_rows(&[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]);
        let rows_b = vec![
            1, 2, 100, 3, 4, 5, 200, 6, 7, 8, 300, 9, 10, 11, 400, 12, 13, 14, 15,
        ];
        let grid_b = grid_with_unique_rows(&rows_b);

        let mut config = DiffConfig::default();
        config.alignment.recursive_align_threshold = 5;
        config.alignment.small_gap_threshold = 2;

        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed with recursive gaps");

        let matched_is_monotonic = alignment
            .matched
            .windows(2)
            .all(|w| w[0].0 <= w[1].0 && w[0].1 <= w[1].1);
        assert!(
            matched_is_monotonic,
            "recursive alignment should produce monotonic matched pairs"
        );

        for &inserted_row in &alignment.inserted {
            assert!(
                !alignment.matched.iter().any(|(_, b)| *b == inserted_row),
                "inserted rows should not appear in matched pairs"
            );
        }

        for &deleted_row in &alignment.deleted {
            assert!(
                !alignment.matched.iter().any(|(a, _)| *a == deleted_row),
                "deleted rows should not appear in matched pairs"
            );
        }
    }

    #[test]
    fn amr_multi_gap_move_detection_produces_expected_row_block_move() {
        let grid_a = grid_with_unique_rows(&[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);
        let grid_b = grid_with_unique_rows(&[1, 2, 6, 7, 8, 3, 4, 5, 9, 10]);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed with moved block");

        assert!(
            !alignment.matched.is_empty(),
            "should have matched pairs even with moves"
        );

        let old_rows: std::collections::HashSet<_> =
            alignment.matched.iter().map(|(a, _)| *a).collect();
        let new_rows: std::collections::HashSet<_> =
            alignment.matched.iter().map(|(_, b)| *b).collect();

        assert!(
            old_rows.len() <= 10 && new_rows.len() <= 10,
            "matched rows should not exceed input size"
        );
    }

    #[test]
    fn amr_alignment_empty_grids() {
        let grid_a = Grid::new(0, 0);
        let grid_b = Grid::new(0, 0);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed for empty grids");

        assert!(alignment.matched.is_empty());
        assert!(alignment.inserted.is_empty());
        assert!(alignment.deleted.is_empty());
        assert!(alignment.moves.is_empty());
    }

    #[test]
    fn align_rows_amr_with_signatures_exposes_row_hashes() {
        let grid_a = grid_with_unique_rows(&[1, 2, 3, 4]);
        let grid_b = grid_with_unique_rows(&[1, 2, 3, 4]);

        let config = DiffConfig::default();
        let result =
            align_rows_amr_with_signatures(&grid_a, &grid_b, &config).expect("should align");

        assert_eq!(result.row_signatures_a.len(), grid_a.nrows as usize);
        assert_eq!(result.row_signatures_b.len(), grid_b.nrows as usize);
        assert_eq!(result.alignment.matched.len(), grid_a.nrows as usize);

        for row in 0..grid_a.nrows {
            let expected_a = grid_a.compute_row_signature(row);
            let expected_b = grid_b.compute_row_signature(row);
            assert_eq!(
                Some(expected_a),
                result.row_signatures_a.get(row as usize).copied(),
                "row {} signature for grid A should match compute_row_signature",
                row
            );
            assert_eq!(
                Some(expected_b),
                result.row_signatures_b.get(row as usize).copied(),
                "row {} signature for grid B should match compute_row_signature",
                row
            );
        }
    }

    #[test]
    fn amr_alignment_all_deleted() {
        let grid_a = grid_with_unique_rows(&[1, 2, 3, 4, 5]);
        let grid_b = Grid::new(0, 1);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed when all rows deleted");

        assert!(alignment.matched.is_empty());
        assert!(alignment.inserted.is_empty());
        assert_eq!(alignment.deleted.len(), 5);
    }

    #[test]
    fn amr_alignment_all_inserted() {
        let grid_a = Grid::new(0, 1);
        let grid_b = grid_with_unique_rows(&[1, 2, 3, 4, 5]);

        let config = DiffConfig::default();
        let alignment = align_rows_amr(&grid_a, &grid_b, &config)
            .expect("alignment should succeed when all rows inserted");

        assert!(alignment.matched.is_empty());
        assert_eq!(alignment.inserted.len(), 5);
        assert!(alignment.deleted.is_empty());
    }

    #[test]
    fn align_small_gap_enforces_cap_with_hash_fallback() {
        let config = DiffConfig::default();
        let large = (config.alignment.max_lcs_gap_size + 1) as usize;
        let old_hashes: Vec<u128> = (0..large as u32).map(|i| i as u128).collect();
        let new_hashes: Vec<u128> = (0..large as u32).map(|i| (10_000 + i) as u128).collect();

        let old_meta = row_meta_from_hashes(10, &old_hashes);
        let new_meta = row_meta_from_hashes(20, &new_hashes);

        let result = align_small_gap(&old_meta, &new_meta, &config);
        assert_eq!(result.matched.len(), large);
        assert!(result.inserted.is_empty());
        assert!(result.deleted.is_empty());
        assert_eq!(result.matched.first(), Some(&(10, 20)));
        assert_eq!(
            result.matched.last(),
            Some(&(10 + large as u32 - 1, 20 + large as u32 - 1))
        );
    }

    #[test]
    fn hash_fallback_produces_monotone_pairs() {
        let old_meta = row_meta_from_hashes(0, &[1, 2, 3, 4]);
        let new_meta = row_meta_from_hashes(0, &[2, 1, 3, 4]);

        let result = align_gap_via_hash(&old_meta, &new_meta);
        assert_eq!(result.matched, vec![(1, 0), (2, 2), (3, 3)]);

        let is_monotone = result
            .matched
            .windows(2)
            .all(|w| w[0].0 <= w[1].0 && w[0].1 <= w[1].1);
        assert!(is_monotone, "hash fallback must preserve monotone ordering");
        assert_eq!(result.inserted, vec![1]);
        assert_eq!(result.deleted, vec![0]);
    }

    #[test]
    fn myers_handles_medium_gap_with_single_insertion() {
        let count = 300usize;
        let old_hashes: Vec<u128> = (0..count as u128).collect();
        let mut new_hashes: Vec<u128> = old_hashes.clone();
        new_hashes.insert(150, 9_999);

        let old_meta = row_meta_from_hashes(0, &old_hashes);
        let new_meta = row_meta_from_hashes(0, &new_hashes);

        let result = align_small_gap(&old_meta, &new_meta, &DiffConfig::default());
        assert_eq!(result.inserted, vec![150]);
        assert!(result.deleted.is_empty());
        assert_eq!(result.matched.len(), count);
        assert_eq!(result.matched.first(), Some(&(0, 0)));
        assert_eq!(
            result.matched.last(),
            Some(&(count as u32 - 1, (count + 1) as u32 - 1))
        );
    }
}

```

---

### File: `core\src\alignment\gap_strategy.rs`

```rust
//! Gap strategy selection for AMR alignment.
//!
//! Implements gap strategy selection as described in the unified grid diff
//! specification Sections 9.6 and 12. After anchors divide the grids into
//! gaps, each gap is processed according to its characteristics:
//!
//! - **Empty**: Both sides empty, nothing to do
//! - **InsertAll**: Old side empty, all new rows are insertions
//! - **DeleteAll**: New side empty, all old rows are deletions
//! - **SmallEdit**: Both sides small enough for O(n*m) LCS alignment
//! - **MoveCandidate**: Gap contains matching unique signatures that may indicate moves
//! - **ConsumeGlobalMoves**: Gap contains validated global moves; consume them first
//! - **RecursiveAlign**: Gap is large; recursively apply AMR with rare anchors
//! - **HashFallback**: Monotone hash/LIS fallback for large gaps

use std::collections::HashSet;

use crate::config::DiffConfig;
use crate::grid_metadata::{FrequencyClass, RowMeta};

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum GapStrategy {
    Empty,
    InsertAll,
    DeleteAll,
    SmallEdit,
    MoveCandidate,
    ConsumeGlobalMoves,
    RecursiveAlign,
    HashFallback,
}

pub fn select_gap_strategy(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    config: &DiffConfig,
    has_recursed: bool,
    has_global_moves: bool,
) -> GapStrategy {
    let old_len = old_slice.len() as u32;
    let new_len = new_slice.len() as u32;

    if old_len == 0 && new_len == 0 {
        return GapStrategy::Empty;
    }
    if old_len == 0 {
        return GapStrategy::InsertAll;
    }
    if new_len == 0 {
        return GapStrategy::DeleteAll;
    }

    if has_global_moves {
        return GapStrategy::ConsumeGlobalMoves;
    }

    let is_move_candidate = has_matching_signatures(old_slice, new_slice);

    let small_threshold = config
        .alignment
        .small_gap_threshold
        .min(config.alignment.max_lcs_gap_size);
    if old_len <= small_threshold && new_len <= small_threshold {
        return if is_move_candidate {
            GapStrategy::MoveCandidate
        } else {
            GapStrategy::SmallEdit
        };
    }

    if (old_len > config.alignment.recursive_align_threshold
        || new_len > config.alignment.recursive_align_threshold)
        && !has_recursed
    {
        return GapStrategy::RecursiveAlign;
    }

    if is_move_candidate {
        return GapStrategy::MoveCandidate;
    }

    if old_len > config.alignment.max_lcs_gap_size || new_len > config.alignment.max_lcs_gap_size {
        return GapStrategy::HashFallback;
    }

    GapStrategy::SmallEdit
}

fn has_matching_signatures(old_slice: &[RowMeta], new_slice: &[RowMeta]) -> bool {
    let set: HashSet<_> = old_slice
        .iter()
        .filter(|m| m.frequency_class == FrequencyClass::Unique)
        .map(|m| m.signature)
        .collect();

    new_slice
        .iter()
        .filter(|m| m.frequency_class == FrequencyClass::Unique)
        .any(|m| set.contains(&m.signature))
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::grid_metadata::{FrequencyClass, RowMeta};
    use crate::workbook::RowSignature;

    fn meta(row_idx: u32, hash: u128) -> RowMeta {
        let signature = RowSignature { hash };
        RowMeta {
            row_idx,
            signature,
            non_blank_count: 1,
            first_non_blank_col: 0,
            frequency_class: FrequencyClass::Common,
            is_low_info: false,
        }
    }

    #[test]
    fn respects_configured_max_lcs_gap_size() {
        let mut config = DiffConfig::default();
        config.alignment.max_lcs_gap_size = 2;
        config.alignment.small_gap_threshold = 10;
        let rows_a = vec![meta(0, 1), meta(1, 2), meta(2, 3)];
        let rows_b = vec![meta(0, 4), meta(1, 5), meta(2, 6)];

        let strategy = select_gap_strategy(&rows_a, &rows_b, &config, false, false);
        assert_eq!(strategy, GapStrategy::HashFallback);
    }
}

```

---

### File: `core\src\alignment\lap.rs`

```rust
//! Linear assignment solver (Hungarian algorithm).
//!
//! Uses a dense O(n^3) Hungarian implementation over integer costs.
//! Intended for small candidate sets after global move extraction caps.

pub(crate) fn solve(costs: &[Vec<i64>]) -> Vec<usize> {
    let n = costs.len();
    if n == 0 {
        return Vec::new();
    }

    debug_assert!(costs.iter().all(|row| row.len() == n));

    let inf = i64::MAX / 4;
    let mut u = vec![0i64; n + 1];
    let mut v = vec![0i64; n + 1];
    let mut p = vec![0usize; n + 1];
    let mut way = vec![0usize; n + 1];

    for i in 1..=n {
        p[0] = i;
        let mut j0 = 0usize;
        let mut minv = vec![inf; n + 1];
        let mut used = vec![false; n + 1];

        loop {
            used[j0] = true;
            let i0 = p[j0];
            let mut delta = inf;
            let mut j1 = 0usize;

            for j in 1..=n {
                if used[j] {
                    continue;
                }
                let cur = costs[i0 - 1][j - 1] - u[i0] - v[j];
                if cur < minv[j] {
                    minv[j] = cur;
                    way[j] = j0;
                }
                if minv[j] < delta {
                    delta = minv[j];
                    j1 = j;
                }
            }

            for j in 0..=n {
                if used[j] {
                    u[p[j]] += delta;
                    v[j] -= delta;
                } else {
                    minv[j] -= delta;
                }
            }

            j0 = j1;
            if p[j0] == 0 {
                break;
            }
        }

        loop {
            let j1 = way[j0];
            p[j0] = p[j1];
            j0 = j1;
            if j0 == 0 {
                break;
            }
        }
    }

    let mut assignment = vec![0usize; n];
    for j in 1..=n {
        if p[j] > 0 {
            assignment[p[j] - 1] = j - 1;
        }
    }
    assignment
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn solves_small_assignment() {
        let costs = vec![
            vec![4, 1, 3],
            vec![2, 0, 5],
            vec![3, 2, 2],
        ];

        let assignment = solve(&costs);
        assert_eq!(assignment.len(), 3);

        let total: i64 = assignment
            .iter()
            .enumerate()
            .map(|(i, &j)| costs[i][j])
            .sum();
        assert_eq!(total, 5, "expected minimal total cost");
    }
}

```

---

### File: `core\src\alignment\mod.rs`

```rust
//! Anchor-Move-Refine (AMR) row alignment algorithm.
//!
//! This module implements a simplified version of the AMR algorithm described in the
//! unified grid diff specification. The implementation follows the general structure:
//!
//! 1. **Row Metadata Collection** (`grid_metadata.rs`, Spec Section 9.11)
//!    - Compute row signatures and classify by frequency (Unique/Rare/Common/LowInfo)
//!
//! 2. **Anchor Discovery** (`anchor_discovery.rs`, Spec Section 10)
//!    - Find rows that are unique in both grids with matching signatures
//!
//! 3. **Anchor Chain Construction** (`anchor_chain.rs`, Spec Section 10)
//!    - Build longest increasing subsequence (LIS) of anchors to preserve relative order
//!
//! 4. **Gap Strategy Selection** (`gap_strategy.rs`, Spec Sections 9.6, 12)
//!    - For each gap between anchors, select appropriate strategy:
//!      Empty, InsertAll, DeleteAll, SmallEdit, MoveCandidate, or RecursiveAlign
//!
//! 5. **Assembly** (`assembly.rs`, Spec Section 12)
//!    - Assemble final alignment by processing gaps and anchors
//!
//! ## Intentional Spec Deviations
//!
//! The current implementation simplifies the full AMR spec in the following ways:
//!
//! - **Global move extraction is bounded**: The implementation performs global
//!   unanchored match extraction with LAP assignment and validation, but caps
//!   candidate counts for determinism and performance.
//!
//! - **RLE fast path**: For highly repetitive grids (>50% compression), the implementation
//!   uses a run-length encoded alignment path (`runs.rs`) that bypasses full AMR.
//!
//! These simplifications are acceptable for most real-world Excel workbooks and keep
//! the implementation maintainable. Future work may implement the full global move
//! extraction if complex reordering scenarios require it.

pub(crate) mod anchor_chain;
pub(crate) mod anchor_discovery;
pub(crate) mod assembly;
pub(crate) mod gap_strategy;
pub(crate) mod lap;
pub(crate) mod move_extraction;
pub(crate) mod runs;

pub(crate) use crate::alignment_types::RowBlockMove;
pub(crate) use assembly::align_rows_amr_with_signatures_from_views;
pub(crate) use assembly::align_meta_with_amr;
#[cfg(feature = "perf-metrics")]
pub(crate) use assembly::align_rows_amr_with_signatures_from_views_with_metrics;

```

---

### File: `core\src\alignment\move_extraction.rs`

```rust
//! Move extraction for AMR alignment.
//!
//! Implements both localized move detection within gaps and the global
//! unanchored match pipeline (Sections 9.5-9.7, 11).
//!
//! ## Current Implementation
//!
//! - `find_block_move`: Scans for contiguous blocks of matching signatures
//!   between old and new slices within a gap. Returns the largest found.
//!
//! - `moves_from_matched_pairs`: Extracts block moves from matched row pairs
//!   where consecutive pairs have the same offset (indicating they moved together).
//!
//! - `extract_global_moves`: Global unanchored match collection, candidate block
//!   construction, LAP assignment, and validation to produce non-overlapping moves.

use std::collections::{HashMap, HashSet};

use crate::alignment::RowBlockMove;
use crate::alignment::anchor_discovery::Anchor;
use crate::alignment::lap;
use crate::config::DiffConfig;
use crate::grid_metadata::{FrequencyClass, RowMeta};
use crate::workbook::RowSignature;

pub fn find_block_move(
    old_slice: &[RowMeta],
    new_slice: &[RowMeta],
    min_len: u32,
    config: &DiffConfig,
) -> Option<RowBlockMove> {
    let max_slice_len = config.moves.move_extraction_max_slice_len as usize;
    if old_slice.len() > max_slice_len || new_slice.len() > max_slice_len {
        return None;
    }

    let mut positions: HashMap<RowSignature, Vec<usize>> = HashMap::new();
    for (idx, meta) in old_slice.iter().enumerate() {
        if meta.is_low_info() {
            continue;
        }
        positions.entry(meta.signature).or_default().push(idx);
    }

    let mut best: Option<RowBlockMove> = None;
    let mut best_len: usize = 0;

    for (new_idx, meta) in new_slice.iter().enumerate() {
        if meta.is_low_info() {
            continue;
        }

        let Some(candidates) = positions.get(&meta.signature) else {
            continue;
        };

        let max_candidates = config.moves.move_extraction_max_candidates_per_sig as usize;
        for &old_idx in candidates.iter().take(max_candidates) {
            let max_possible = (old_slice.len() - old_idx).min(new_slice.len() - new_idx);
            if max_possible <= best_len {
                continue;
            }

            let mut len = 0usize;
            while len < max_possible
                && old_slice[old_idx + len].signature == new_slice[new_idx + len].signature
            {
                len += 1;
            }

            if len >= min_len as usize && len > best_len {
                best_len = len;
                best = Some(RowBlockMove {
                    src_start_row: old_slice[old_idx].row_idx,
                    dst_start_row: new_slice[new_idx].row_idx,
                    row_count: len as u32,
                });
            }
        }
    }

    best
}

pub fn moves_from_matched_pairs(pairs: &[(u32, u32)]) -> Vec<RowBlockMove> {
    if pairs.is_empty() {
        return Vec::new();
    }

    let mut sorted = pairs.to_vec();
    sorted.sort_by_key(|(a, b)| (*a, *b));

    let mut moves = Vec::new();
    let mut start = sorted[0];
    let mut prev = sorted[0];
    let mut run_len = 1u32;
    let mut current_offset: i64 = prev.1 as i64 - prev.0 as i64;

    for &(a, b) in sorted.iter().skip(1) {
        let offset = b as i64 - a as i64;
        if offset == current_offset && a == prev.0 + 1 && b == prev.1 + 1 {
            run_len += 1;
            prev = (a, b);
            continue;
        }

        if run_len > 1 && current_offset != 0 {
            moves.push(RowBlockMove {
                src_start_row: start.0,
                dst_start_row: start.1,
                row_count: run_len,
            });
        }

        start = (a, b);
        prev = (a, b);
        current_offset = offset;
        run_len = 1;
    }

    if run_len > 1 && current_offset != 0 {
        moves.push(RowBlockMove {
            src_start_row: start.0,
            dst_start_row: start.1,
            row_count: run_len,
        });
    }

    moves
}

#[derive(Clone, Copy, Debug)]
struct MoveCandidate {
    src_start_row: u32,
    dst_start_row: u32,
    row_count: u32,
    similarity: f64,
}

#[derive(Clone, Copy, Debug)]
struct CandidateSeed {
    src_start_row: u32,
    dst_start_row: u32,
    row_count: u32,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
struct BlockRange {
    start: u32,
    len: u32,
}

#[derive(Clone, Copy, Debug)]
struct MatchPair {
    a: u32,
    b: u32,
    offset: i64,
}

pub(crate) fn extract_global_moves(
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    anchors: &[Anchor],
    config: &DiffConfig,
) -> Vec<RowBlockMove> {
    let pairs = collect_unanchored_pairs(old_meta, new_meta, anchors, config);
    if pairs.is_empty() {
        return Vec::new();
    }

    let candidates = build_candidate_blocks(&pairs, old_meta, new_meta, config);
    if candidates.is_empty() {
        return Vec::new();
    }

    let assigned = assign_moves(&candidates, config);
    let validated = validate_moves(assigned, old_meta, new_meta, config);
    let resolved = resolve_overlaps(validated);

    resolved
        .into_iter()
        .map(|cand| RowBlockMove {
            src_start_row: cand.src_start_row,
            dst_start_row: cand.dst_start_row,
            row_count: cand.row_count,
        })
        .collect()
}

fn collect_unanchored_pairs(
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    anchors: &[Anchor],
    config: &DiffConfig,
) -> Vec<(u32, u32)> {
    let anchored_old: HashSet<u32> = anchors.iter().map(|a| a.old_row).collect();
    let anchored_new: HashSet<u32> = anchors.iter().map(|a| a.new_row).collect();

    let old_map = collect_unanchored_by_signature(old_meta, &anchored_old, config);
    let new_map = collect_unanchored_by_signature(new_meta, &anchored_new, config);

    let max_per_sig = config.moves.move_extraction_max_candidates_per_sig as usize;
    let max_total = config.moves.move_extraction_max_slice_len as usize;

    let mut pairs = Vec::new();
    for (sig, old_rows) in old_map {
        let Some(new_rows) = new_map.get(&sig) else {
            continue;
        };

        for &a in old_rows.iter().take(max_per_sig) {
            for &b in new_rows.iter().take(max_per_sig) {
                pairs.push((a, b));
                if pairs.len() >= max_total {
                    pairs.sort_by_key(|(x, y)| (*x, *y));
                    return pairs;
                }
            }
        }
    }

    pairs.sort_by_key(|(a, b)| (*a, *b));
    pairs
}

fn collect_unanchored_by_signature(
    rows: &[RowMeta],
    anchored: &HashSet<u32>,
    config: &DiffConfig,
) -> HashMap<RowSignature, Vec<u32>> {
    let mut map: HashMap<RowSignature, Vec<u32>> = HashMap::new();

    for meta in rows {
        if anchored.contains(&meta.row_idx) {
            continue;
        }
        if meta.is_low_info() {
            continue;
        }
        if !matches!(meta.frequency_class, FrequencyClass::Unique | FrequencyClass::Rare) {
            continue;
        }
        if config.moves.move_extraction_max_candidates_per_sig == 0 {
            continue;
        }
        map.entry(meta.signature).or_default().push(meta.row_idx);
    }

    map
}

fn build_candidate_blocks(
    pairs: &[(u32, u32)],
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    config: &DiffConfig,
) -> Vec<MoveCandidate> {
    if pairs.is_empty() {
        return Vec::new();
    }

    let max_gap = config.alignment.small_gap_threshold.max(1) as u32;
    let min_len = config.moves.min_block_size_for_move.max(1);
    let max_len = config.moves.move_extraction_max_slice_len.max(1);
    let threshold = move_similarity_threshold(config);

    let mut sorted: Vec<MatchPair> = pairs
        .iter()
        .map(|(a, b)| MatchPair {
            a: *a,
            b: *b,
            offset: *b as i64 - *a as i64,
        })
        .collect();
    sorted.sort_by_key(|p| (p.a, p.b));

    let mut seeds: Vec<CandidateSeed> = Vec::new();
    let mut seen: HashSet<(u32, u32, u32)> = HashSet::new();

    let mut start = sorted[0];
    let mut prev = sorted[0];
    let mut offset = sorted[0].offset;

    for pair in sorted.iter().skip(1) {
        let delta = pair.a.saturating_sub(prev.a);
        if pair.offset == offset && delta > 0 && delta <= max_gap {
            prev = *pair;
            continue;
        }

        push_candidate_seed(
            start,
            prev,
            min_len,
            max_len,
            &mut seeds,
            &mut seen,
        );

        start = *pair;
        prev = *pair;
        offset = pair.offset;
    }

    push_candidate_seed(
        start,
        prev,
        min_len,
        max_len,
        &mut seeds,
        &mut seen,
    );

    let mut candidates = score_candidates(old_meta, new_meta, threshold, &seeds);
    candidates.sort_by(|a, b| {
        b.similarity
            .partial_cmp(&a.similarity)
            .unwrap_or(std::cmp::Ordering::Equal)
            .then_with(|| a.src_start_row.cmp(&b.src_start_row))
            .then_with(|| a.dst_start_row.cmp(&b.dst_start_row))
            .then_with(|| b.row_count.cmp(&a.row_count))
    });
    candidates
}

fn push_candidate_seed(
    start: MatchPair,
    end: MatchPair,
    min_len: u32,
    max_len: u32,
    seeds: &mut Vec<CandidateSeed>,
    seen: &mut HashSet<(u32, u32, u32)>,
) {
    if end.a < start.a {
        return;
    }
    let row_count = end.a - start.a + 1;
    if row_count < min_len || row_count > max_len {
        return;
    }

    let key = (start.a, start.b, row_count);
    if !seen.insert(key) {
        return;
    }

    seeds.push(CandidateSeed {
        src_start_row: start.a,
        dst_start_row: start.b,
        row_count,
    });
}

fn score_candidates(
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    threshold: f64,
    seeds: &[CandidateSeed],
) -> Vec<MoveCandidate> {
    if seeds.is_empty() {
        return Vec::new();
    }

    #[cfg(feature = "parallel")]
    {
        use rayon::prelude::*;
        let scored: Vec<Option<MoveCandidate>> = seeds
            .par_iter()
            .map(|seed| {
                let similarity = block_similarity(
                    old_meta,
                    new_meta,
                    seed.src_start_row,
                    seed.dst_start_row,
                    seed.row_count,
                );
                if similarity < threshold {
                    None
                } else {
                    Some(MoveCandidate {
                        src_start_row: seed.src_start_row,
                        dst_start_row: seed.dst_start_row,
                        row_count: seed.row_count,
                        similarity,
                    })
                }
            })
            .collect();
        scored.into_iter().flatten().collect()
    }

    #[cfg(not(feature = "parallel"))]
    let mut out = Vec::new();
    #[cfg(not(feature = "parallel"))]
    for seed in seeds {
        let similarity = block_similarity(
            old_meta,
            new_meta,
            seed.src_start_row,
            seed.dst_start_row,
            seed.row_count,
        );
        if similarity < threshold {
            continue;
        }
        out.push(MoveCandidate {
            src_start_row: seed.src_start_row,
            dst_start_row: seed.dst_start_row,
            row_count: seed.row_count,
            similarity,
        });
    }
    #[cfg(not(feature = "parallel"))]
    out
}

fn assign_moves(candidates: &[MoveCandidate], config: &DiffConfig) -> Vec<MoveCandidate> {
    if candidates.is_empty() {
        return Vec::new();
    }

    let mut src_blocks: Vec<BlockRange> = Vec::new();
    let mut dst_blocks: Vec<BlockRange> = Vec::new();
    let mut src_index: HashMap<BlockRange, usize> = HashMap::new();
    let mut dst_index: HashMap<BlockRange, usize> = HashMap::new();

    for cand in candidates {
        let src = BlockRange {
            start: cand.src_start_row,
            len: cand.row_count,
        };
        let dst = BlockRange {
            start: cand.dst_start_row,
            len: cand.row_count,
        };
        if !src_index.contains_key(&src) {
            src_index.insert(src, src_blocks.len());
            src_blocks.push(src);
        }
        if !dst_index.contains_key(&dst) {
            dst_index.insert(dst, dst_blocks.len());
            dst_blocks.push(dst);
        }
    }

    if src_blocks.is_empty() || dst_blocks.is_empty() {
        return Vec::new();
    }

    let mut similarity_map: HashMap<(usize, usize), f64> = HashMap::new();
    for cand in candidates {
        let src = BlockRange {
            start: cand.src_start_row,
            len: cand.row_count,
        };
        let dst = BlockRange {
            start: cand.dst_start_row,
            len: cand.row_count,
        };
        let Some(&i) = src_index.get(&src) else {
            continue;
        };
        let Some(&j) = dst_index.get(&dst) else {
            continue;
        };
        let entry = similarity_map.entry((i, j)).or_insert(0.0);
        if cand.similarity > *entry {
            *entry = cand.similarity;
        }
    }

    let size = src_blocks.len().max(dst_blocks.len());
    let scale = 1000i64;
    let mut costs = vec![vec![scale; size]; size];

    let threshold = move_similarity_threshold(config);
    for ((i, j), sim) in similarity_map.iter() {
        if *sim < threshold {
            continue;
        }
        if let (Some(src), Some(dst)) = (src_blocks.get(*i), dst_blocks.get(*j)) {
            if src.len != dst.len {
                continue;
            }
            let cost = ((1.0 - sim) * scale as f64).round() as i64;
            costs[*i][*j] = cost;
        }
    }

    let assignment = lap::solve(&costs);
    let mut out = Vec::new();
    for (i, &j) in assignment.iter().enumerate() {
        if i >= src_blocks.len() || j >= dst_blocks.len() {
            continue;
        }
        let Some(sim) = similarity_map.get(&(i, j)).copied() else {
            continue;
        };
        if sim < threshold {
            continue;
        }
        let src = src_blocks[i];
        let dst = dst_blocks[j];
        if src.len != dst.len || src.len == 0 {
            continue;
        }
        out.push(MoveCandidate {
            src_start_row: src.start,
            dst_start_row: dst.start,
            row_count: src.len,
            similarity: sim,
        });
    }
    out
}

fn validate_moves(
    candidates: Vec<MoveCandidate>,
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    config: &DiffConfig,
) -> Vec<MoveCandidate> {
    candidates
        .into_iter()
        .filter(|cand| {
            block_similarity(
                old_meta,
                new_meta,
                cand.src_start_row,
                cand.dst_start_row,
                cand.row_count,
            ) >= move_similarity_threshold(config)
        })
        .collect()
}

fn resolve_overlaps(mut candidates: Vec<MoveCandidate>) -> Vec<MoveCandidate> {
    candidates.sort_by(|a, b| {
        b.similarity
            .partial_cmp(&a.similarity)
            .unwrap_or(std::cmp::Ordering::Equal)
            .then_with(|| b.row_count.cmp(&a.row_count))
            .then_with(|| a.src_start_row.cmp(&b.src_start_row))
            .then_with(|| a.dst_start_row.cmp(&b.dst_start_row))
    });

    let mut accepted: Vec<MoveCandidate> = Vec::new();
    for cand in candidates {
        if accepted.iter().any(|m| ranges_overlap(m, &cand)) {
            continue;
        }
        accepted.push(cand);
    }

    accepted.sort_by_key(|m| (m.src_start_row, m.dst_start_row, m.row_count));
    accepted
}

fn ranges_overlap(a: &MoveCandidate, b: &MoveCandidate) -> bool {
    let a_src_end = a.src_start_row.saturating_add(a.row_count);
    let b_src_end = b.src_start_row.saturating_add(b.row_count);
    let a_dst_end = a.dst_start_row.saturating_add(a.row_count);
    let b_dst_end = b.dst_start_row.saturating_add(b.row_count);

    let src_overlap = a.src_start_row < b_src_end && b.src_start_row < a_src_end;
    let dst_overlap = a.dst_start_row < b_dst_end && b.dst_start_row < a_dst_end;

    src_overlap || dst_overlap
}

fn block_similarity(
    old_meta: &[RowMeta],
    new_meta: &[RowMeta],
    src_start: u32,
    dst_start: u32,
    row_count: u32,
) -> f64 {
    if row_count == 0 {
        return 0.0;
    }
    let len = row_count as usize;
    let end_a = src_start as usize + len;
    let end_b = dst_start as usize + len;
    if end_a > old_meta.len() || end_b > new_meta.len() {
        return 0.0;
    }

    let mut matches = 0u32;
    let mut total = 0u32;

    for offset in 0..len {
        let a = &old_meta[src_start as usize + offset];
        let b = &new_meta[dst_start as usize + offset];
        total = total.saturating_add(1);
        if a.is_low_info() || b.is_low_info() {
            continue;
        }
        if a.signature == b.signature {
            matches = matches.saturating_add(1);
        }
    }

    if total == 0 {
        0.0
    } else {
        (matches as f64 + 1.0) / (total as f64 + 1.0)
    }
}

fn move_similarity_threshold(config: &DiffConfig) -> f64 {
    if config.moves.enable_fuzzy_moves {
        config.moves.fuzzy_similarity_threshold
    } else {
        1.0
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::DiffConfig;
    use crate::workbook::RowSignature;

    fn make_meta(row_idx: u32, hash: u128, class: FrequencyClass) -> RowMeta {
        RowMeta {
            row_idx,
            signature: RowSignature { hash },
            non_blank_count: 1,
            first_non_blank_col: 0,
            frequency_class: class,
            is_low_info: matches!(class, FrequencyClass::LowInfo),
        }
    }

    #[test]
    fn global_moves_ignore_low_info_rows() {
        let old = vec![
            make_meta(0, 1, FrequencyClass::LowInfo),
            make_meta(1, 2, FrequencyClass::LowInfo),
        ];
        let new = vec![
            make_meta(0, 1, FrequencyClass::LowInfo),
            make_meta(1, 2, FrequencyClass::LowInfo),
        ];

        let config = DiffConfig::default();
        let moves = extract_global_moves(&old, &new, &[], &config);
        assert!(moves.is_empty());
    }

    #[test]
    fn global_moves_detects_simple_block_move() {
        let old = vec![
            make_meta(0, 1, FrequencyClass::Unique),
            make_meta(1, 2, FrequencyClass::Unique),
            make_meta(2, 3, FrequencyClass::Unique),
            make_meta(3, 4, FrequencyClass::Unique),
            make_meta(4, 5, FrequencyClass::Unique),
            make_meta(5, 6, FrequencyClass::Unique),
        ];
        let new = vec![
            make_meta(0, 1, FrequencyClass::Unique),
            make_meta(1, 5, FrequencyClass::Unique),
            make_meta(2, 6, FrequencyClass::Unique),
            make_meta(3, 2, FrequencyClass::Unique),
            make_meta(4, 3, FrequencyClass::Unique),
            make_meta(5, 4, FrequencyClass::Unique),
        ];

        let config = DiffConfig::default();
        let moves = extract_global_moves(&old, &new, &[], &config);

        assert!(
            moves
                .iter()
                .any(|mv| mv.src_start_row == 1 && mv.dst_start_row == 3 && mv.row_count == 3),
            "expected move for block [2,3,4] shifted down"
        );
    }

    #[test]
    fn candidate_pairs_are_capped_per_signature() {
        let mut old = Vec::new();
        let mut new = Vec::new();
        for idx in 0..5u32 {
            old.push(make_meta(idx, 42, FrequencyClass::Rare));
            new.push(make_meta(idx, 42, FrequencyClass::Rare));
        }

        let mut config = DiffConfig::default();
        config.moves.move_extraction_max_candidates_per_sig = 2;
        config.moves.move_extraction_max_slice_len = 100;

        let pairs = collect_unanchored_pairs(&old, &new, &[], &config);
        assert!(pairs.len() <= 4);
    }
}

```

---

### File: `core\src\alignment\runs.rs`

```rust
//! Run-length encoding for repetitive row patterns.
//!
//! Implements run-length compression as described in the unified grid diff
//! specification Section 2.6 (optional optimization). For grids where >50%
//! of rows share signatures with adjacent rows, this provides a fast path
//! that avoids full AMR computation.
//!
//! This is particularly effective for:
//! - Template-based workbooks with many identical rows
//! - Data with long runs of blank or placeholder rows
//! - Adversarial cases designed to stress the alignment algorithm

use crate::grid_metadata::RowMeta;
use crate::workbook::RowSignature;

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct RowRun {
    pub signature: RowSignature,
    pub start_row: u32,
    pub count: u32,
}

pub fn compress_to_runs(meta: &[RowMeta]) -> Vec<RowRun> {
    let mut runs = Vec::new();
    let mut i = 0usize;
    while i < meta.len() {
        let sig = meta[i].signature;
        let start = i;
        while i < meta.len() && meta[i].signature == sig {
            i += 1;
        }
        runs.push(RowRun {
            signature: sig,
            start_row: meta[start].row_idx,
            count: (i - start) as u32,
        });
    }
    runs
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_meta(idx: u32, hash: u128) -> RowMeta {
        let sig = RowSignature { hash };
        RowMeta {
            row_idx: idx,
            signature: sig,
            non_blank_count: 1,
            first_non_blank_col: 0,
            frequency_class: crate::grid_metadata::FrequencyClass::Common,
            is_low_info: false,
        }
    }

    #[test]
    fn compresses_identical_rows() {
        let meta = vec![make_meta(0, 1), make_meta(1, 1), make_meta(2, 2)];
        let runs = compress_to_runs(&meta);
        assert_eq!(runs.len(), 2);
        assert_eq!(runs[0].count, 2);
        assert_eq!(runs[1].count, 1);
    }

    #[test]
    fn compresses_10k_identical_rows_to_single_run() {
        let meta: Vec<RowMeta> = (0..10_000).map(|i| make_meta(i, 42)).collect();
        let runs = compress_to_runs(&meta);

        assert_eq!(
            runs.len(),
            1,
            "10K identical rows should compress to a single run"
        );
        assert_eq!(
            runs[0].count, 10_000,
            "single run should have count of 10,000"
        );
        assert_eq!(
            runs[0].signature.hash, 42,
            "run signature should match input"
        );
        assert_eq!(runs[0].start_row, 0, "run should start at row 0");
    }

    #[test]
    fn alternating_pattern_ab_does_not_overcompress() {
        let meta: Vec<RowMeta> = (0..10_000)
            .map(|i| {
                let hash = if i % 2 == 0 { 1 } else { 2 };
                make_meta(i, hash)
            })
            .collect();
        let runs = compress_to_runs(&meta);

        assert_eq!(
            runs.len(),
            10_000,
            "alternating A-B pattern should produce 10K runs (no compression benefit)"
        );

        for (i, run) in runs.iter().enumerate() {
            assert_eq!(
                run.count, 1,
                "each run should have count of 1 for alternating pattern"
            );
            let expected_hash = if i % 2 == 0 { 1 } else { 2 };
            assert_eq!(
                run.signature.hash, expected_hash,
                "run signature should alternate"
            );
        }
    }

    #[test]
    fn mixed_runs_with_varying_lengths() {
        let mut meta = Vec::new();
        let mut row_idx = 0u32;

        for _ in 0..100 {
            meta.push(make_meta(row_idx, 1));
            row_idx += 1;
        }
        for _ in 0..50 {
            meta.push(make_meta(row_idx, 2));
            row_idx += 1;
        }
        for _ in 0..200 {
            meta.push(make_meta(row_idx, 3));
            row_idx += 1;
        }
        for _ in 0..1 {
            meta.push(make_meta(row_idx, 4));
            row_idx += 1;
        }

        let runs = compress_to_runs(&meta);

        assert_eq!(
            runs.len(),
            4,
            "should produce 4 runs for 4 distinct signatures"
        );
        assert_eq!(runs[0].count, 100);
        assert_eq!(runs[1].count, 50);
        assert_eq!(runs[2].count, 200);
        assert_eq!(runs[3].count, 1);
    }

    #[test]
    fn empty_input_produces_empty_runs() {
        let meta: Vec<RowMeta> = vec![];
        let runs = compress_to_runs(&meta);
        assert!(runs.is_empty(), "empty input should produce empty runs");
    }

    #[test]
    fn single_row_produces_single_run() {
        let meta = vec![make_meta(0, 999)];
        let runs = compress_to_runs(&meta);

        assert_eq!(runs.len(), 1);
        assert_eq!(runs[0].count, 1);
        assert_eq!(runs[0].start_row, 0);
        assert_eq!(runs[0].signature.hash, 999);
    }

    #[test]
    fn run_compression_preserves_row_indices() {
        let meta: Vec<RowMeta> = (0..1000u32)
            .map(|i| make_meta(i, (i / 100) as u128))
            .collect();
        let runs = compress_to_runs(&meta);

        assert_eq!(runs.len(), 10, "should have 10 runs (one per 100 rows)");

        for (group_idx, run) in runs.iter().enumerate() {
            let expected_start = (group_idx * 100) as u32;
            assert_eq!(
                run.start_row, expected_start,
                "run {} should start at row {}",
                group_idx, expected_start
            );
            assert_eq!(run.count, 100, "each run should have 100 rows");
        }
    }
}

```

---

### File: `core\src\alignment_types.rs`

```rust
#[derive(Debug, Clone, PartialEq, Eq, Default)]
pub struct RowAlignment {
    pub matched: Vec<(u32, u32)>,
    pub inserted: Vec<u32>,
    pub deleted: Vec<u32>,
    pub moves: Vec<RowBlockMove>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct RowBlockMove {
    pub src_start_row: u32,
    pub dst_start_row: u32,
    pub row_count: u32,
}

```

---

### File: `core\src\bin\wasm_smoke.rs`

```rust
use excel_diff::{
    CallbackSink, CellValue, DiffConfig, DiffSession, Grid, Sheet, SheetKind, Workbook,
    try_diff_workbooks_streaming,
};
use core::hint::black_box;

fn make_workbook(session: &mut DiffSession, value: f64) -> Workbook {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(value)), None);

    let sheet_name = session.strings.intern("WasmSmoke");

    Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn main() {
    let mut session = DiffSession::new();
    let wb_a = make_workbook(&mut session, 1.0);
    let wb_b = make_workbook(&mut session, 2.0);

    let mut op_count = 0usize;
    let mut sink = CallbackSink::new(|_op| op_count += 1);
    if let Ok(summary) = try_diff_workbooks_streaming(
        &wb_a,
        &wb_b,
        &mut session.strings,
        &DiffConfig::default(),
        &mut sink,
    ) {
        black_box(summary.complete);
        black_box(summary.op_count);
        black_box(op_count);
    }
}

```

---

### File: `core\src\capabilities.rs`

```rust
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct EngineFeatures {
    pub vba: bool,
    pub model_diff: bool,
    pub parallel: bool,
    pub std_fs: bool,
}

pub fn engine_features() -> EngineFeatures {
    EngineFeatures {
        vba: cfg!(feature = "vba"),
        model_diff: cfg!(feature = "model-diff"),
        parallel: cfg!(feature = "parallel"),
        std_fs: cfg!(feature = "std-fs"),
    }
}

```

---

### File: `core\src\column_alignment.rs`

```rust
use crate::alignment::align_meta_with_amr;
use crate::config::DiffConfig;
use crate::grid_metadata::{FrequencyClass, RowMeta, classify_row_frequencies};
use crate::grid_view::{ColHash, ColMeta, GridView, HashStats};
use crate::hashing::hash_col_content_unordered_128;
use crate::workbook::{ColSignature, Grid, RowSignature};

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct ColumnAlignment {
    pub(crate) matched: Vec<(u32, u32)>, // (col_idx_a, col_idx_b)
    pub(crate) inserted: Vec<u32>,       // columns present only in B
    pub(crate) deleted: Vec<u32>,        // columns present only in A
    pub(crate) moves: Vec<ColumnBlockMove>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub(crate) struct ColumnBlockMove {
    pub src_start_col: u32,
    pub dst_start_col: u32,
    pub col_count: u32,
}

fn unordered_col_hashes(grid: &Grid) -> Vec<ColHash> {
    let mut col_cells: Vec<Vec<&crate::workbook::Cell>> = vec![Vec::new(); grid.ncols as usize];
    for ((_, col), cell) in grid.iter_cells() {
        let idx = col as usize;
        if idx < col_cells.len() {
            col_cells[idx].push(cell);
        }
    }
    col_cells
        .iter()
        .map(|cells| ColSignature {
            hash: hash_col_content_unordered_128(cells),
        })
        .collect()
}

pub(crate) fn detect_exact_column_block_move(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<ColumnBlockMove> {
    if old.ncols != new.ncols || old.nrows != new.nrows {
        return None;
    }

    if old.ncols == 0 {
        return None;
    }

    if !is_within_size_bounds(old, new, config) {
        return None;
    }

    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);

    let unordered_a = unordered_col_hashes(old);
    let unordered_b = unordered_col_hashes(new);

    let col_meta_a: Vec<ColMeta> = view_a
        .col_meta
        .iter()
        .enumerate()
        .map(|(idx, meta)| ColMeta {
            hash: *unordered_a.get(idx).unwrap_or(&meta.hash),
            ..*meta
        })
        .collect();
    let col_meta_b: Vec<ColMeta> = view_b
        .col_meta
        .iter()
        .enumerate()
        .map(|(idx, meta)| ColMeta {
            hash: *unordered_b.get(idx).unwrap_or(&meta.hash),
            ..*meta
        })
        .collect();

    if view_a.is_blank_dominated() || view_b.is_blank_dominated() {
        return None;
    }

    let stats = HashStats::from_col_meta(&col_meta_a, &col_meta_b);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    let meta_a = &col_meta_a;
    let meta_b = &col_meta_b;
    let n = meta_a.len();

    if meta_a
        .iter()
        .zip(meta_b.iter())
        .all(|(a, b)| a.hash == b.hash)
    {
        return None;
    }

    let prefix = (0..n).find(|&idx| meta_a[idx].hash != meta_b[idx].hash)?;

    let mut suffix_len = 0usize;
    while suffix_len < n.saturating_sub(prefix) {
        let idx_a = n - 1 - suffix_len;
        let idx_b = n - 1 - suffix_len;
        if meta_a[idx_a].hash == meta_b[idx_b].hash {
            suffix_len += 1;
        } else {
            break;
        }
    }
    let tail_start = n - suffix_len;

    let try_candidate = |src_start: usize, dst_start: usize| -> Option<ColumnBlockMove> {
        if src_start >= tail_start || dst_start >= tail_start {
            return None;
        }

        let mut len = 0usize;
        while src_start + len < tail_start && dst_start + len < tail_start {
            if meta_a[src_start + len].hash != meta_b[dst_start + len].hash {
                break;
            }
            len += 1;
        }

        if len == 0 {
            return None;
        }

        let src_end = src_start + len;
        let dst_end = dst_start + len;

        if !(src_end <= dst_start || dst_end <= src_start) {
            return None;
        }

        let mut idx_a = 0usize;
        let mut idx_b = 0usize;

        loop {
            if idx_a == src_start {
                idx_a = src_end;
            }
            if idx_b == dst_start {
                idx_b = dst_end;
            }

            if idx_a >= n && idx_b >= n {
                break;
            }

            if idx_a >= n || idx_b >= n {
                return None;
            }

            if meta_a[idx_a].hash != meta_b[idx_b].hash {
                return None;
            }

            idx_a += 1;
            idx_b += 1;
        }

        for meta in &meta_a[src_start..src_end] {
            if stats.freq_a.get(&meta.hash).copied().unwrap_or(0) != 1
                || stats.freq_b.get(&meta.hash).copied().unwrap_or(0) != 1
            {
                return None;
            }
        }

        Some(ColumnBlockMove {
            src_start_col: meta_a[src_start].col_idx,
            dst_start_col: meta_b[dst_start].col_idx,
            col_count: len as u32,
        })
    };

    if let Some(src_start) =
        (prefix..tail_start).find(|&idx| meta_a[idx].hash == meta_b[prefix].hash)
        && let Some(mv) = try_candidate(src_start, prefix)
    {
        return Some(mv);
    }

    if let Some(dst_start) =
        (prefix..tail_start).find(|&idx| meta_b[idx].hash == meta_a[prefix].hash)
        && let Some(mv) = try_candidate(prefix, dst_start)
    {
        return Some(mv);
    }

    None
}

#[cfg(test)]
pub(crate) fn align_single_column_change(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<ColumnAlignment> {
    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);
    align_single_column_change_from_views(&view_a, &view_b, config)
}

pub(crate) fn align_single_column_change_from_views(
    view_a: &GridView,
    view_b: &GridView,
    config: &DiffConfig,
) -> Option<ColumnAlignment> {
    if !is_within_size_bounds(view_a.source, view_b.source, config) {
        return None;
    }

    if view_a.source.nrows != view_b.source.nrows {
        return None;
    }

    let col_diff = view_b.source.ncols as i64 - view_a.source.ncols as i64;
    if col_diff.abs() != 1 {
        return None;
    }

    let stats = HashStats::from_col_meta(&view_a.col_meta, &view_b.col_meta);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    if col_diff == 1 {
        find_single_gap_alignment(
            &view_a.col_meta,
            &view_b.col_meta,
            &stats,
            ColumnChange::Insert,
        )
    } else {
        find_single_gap_alignment(
            &view_a.col_meta,
            &view_b.col_meta,
            &stats,
            ColumnChange::Delete,
        )
    }
}

pub(crate) fn align_columns_amr_from_views(
    view_a: &GridView,
    view_b: &GridView,
    config: &DiffConfig,
) -> Option<ColumnAlignment> {
    let stats = HashStats::from_col_meta(&view_a.col_meta, &view_b.col_meta);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    if view_a.col_meta.len() == view_b.col_meta.len()
        && view_a
            .col_meta
            .iter()
            .zip(view_b.col_meta.iter())
            .all(|(a, b)| a.hash == b.hash)
    {
        return None;
    }

    let meta_a = build_column_alignment_meta(view_a, config);
    let meta_b = build_column_alignment_meta(view_b, config);

    let alignment = align_meta_with_amr(&meta_a, &meta_b, config)?;
    let moves: Vec<ColumnBlockMove> = alignment
        .moves
        .iter()
        .map(|mv| ColumnBlockMove {
            src_start_col: mv.src_start_row,
            dst_start_col: mv.dst_start_row,
            col_count: mv.row_count,
        })
        .collect();

    let col_alignment = ColumnAlignment {
        matched: alignment.matched,
        inserted: alignment.inserted,
        deleted: alignment.deleted,
        moves,
    };

    let has_structural = !col_alignment.inserted.is_empty() || !col_alignment.deleted.is_empty();
    if has_structural {
        let has_column_edits = col_alignment.matched.iter().any(|(a, b)| {
            let sig_a = view_a.col_meta.get(*a as usize).map(|m| m.hash);
            let sig_b = view_b.col_meta.get(*b as usize).map(|m| m.hash);
            sig_a.is_some() && sig_b.is_some() && sig_a != sig_b
        });
        if has_column_edits {
            return None;
        }
    }

    if col_alignment.moves.is_empty()
        && col_alignment.inserted.is_empty()
        && col_alignment.deleted.is_empty()
    {
        return None;
    }

    Some(col_alignment)
}

fn build_column_alignment_meta(view: &GridView, config: &DiffConfig) -> Vec<RowMeta> {
    let mut meta: Vec<RowMeta> = view
        .col_meta
        .iter()
        .map(|col| RowMeta {
            row_idx: col.col_idx,
            signature: RowSignature { hash: col.hash.hash },
            non_blank_count: col.non_blank_count,
            first_non_blank_col: col.first_non_blank_row,
            frequency_class: FrequencyClass::Common,
            is_low_info: false,
        })
        .collect();

    classify_row_frequencies(&mut meta, config);
    meta
}

enum ColumnChange {
    Insert,
    Delete,
}

fn find_single_gap_alignment(
    cols_a: &[ColMeta],
    cols_b: &[ColMeta],
    stats: &HashStats<ColHash>,
    change: ColumnChange,
) -> Option<ColumnAlignment> {
    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();
    let mut skipped = false;

    let mut idx_a = 0usize;
    let mut idx_b = 0usize;

    while idx_a < cols_a.len() && idx_b < cols_b.len() {
        let meta_a = cols_a[idx_a];
        let meta_b = cols_b[idx_b];

        if meta_a.hash == meta_b.hash {
            matched.push((meta_a.col_idx, meta_b.col_idx));
            idx_a += 1;
            idx_b += 1;
            continue;
        }

        if skipped {
            return None;
        }

        match change {
            ColumnChange::Insert => {
                if !stats.is_unique_to_b(meta_b.hash) {
                    return None;
                }
                inserted.push(meta_b.col_idx);
                idx_b += 1;
            }
            ColumnChange::Delete => {
                if !stats.is_unique_to_a(meta_a.hash) {
                    return None;
                }
                deleted.push(meta_a.col_idx);
                idx_a += 1;
            }
        }

        skipped = true;
    }

    if idx_a < cols_a.len() || idx_b < cols_b.len() {
        if skipped {
            return None;
        }

        match change {
            ColumnChange::Insert if idx_a == cols_a.len() && cols_b.len() == idx_b + 1 => {
                let meta_b = cols_b[idx_b];
                if !stats.is_unique_to_b(meta_b.hash) {
                    return None;
                }
                inserted.push(meta_b.col_idx);
            }
            ColumnChange::Delete if idx_b == cols_b.len() && cols_a.len() == idx_a + 1 => {
                let meta_a = cols_a[idx_a];
                if !stats.is_unique_to_a(meta_a.hash) {
                    return None;
                }
                deleted.push(meta_a.col_idx);
            }
            _ => return None,
        }
    }

    if inserted.len() + deleted.len() != 1 {
        return None;
    }

    let alignment = ColumnAlignment {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    };

    debug_assert!(
        is_monotonic(&alignment.matched),
        "matched pairs must be strictly increasing in both dimensions"
    );

    Some(alignment)
}

fn is_monotonic(pairs: &[(u32, u32)]) -> bool {
    pairs.windows(2).all(|w| w[0].0 < w[1].0 && w[0].1 < w[1].1)
}

fn is_within_size_bounds(old: &Grid, new: &Grid, config: &DiffConfig) -> bool {
    let rows = old.nrows.max(new.nrows);
    let cols = old.ncols.max(new.ncols);
    rows <= config.alignment.max_align_rows && cols <= config.alignment.max_align_cols
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::workbook::CellValue;

    fn grid_from_numbers(rows: &[&[i32]]) -> Grid {
        let nrows = rows.len() as u32;
        let ncols = if nrows == 0 { 0 } else { rows[0].len() as u32 };
        let mut grid = Grid::new(nrows, ncols);

        for (r_idx, row_vals) in rows.iter().enumerate() {
            for (c_idx, value) in row_vals.iter().enumerate() {
                grid.insert_cell(
                    r_idx as u32,
                    c_idx as u32,
                    Some(CellValue::Number(*value as f64)),
                    None,
                );
            }
        }

        grid
    }

    #[test]
    fn single_insert_aligns_all_columns() {
        let base_rows: Vec<Vec<i32>> =
            vec![vec![1, 2, 3, 4], vec![5, 6, 7, 8], vec![9, 10, 11, 12]];
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|r| r.as_slice()).collect();
        let grid_a = grid_from_numbers(&base_refs);

        let inserted_rows: Vec<Vec<i32>> = base_rows
            .iter()
            .enumerate()
            .map(|(idx, row)| {
                let mut new_row = row.clone();
                new_row.insert(2, 100 + idx as i32); // insert at index 2 (0-based)
                new_row
            })
            .collect();
        let inserted_refs: Vec<&[i32]> = inserted_rows.iter().map(|r| r.as_slice()).collect();
        let grid_b = grid_from_numbers(&inserted_refs);

        let alignment = align_single_column_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");

        assert_eq!(alignment.inserted, vec![2]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 4);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[1], (1, 1));
        assert_eq!(alignment.matched[2], (2, 3));
        assert_eq!(alignment.matched[3], (3, 4));
    }

    #[test]
    fn multiple_unique_columns_causes_bailout() {
        let base_rows: Vec<Vec<i32>> = vec![vec![1, 2, 3], vec![4, 5, 6], vec![7, 8, 9]];
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|r| r.as_slice()).collect();
        let grid_a = grid_from_numbers(&base_refs);

        let mut rows_b: Vec<Vec<i32>> = base_rows
            .iter()
            .enumerate()
            .map(|(idx, row)| {
                let mut new_row = row.clone();
                new_row.insert(1, 100 + idx as i32); // inserted column
                new_row
            })
            .collect();
        if let Some(cell) = rows_b.get_mut(1).and_then(|row| row.get_mut(3)) {
            *cell = 999;
        }
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
        let grid_b = grid_from_numbers(&rows_b_refs);

        assert!(align_single_column_change(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn heavy_repetition_causes_bailout() {
        let repetitive_cols = 9;
        let rows: usize = 3;

        let values_a: Vec<Vec<i32>> = (0..rows).map(|_| vec![1; repetitive_cols]).collect();
        let refs_a: Vec<&[i32]> = values_a.iter().map(|r| r.as_slice()).collect();
        let grid_a = grid_from_numbers(&refs_a);

        let values_b: Vec<Vec<i32>> = (0..rows)
            .map(|row_idx| {
                let mut row = vec![1; repetitive_cols];
                row.insert(4, 2 + row_idx as i32);
                row
            })
            .collect();
        let refs_b: Vec<&[i32]> = values_b.iter().map(|r| r.as_slice()).collect();
        let grid_b = grid_from_numbers(&refs_b);

        assert!(align_single_column_change(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn detect_exact_column_block_move_simple_case() {
        let grid_a = grid_from_numbers(&[&[10, 20, 30, 40], &[11, 21, 31, 41]]);

        let grid_b = grid_from_numbers(&[&[10, 30, 40, 20], &[11, 31, 41, 21]]);

        let mv = detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("expected column move found");
        assert_eq!(mv.src_start_col, 1);
        assert_eq!(mv.col_count, 1);
        assert_eq!(mv.dst_start_col, 3);
    }

    #[test]
    fn detect_exact_column_block_move_rejects_internal_edits() {
        let grid_a = grid_from_numbers(&[&[1, 2, 3, 4], &[5, 6, 7, 8], &[9, 10, 11, 12]]);

        let grid_b = grid_from_numbers(&[
            &[1, 3, 4, 2],
            &[5, 7, 8, 6],
            &[9, 11, 12, 999], // edit inside moved column
        ]);

        assert!(detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn detect_exact_column_block_move_rejects_repetition() {
        let grid_a = grid_from_numbers(&[&[1, 1, 2, 2], &[10, 10, 20, 20]]);
        let grid_b = grid_from_numbers(&[&[2, 2, 1, 1], &[20, 20, 10, 10]]);

        assert!(detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn detect_exact_column_block_move_multi_column_block() {
        let grid_a = grid_from_numbers(&[
            &[10, 20, 30, 40, 50, 60],
            &[11, 21, 31, 41, 51, 61],
            &[12, 22, 32, 42, 52, 62],
        ]);

        let grid_b = grid_from_numbers(&[
            &[10, 40, 50, 20, 30, 60],
            &[11, 41, 51, 21, 31, 61],
            &[12, 42, 52, 22, 32, 62],
        ]);

        let mv = detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("expected multi-column move");
        assert_eq!(mv.src_start_col, 3);
        assert_eq!(mv.col_count, 2);
        assert_eq!(mv.dst_start_col, 1);
    }

    #[test]
    fn detect_exact_column_block_move_rejects_two_independent_moves() {
        let grid_a = grid_from_numbers(&[&[10, 20, 30, 40, 50, 60], &[11, 21, 31, 41, 51, 61]]);

        let grid_b = grid_from_numbers(&[&[20, 10, 30, 40, 60, 50], &[21, 11, 31, 41, 61, 51]]);

        assert!(
            detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "two independent column swaps must not be detected as a single block move"
        );
    }

    #[test]
    fn detect_exact_column_block_move_swap_as_single_move() {
        let grid_a = grid_from_numbers(&[&[10, 20, 30, 40], &[11, 21, 31, 41]]);

        let grid_b = grid_from_numbers(&[&[20, 10, 30, 40], &[21, 11, 31, 41]]);

        let mv = detect_exact_column_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("swap of adjacent columns should be detected as single-column move");
        assert_eq!(mv.col_count, 1);
        assert!(
            (mv.src_start_col == 0 && mv.dst_start_col == 1)
                || (mv.src_start_col == 1 && mv.dst_start_col == 0),
            "swap should be represented as moving one column past the other"
        );
    }
}

```

---

### File: `core\src\config.rs`

```rust
//! Configuration for the diff engine.
//!
//! `DiffConfig` centralizes all algorithm thresholds and behavioral knobs
//! to avoid hardcoded constants scattered throughout the codebase.

use serde::{Deserialize, Serialize};
use thiserror::Error;

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum LimitBehavior {
    FallbackToPositional,
    ReturnPartialResult,
    ReturnError,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum SemanticNoisePolicy {
    SuppressFormattingOnly,
    ReportFormattingOnly,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct PreflightConfig {
    /// Preflight: minimum row count to consider short-circuit bailouts.
    /// Grids smaller than this always run full move detection/alignment.
    pub preflight_min_rows: u32,
    /// Preflight: maximum number of in-order row mismatches to trigger near-identical bailout.
    pub preflight_in_order_mismatch_max: u32,
    /// Preflight: minimum ratio of in-order matching rows (0.0..=1.0) for near-identical bailout.
    pub preflight_in_order_match_ratio_min: f64,
    /// Preflight: Jaccard similarity threshold below which grids are considered dissimilar
    /// and move detection/alignment are skipped.
    pub bailout_similarity_threshold: f64,
    pub max_context_rows: u32,
}

impl Default for PreflightConfig {
    fn default() -> Self {
        Self {
            preflight_min_rows: 5000,
            preflight_in_order_mismatch_max: 32,
            preflight_in_order_match_ratio_min: 0.995,
            bailout_similarity_threshold: 0.05,
            max_context_rows: 3,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct AlignmentConfig {
    pub max_align_rows: u32,
    pub max_align_cols: u32,
    pub max_block_gap: u32,
    pub max_hash_repeat: u32,
    #[serde(alias = "rare_frequency_threshold")]
    pub rare_threshold: u32,
    #[serde(alias = "low_info_cell_threshold")]
    pub low_info_threshold: u32,
    /// Row-count threshold for recursive gap alignment. Does not gate masked move detection.
    #[serde(alias = "recursive_threshold")]
    pub recursive_align_threshold: u32,
    pub small_gap_threshold: u32,
    pub max_recursion_depth: u32,
    pub max_lcs_gap_size: u32,
    pub lcs_dp_work_limit: usize,
    pub context_anchor_k1: u32,
    pub context_anchor_k2: u32,
}

impl Default for AlignmentConfig {
    fn default() -> Self {
        Self {
            max_align_rows: 500_000,
            max_align_cols: 16_384,
            max_block_gap: 10_000,
            max_hash_repeat: 8,
            rare_threshold: 5,
            low_info_threshold: 2,
            recursive_align_threshold: 200,
            small_gap_threshold: 50,
            max_recursion_depth: 10,
            max_lcs_gap_size: 1_500,
            lcs_dp_work_limit: 20_000,
            context_anchor_k1: 4,
            context_anchor_k2: 8,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct MoveConfig {
    /// Maximum number of masked move-detection iterations per sheet.
    /// Set to 0 to disable move detection and represent moves as insert/delete.
    pub max_move_iterations: u32,
    pub enable_fuzzy_moves: bool,
    pub fuzzy_similarity_threshold: f64,
    pub max_fuzzy_block_rows: u32,
    pub min_block_size_for_move: u32,
    pub move_extraction_max_slice_len: u32,
    pub move_extraction_max_candidates_per_sig: u32,
    /// Masked move detection runs only when max(old.nrows, new.nrows) <= this.
    pub max_move_detection_rows: u32,
    /// Masked move detection runs only when max(old.ncols, new.ncols) <= this.
    pub max_move_detection_cols: u32,
}

impl Default for MoveConfig {
    fn default() -> Self {
        Self {
            max_move_iterations: 20,
            enable_fuzzy_moves: true,
            fuzzy_similarity_threshold: 0.80,
            max_fuzzy_block_rows: 32,
            min_block_size_for_move: 3,
            move_extraction_max_slice_len: 10_000,
            move_extraction_max_candidates_per_sig: 16,
            max_move_detection_rows: 200,
            max_move_detection_cols: 256,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct SemanticConfig {
    pub enable_m_semantic_diff: bool,
    pub enable_formula_semantic_diff: bool,
    pub enable_dax_semantic_diff: bool,
    /// Policy for handling formatting-only M changes when semantic diff is enabled.
    pub semantic_noise_policy: SemanticNoisePolicy,
    /// When true, emits CellEdited ops even when values are unchanged (diagnostic);
    /// downstream consumers should treat edits as semantic only if from != to.
    pub include_unchanged_cells: bool,
    /// Ratio of differing cells required to emit dense row/rect replacement ops.
    /// Set to 0.0 to disable dense replacement.
    pub dense_row_replace_ratio: f64,
    /// Minimum column count to consider dense row replacement.
    pub dense_row_replace_min_cols: u32,
    /// Minimum consecutive replaced rows to emit a RectReplaced op.
    pub dense_rect_replace_min_rows: u32,
}

impl Default for SemanticConfig {
    fn default() -> Self {
        Self {
            enable_m_semantic_diff: true,
            enable_formula_semantic_diff: false,
            enable_dax_semantic_diff: false,
            semantic_noise_policy: SemanticNoisePolicy::ReportFormattingOnly,
            include_unchanged_cells: false,
            dense_row_replace_ratio: 0.90,
            dense_row_replace_min_cols: 64,
            dense_rect_replace_min_rows: 4,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct HardeningConfig {
    pub on_limit_exceeded: LimitBehavior,
    /// Optional soft cap on estimated memory usage (in MB) for advanced strategies.
    ///
    /// When the estimate exceeds this cap, the engine falls back to positional diff for the
    /// affected sheet and marks the overall diff as incomplete with a warning.
    pub max_memory_mb: Option<u32>,
    /// Optional timeout (in seconds) for the diff engine.
    ///
    /// When exceeded, the engine aborts early, preserving any already-emitted ops, and marks the
    /// result as incomplete with a warning.
    pub timeout_seconds: Option<u32>,
    /// Optional maximum number of operations to emit.
    ///
    /// When the limit is reached, the engine stops emitting further ops and marks the result
    /// as incomplete with a warning. This bounds both time and memory for pathological "everything
    /// changed" cases.
    pub max_ops: Option<usize>,
}

impl Default for HardeningConfig {
    fn default() -> Self {
        Self {
            on_limit_exceeded: LimitBehavior::FallbackToPositional,
            max_memory_mb: None,
            timeout_seconds: None,
            max_ops: None,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(default)]
pub struct DiffConfig {
    #[serde(flatten)]
    pub preflight: PreflightConfig,
    #[serde(flatten)]
    pub alignment: AlignmentConfig,
    #[serde(flatten)]
    pub moves: MoveConfig,
    #[serde(flatten)]
    pub semantic: SemanticConfig,
    #[serde(flatten)]
    pub hardening: HardeningConfig,
}

impl Default for DiffConfig {
    fn default() -> Self {
        Self {
            preflight: PreflightConfig::default(),
            alignment: AlignmentConfig::default(),
            moves: MoveConfig::default(),
            semantic: SemanticConfig::default(),
            hardening: HardeningConfig::default(),
        }
    }
}

impl DiffConfig {
    pub fn fastest() -> Self {
        let mut cfg = Self::default();
        cfg.moves.max_move_iterations = 5;
        cfg.alignment.max_block_gap = 1_000;
        cfg.alignment.small_gap_threshold = 20;
        cfg.alignment.recursive_align_threshold = 80;
        cfg.moves.max_move_detection_rows = 80;
        cfg.moves.enable_fuzzy_moves = false;
        cfg.semantic.enable_m_semantic_diff = false;
        cfg
    }

    pub fn balanced() -> Self {
        Self::default()
    }

    pub fn most_precise() -> Self {
        let mut cfg = Self::default();
        cfg.moves.max_move_iterations = 30;
        cfg.alignment.max_block_gap = 20_000;
        cfg.moves.fuzzy_similarity_threshold = 0.95;
        cfg.alignment.small_gap_threshold = 80;
        cfg.alignment.recursive_align_threshold = 400;
        cfg.semantic.enable_formula_semantic_diff = true;
        cfg.semantic.enable_dax_semantic_diff = true;
        cfg.alignment.max_lcs_gap_size = 1_500;
        cfg.alignment.lcs_dp_work_limit = 20_000;
        cfg.moves.move_extraction_max_slice_len = 10_000;
        cfg.moves.move_extraction_max_candidates_per_sig = 16;
        cfg.moves.max_move_detection_rows = 400;
        cfg.moves.max_move_detection_cols = 256;
        cfg
    }

    pub fn builder() -> DiffConfigBuilder {
        DiffConfigBuilder {
            inner: DiffConfig::default(),
        }
    }

    pub fn validate(&self) -> Result<(), ConfigError> {
        if !self.moves.fuzzy_similarity_threshold.is_finite()
            || self.moves.fuzzy_similarity_threshold < 0.0
            || self.moves.fuzzy_similarity_threshold > 1.0
        {
            return Err(ConfigError::InvalidFuzzySimilarity {
                value: self.moves.fuzzy_similarity_threshold,
            });
        }

        ensure_non_zero_u32(self.alignment.max_align_rows, "max_align_rows")?;
        ensure_non_zero_u32(self.alignment.max_align_cols, "max_align_cols")?;
        ensure_non_zero_u32(self.alignment.max_lcs_gap_size, "max_lcs_gap_size")?;
        ensure_non_zero_u32(
            self.moves.move_extraction_max_slice_len,
            "move_extraction_max_slice_len",
        )?;
        ensure_non_zero_u32(
            self.moves.move_extraction_max_candidates_per_sig,
            "move_extraction_max_candidates_per_sig",
        )?;
        ensure_non_zero_u32(self.alignment.context_anchor_k1, "context_anchor_k1")?;
        ensure_non_zero_u32(self.alignment.context_anchor_k2, "context_anchor_k2")?;
        ensure_non_zero_u32(self.moves.max_move_detection_rows, "max_move_detection_rows")?;
        ensure_non_zero_u32(self.moves.max_move_detection_cols, "max_move_detection_cols")?;
        ensure_non_zero_u32(self.preflight.max_context_rows, "max_context_rows")?;
        ensure_non_zero_u32(self.moves.min_block_size_for_move, "min_block_size_for_move")?;

        if self.alignment.lcs_dp_work_limit == 0 {
            return Err(ConfigError::NonPositiveLimit {
                field: "lcs_dp_work_limit",
                value: 0,
            });
        }

        if !self.preflight.preflight_in_order_match_ratio_min.is_finite()
            || self.preflight.preflight_in_order_match_ratio_min < 0.0
            || self.preflight.preflight_in_order_match_ratio_min > 1.0
        {
            return Err(ConfigError::InvalidPreflightRatio {
                value: self.preflight.preflight_in_order_match_ratio_min,
            });
        }

        if !self.semantic.dense_row_replace_ratio.is_finite()
            || self.semantic.dense_row_replace_ratio < 0.0
            || self.semantic.dense_row_replace_ratio > 1.0
        {
            return Err(ConfigError::InvalidDenseRowReplaceRatio {
                value: self.semantic.dense_row_replace_ratio,
            });
        }

        if !self.preflight.bailout_similarity_threshold.is_finite()
            || self.preflight.bailout_similarity_threshold < 0.0
            || self.preflight.bailout_similarity_threshold > 1.0
        {
            return Err(ConfigError::InvalidBailoutSimilarity {
                value: self.preflight.bailout_similarity_threshold,
            });
        }

        Ok(())
    }
}

#[derive(Debug, Clone, PartialEq, Error)]
pub enum ConfigError {
    #[error("fuzzy_similarity_threshold must be in [0.0, 1.0] and finite (got {value})")]
    InvalidFuzzySimilarity { value: f64 },
    #[error("{field} must be greater than zero (got {value})")]
    NonPositiveLimit { field: &'static str, value: u64 },
    #[error("preflight_in_order_match_ratio_min must be in [0.0, 1.0] and finite (got {value})")]
    InvalidPreflightRatio { value: f64 },
    #[error("dense_row_replace_ratio must be in [0.0, 1.0] and finite (got {value})")]
    InvalidDenseRowReplaceRatio { value: f64 },
    #[error("bailout_similarity_threshold must be in [0.0, 1.0] and finite (got {value})")]
    InvalidBailoutSimilarity { value: f64 },
}

fn ensure_non_zero_u32(value: u32, field: &'static str) -> Result<(), ConfigError> {
    if value == 0 {
        return Err(ConfigError::NonPositiveLimit {
            field,
            value: value as u64,
        });
    }
    Ok(())
}

#[derive(Debug, Clone)]
pub struct DiffConfigBuilder {
    inner: DiffConfig,
}

impl Default for DiffConfigBuilder {
    fn default() -> Self {
        Self::new()
    }
}

impl DiffConfigBuilder {
    pub fn new() -> Self {
        DiffConfig::builder()
    }

    pub fn max_move_iterations(mut self, value: u32) -> Self {
        self.inner.moves.max_move_iterations = value;
        self
    }

    pub fn max_align_rows(mut self, value: u32) -> Self {
        self.inner.alignment.max_align_rows = value;
        self
    }

    pub fn max_align_cols(mut self, value: u32) -> Self {
        self.inner.alignment.max_align_cols = value;
        self
    }

    pub fn max_block_gap(mut self, value: u32) -> Self {
        self.inner.alignment.max_block_gap = value;
        self
    }

    pub fn max_hash_repeat(mut self, value: u32) -> Self {
        self.inner.alignment.max_hash_repeat = value;
        self
    }

    pub fn fuzzy_similarity_threshold(mut self, value: f64) -> Self {
        self.inner.moves.fuzzy_similarity_threshold = value;
        self
    }

    pub fn max_fuzzy_block_rows(mut self, value: u32) -> Self {
        self.inner.moves.max_fuzzy_block_rows = value;
        self
    }

    pub fn rare_threshold(mut self, value: u32) -> Self {
        self.inner.alignment.rare_threshold = value;
        self
    }

    pub fn low_info_threshold(mut self, value: u32) -> Self {
        self.inner.alignment.low_info_threshold = value;
        self
    }

    pub fn recursive_align_threshold(mut self, value: u32) -> Self {
        self.inner.alignment.recursive_align_threshold = value;
        self
    }

    pub fn small_gap_threshold(mut self, value: u32) -> Self {
        self.inner.alignment.small_gap_threshold = value;
        self
    }

    pub fn max_recursion_depth(mut self, value: u32) -> Self {
        self.inner.alignment.max_recursion_depth = value;
        self
    }

    pub fn on_limit_exceeded(mut self, value: LimitBehavior) -> Self {
        self.inner.hardening.on_limit_exceeded = value;
        self
    }

    pub fn enable_fuzzy_moves(mut self, value: bool) -> Self {
        self.inner.moves.enable_fuzzy_moves = value;
        self
    }

    pub fn enable_m_semantic_diff(mut self, value: bool) -> Self {
        self.inner.semantic.enable_m_semantic_diff = value;
        self
    }

    pub fn enable_formula_semantic_diff(mut self, value: bool) -> Self {
        self.inner.semantic.enable_formula_semantic_diff = value;
        self
    }

    pub fn enable_dax_semantic_diff(mut self, value: bool) -> Self {
        self.inner.semantic.enable_dax_semantic_diff = value;
        self
    }

    pub fn semantic_noise_policy(mut self, value: SemanticNoisePolicy) -> Self {
        self.inner.semantic.semantic_noise_policy = value;
        self
    }

    pub fn include_unchanged_cells(mut self, value: bool) -> Self {
        self.inner.semantic.include_unchanged_cells = value;
        self
    }

    pub fn dense_row_replace_ratio(mut self, value: f64) -> Self {
        self.inner.semantic.dense_row_replace_ratio = value;
        self
    }

    pub fn dense_row_replace_min_cols(mut self, value: u32) -> Self {
        self.inner.semantic.dense_row_replace_min_cols = value;
        self
    }

    pub fn dense_rect_replace_min_rows(mut self, value: u32) -> Self {
        self.inner.semantic.dense_rect_replace_min_rows = value;
        self
    }

    pub fn max_context_rows(mut self, value: u32) -> Self {
        self.inner.preflight.max_context_rows = value;
        self
    }

    pub fn min_block_size_for_move(mut self, value: u32) -> Self {
        self.inner.moves.min_block_size_for_move = value;
        self
    }

    pub fn max_lcs_gap_size(mut self, value: u32) -> Self {
        self.inner.alignment.max_lcs_gap_size = value;
        self
    }

    pub fn lcs_dp_work_limit(mut self, value: usize) -> Self {
        self.inner.alignment.lcs_dp_work_limit = value;
        self
    }

    pub fn move_extraction_max_slice_len(mut self, value: u32) -> Self {
        self.inner.moves.move_extraction_max_slice_len = value;
        self
    }

    pub fn move_extraction_max_candidates_per_sig(mut self, value: u32) -> Self {
        self.inner.moves.move_extraction_max_candidates_per_sig = value;
        self
    }

    pub fn context_anchor_k1(mut self, value: u32) -> Self {
        self.inner.alignment.context_anchor_k1 = value;
        self
    }

    pub fn context_anchor_k2(mut self, value: u32) -> Self {
        self.inner.alignment.context_anchor_k2 = value;
        self
    }

    pub fn max_move_detection_rows(mut self, value: u32) -> Self {
        self.inner.moves.max_move_detection_rows = value;
        self
    }

    pub fn max_move_detection_cols(mut self, value: u32) -> Self {
        self.inner.moves.max_move_detection_cols = value;
        self
    }

    pub fn preflight_min_rows(mut self, value: u32) -> Self {
        self.inner.preflight.preflight_min_rows = value;
        self
    }

    pub fn preflight_in_order_mismatch_max(mut self, value: u32) -> Self {
        self.inner.preflight.preflight_in_order_mismatch_max = value;
        self
    }

    pub fn preflight_in_order_match_ratio_min(mut self, value: f64) -> Self {
        self.inner.preflight.preflight_in_order_match_ratio_min = value;
        self
    }

    pub fn bailout_similarity_threshold(mut self, value: f64) -> Self {
        self.inner.preflight.bailout_similarity_threshold = value;
        self
    }

    pub fn max_memory_mb(mut self, value: Option<u32>) -> Self {
        self.inner.hardening.max_memory_mb = value;
        self
    }

    pub fn timeout_seconds(mut self, value: Option<u32>) -> Self {
        self.inner.hardening.timeout_seconds = value;
        self
    }

    pub fn max_ops(mut self, value: Option<usize>) -> Self {
        self.inner.hardening.max_ops = value;
        self
    }

    pub fn build(self) -> Result<DiffConfig, ConfigError> {
        self.inner.validate()?;
        Ok(self.inner)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn defaults_match_limit_spec() {
        let cfg = DiffConfig::default();

        assert_eq!(cfg.alignment.max_align_rows, 500_000);
        assert_eq!(cfg.alignment.max_align_cols, 16_384);
        assert_eq!(cfg.alignment.max_recursion_depth, 10);
        assert!(matches!(
            cfg.hardening.on_limit_exceeded,
            LimitBehavior::FallbackToPositional
        ));

        assert_eq!(cfg.moves.fuzzy_similarity_threshold, 0.80);
        assert_eq!(cfg.moves.min_block_size_for_move, 3);
        assert_eq!(cfg.moves.max_move_iterations, 20);

        assert_eq!(cfg.alignment.recursive_align_threshold, 200);
        assert_eq!(cfg.alignment.small_gap_threshold, 50);
        assert_eq!(cfg.alignment.low_info_threshold, 2);
        assert_eq!(cfg.alignment.rare_threshold, 5);
        assert_eq!(cfg.alignment.max_block_gap, 10_000);

        assert_eq!(cfg.moves.max_move_detection_rows, 200);
        assert_eq!(cfg.moves.max_move_detection_cols, 256);

        assert_eq!(cfg.preflight.preflight_min_rows, 5000);
        assert_eq!(cfg.preflight.preflight_in_order_mismatch_max, 32);
        assert!((cfg.preflight.preflight_in_order_match_ratio_min - 0.995).abs() < f64::EPSILON);
        assert!((cfg.preflight.bailout_similarity_threshold - 0.05).abs() < f64::EPSILON);

        assert_eq!(cfg.hardening.max_memory_mb, None);
        assert_eq!(cfg.hardening.timeout_seconds, None);

        assert!(!cfg.semantic.include_unchanged_cells);
        assert!((cfg.semantic.dense_row_replace_ratio - 0.90).abs() < f64::EPSILON);
        assert_eq!(cfg.semantic.dense_row_replace_min_cols, 64);
        assert_eq!(cfg.semantic.dense_rect_replace_min_rows, 4);
        assert_eq!(cfg.preflight.max_context_rows, 3);

        assert!(cfg.moves.enable_fuzzy_moves);
        assert!(cfg.semantic.enable_m_semantic_diff);
        assert!(!cfg.semantic.enable_formula_semantic_diff);
        assert!(!cfg.semantic.enable_dax_semantic_diff);
        assert!(matches!(
            cfg.semantic.semantic_noise_policy,
            SemanticNoisePolicy::ReportFormattingOnly
        ));
    }

    #[test]
    fn serde_roundtrip_preserves_defaults() {
        let cfg = DiffConfig::default();
        let json = serde_json::to_string(&cfg).expect("serialize default config");
        let parsed: DiffConfig = serde_json::from_str(&json).expect("deserialize default config");
        assert_eq!(cfg, parsed);
    }

    #[test]
    fn serde_flatten_keeps_flat_shape() {
        let cfg = DiffConfig::default();
        let value = serde_json::to_value(&cfg).expect("serialize default config");
        let obj = value
            .as_object()
            .expect("default config should serialize to an object");
        assert!(obj.contains_key("max_align_rows"));
        assert!(!obj.contains_key("alignment"));
        assert!(!obj.contains_key("moves"));
        assert!(!obj.contains_key("preflight"));
        assert!(!obj.contains_key("semantic"));
        assert!(!obj.contains_key("hardening"));
    }

    #[test]
    fn serde_aliases_populate_fields() {
        let json = r#"{
            "rare_frequency_threshold": 9,
            "low_info_cell_threshold": 3,
            "recursive_threshold": 123
        }"#;
        let cfg: DiffConfig = serde_json::from_str(json).expect("deserialize with aliases");
        assert_eq!(cfg.alignment.rare_threshold, 9);
        assert_eq!(cfg.alignment.low_info_threshold, 3);
        assert_eq!(cfg.alignment.recursive_align_threshold, 123);
    }

    #[test]
    fn builder_rejects_invalid_similarity_threshold() {
        let err = DiffConfig::builder()
            .fuzzy_similarity_threshold(2.0)
            .build()
            .expect_err("builder should reject invalid probability");
        assert!(matches!(
            err,
            ConfigError::InvalidFuzzySimilarity { value } if (value - 2.0).abs() < f64::EPSILON
        ));
    }

    #[test]
    fn presets_differ_in_expected_directions() {
        let fastest = DiffConfig::fastest();
        let balanced = DiffConfig::balanced();
        let precise = DiffConfig::most_precise();

        assert!(!fastest.moves.enable_fuzzy_moves);
        assert!(!fastest.semantic.enable_m_semantic_diff);
        assert!(precise.moves.max_move_iterations >= balanced.moves.max_move_iterations);
        assert!(precise.alignment.max_block_gap >= balanced.alignment.max_block_gap);
        assert!(
            precise.moves.fuzzy_similarity_threshold >= balanced.moves.fuzzy_similarity_threshold
        );
    }

    #[test]
    fn most_precise_matches_sprint_plan_values() {
        let cfg = DiffConfig::most_precise();
        assert_eq!(cfg.moves.fuzzy_similarity_threshold, 0.95);
        assert!(cfg.semantic.enable_formula_semantic_diff);
        assert!(cfg.semantic.enable_dax_semantic_diff);
    }

    #[test]
    fn builder_rejects_invalid_preflight_ratio() {
        let err = DiffConfig::builder()
            .preflight_in_order_match_ratio_min(1.5)
            .build()
            .expect_err("builder should reject invalid preflight ratio");
        assert!(matches!(
            err,
            ConfigError::InvalidPreflightRatio { value } if (value - 1.5).abs() < f64::EPSILON
        ));

        let err = DiffConfig::builder()
            .preflight_in_order_match_ratio_min(-0.1)
            .build()
            .expect_err("builder should reject negative preflight ratio");
        assert!(matches!(err, ConfigError::InvalidPreflightRatio { .. }));
    }

    #[test]
    fn builder_rejects_invalid_dense_row_replace_ratio() {
        let err = DiffConfig::builder()
            .dense_row_replace_ratio(2.0)
            .build()
            .expect_err("builder should reject invalid dense row replace ratio");
        assert!(matches!(
            err,
            ConfigError::InvalidDenseRowReplaceRatio { value } if (value - 2.0).abs() < f64::EPSILON
        ));

        let err = DiffConfig::builder()
            .dense_row_replace_ratio(-0.5)
            .build()
            .expect_err("builder should reject negative dense row replace ratio");
        assert!(matches!(err, ConfigError::InvalidDenseRowReplaceRatio { .. }));
    }

    #[test]
    fn builder_rejects_invalid_bailout_similarity() {
        let err = DiffConfig::builder()
            .bailout_similarity_threshold(2.0)
            .build()
            .expect_err("builder should reject invalid bailout similarity");
        assert!(matches!(
            err,
            ConfigError::InvalidBailoutSimilarity { value } if (value - 2.0).abs() < f64::EPSILON
        ));

        let err = DiffConfig::builder()
            .bailout_similarity_threshold(-0.5)
            .build()
            .expect_err("builder should reject negative bailout similarity");
        assert!(matches!(err, ConfigError::InvalidBailoutSimilarity { .. }));
    }

    #[test]
    fn preflight_config_builder_setters_work() {
        let cfg = DiffConfig::builder()
            .preflight_min_rows(10000)
            .preflight_in_order_mismatch_max(64)
            .preflight_in_order_match_ratio_min(0.99)
            .bailout_similarity_threshold(0.10)
            .max_memory_mb(Some(64))
            .timeout_seconds(Some(5))
            .dense_row_replace_ratio(0.75)
            .dense_row_replace_min_cols(16)
            .dense_rect_replace_min_rows(2)
            .semantic_noise_policy(SemanticNoisePolicy::SuppressFormattingOnly)
            .build()
            .expect("valid config should build");

        assert_eq!(cfg.preflight.preflight_min_rows, 10000);
        assert_eq!(cfg.preflight.preflight_in_order_mismatch_max, 64);
        assert!((cfg.preflight.preflight_in_order_match_ratio_min - 0.99).abs() < f64::EPSILON);
        assert!((cfg.preflight.bailout_similarity_threshold - 0.10).abs() < f64::EPSILON);
        assert_eq!(cfg.hardening.max_memory_mb, Some(64));
        assert_eq!(cfg.hardening.timeout_seconds, Some(5));
        assert!((cfg.semantic.dense_row_replace_ratio - 0.75).abs() < f64::EPSILON);
        assert_eq!(cfg.semantic.dense_row_replace_min_cols, 16);
        assert_eq!(cfg.semantic.dense_rect_replace_min_rows, 2);
        assert!(matches!(
            cfg.semantic.semantic_noise_policy,
            SemanticNoisePolicy::SuppressFormattingOnly
        ));
    }
}

```

---

### File: `core\src\container.rs`

```rust
//! ZIP container handling.
//!
//! Provides abstraction over ZIP-based packages and validates OPC
//! requirements for Office Open XML containers.

use std::io::{Read, Seek};
use thiserror::Error;
use zip::ZipArchive;
use zip::result::ZipError;

use crate::error_codes;

#[derive(Debug, Clone, Copy)]
pub struct ContainerLimits {
    pub max_entries: usize,
    pub max_part_uncompressed_bytes: u64,
    pub max_total_uncompressed_bytes: u64,
}

impl Default for ContainerLimits {
    fn default() -> Self {
        Self {
            max_entries: 10_000,
            max_part_uncompressed_bytes: 100 * 1024 * 1024,
            max_total_uncompressed_bytes: 500 * 1024 * 1024,
        }
    }
}

#[derive(Debug, Error)]
#[non_exhaustive]
pub enum ContainerError {
    #[error("[EXDIFF_CTR_001] I/O error: {0}. Suggestion: check the file path and permissions.")]
    Io(#[from] std::io::Error),
    #[error("[EXDIFF_CTR_002] ZIP error: {0}. Suggestion: verify the file is a valid .xlsx and not corrupt.")]
    Zip(String),
    #[error("[EXDIFF_CTR_003] not a ZIP container. Suggestion: verify the input is a ZIP-based .xlsx file.")]
    NotZipContainer,
    #[error("[EXDIFF_CTR_004] not an OPC package (missing [Content_Types].xml). Suggestion: verify the file is a valid .xlsx workbook.")]
    NotOpcPackage,
    #[error("[EXDIFF_CTR_005] archive has too many entries: {entries} (limit: {max_entries}). Suggestion: possible ZIP bomb; increase limits only for trusted files.")]
    TooManyEntries { entries: usize, max_entries: usize },
    #[error("[EXDIFF_CTR_006] part '{path}' is too large: {size} bytes (limit: {limit} bytes). Suggestion: possible ZIP bomb; increase limits only for trusted files.")]
    PartTooLarge { path: String, size: u64, limit: u64 },
    #[error("[EXDIFF_CTR_007] total uncompressed size exceeds limit: would exceed {limit} bytes. Suggestion: possible ZIP bomb; increase limits only for trusted files.")]
    TotalTooLarge { limit: u64 },
    #[error("[EXDIFF_CTR_002] failed to read ZIP entry '{path}': {reason}. Suggestion: the file may be corrupt or truncated.")]
    ZipRead { path: String, reason: String },
    #[error("[EXDIFF_CTR_002] file not found in archive: {path}. Suggestion: the file may be corrupt or incomplete.")]
    FileNotFound { path: String },
}

impl ContainerError {
    pub fn code(&self) -> &'static str {
        match self {
            ContainerError::Io(_) => error_codes::CONTAINER_IO,
            ContainerError::Zip(_) => error_codes::CONTAINER_ZIP,
            ContainerError::NotZipContainer => error_codes::CONTAINER_NOT_ZIP,
            ContainerError::NotOpcPackage => error_codes::CONTAINER_NOT_OPC,
            ContainerError::TooManyEntries { .. } => error_codes::CONTAINER_TOO_MANY_ENTRIES,
            ContainerError::PartTooLarge { .. } => error_codes::CONTAINER_PART_TOO_LARGE,
            ContainerError::TotalTooLarge { .. } => error_codes::CONTAINER_TOTAL_TOO_LARGE,
            ContainerError::ZipRead { .. } => error_codes::CONTAINER_ZIP,
            ContainerError::FileNotFound { .. } => error_codes::CONTAINER_ZIP,
        }
    }
}

pub(crate) trait ReadSeek: Read + Seek {}
impl<T: Read + Seek> ReadSeek for T {}

pub struct ZipContainer {
    archive: ZipArchive<Box<dyn ReadSeek>>,
    limits: ContainerLimits,
    total_read: u64,
}

impl ZipContainer {
    pub fn open_from_reader<R: Read + Seek + 'static>(
        reader: R,
    ) -> Result<Self, ContainerError> {
        Self::open_from_reader_with_limits(reader, ContainerLimits::default())
    }

    pub fn open_from_reader_with_limits<R: Read + Seek + 'static>(
        reader: R,
        limits: ContainerLimits,
    ) -> Result<Self, ContainerError> {
        let reader: Box<dyn ReadSeek> = Box::new(reader);
        let archive = ZipArchive::new(reader).map_err(|err| match err {
            ZipError::InvalidArchive(_) | ZipError::UnsupportedArchive(_) => {
                ContainerError::NotZipContainer
            }
            ZipError::Io(e) => match e.kind() {
                std::io::ErrorKind::UnexpectedEof | std::io::ErrorKind::InvalidData => {
                    ContainerError::NotZipContainer
                }
                _ => ContainerError::Io(e),
            },
            other => ContainerError::Zip(other.to_string()),
        })?;

        if archive.len() > limits.max_entries {
            return Err(ContainerError::TooManyEntries {
                entries: archive.len(),
                max_entries: limits.max_entries,
            });
        }

        Ok(Self {
            archive,
            limits,
            total_read: 0,
        })
    }

    #[cfg(feature = "std-fs")]
    pub fn open_from_path(
        path: impl AsRef<std::path::Path>,
    ) -> Result<Self, ContainerError> {
        Self::open_from_path_with_limits(path, ContainerLimits::default())
    }

    #[cfg(feature = "std-fs")]
    pub fn open_from_path_with_limits(
        path: impl AsRef<std::path::Path>,
        limits: ContainerLimits,
    ) -> Result<Self, ContainerError> {
        let file = std::fs::File::open(path)?;
        Self::open_from_reader_with_limits(file, limits)
    }

    #[cfg(feature = "std-fs")]
    pub fn open(path: impl AsRef<std::path::Path>) -> Result<Self, ContainerError> {
        Self::open_from_path(path)
    }

    pub fn read_file(&mut self, name: &str) -> Result<Vec<u8>, ZipError> {
        let mut file = self.archive.by_name(name)?;
        let mut buf = Vec::new();
        file.read_to_end(&mut buf)?;
        Ok(buf)
    }

    pub fn read_file_checked(&mut self, name: &str) -> Result<Vec<u8>, ContainerError> {
        let size = {
            let file = self.archive.by_name(name).map_err(|e| match e {
                ZipError::FileNotFound => ContainerError::FileNotFound {
                    path: name.to_string(),
                },
                ZipError::Io(io_err) => ContainerError::ZipRead {
                    path: name.to_string(),
                    reason: io_err.to_string(),
                },
                other => ContainerError::ZipRead {
                    path: name.to_string(),
                    reason: other.to_string(),
                },
            })?;
            file.size()
        };

        if size > self.limits.max_part_uncompressed_bytes {
            return Err(ContainerError::PartTooLarge {
                path: name.to_string(),
                size,
                limit: self.limits.max_part_uncompressed_bytes,
            });
        }

        let new_total = self.total_read.saturating_add(size);
        if new_total > self.limits.max_total_uncompressed_bytes {
            return Err(ContainerError::TotalTooLarge {
                limit: self.limits.max_total_uncompressed_bytes,
            });
        }

        let mut file = self.archive.by_name(name).map_err(|e| ContainerError::ZipRead {
            path: name.to_string(),
            reason: e.to_string(),
        })?;

        let mut buf = Vec::new();
        file.read_to_end(&mut buf).map_err(|e| ContainerError::ZipRead {
            path: name.to_string(),
            reason: e.to_string(),
        })?;

        self.total_read = new_total;
        Ok(buf)
    }

    pub fn read_file_optional(&mut self, name: &str) -> Result<Option<Vec<u8>>, std::io::Error> {
        match self.read_file(name) {
            Ok(bytes) => Ok(Some(bytes)),
            Err(ZipError::FileNotFound) => Ok(None),
            Err(ZipError::Io(e)) => Err(e),
            Err(e) => Err(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                e.to_string(),
            )),
        }
    }

    pub fn read_file_optional_checked(
        &mut self,
        name: &str,
    ) -> Result<Option<Vec<u8>>, ContainerError> {
        match self.read_file_checked(name) {
            Ok(bytes) => Ok(Some(bytes)),
            Err(ContainerError::FileNotFound { .. }) => Ok(None),
            Err(e) => Err(e),
        }
    }

    pub fn file_names(&self) -> impl Iterator<Item = &str> + '_ {
        self.archive.file_names()
    }

    pub fn len(&self) -> usize {
        self.archive.len()
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    pub fn limits(&self) -> &ContainerLimits {
        &self.limits
    }
}

pub struct OpcContainer {
    inner: ZipContainer,
}

impl OpcContainer {
    pub fn open_from_reader<R: Read + Seek + 'static>(
        reader: R,
    ) -> Result<OpcContainer, ContainerError> {
        Self::open_from_reader_with_limits(reader, ContainerLimits::default())
    }

    pub fn open_from_reader_with_limits<R: Read + Seek + 'static>(
        reader: R,
        limits: ContainerLimits,
    ) -> Result<OpcContainer, ContainerError> {
        let mut inner = ZipContainer::open_from_reader_with_limits(reader, limits)?;

        match inner.archive.by_name("[Content_Types].xml") {
            Ok(file) => {
                let size = file.size();
                if size > inner.limits.max_part_uncompressed_bytes {
                    return Err(ContainerError::PartTooLarge {
                        path: "[Content_Types].xml".to_string(),
                        size,
                        limit: inner.limits.max_part_uncompressed_bytes,
                    });
                }
            }
            Err(ZipError::FileNotFound) => return Err(ContainerError::NotOpcPackage),
            Err(ZipError::Io(e)) => return Err(ContainerError::Io(e)),
            Err(other) => return Err(ContainerError::Zip(other.to_string())),
        }

        Ok(Self { inner })
    }

    #[cfg(feature = "std-fs")]
    pub fn open_from_path(
        path: impl AsRef<std::path::Path>,
    ) -> Result<OpcContainer, ContainerError> {
        Self::open_from_path_with_limits(path, ContainerLimits::default())
    }

    #[cfg(feature = "std-fs")]
    pub fn open_from_path_with_limits(
        path: impl AsRef<std::path::Path>,
        limits: ContainerLimits,
    ) -> Result<OpcContainer, ContainerError> {
        let file = std::fs::File::open(path)?;
        Self::open_from_reader_with_limits(file, limits)
    }

    #[cfg(feature = "std-fs")]
    pub fn open(path: impl AsRef<std::path::Path>) -> Result<OpcContainer, ContainerError> {
        Self::open_from_path(path)
    }

    pub fn read_file(&mut self, name: &str) -> Result<Vec<u8>, ZipError> {
        self.inner.read_file(name)
    }

    pub fn read_file_checked(&mut self, name: &str) -> Result<Vec<u8>, ContainerError> {
        self.inner.read_file_checked(name)
    }

    pub fn read_file_optional(&mut self, name: &str) -> Result<Option<Vec<u8>>, std::io::Error> {
        self.inner.read_file_optional(name)
    }

    pub fn read_file_optional_checked(
        &mut self,
        name: &str,
    ) -> Result<Option<Vec<u8>>, ContainerError> {
        self.inner.read_file_optional_checked(name)
    }

    pub fn file_names(&self) -> impl Iterator<Item = &str> + '_ {
        self.inner.file_names()
    }

    pub fn len(&self) -> usize {
        self.inner.len()
    }

    pub fn is_empty(&self) -> bool {
        self.inner.is_empty()
    }

    pub fn limits(&self) -> &ContainerLimits {
        self.inner.limits()
    }
}

#[cfg(test)]
mod tests {
    use super::ZipContainer;
    use std::io::{Cursor, Write};
    use zip::CompressionMethod;
    use zip::write::FileOptions;
    use zip::ZipWriter;

    fn make_zip(entries: &[(&str, &str)]) -> Vec<u8> {
        let mut buf = Vec::new();
        {
            let cursor = Cursor::new(&mut buf);
            let mut writer = ZipWriter::new(cursor);
            let options = FileOptions::default().compression_method(CompressionMethod::Stored);
            for (name, contents) in entries {
                writer.start_file(*name, options).expect("start zip entry");
                writer
                    .write_all(contents.as_bytes())
                    .expect("write zip entry");
            }
            writer.finish().expect("finish zip");
        }
        buf
    }

    #[test]
    fn zip_container_opens_non_opc_zip() {
        let bytes = make_zip(&[("hello.txt", "world")]);
        let cursor = Cursor::new(bytes);
        let result = ZipContainer::open_from_reader(cursor);
        assert!(result.is_ok(), "ZipContainer should open non-OPC ZIPs");
    }
}

```

---

### File: `core\src\database_alignment.rs`

```rust
use crate::hashing::normalize_float_for_hash;
use crate::string_pool::{StringId, StringPool};
use crate::workbook::{CellValue, Grid};
use std::collections::{HashMap, HashSet};

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct KeyColumnSpec {
    pub columns: Vec<u32>,
}

impl KeyColumnSpec {
    pub fn new(columns: Vec<u32>) -> KeyColumnSpec {
        KeyColumnSpec { columns }
    }

    #[cfg(test)]
    pub fn is_key_column(&self, col: u32) -> bool {
        self.columns.contains(&col)
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub(crate) enum KeyValueRepr {
    None,
    Number(u64),
    Text(StringId),
    Bool(bool),
    Error(StringId),
}

impl KeyValueRepr {
    fn from_cell_value(value: Option<&CellValue>) -> KeyValueRepr {
        match value {
            Some(CellValue::Number(n)) => KeyValueRepr::Number(normalize_float_for_hash(*n)),
            Some(CellValue::Text(id)) => KeyValueRepr::Text(*id),
            Some(CellValue::Bool(b)) => KeyValueRepr::Bool(*b),
            Some(CellValue::Blank) => KeyValueRepr::None,
            Some(CellValue::Error(id)) => KeyValueRepr::Error(*id),
            None => KeyValueRepr::None,
        }
    }

    fn to_cell_value(&self) -> Option<CellValue> {
        match self {
            KeyValueRepr::None => None,
            KeyValueRepr::Number(bits) => Some(CellValue::Number(f64::from_bits(*bits))),
            KeyValueRepr::Text(id) => Some(CellValue::Text(*id)),
            KeyValueRepr::Bool(b) => Some(CellValue::Bool(*b)),
            KeyValueRepr::Error(id) => Some(CellValue::Error(*id)),
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub(crate) struct KeyValue {
    components: Vec<KeyValueRepr>,
}

impl KeyValue {
    fn new(components: Vec<KeyValueRepr>) -> KeyValue {
        KeyValue { components }
    }

    pub(crate) fn as_cell_values(&self) -> Vec<Option<CellValue>> {
        self.components
            .iter()
            .map(KeyValueRepr::to_cell_value)
            .collect()
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct KeyedRow {
    pub key: KeyValue,
    pub row_idx: u32,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct KeyedAlignment {
    pub matched_rows: Vec<(u32, u32)>, // (row_idx_a, row_idx_b)
    pub left_only_rows: Vec<u32>,
    pub right_only_rows: Vec<u32>,
    pub duplicate_clusters: Vec<DuplicateKeyCluster>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct DuplicateKeyCluster {
    pub key: KeyValue,
    pub left_rows: Vec<u32>,
    pub right_rows: Vec<u32>,
}

pub(crate) fn diff_table_by_key(
    old: &Grid,
    new: &Grid,
    key_columns: &[u32],
) -> KeyedAlignment {
    let spec = KeyColumnSpec::new(key_columns.to_vec());
    let (left_rows, left_lookup) = build_keyed_rows(old, &spec);
    let (right_rows, right_lookup) = build_keyed_rows(new, &spec);

    let mut matched_rows = Vec::new();
    let mut left_only_rows = Vec::new();
    let mut right_only_rows = Vec::new();
    let mut duplicate_clusters = Vec::new();

    let mut ordered_keys: Vec<KeyValue> = Vec::new();
    let mut seen_keys: HashSet<KeyValue> = HashSet::new();

    for row in &left_rows {
        if seen_keys.insert(row.key.clone()) {
            ordered_keys.push(row.key.clone());
        }
    }

    for row in &right_rows {
        if seen_keys.insert(row.key.clone()) {
            ordered_keys.push(row.key.clone());
        }
    }

    for key in ordered_keys {
        let left = left_lookup.get(&key).cloned().unwrap_or_default();
        let right = right_lookup.get(&key).cloned().unwrap_or_default();

        let left_dupe = left.len() > 1;
        let right_dupe = right.len() > 1;

        if left_dupe || right_dupe {
            duplicate_clusters.push(DuplicateKeyCluster {
                key,
                left_rows: left,
                right_rows: right,
            });
            continue;
        }

        match (left.first().copied(), right.first().copied()) {
            (Some(l), Some(r)) => matched_rows.push((l, r)),
            (Some(l), None) => left_only_rows.push(l),
            (None, Some(r)) => right_only_rows.push(r),
            (None, None) => {}
        }
    }

    KeyedAlignment {
        matched_rows,
        left_only_rows,
        right_only_rows,
        duplicate_clusters,
    }
}

fn build_keyed_rows(
    grid: &Grid,
    spec: &KeyColumnSpec,
) -> (Vec<KeyedRow>, HashMap<KeyValue, Vec<u32>>) {
    let mut rows = Vec::with_capacity(grid.nrows as usize);
    let mut lookup = HashMap::new();

    for row_idx in 0..grid.nrows {
        let key = extract_key(grid, row_idx, spec);
        lookup.entry(key.clone()).or_insert_with(Vec::new).push(row_idx);
        rows.push(KeyedRow { key, row_idx });
    }

    (rows, lookup)
}

fn extract_key(grid: &Grid, row_idx: u32, spec: &KeyColumnSpec) -> KeyValue {
    let mut components = Vec::with_capacity(spec.columns.len());

    for &col in &spec.columns {
        let value = grid
            .get(row_idx, col)
            .map(|cell| KeyValueRepr::from_cell_value(cell.value.as_ref()))
            .unwrap_or(KeyValueRepr::None);
        components.push(value);
    }

    KeyValue::new(components)
}

pub fn suggest_key_columns(grid: &Grid, pool: &StringPool) -> Vec<u32> {
    if grid.nrows == 0 || grid.ncols == 0 {
        return Vec::new();
    }

    let header_matches_key_pattern = |col: u32| -> bool {
        if let Some(cell) = grid.get(0, col) {
            if let Some(CellValue::Text(id)) = &cell.value {
                let text = pool.resolve(*id).trim().to_lowercase();
                return text == "id" || text == "key" || text == "sku" 
                    || text.contains("_id") || text.ends_with("id");
            }
        }
        false
    };

    let data_start = if grid.nrows > 1 { 1 } else { 0 };
    let data_rows = grid.nrows.saturating_sub(data_start);
    if data_rows == 0 {
        return Vec::new();
    }

    let column_non_empty = |col: u32| -> bool {
        for row in data_start..grid.nrows {
            let value = grid
                .get(row, col)
                .map(|cell| KeyValueRepr::from_cell_value(cell.value.as_ref()))
                .unwrap_or(KeyValueRepr::None);
            if matches!(value, KeyValueRepr::None) {
                return false;
            }
        }
        true
    };

    let column_has_unique_values = |col: u32| -> bool {
        let mut seen: HashSet<KeyValueRepr> = HashSet::new();
        for row in data_start..grid.nrows {
            let value = grid
                .get(row, col)
                .map(|cell| KeyValueRepr::from_cell_value(cell.value.as_ref()))
                .unwrap_or(KeyValueRepr::None);
            if !seen.insert(value) {
                return false;
            }
        }
        true
    };

    let column_unique_ratio = |col: u32| -> f64 {
        let mut seen: HashSet<KeyValueRepr> = HashSet::new();
        for row in data_start..grid.nrows {
            let value = grid
                .get(row, col)
                .map(|cell| KeyValueRepr::from_cell_value(cell.value.as_ref()))
                .unwrap_or(KeyValueRepr::None);
            seen.insert(value);
        }
        if data_rows == 0 {
            return 0.0;
        }
        seen.len() as f64 / data_rows as f64
    };

    let mut unique_cols = Vec::new();
    let mut header_unique = Vec::new();
    let mut header_candidates = Vec::new();
    for col in 0..grid.ncols {
        if !column_non_empty(col) {
            continue;
        }
        if header_matches_key_pattern(col) {
            header_candidates.push(col);
        }
        if column_has_unique_values(col) {
            unique_cols.push(col);
            if header_matches_key_pattern(col) {
                header_unique.push(col);
            }
        }
    }

    if header_unique.len() == 1 {
        return vec![header_unique[0]];
    }
    if header_unique.len() > 1 {
        return Vec::new();
    }

    let min_unique_ratio = 0.5;
    let mut candidates: Vec<u32> = (0..grid.ncols)
        .filter(|&col| column_non_empty(col) && column_unique_ratio(col) >= min_unique_ratio)
        .collect();

    if candidates.is_empty() {
        return Vec::new();
    }

    if !header_candidates.is_empty() {
        candidates.retain(|col| header_candidates.contains(col) || !unique_cols.contains(col));
    }

    candidates.sort_unstable();
    const MAX_CANDIDATES: usize = 6;
    if candidates.len() > MAX_CANDIDATES {
        candidates.truncate(MAX_CANDIDATES);
    }

    let composite_unique = |cols: &[u32]| -> bool {
        let mut seen: HashSet<Vec<KeyValueRepr>> = HashSet::new();
        for row in data_start..grid.nrows {
            let mut key = Vec::with_capacity(cols.len());
            for &col in cols {
                let value = grid
                    .get(row, col)
                    .map(|cell| KeyValueRepr::from_cell_value(cell.value.as_ref()))
                    .unwrap_or(KeyValueRepr::None);
                if matches!(value, KeyValueRepr::None) {
                    return false;
                }
                key.push(value);
            }
            if !seen.insert(key) {
                return false;
            }
        }
        true
    };

    let mut unique_pairs = Vec::new();
    for i in 0..candidates.len() {
        for j in (i + 1)..candidates.len() {
            let cols = vec![candidates[i], candidates[j]];
            if composite_unique(&cols) {
                unique_pairs.push(cols);
            }
        }
    }

    if !header_candidates.is_empty() {
        let mut preferred_pairs = Vec::new();
        for pair in &unique_pairs {
            if pair.iter().any(|col| header_candidates.contains(col)) {
                preferred_pairs.push(pair.clone());
            }
        }
        if preferred_pairs.len() == 1 {
            return preferred_pairs.swap_remove(0);
        }
        if preferred_pairs.len() > 1 {
            return Vec::new();
        }
    }

    if unique_cols.len() == 1 {
        return vec![unique_cols[0]];
    }
    if unique_cols.len() > 1 {
        return Vec::new();
    }

    if unique_pairs.len() == 1 {
        return unique_pairs.swap_remove(0);
    }
    if unique_pairs.len() > 1 {
        return Vec::new();
    }

    Vec::new()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::string_pool::StringPool;
    use crate::workbook::CellValue;

    fn grid_from_rows(rows: &[&[i32]]) -> Grid {
        let nrows = rows.len() as u32;
        let ncols = if nrows == 0 { 0 } else { rows[0].len() as u32 };
        let mut grid = Grid::new(nrows, ncols);

        for (r_idx, row_vals) in rows.iter().enumerate() {
            for (c_idx, value) in row_vals.iter().enumerate() {
                grid.insert_cell(
                    r_idx as u32,
                    c_idx as u32,
                    Some(CellValue::Number(*value as f64)),
                    None,
                );
            }
        }

        grid
    }

    #[test]
    fn unique_keys_reorder_no_changes() {
        let grid_a = grid_from_rows(&[&[1, 10], &[2, 20], &[3, 30]]);
        let grid_b = grid_from_rows(&[&[3, 30], &[1, 10], &[2, 20]]);

        let alignment = diff_table_by_key(&grid_a, &grid_b, &[0]);
        assert_eq!(
            alignment.matched_rows,
            vec![(0, 1), (1, 2), (2, 0)],
            "all keys should align regardless of order"
        );
        assert!(alignment.left_only_rows.is_empty());
        assert!(alignment.right_only_rows.is_empty());
        assert!(alignment.duplicate_clusters.is_empty());
    }

    #[test]
    fn unique_keys_insert_delete_classified() {
        let grid_a = grid_from_rows(&[&[1, 10], &[2, 20]]);
        let grid_b = grid_from_rows(&[&[1, 10], &[2, 20], &[3, 30]]);

        let alignment = diff_table_by_key(&grid_a, &grid_b, &[0]);
        assert_eq!(alignment.matched_rows, vec![(0, 0), (1, 1)]);
        assert!(alignment.left_only_rows.is_empty());
        assert_eq!(alignment.right_only_rows, vec![2]);
        assert!(alignment.duplicate_clusters.is_empty());
    }

    #[test]
    fn duplicate_keys_form_cluster() {
        let grid_a = grid_from_rows(&[&[1, 10], &[1, 99]]);
        let grid_b = grid_from_rows(&[&[1, 10], &[1, 100]]);

        let alignment = diff_table_by_key(&grid_a, &grid_b, &[0]);
        assert_eq!(alignment.duplicate_clusters.len(), 1);
        let cluster = &alignment.duplicate_clusters[0];
        assert_eq!(cluster.left_rows, vec![0, 1]);
        assert_eq!(cluster.right_rows, vec![0, 1]);
    }

    #[test]
    fn composite_key_alignment_matches_rows_correctly() {
        let grid_a = grid_from_rows(&[&[1, 10, 100], &[1, 20, 200], &[2, 10, 300]]);
        let grid_b = grid_from_rows(&[&[1, 20, 200], &[2, 10, 300], &[1, 10, 100]]);

        let alignment =
            diff_table_by_key(&grid_a, &grid_b, &[0, 1]);

        assert!(
            alignment.left_only_rows.is_empty(),
            "no left-only rows expected"
        );
        assert!(
            alignment.right_only_rows.is_empty(),
            "no right-only rows expected"
        );

        let mut matched = alignment.matched_rows.clone();
        matched.sort_unstable();

        let mut expected = vec![(0, 2), (1, 0), (2, 1)];
        expected.sort_unstable();

        assert_eq!(
            matched, expected,
            "composite keys should align rows sharing the same key tuple regardless of order"
        );
    }

    #[test]
    fn non_contiguous_key_columns_alignment() {
        let grid_a = grid_from_rows(&[&[1, 999, 10, 100], &[1, 888, 20, 200], &[2, 777, 10, 300]]);
        let grid_b = grid_from_rows(&[&[2, 777, 10, 300], &[1, 999, 10, 100], &[1, 888, 20, 200]]);

        let alignment =
            diff_table_by_key(&grid_a, &grid_b, &[0, 2]);

        assert!(alignment.left_only_rows.is_empty());
        assert!(alignment.right_only_rows.is_empty());

        let mut matched = alignment.matched_rows.clone();
        matched.sort_unstable();

        let mut expected = vec![(0, 1), (1, 2), (2, 0)];
        expected.sort_unstable();

        assert_eq!(
            matched, expected,
            "non-contiguous key columns [0,2] should align correctly"
        );
    }

    #[test]
    fn three_column_composite_key_alignment() {
        let grid_a = grid_from_rows(&[
            &[1, 10, 100, 1000],
            &[1, 10, 200, 2000],
            &[1, 20, 100, 3000],
            &[2, 10, 100, 4000],
        ]);
        let grid_b = grid_from_rows(&[
            &[2, 10, 100, 4000],
            &[1, 20, 100, 3000],
            &[1, 10, 200, 2000],
            &[1, 10, 100, 1000],
        ]);

        let alignment =
            diff_table_by_key(&grid_a, &grid_b, &[0, 1, 2]);

        assert!(alignment.left_only_rows.is_empty());
        assert!(alignment.right_only_rows.is_empty());

        let mut matched = alignment.matched_rows.clone();
        matched.sort_unstable();

        let mut expected = vec![(0, 3), (1, 2), (2, 1), (3, 0)];
        expected.sort_unstable();

        assert_eq!(
            matched, expected,
            "three-column composite keys should align correctly"
        );
    }

    #[test]
    fn is_key_column_single_column() {
        let spec = KeyColumnSpec::new(vec![0]);
        assert!(spec.is_key_column(0), "column 0 should be a key column");
        assert!(
            !spec.is_key_column(1),
            "column 1 should not be a key column"
        );
        assert!(
            !spec.is_key_column(2),
            "column 2 should not be a key column"
        );
    }

    #[test]
    fn is_key_column_contiguous_columns() {
        let spec = KeyColumnSpec::new(vec![0, 1]);
        assert!(spec.is_key_column(0), "column 0 should be a key column");
        assert!(spec.is_key_column(1), "column 1 should be a key column");
        assert!(
            !spec.is_key_column(2),
            "column 2 should not be a key column"
        );
        assert!(
            !spec.is_key_column(3),
            "column 3 should not be a key column"
        );
    }

    #[test]
    fn is_key_column_non_contiguous_columns() {
        let spec = KeyColumnSpec::new(vec![0, 2]);
        assert!(spec.is_key_column(0), "column 0 should be a key column");
        assert!(
            !spec.is_key_column(1),
            "column 1 should not be a key column"
        );
        assert!(spec.is_key_column(2), "column 2 should be a key column");
        assert!(
            !spec.is_key_column(3),
            "column 3 should not be a key column"
        );
    }

    #[test]
    fn is_key_column_three_columns() {
        let spec = KeyColumnSpec::new(vec![0, 1, 2]);
        assert!(spec.is_key_column(0));
        assert!(spec.is_key_column(1));
        assert!(spec.is_key_column(2));
        assert!(!spec.is_key_column(3));
    }

    #[test]
    fn is_key_column_non_contiguous_three_columns() {
        let spec = KeyColumnSpec::new(vec![1, 3, 5]);
        assert!(
            !spec.is_key_column(0),
            "column 0 should not be a key column"
        );
        assert!(spec.is_key_column(1), "column 1 should be a key column");
        assert!(
            !spec.is_key_column(2),
            "column 2 should not be a key column"
        );
        assert!(spec.is_key_column(3), "column 3 should be a key column");
        assert!(
            !spec.is_key_column(4),
            "column 4 should not be a key column"
        );
        assert!(spec.is_key_column(5), "column 5 should be a key column");
        assert!(
            !spec.is_key_column(6),
            "column 6 should not be a key column"
        );
    }

    #[test]
    fn suggest_key_columns_prefers_composite_when_single_not_unique() {
        let mut pool = StringPool::new();
        let mut grid = Grid::new(4, 3);

        let headers = ["Country", "CustomerID", "Amount"];
        for (col, header) in headers.iter().enumerate() {
            grid.insert_cell(
                0,
                col as u32,
                Some(CellValue::Text(pool.intern(header))),
                None,
            );
        }

        let rows = [
            ("US", 1.0, 100.0),
            ("US", 2.0, 200.0),
            ("CA", 1.0, 300.0),
        ];
        for (idx, (country, customer, amount)) in rows.iter().enumerate() {
            let row = (idx + 1) as u32;
            grid.insert_cell(
                row,
                0,
                Some(CellValue::Text(pool.intern(country))),
                None,
            );
            grid.insert_cell(row, 1, Some(CellValue::Number(*customer)), None);
            grid.insert_cell(row, 2, Some(CellValue::Number(*amount)), None);
        }

        let keys = suggest_key_columns(&grid, &pool);
        assert_eq!(keys, vec![0, 1], "composite key should be inferred");
    }

    #[test]
    fn suggest_key_columns_returns_empty_on_ambiguous_unique_columns() {
        let mut pool = StringPool::new();
        let mut grid = Grid::new(3, 2);

        let headers = ["ID", "SKU"];
        for (col, header) in headers.iter().enumerate() {
            grid.insert_cell(
                0,
                col as u32,
                Some(CellValue::Text(pool.intern(header))),
                None,
            );
        }

        let values = [(1.0, 10.0), (2.0, 11.0)];
        for (idx, (id, sku)) in values.iter().enumerate() {
            let row = (idx + 1) as u32;
            grid.insert_cell(row, 0, Some(CellValue::Number(*id)), None);
            grid.insert_cell(row, 1, Some(CellValue::Number(*sku)), None);
        }

        let keys = suggest_key_columns(&grid, &pool);
        assert!(keys.is_empty(), "multiple unique headers should be ambiguous");
    }
}

```

---

### File: `core\src\datamashup.rs`

```rust
//! High-level DataMashup (Power Query) parsing and query extraction.
//!
//! Builds on the low-level framing and package parsing to provide structured
//! access to queries, permissions, and metadata stored in Excel DataMashup sections.

use std::collections::HashMap;

use crate::datamashup_framing::{DataMashupError, RawDataMashup};
use crate::datamashup_package::{PackageParts, parse_package_parts};
use crate::m_section::{SectionParseError, parse_section_members};
use crate::permission_bindings::{
    DpapiDecryptor, PermissionBindingsStatus, default_dpapi_decryptor, effective_permissions,
    validate_permission_bindings,
};
use quick_xml::Reader;
use quick_xml::events::Event;

#[derive(Debug, Clone, PartialEq, Eq)]
#[non_exhaustive]
pub struct DataMashup {
    pub version: u32,
    pub package_parts: PackageParts,
    pub permissions: Permissions,
    pub metadata: Metadata,
    pub permission_bindings_raw: Vec<u8>,
    pub permission_bindings_status: PermissionBindingsStatus,
}

impl DataMashup {
    pub fn new(
        version: u32,
        package_parts: PackageParts,
        permissions: Permissions,
        metadata: Metadata,
        permission_bindings_raw: Vec<u8>,
        permission_bindings_status: PermissionBindingsStatus,
    ) -> Self {
        Self {
            version,
            package_parts,
            permissions,
            metadata,
            permission_bindings_raw,
            permission_bindings_status,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Permissions {
    pub can_evaluate_future_packages: bool,
    pub firewall_enabled: bool,
    pub workbook_group_type: Option<String>,
}

impl Default for Permissions {
    fn default() -> Self {
        Permissions {
            can_evaluate_future_packages: false,
            firewall_enabled: true,
            workbook_group_type: None,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Metadata {
    pub formulas: Vec<QueryMetadata>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct QueryMetadata {
    pub item_path: String,
    pub section_name: String,
    pub formula_name: String,
    pub load_to_sheet: bool,
    pub load_to_model: bool,
    pub is_connection_only: bool,
    pub group_path: Option<String>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Query {
    pub name: String,
    pub section_member: String,
    pub expression_m: String,
    pub metadata: QueryMetadata,
}

pub fn build_data_mashup(raw: &RawDataMashup) -> Result<DataMashup, DataMashupError> {
    let decryptor = default_dpapi_decryptor();
    build_data_mashup_with_decryptor(raw, decryptor)
}

pub fn build_data_mashup_with_decryptor(
    raw: &RawDataMashup,
    decryptor: &dyn DpapiDecryptor,
) -> Result<DataMashup, DataMashupError> {
    let package_parts = parse_package_parts(&raw.package_parts)?;
    let parsed_permissions = parse_permissions(&raw.permissions);
    let permission_bindings_status = validate_permission_bindings(raw, decryptor);
    let permissions = effective_permissions(parsed_permissions, permission_bindings_status);
    let metadata = parse_metadata(&raw.metadata)?;

    Ok(DataMashup {
        version: raw.version,
        package_parts,
        permissions,
        metadata,
        permission_bindings_raw: raw.permission_bindings.clone(),
        permission_bindings_status,
    })
}

pub fn build_queries(dm: &DataMashup) -> Result<Vec<Query>, SectionParseError> {
    let members = parse_section_members(&dm.package_parts.main_section.source)?;

    let mut metadata_index: HashMap<(String, String), QueryMetadata> = HashMap::new();
    for meta in &dm.metadata.formulas {
        metadata_index.insert(
            (meta.section_name.clone(), meta.formula_name.clone()),
            meta.clone(),
        );
    }

    let mut positions: HashMap<String, usize> = HashMap::new();
    let mut queries = Vec::new();

    for member in members {
        let section_name = member.section_name.clone();
        let member_name = member.member_name.clone();
        let key = (section_name.clone(), member_name.clone());
        let metadata = metadata_index
            .get(&key)
            .cloned()
            .unwrap_or_else(|| QueryMetadata {
                item_path: format!("{}/{}", section_name, member_name),
                section_name: section_name.clone(),
                formula_name: member_name.clone(),
                load_to_sheet: false,
                load_to_model: false,
                is_connection_only: true,
                group_path: None,
            });

        let name = format!("{}/{}", section_name, member_name);
        let query = Query {
            name: name.clone(),
            section_member: member.member_name,
            expression_m: member.expression_m,
            metadata,
        };

        if let Some(idx) = positions.get(&name) {
            debug_assert!(
                false,
                "duplicate query name '{}' found in DataMashup section; \
                 later definition will overwrite earlier one",
                name
            );
            queries[*idx] = query;
        } else {
            positions.insert(name, queries.len());
            queries.push(query);
        }
    }

    Ok(queries)
}

pub fn build_embedded_queries(dm: &DataMashup) -> Vec<Query> {
    let mut queries: Vec<Query> = Vec::new();
    let mut positions: HashMap<String, usize> = HashMap::new();

    for embedded in &dm.package_parts.embedded_contents {
        let members = match parse_section_members(&embedded.section.source) {
            Ok(m) => m,
            Err(_) => continue,
        };

        for member in members {
            let section_name = format!("Embedded/{}/{}", embedded.name, member.section_name);
            let name = format!("{}/{}", section_name, member.member_name);

            let q = Query {
                name: name.clone(),
                section_member: member.member_name.clone(),
                expression_m: member.expression_m,
                metadata: QueryMetadata {
                    item_path: name.clone(),
                    section_name,
                    formula_name: member.member_name,
                    load_to_sheet: false,
                    load_to_model: false,
                    is_connection_only: true,
                    group_path: None,
                },
            };

            if let Some(idx) = positions.get(&q.name).copied() {
                queries[idx] = q;
            } else {
                positions.insert(q.name.clone(), queries.len());
                queries.push(q);
            }
        }
    }

    queries
}

pub fn parse_permissions(xml_bytes: &[u8]) -> Permissions {
    if xml_bytes.is_empty() {
        return Permissions::default();
    }

    let Ok(mut text) = String::from_utf8(xml_bytes.to_vec()) else {
        return Permissions::default();
    };
    if let Some(stripped) = text.strip_prefix('\u{FEFF}') {
        text = stripped.to_string();
    }

    let mut reader = Reader::from_str(&text);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut current_tag: Option<String> = None;
    let mut permissions = Permissions::default();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) => {
                current_tag =
                    Some(String::from_utf8_lossy(local_name(e.name().as_ref())).to_string());
            }
            Ok(Event::Text(t)) => {
                if let Some(tag) = current_tag.as_deref() {
                    let value = match t.unescape() {
                        Ok(v) => v.into_owned(),
                        Err(_) => {
                            // Any unescape failure means the permissions payload is unusable; fall back to defaults.
                            return Permissions::default();
                        }
                    };
                    match tag {
                        "CanEvaluateFuturePackages" => {
                            if let Some(v) = parse_bool(&value) {
                                permissions.can_evaluate_future_packages = v;
                            }
                        }
                        "FirewallEnabled" => {
                            if let Some(v) = parse_bool(&value) {
                                permissions.firewall_enabled = v;
                            }
                        }
                        "WorkbookGroupType" => {
                            let trimmed = value.trim();
                            if !trimmed.is_empty() {
                                permissions.workbook_group_type = Some(trimmed.to_string());
                            }
                        }
                        _ => {}
                    }
                }
            }
            Ok(Event::CData(t)) => {
                if let Some(tag) = current_tag.as_deref() {
                    let value = String::from_utf8_lossy(&t.into_inner()).to_string();
                    match tag {
                        "CanEvaluateFuturePackages" => {
                            if let Some(v) = parse_bool(&value) {
                                permissions.can_evaluate_future_packages = v;
                            }
                        }
                        "FirewallEnabled" => {
                            if let Some(v) = parse_bool(&value) {
                                permissions.firewall_enabled = v;
                            }
                        }
                        "WorkbookGroupType" => {
                            let trimmed = value.trim();
                            if !trimmed.is_empty() {
                                permissions.workbook_group_type = Some(trimmed.to_string());
                            }
                        }
                        _ => {}
                    }
                }
            }
            Ok(Event::End(_)) => current_tag = None,
            Ok(Event::Eof) => break,
            Err(_) => return Permissions::default(),
            _ => {}
        }
        buf.clear();
    }

    permissions
}

pub fn parse_metadata(metadata_bytes: &[u8]) -> Result<Metadata, DataMashupError> {
    if metadata_bytes.is_empty() {
        return Ok(Metadata {
            formulas: Vec::new(),
        });
    }

    let xml_bytes = metadata_xml_bytes(metadata_bytes)?;
    let mut text = String::from_utf8(xml_bytes)
        .map_err(|_| DataMashupError::XmlError("metadata is not valid UTF-8".into()))?;
    if let Some(stripped) = text.strip_prefix('\u{FEFF}') {
        text = stripped.to_string();
    }

    let mut reader = Reader::from_str(&text);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();

    let mut element_stack: Vec<String> = Vec::new();
    let mut item_type: Option<String> = None;
    let mut item_path: Option<String> = None;
    let mut entries: Vec<(String, String)> = Vec::new();
    let mut formulas: Vec<QueryMetadata> = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Empty(e)) => {
                let name = String::from_utf8_lossy(local_name(e.name().as_ref())).to_string();
                if name == "Entry"
                    && let Some((typ, val)) = parse_entry_attributes(&e)?
                {
                    entries.push((typ, val));
                }
            }
            Ok(Event::Start(e)) => {
                let name = String::from_utf8_lossy(local_name(e.name().as_ref())).to_string();
                if name == "Item" {
                    item_type = None;
                    item_path = None;
                    entries.clear();
                }
                if name == "Entry"
                    && let Some((typ, val)) = parse_entry_attributes(&e)?
                {
                    entries.push((typ, val));
                }
                element_stack.push(name);
            }
            Ok(Event::Text(t)) => {
                if let Some(tag) = element_stack.last() {
                    let value = t
                        .unescape()
                        .map_err(|e| DataMashupError::XmlError(e.to_string()))?
                        .into_owned();
                    match tag.as_str() {
                        "ItemType" => {
                            item_type = Some(value.trim().to_string());
                        }
                        "ItemPath" => {
                            item_path = Some(value.trim().to_string());
                        }
                        _ => {}
                    }
                }
            }
            Ok(Event::CData(t)) => {
                if let Some(tag) = element_stack.last() {
                    let value = String::from_utf8_lossy(&t.into_inner()).to_string();
                    match tag.as_str() {
                        "ItemType" => {
                            item_type = Some(value.trim().to_string());
                        }
                        "ItemPath" => {
                            item_path = Some(value.trim().to_string());
                        }
                        _ => {}
                    }
                }
            }
            Ok(Event::End(e)) => {
                let name_bytes = local_name(e.name().as_ref()).to_vec();
                if name_bytes.as_slice() == b"Item" && item_type.as_deref() == Some("Formula") {
                    let raw_path = item_path.clone().ok_or_else(|| {
                        DataMashupError::XmlError("Formula item missing ItemPath".into())
                    })?;
                    let decoded_path = decode_item_path(&raw_path)?;
                    let (section_name, formula_name) = split_item_path(&decoded_path)?;
                    let load_to_sheet =
                        entry_bool(&entries, &["FillEnabled", "LoadEnabled"]).unwrap_or(false);
                    let load_to_model = entry_bool(
                        &entries,
                        &[
                            "FillToDataModelEnabled",
                            "AddedToDataModel",
                            "LoadToDataModel",
                        ],
                    )
                    .unwrap_or(false);
                    // Group paths are derived solely from per-formula entries for now; the AllFormulas tree is not parsed yet.
                    let group_path = entry_string(
                        &entries,
                        &[
                            "QueryGroupId",
                            "QueryGroupID",
                            "QueryGroupPath",
                            "QueryGroup",
                        ],
                    );

                    let metadata = QueryMetadata {
                        item_path: decoded_path.clone(),
                        section_name,
                        formula_name,
                        load_to_sheet,
                        load_to_model,
                        is_connection_only: !(load_to_sheet || load_to_model),
                        group_path,
                    };
                    formulas.push(metadata);
                }

                if let Some(last) = element_stack.last()
                    && last.as_bytes() == name_bytes.as_slice()
                {
                    element_stack.pop();
                }
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(DataMashupError::XmlError(e.to_string())),
            _ => {}
        }

        buf.clear();
    }

    Ok(Metadata { formulas })
}

fn metadata_xml_bytes(metadata_bytes: &[u8]) -> Result<Vec<u8>, DataMashupError> {
    if looks_like_xml(metadata_bytes) {
        return Ok(metadata_bytes.to_vec());
    }

    if metadata_bytes.len() >= 8 {
        let content_len_bytes: [u8; 4] = metadata_bytes[0..4]
            .try_into()
            .map_err(|_| DataMashupError::InvalidHeader("cannot read content length".into()))?;
        let content_len = u32::from_le_bytes(content_len_bytes) as usize;

        let xml_len_bytes: [u8; 4] = metadata_bytes[4..8]
            .try_into()
            .map_err(|_| DataMashupError::InvalidHeader("cannot read XML length".into()))?;
        let xml_len = u32::from_le_bytes(xml_len_bytes) as usize;

        let start = 8usize
            .checked_add(content_len)
            .ok_or_else(|| DataMashupError::InvalidHeader("metadata length overflow".into()))?;
        let end = start
            .checked_add(xml_len)
            .ok_or_else(|| DataMashupError::InvalidHeader("metadata length overflow".into()))?;
        if end <= metadata_bytes.len() {
            return Ok(metadata_bytes[start..end].to_vec());
        }
        return Err(DataMashupError::InvalidHeader(
            "metadata length prefix invalid".into(),
        ));
    }

    Err(DataMashupError::XmlError("metadata XML not found".into()))
}

fn looks_like_xml(bytes: &[u8]) -> bool {
    let mut idx = 0;
    while idx < bytes.len() && bytes[idx].is_ascii_whitespace() {
        idx += 1;
    }

    if idx >= bytes.len() {
        return false;
    }

    let slice = &bytes[idx..];
    slice.starts_with(b"<")
        || slice.starts_with(&[0xEF, 0xBB, 0xBF])
        || slice.starts_with(&[0xFE, 0xFF])
        || slice.starts_with(&[0xFF, 0xFE])
}

fn local_name(name: &[u8]) -> &[u8] {
    match name.iter().rposition(|&b| b == b':') {
        Some(idx) => name.get(idx + 1..).unwrap_or(name),
        None => name,
    }
}

fn parse_bool(text: &str) -> Option<bool> {
    let trimmed = text.trim();
    let payload = trimmed
        .strip_prefix(|c| c == 'l' || c == 'L')
        .unwrap_or(trimmed);
    let lowered = payload.to_ascii_lowercase();
    match lowered.as_str() {
        "1" | "true" | "yes" => Some(true),
        "0" | "false" | "no" => Some(false),
        _ => None,
    }
}

fn parse_entry_attributes(
    e: &quick_xml::events::BytesStart<'_>,
) -> Result<Option<(String, String)>, DataMashupError> {
    let mut typ: Option<String> = None;
    let mut value: Option<String> = None;

    for attr in e.attributes().with_checks(false) {
        let attr = attr.map_err(|e| DataMashupError::XmlError(e.to_string()))?;
        let key = local_name(attr.key.as_ref());
        if key == b"Type" {
            typ = Some(
                String::from_utf8(attr.value.as_ref().to_vec())
                    .map_err(|e| DataMashupError::XmlError(e.to_string()))?,
            );
        } else if key == b"Value" {
            value = Some(
                String::from_utf8(attr.value.as_ref().to_vec())
                    .map_err(|e| DataMashupError::XmlError(e.to_string()))?,
            );
        }
    }

    match (typ, value) {
        (Some(t), Some(v)) => Ok(Some((t, v))),
        _ => Ok(None),
    }
}

fn entry_bool(entries: &[(String, String)], keys: &[&str]) -> Option<bool> {
    for (key, val) in entries {
        if keys.iter().any(|k| k.eq_ignore_ascii_case(key))
            && let Some(b) = parse_bool(val)
        {
            return Some(b);
        }
    }
    None
}

fn entry_string(entries: &[(String, String)], keys: &[&str]) -> Option<String> {
    for (key, val) in entries {
        if keys.iter().any(|k| k.eq_ignore_ascii_case(key)) {
            let trimmed = val.trim();
            let without_prefix = trimmed
                .strip_prefix('s')
                .or_else(|| trimmed.strip_prefix('S'))
                .unwrap_or(trimmed);
            if without_prefix.is_empty() {
                return None;
            }
            return Some(without_prefix.to_string());
        }
    }
    None
}

fn decode_item_path(path: &str) -> Result<String, DataMashupError> {
    let mut decoded = Vec::with_capacity(path.len());
    let bytes = path.as_bytes();
    let mut idx = 0;
    while idx < bytes.len() {
        let b = bytes[idx];
        if b == b'%' {
            if idx + 2 >= bytes.len() {
                return Err(DataMashupError::XmlError(
                    "invalid percent-encoding in ItemPath".into(),
                ));
            }
            let hi = hex_value(bytes[idx + 1]).ok_or_else(|| {
                DataMashupError::XmlError("invalid percent-encoding in ItemPath".into())
            })?;
            let lo = hex_value(bytes[idx + 2]).ok_or_else(|| {
                DataMashupError::XmlError("invalid percent-encoding in ItemPath".into())
            })?;
            decoded.push(hi << 4 | lo);
            idx += 3;
            continue;
        }
        decoded.push(b);
        idx += 1;
    }
    String::from_utf8(decoded)
        .map_err(|_| DataMashupError::XmlError("invalid UTF-8 in ItemPath".into()))
}

fn hex_value(b: u8) -> Option<u8> {
    match b {
        b'0'..=b'9' => Some(b - b'0'),
        b'a'..=b'f' => Some(10 + b - b'a'),
        b'A'..=b'F' => Some(10 + b - b'A'),
        _ => None,
    }
}

fn split_item_path(path: &str) -> Result<(String, String), DataMashupError> {
    let mut parts = path.split('/');
    let section = parts.next().unwrap_or_default();
    let rest: Vec<&str> = parts.collect();
    if section.is_empty() || rest.is_empty() {
        return Err(DataMashupError::XmlError(
            "invalid ItemPath in metadata".into(),
        ));
    }
    let formula = rest.join("/");
    Ok((section.to_string(), formula))
}

```

---

### File: `core\src\datamashup_framing.rs`

```rust
use base64::Engine;
use base64::engine::general_purpose::STANDARD;
use quick_xml::Reader;
use quick_xml::events::Event;
use thiserror::Error;

use crate::error_codes;

#[derive(Debug, Error)]
#[non_exhaustive]
pub enum DataMashupError {
    #[error("[EXDIFF_DM_001] base64 decoding failed. Suggestion: the workbook may be corrupt; re-save the file in Excel.")]
    Base64Invalid,
    #[error("[EXDIFF_DM_002] unsupported DataMashup version: {0}. Suggestion: update excel_diff or re-save the file in Excel.")]
    UnsupportedVersion(u32),
    #[error("[EXDIFF_DM_003] invalid framing structure. Suggestion: the workbook may be corrupt.")]
    FramingInvalid,
    #[error("[EXDIFF_DM_004] XML parse error: {0}. Suggestion: re-save the file in Excel.")]
    XmlError(String),
    #[error("[EXDIFF_DM_006] invalid header: {0}. Suggestion: the workbook may be corrupt.")]
    InvalidHeader(String),
    #[error("[EXDIFF_DM_005] inner package part too large: '{path}' ({size} bytes, limit {limit} bytes). Suggestion: possible nested ZIP bomb; increase limits only for trusted files.")]
    InnerPartTooLarge { path: String, size: u64, limit: u64 },
    #[error("[EXDIFF_DM_007] inner package has too many entries: {entries} (limit: {max_entries}). Suggestion: possible nested ZIP bomb; increase limits only for trusted files.")]
    InnerTooManyEntries { entries: usize, max_entries: usize },
    #[error("[EXDIFF_DM_008] inner package total uncompressed size exceeds limit: would exceed {limit} bytes. Suggestion: possible nested ZIP bomb; increase limits only for trusted files.")]
    InnerTotalTooLarge { limit: u64 },
}

impl DataMashupError {
    pub fn code(&self) -> &'static str {
        match self {
            DataMashupError::Base64Invalid => error_codes::DM_BASE64_INVALID,
            DataMashupError::UnsupportedVersion(_) => error_codes::DM_UNSUPPORTED_VERSION,
            DataMashupError::FramingInvalid => error_codes::DM_FRAMING_INVALID,
            DataMashupError::XmlError(_) => error_codes::DM_XML_ERROR,
            DataMashupError::InvalidHeader(_) => error_codes::DM_INVALID_HEADER,
            DataMashupError::InnerPartTooLarge { .. } => error_codes::DM_INNER_PART_TOO_LARGE,
            DataMashupError::InnerTooManyEntries { .. } => error_codes::DM_INNER_TOO_MANY_ENTRIES,
            DataMashupError::InnerTotalTooLarge { .. } => error_codes::DM_INNER_TOTAL_TOO_LARGE,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct RawDataMashup {
    pub version: u32,
    pub package_parts: Vec<u8>,
    pub permissions: Vec<u8>,
    pub metadata: Vec<u8>,
    pub permission_bindings: Vec<u8>,
}

pub fn parse_data_mashup(bytes: &[u8]) -> Result<RawDataMashup, DataMashupError> {
    const MIN_SIZE: usize = 4 + 4 * 4;
    if bytes.len() < MIN_SIZE {
        return Err(DataMashupError::FramingInvalid);
    }

    let mut offset: usize = 0;
    let version = read_u32_at(bytes, offset).ok_or(DataMashupError::FramingInvalid)?;
    offset += 4;

    if version != 0 {
        return Err(DataMashupError::UnsupportedVersion(version));
    }

    let package_parts_len = read_length(bytes, offset)?;
    offset += 4;
    let package_parts = take_segment(bytes, &mut offset, package_parts_len)?;

    let permissions_len = read_length(bytes, offset)?;
    offset += 4;
    let permissions = take_segment(bytes, &mut offset, permissions_len)?;

    let metadata_len = read_length(bytes, offset)?;
    offset += 4;
    let metadata = take_segment(bytes, &mut offset, metadata_len)?;

    let permission_bindings_len = read_length(bytes, offset)?;
    offset += 4;
    let permission_bindings = take_segment(bytes, &mut offset, permission_bindings_len)?;

    if offset != bytes.len() {
        return Err(DataMashupError::FramingInvalid);
    }

    Ok(RawDataMashup {
        version,
        package_parts,
        permissions,
        metadata,
        permission_bindings,
    })
}

pub fn read_datamashup_text(xml: &[u8]) -> Result<Option<String>, DataMashupError> {
    let utf8_xml = decode_datamashup_xml(xml)?;

    let mut reader = Reader::from_reader(utf8_xml.as_deref().unwrap_or(xml));
    reader.config_mut().trim_text(false);
    let mut buf = Vec::new();
    let mut in_datamashup = false;
    let mut found_content: Option<String> = None;
    let mut content = String::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if is_datamashup_element(e.name().as_ref()) => {
                if in_datamashup || found_content.is_some() {
                    return Err(DataMashupError::FramingInvalid);
                }
                in_datamashup = true;
                content.clear();
            }
            Ok(Event::Text(t)) if in_datamashup => {
                let text = t
                    .unescape()
                    .map_err(|e| DataMashupError::XmlError(e.to_string()))?
                    .into_owned();
                content.push_str(&text);
            }
            Ok(Event::CData(t)) if in_datamashup => {
                let data = t.into_inner();
                content.push_str(&String::from_utf8_lossy(&data));
            }
            Ok(Event::End(e)) if is_datamashup_element(e.name().as_ref()) => {
                if !in_datamashup {
                    return Err(DataMashupError::FramingInvalid);
                }
                in_datamashup = false;
                found_content = Some(content.clone());
            }
            Ok(Event::Eof) if in_datamashup => {
                return Err(DataMashupError::FramingInvalid);
            }
            Ok(Event::Eof) => return Ok(found_content),
            Err(e) => return Err(DataMashupError::XmlError(e.to_string())),
            _ => {}
        }
        buf.clear();
    }
}

pub fn decode_datamashup_base64(text: &str) -> Result<Vec<u8>, DataMashupError> {
    let cleaned: String = text.split_whitespace().collect();
    STANDARD
        .decode(cleaned.as_bytes())
        .map_err(|_| DataMashupError::Base64Invalid)
}

pub(crate) fn decode_datamashup_xml(xml: &[u8]) -> Result<Option<Vec<u8>>, DataMashupError> {
    if xml.starts_with(&[0xFF, 0xFE]) {
        return Ok(Some(decode_utf16_xml(xml, true, true)?));
    }
    if xml.starts_with(&[0xFE, 0xFF]) {
        return Ok(Some(decode_utf16_xml(xml, false, true)?));
    }

    decode_declared_utf16_without_bom(xml)
}

fn decode_declared_utf16_without_bom(xml: &[u8]) -> Result<Option<Vec<u8>>, DataMashupError> {
    let attempt_decode = |little_endian| -> Result<Option<Vec<u8>>, DataMashupError> {
        if !looks_like_utf16(xml, little_endian) {
            return Ok(None);
        }
        let decoded = decode_utf16_xml(xml, little_endian, false)?;
        let lower = String::from_utf8_lossy(&decoded).to_ascii_lowercase();
        if lower.contains("encoding=\"utf-16\"") || lower.contains("encoding='utf-16'") {
            Ok(Some(decoded))
        } else {
            Ok(None)
        }
    };

    if let Some(decoded) = attempt_decode(true)? {
        return Ok(Some(decoded));
    }
    attempt_decode(false)
}

fn looks_like_utf16(xml: &[u8], little_endian: bool) -> bool {
    if xml.len() < 4 {
        return false;
    }

    if little_endian {
        xml[0] == b'<' && xml[1] == 0 && xml[2] == b'?' && xml[3] == 0
    } else {
        xml[0] == 0 && xml[1] == b'<' && xml[2] == 0 && xml[3] == b'?'
    }
}

fn decode_utf16_xml(
    xml: &[u8],
    little_endian: bool,
    has_bom: bool,
) -> Result<Vec<u8>, DataMashupError> {
    let start = if has_bom { 2 } else { 0 };
    let body = xml
        .get(start..)
        .ok_or_else(|| DataMashupError::XmlError("invalid UTF-16 XML".into()))?;
    if body.len() % 2 != 0 {
        return Err(DataMashupError::XmlError(
            "invalid UTF-16 byte length".into(),
        ));
    }

    let mut code_units = Vec::with_capacity(body.len() / 2);
    for chunk in body.chunks_exact(2) {
        let unit = if little_endian {
            u16::from_le_bytes([chunk[0], chunk[1]])
        } else {
            u16::from_be_bytes([chunk[0], chunk[1]])
        };
        code_units.push(unit);
    }

    let utf8 = String::from_utf16(&code_units)
        .map_err(|_| DataMashupError::XmlError("invalid UTF-16 XML".into()))?;
    Ok(utf8.into_bytes())
}

fn is_datamashup_element(name: &[u8]) -> bool {
    match name.iter().rposition(|&b| b == b':') {
        Some(idx) => name.get(idx + 1..) == Some(b"DataMashup".as_slice()),
        None => name == b"DataMashup",
    }
}

fn read_u32_at(bytes: &[u8], offset: usize) -> Option<u32> {
    let slice = bytes.get(offset..offset + 4)?;
    let array: [u8; 4] = slice.try_into().ok()?;
    Some(u32::from_le_bytes(array))
}

fn read_length(bytes: &[u8], offset: usize) -> Result<usize, DataMashupError> {
    let len = read_u32_at(bytes, offset).ok_or(DataMashupError::FramingInvalid)?;
    usize::try_from(len).map_err(|_| DataMashupError::FramingInvalid)
}

fn take_segment(bytes: &[u8], offset: &mut usize, len: usize) -> Result<Vec<u8>, DataMashupError> {
    let start = *offset;
    let end = start
        .checked_add(len)
        .ok_or(DataMashupError::FramingInvalid)?;
    if end > bytes.len() {
        return Err(DataMashupError::FramingInvalid);
    }

    let segment = bytes[start..end].to_vec();
    *offset = end;
    Ok(segment)
}

#[cfg(test)]
mod tests {
    use super::{
        DataMashupError, RawDataMashup, decode_datamashup_base64, parse_data_mashup,
        read_datamashup_text,
    };

    fn build_dm_bytes(
        version: u32,
        package_parts: &[u8],
        permissions: &[u8],
        metadata: &[u8],
        permission_bindings: &[u8],
    ) -> Vec<u8> {
        let mut bytes = Vec::new();
        bytes.extend_from_slice(&version.to_le_bytes());
        bytes.extend_from_slice(&(package_parts.len() as u32).to_le_bytes());
        bytes.extend_from_slice(package_parts);
        bytes.extend_from_slice(&(permissions.len() as u32).to_le_bytes());
        bytes.extend_from_slice(permissions);
        bytes.extend_from_slice(&(metadata.len() as u32).to_le_bytes());
        bytes.extend_from_slice(metadata);
        bytes.extend_from_slice(&(permission_bindings.len() as u32).to_le_bytes());
        bytes.extend_from_slice(permission_bindings);
        bytes
    }

    #[test]
    fn parse_zero_length_stream_succeeds() {
        let bytes = build_dm_bytes(0, b"", b"", b"", b"");
        let parsed = parse_data_mashup(&bytes).expect("zero-length sections should parse");
        assert_eq!(
            parsed,
            RawDataMashup {
                version: 0,
                package_parts: Vec::new(),
                permissions: Vec::new(),
                metadata: Vec::new(),
                permission_bindings: Vec::new(),
            }
        );
    }

    #[test]
    fn parse_basic_non_zero_lengths() {
        let bytes = build_dm_bytes(0, b"AAAA", b"BBBB", b"CCCC", b"DDDD");
        let parsed = parse_data_mashup(&bytes).expect("non-zero lengths should parse");
        assert_eq!(parsed.version, 0);
        assert_eq!(parsed.package_parts, b"AAAA");
        assert_eq!(parsed.permissions, b"BBBB");
        assert_eq!(parsed.metadata, b"CCCC");
        assert_eq!(parsed.permission_bindings, b"DDDD");
    }

    #[test]
    fn unsupported_version_is_rejected() {
        let bytes = build_dm_bytes(1, b"AAAA", b"BBBB", b"CCCC", b"DDDD");
        let err = parse_data_mashup(&bytes).expect_err("version 1 should be unsupported");
        assert!(matches!(err, DataMashupError::UnsupportedVersion(1)));
    }

    #[test]
    fn truncated_stream_errors() {
        let mut bytes = Vec::new();
        bytes.extend_from_slice(&0u32.to_le_bytes());
        bytes.extend_from_slice(&100u32.to_le_bytes());
        bytes.extend_from_slice(&0u32.to_le_bytes());
        bytes.extend_from_slice(&0u32.to_le_bytes());
        bytes.extend_from_slice(&0u32.to_le_bytes());
        let err = parse_data_mashup(&bytes).expect_err("length overflows buffer");
        assert!(matches!(err, DataMashupError::FramingInvalid));
    }

    #[test]
    fn trailing_bytes_are_invalid() {
        let mut bytes = build_dm_bytes(0, b"", b"", b"", b"");
        bytes.push(0xFF);
        let err = parse_data_mashup(&bytes).expect_err("trailing bytes should fail");
        assert!(matches!(err, DataMashupError::FramingInvalid));
    }

    #[test]
    fn too_short_stream_is_framing_invalid() {
        let bytes = vec![0u8; 8];
        let err =
            parse_data_mashup(&bytes).expect_err("buffer shorter than header must be invalid");
        assert!(matches!(err, DataMashupError::FramingInvalid));
    }

    #[test]
    fn utf16_datamashup_xml_decodes_correctly() {
        let xml_text = r#"<?xml version="1.0" encoding="utf-16"?><root xmlns:dm="http://schemas.microsoft.com/DataMashup"><dm:DataMashup>QQ==</dm:DataMashup></root>"#;
        let mut xml_bytes = Vec::with_capacity(2 + xml_text.len() * 2);
        xml_bytes.extend_from_slice(&[0xFF, 0xFE]);
        for unit in xml_text.encode_utf16() {
            xml_bytes.extend_from_slice(&unit.to_le_bytes());
        }

        let text = read_datamashup_text(&xml_bytes)
            .expect("UTF-16 XML should parse")
            .expect("DataMashup element should be found");
        assert_eq!(text.trim(), "QQ==");
    }

    #[test]
    fn utf16_without_bom_with_declared_encoding_parses() {
        let xml_text = r#"<?xml version="1.0" encoding="utf-16"?><root xmlns:dm="http://schemas.microsoft.com/DataMashup"><dm:DataMashup>QQ==</dm:DataMashup></root>"#;
        for &little_endian in &[true, false] {
            let mut xml_bytes = Vec::with_capacity(xml_text.len() * 2);
            for unit in xml_text.encode_utf16() {
                let bytes = if little_endian {
                    unit.to_le_bytes()
                } else {
                    unit.to_be_bytes()
                };
                xml_bytes.extend_from_slice(&bytes);
            }

            let text = read_datamashup_text(&xml_bytes)
                .expect("UTF-16 XML without BOM should parse when declared")
                .expect("DataMashup element should be found");
            assert_eq!(text.trim(), "QQ==");
        }
    }

    #[test]
    fn elements_with_datamashup_suffix_are_ignored() {
        let xml = br#"<?xml version="1.0"?><root><FooDataMashup>QQ==</FooDataMashup></root>"#;
        let result = read_datamashup_text(xml).expect("parsing should succeed");
        assert!(result.is_none());
    }

    #[test]
    fn duplicate_sibling_datamashup_elements_error() {
        let xml = br#"<?xml version="1.0"?>
<root xmlns:dm="http://schemas.microsoft.com/DataMashup">
  <dm:DataMashup>QQ==</dm:DataMashup>
  <dm:DataMashup>QQ==</dm:DataMashup>
</root>"#;
        let err = read_datamashup_text(xml).expect_err("duplicate DataMashup elements should fail");
        assert!(matches!(err, DataMashupError::FramingInvalid));
    }

    #[test]
    fn decode_datamashup_base64_rejects_invalid() {
        let err = decode_datamashup_base64("!!!").expect_err("invalid base64 should fail");
        assert!(matches!(err, DataMashupError::Base64Invalid));
    }

    #[test]
    fn fuzz_style_never_panics() {
        for seed in 0u64..32 {
            let len = (seed as usize * 7 % 48) + (seed as usize % 5);
            let mut state = seed.wrapping_mul(6364136223846793005).wrapping_add(1);
            let mut bytes = Vec::with_capacity(len);
            for _ in 0..len {
                state = state
                    .wrapping_mul(2862933555777941757)
                    .wrapping_add(3037000493);
                bytes.push((state >> 32) as u8);
            }
            let _ = parse_data_mashup(&bytes);
        }
    }
}

```

---

### File: `core\src\datamashup_package.rs`

```rust
use crate::datamashup_framing::DataMashupError;
use std::io::{Cursor, Read, Seek};
use zip::ZipArchive;

#[derive(Debug, Clone, Copy)]
pub struct DataMashupLimits {
    pub max_inner_entries: usize,
    pub max_inner_part_bytes: u64,
    pub max_inner_total_bytes: u64,
}

impl Default for DataMashupLimits {
    fn default() -> Self {
        Self {
            max_inner_entries: 10_000,
            max_inner_part_bytes: 100 * 1024 * 1024,
            max_inner_total_bytes: 500 * 1024 * 1024,
        }
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct PackageXml {
    pub raw_xml: String,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct SectionDocument {
    pub source: String,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct EmbeddedContent {
    /// Normalized PackageParts path for the embedded package (never starts with '/').
    pub name: String,
    pub section: SectionDocument,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct PackageParts {
    pub package_xml: PackageXml,
    pub main_section: SectionDocument,
    pub embedded_contents: Vec<EmbeddedContent>,
}

pub fn parse_package_parts(bytes: &[u8]) -> Result<PackageParts, DataMashupError> {
    parse_package_parts_with_limits(bytes, DataMashupLimits::default())
}

pub fn parse_package_parts_with_limits(
    bytes: &[u8],
    limits: DataMashupLimits,
) -> Result<PackageParts, DataMashupError> {
    let cursor = Cursor::new(bytes);
    let mut archive = ZipArchive::new(cursor).map_err(|_| DataMashupError::FramingInvalid)?;

    if archive.len() > limits.max_inner_entries {
        return Err(DataMashupError::InnerTooManyEntries {
            entries: archive.len(),
            max_entries: limits.max_inner_entries,
        });
    }

    let mut total_read: u64 = 0;
    let mut package_xml: Option<PackageXml> = None;
    let mut main_section: Option<SectionDocument> = None;
    let mut embedded_contents: Vec<EmbeddedContent> = Vec::new();

    for i in 0..archive.len() {
        let mut file = archive
            .by_index(i)
            .map_err(|_| DataMashupError::FramingInvalid)?;
        if file.is_dir() {
            continue;
        }

        let name = normalize_path(file.name());
        if package_xml.is_none() && name == "Config/Package.xml" {
            reserve_inner_read_budget(&mut total_read, &name, file.size(), limits)?;
            let text = read_file_to_string(&mut file)?;
            package_xml = Some(PackageXml { raw_xml: text });
            continue;
        }
        if main_section.is_none() && name == "Formulas/Section1.m" {
            reserve_inner_read_budget(&mut total_read, &name, file.size(), limits)?;
            let text = strip_leading_bom(read_file_to_string(&mut file)?);
            main_section = Some(SectionDocument { source: text });
            continue;
        }
        if name.starts_with("Content/") {
            reserve_inner_read_budget(&mut total_read, &name, file.size(), limits)?;
            let mut content_bytes = Vec::new();
            if file.read_to_end(&mut content_bytes).is_err() {
                continue;
            }

            let unpacked_suffix = "/Formulas/Section1.m";
            if name.ends_with(unpacked_suffix) {
                if let Some(root) = name.strip_suffix(unpacked_suffix) {
                    if embedded_contents.iter().all(|e| e.name != root) {
                        if let Ok(text) = std::str::from_utf8(&content_bytes) {
                            let s = strip_leading_bom(text.to_string());
                            embedded_contents.push(EmbeddedContent {
                                name: root.to_string(),
                                section: SectionDocument { source: s },
                            });
                        }
                    }
                }
                continue;
            }

            if let Some(section) = extract_embedded_section(&content_bytes, limits, &name)? {
                embedded_contents.push(EmbeddedContent {
                    name: name.clone(),
                    section: SectionDocument { source: section },
                });
            }
        }
    }

    let package_xml = package_xml.ok_or(DataMashupError::FramingInvalid)?;
    let main_section = main_section.ok_or(DataMashupError::FramingInvalid)?;

    Ok(PackageParts {
        package_xml,
        main_section,
        embedded_contents,
    })
}

fn normalize_path(name: &str) -> String {
    let trimmed = name.trim_start_matches(|c| c == '/' || c == '\\');
    trimmed.replace('\\', "/")
}

fn read_file_to_string(file: &mut zip::read::ZipFile<'_>) -> Result<String, DataMashupError> {
    let mut buf = Vec::new();
    file.read_to_end(&mut buf)
        .map_err(|_| DataMashupError::FramingInvalid)?;
    String::from_utf8(buf).map_err(|_| DataMashupError::FramingInvalid)
}

fn reserve_inner_read_budget(
    total_read: &mut u64,
    path: &str,
    size: u64,
    limits: DataMashupLimits,
) -> Result<(), DataMashupError> {
    if size > limits.max_inner_part_bytes {
        return Err(DataMashupError::InnerPartTooLarge {
            path: path.to_string(),
            size,
            limit: limits.max_inner_part_bytes,
        });
    }

    let new_total = total_read.saturating_add(size);
    if new_total > limits.max_inner_total_bytes {
        return Err(DataMashupError::InnerTotalTooLarge {
            limit: limits.max_inner_total_bytes,
        });
    }

    *total_read = new_total;
    Ok(())
}

fn extract_embedded_section(
    bytes: &[u8],
    limits: DataMashupLimits,
    outer_name: &str,
) -> Result<Option<String>, DataMashupError> {
    let cursor = Cursor::new(bytes);
    let mut archive = match ZipArchive::new(cursor) {
        Ok(archive) => archive,
        Err(_) => return Ok(None),
    };

    if archive.len() > limits.max_inner_entries {
        return Err(DataMashupError::InnerTooManyEntries {
            entries: archive.len(),
            max_entries: limits.max_inner_entries,
        });
    }

    find_section_document(&mut archive, limits, outer_name)
}

fn find_section_document<R: Read + Seek>(
    archive: &mut ZipArchive<R>,
    limits: DataMashupLimits,
    outer_name: &str,
) -> Result<Option<String>, DataMashupError> {
    let mut total_read: u64 = 0;

    for idx in 0..archive.len() {
        let mut file = match archive.by_index(idx) {
            Ok(file) => file,
            Err(_) => continue,
        };
        if file.is_dir() {
            continue;
        }

        let inner_name = normalize_path(file.name());
        if inner_name == "Formulas/Section1.m" {
            let combined_name = format!("{outer_name}/{inner_name}");
            reserve_inner_read_budget(&mut total_read, &combined_name, file.size(), limits)?;
            let mut buf = Vec::new();
            if file.read_to_end(&mut buf).is_ok() {
                if let Ok(text) = String::from_utf8(buf) {
                    return Ok(Some(strip_leading_bom(text)));
                }
            }
            return Ok(None);
        }
    }
    Ok(None)
}

fn strip_leading_bom(text: String) -> String {
    text.strip_prefix('\u{FEFF}')
        .map(|s| s.to_string())
        .unwrap_or(text)
}

```

---

### File: `core\src\dax.rs`

```rust
use std::hash::{Hash, Hasher};

use xxhash_rust::xxh64::Xxh64;

use crate::hashing::XXH64_SEED;

#[derive(Debug, Clone, PartialEq, Eq)]
pub(crate) struct DaxParseError {
    message: String,
}

impl DaxParseError {
    fn new(message: impl Into<String>) -> Self {
        Self {
            message: message.into(),
        }
    }
}

impl std::fmt::Display for DaxParseError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.message)
    }
}

impl std::error::Error for DaxParseError {}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
enum BinaryOp {
    Or,
    And,
    Eq,
    Ne,
    Lt,
    Le,
    Gt,
    Ge,
    Add,
    Sub,
    Mul,
    Div,
    Pow,
    Concat,
}

impl BinaryOp {
    fn precedence(self) -> u8 {
        match self {
            BinaryOp::Or => 1,
            BinaryOp::And => 2,
            BinaryOp::Eq | BinaryOp::Ne | BinaryOp::Lt | BinaryOp::Le | BinaryOp::Gt | BinaryOp::Ge => 3,
            BinaryOp::Concat => 4,
            BinaryOp::Add | BinaryOp::Sub => 5,
            BinaryOp::Mul | BinaryOp::Div => 6,
            BinaryOp::Pow => 7,
        }
    }

    fn right_assoc(self) -> bool {
        matches!(self, BinaryOp::Pow)
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
enum UnaryOp {
    Pos,
    Neg,
    Not,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
enum Expr {
    Number(u64),
    String(String),
    Boolean(bool),
    Identifier(String),
    BracketRef(String),
    TableColumnRef { table: String, column: String },
    Call { name: String, args: Vec<Expr> },
    Unary { op: UnaryOp, expr: Box<Expr> },
    Binary { op: BinaryOp, left: Box<Expr>, right: Box<Expr> },
    VarBlock { vars: Vec<(String, Expr)>, body: Box<Expr> },
}

#[derive(Debug, Clone, PartialEq, Eq)]
enum TokenKind {
    Ident(String),
    BracketIdent(String),
    StringLiteral(String),
    Number(u64),
    Operator(BinaryOp),
    Plus,
    Minus,
    LParen,
    RParen,
    Comma,
    End,
}

#[derive(Debug, Clone, PartialEq, Eq)]
struct Token {
    kind: TokenKind,
}

struct Lexer {
    chars: Vec<char>,
    pos: usize,
}

impl Lexer {
    fn new(input: &str) -> Self {
        Self {
            chars: input.chars().collect(),
            pos: 0,
        }
    }

    fn peek(&self) -> Option<char> {
        self.chars.get(self.pos).copied()
    }

    fn peek_next(&self) -> Option<char> {
        self.chars.get(self.pos + 1).copied()
    }

    fn advance(&mut self) -> Option<char> {
        let ch = self.peek()?;
        self.pos += 1;
        Some(ch)
    }

    fn skip_whitespace_and_comments(&mut self) {
        loop {
            while matches!(self.peek(), Some(ch) if ch.is_whitespace()) {
                self.advance();
            }

            match (self.peek(), self.peek_next()) {
                (Some('/'), Some('/')) => {
                    self.advance();
                    self.advance();
                    while let Some(ch) = self.peek() {
                        self.advance();
                        if ch == '\n' || ch == '\r' {
                            break;
                        }
                    }
                }
                (Some('/'), Some('*')) => {
                    self.advance();
                    self.advance();
                    while let Some(ch) = self.advance() {
                        if ch == '*' && self.peek() == Some('/') {
                            self.advance();
                            break;
                        }
                    }
                }
                (Some('-'), Some('-')) => {
                    self.advance();
                    self.advance();
                    while let Some(ch) = self.peek() {
                        self.advance();
                        if ch == '\n' || ch == '\r' {
                            break;
                        }
                    }
                }
                _ => break,
            }
        }
    }

    fn next_token(&mut self) -> Result<Token, DaxParseError> {
        self.skip_whitespace_and_comments();
        let Some(ch) = self.peek() else {
            return Ok(Token { kind: TokenKind::End });
        };

        match ch {
            '(' => {
                self.advance();
                Ok(Token { kind: TokenKind::LParen })
            }
            ')' => {
                self.advance();
                Ok(Token { kind: TokenKind::RParen })
            }
            ',' => {
                self.advance();
                Ok(Token { kind: TokenKind::Comma })
            }
            '[' => self.read_bracket_ident(),
            '\'' => self.read_quoted_ident(),
            '"' => self.read_string_literal(),
            '+' => {
                self.advance();
                Ok(Token { kind: TokenKind::Plus })
            }
            '-' => {
                self.advance();
                Ok(Token { kind: TokenKind::Minus })
            }
            '*' => {
                self.advance();
                Ok(Token { kind: TokenKind::Operator(BinaryOp::Mul) })
            }
            '/' => {
                self.advance();
                Ok(Token { kind: TokenKind::Operator(BinaryOp::Div) })
            }
            '^' => {
                self.advance();
                Ok(Token { kind: TokenKind::Operator(BinaryOp::Pow) })
            }
            '&' => {
                self.advance();
                if self.peek() == Some('&') {
                    self.advance();
                    Ok(Token { kind: TokenKind::Operator(BinaryOp::And) })
                } else {
                    Ok(Token { kind: TokenKind::Operator(BinaryOp::Concat) })
                }
            }
            '|' => {
                self.advance();
                if self.peek() == Some('|') {
                    self.advance();
                    Ok(Token { kind: TokenKind::Operator(BinaryOp::Or) })
                } else {
                    Err(DaxParseError::new("unexpected '|'"))
                }
            }
            '=' => {
                self.advance();
                Ok(Token { kind: TokenKind::Operator(BinaryOp::Eq) })
            }
            '<' => {
                self.advance();
                match self.peek() {
                    Some('=') => {
                        self.advance();
                        Ok(Token { kind: TokenKind::Operator(BinaryOp::Le) })
                    }
                    Some('>') => {
                        self.advance();
                        Ok(Token { kind: TokenKind::Operator(BinaryOp::Ne) })
                    }
                    _ => Ok(Token { kind: TokenKind::Operator(BinaryOp::Lt) }),
                }
            }
            '>' => {
                self.advance();
                if self.peek() == Some('=') {
                    self.advance();
                    Ok(Token { kind: TokenKind::Operator(BinaryOp::Ge) })
                } else {
                    Ok(Token { kind: TokenKind::Operator(BinaryOp::Gt) })
                }
            }
            _ => {
                if ch.is_ascii_digit() || (ch == '.' && self.peek_next().map(|n| n.is_ascii_digit()).unwrap_or(false)) {
                    self.read_number()
                } else if ch.is_ascii_alphabetic() || ch == '_' {
                    self.read_ident()
                } else {
                    Err(DaxParseError::new(format!("unexpected character '{}'", ch)))
                }
            }
        }
    }

    fn read_ident(&mut self) -> Result<Token, DaxParseError> {
        let mut buf = String::new();
        while let Some(ch) = self.peek() {
            if ch.is_ascii_alphanumeric() || ch == '_' || ch == '.' {
                buf.push(ch);
                self.advance();
            } else {
                break;
            }
        }
        Ok(Token {
            kind: TokenKind::Ident(normalize_ident(&buf)),
        })
    }

    fn read_number(&mut self) -> Result<Token, DaxParseError> {
        let mut buf = String::new();
        let mut seen_dot = false;
        while let Some(ch) = self.peek() {
            if ch.is_ascii_digit() {
                buf.push(ch);
                self.advance();
            } else if ch == '.' && !seen_dot {
                seen_dot = true;
                buf.push(ch);
                self.advance();
            } else {
                break;
            }
        }

        if let Some(ch) = self.peek() {
            if ch == 'e' || ch == 'E' {
                buf.push(ch);
                self.advance();
                if let Some(sign) = self.peek() {
                    if sign == '+' || sign == '-' {
                        buf.push(sign);
                        self.advance();
                    }
                }
                while let Some(d) = self.peek() {
                    if d.is_ascii_digit() {
                        buf.push(d);
                        self.advance();
                    } else {
                        break;
                    }
                }
            }
        }

        let num = buf.parse::<f64>().map_err(|_| {
            DaxParseError::new(format!("invalid number literal '{}'", buf))
        })?;
        Ok(Token {
            kind: TokenKind::Number(num.to_bits()),
        })
    }

    fn read_string_literal(&mut self) -> Result<Token, DaxParseError> {
        let mut buf = String::new();
        self.advance();
        while let Some(ch) = self.peek() {
            self.advance();
            if ch == '"' {
                if self.peek() == Some('"') {
                    self.advance();
                    buf.push('"');
                    continue;
                }
                return Ok(Token {
                    kind: TokenKind::StringLiteral(buf),
                });
            }
            buf.push(ch);
        }
        Err(DaxParseError::new("unterminated string literal"))
    }

    fn read_bracket_ident(&mut self) -> Result<Token, DaxParseError> {
        let mut buf = String::new();
        self.advance();
        while let Some(ch) = self.peek() {
            self.advance();
            if ch == ']' {
                if self.peek() == Some(']') {
                    self.advance();
                    buf.push(']');
                    continue;
                }
                return Ok(Token {
                    kind: TokenKind::BracketIdent(normalize_ident(&buf)),
                });
            }
            buf.push(ch);
        }
        Err(DaxParseError::new("unterminated bracket identifier"))
    }

    fn read_quoted_ident(&mut self) -> Result<Token, DaxParseError> {
        let mut buf = String::new();
        self.advance();
        while let Some(ch) = self.peek() {
            self.advance();
            if ch == '\'' {
                if self.peek() == Some('\'') {
                    self.advance();
                    buf.push('\'');
                    continue;
                }
                return Ok(Token {
                    kind: TokenKind::Ident(normalize_ident(&buf)),
                });
            }
            buf.push(ch);
        }
        Err(DaxParseError::new("unterminated quoted identifier"))
    }
}

struct Parser {
    tokens: Vec<Token>,
    pos: usize,
}

impl Parser {
    fn new(tokens: Vec<Token>) -> Self {
        Self { tokens, pos: 0 }
    }

    fn peek(&self) -> &TokenKind {
        &self.tokens[self.pos].kind
    }

    fn next(&mut self) -> &TokenKind {
        let tok = &self.tokens[self.pos].kind;
        self.pos = self.pos.saturating_add(1);
        tok
    }

    fn consume(&mut self, expected: &TokenKind) -> Result<(), DaxParseError> {
        if self.peek() == expected {
            self.next();
            Ok(())
        } else {
            Err(DaxParseError::new(format!(
                "expected {:?}, got {:?}",
                expected,
                self.peek()
            )))
        }
    }

    fn parse(&mut self) -> Result<Expr, DaxParseError> {
        let expr = self.parse_expr_bp(0)?;
        if !matches!(self.peek(), TokenKind::End) {
            return Err(DaxParseError::new("unexpected trailing tokens"));
        }
        Ok(expr)
    }

    fn parse_expr_bp(&mut self, min_bp: u8) -> Result<Expr, DaxParseError> {
        let mut lhs = self.parse_prefix()?;
        loop {
            let op = match self.peek() {
                TokenKind::Operator(op) => *op,
                TokenKind::Plus => BinaryOp::Add,
                TokenKind::Minus => BinaryOp::Sub,
                _ => break,
            };

            let prec = op.precedence();
            if prec < min_bp {
                break;
            }
            let next_min = if op.right_assoc() { prec } else { prec + 1 };
            self.next();
            let rhs = self.parse_expr_bp(next_min)?;
            lhs = Expr::Binary {
                op,
                left: Box::new(lhs),
                right: Box::new(rhs),
            };
        }
        Ok(lhs)
    }

    fn parse_prefix(&mut self) -> Result<Expr, DaxParseError> {
        let token = self.next().clone();
        match token {
            TokenKind::Ident(name) if name == "var" => self.parse_var_block(),
            TokenKind::Ident(name) if name == "not" => {
                let expr = self.parse_expr_bp(8)?;
                Ok(Expr::Unary {
                    op: UnaryOp::Not,
                    expr: Box::new(expr),
                })
            }
            TokenKind::Ident(name) if name == "true" => Ok(Expr::Boolean(true)),
            TokenKind::Ident(name) if name == "false" => Ok(Expr::Boolean(false)),
            TokenKind::Ident(name) => {
                if matches!(self.peek(), TokenKind::LParen) {
                    self.next();
                    let args = self.parse_call_args()?;
                    Ok(Expr::Call { name, args })
                } else if let TokenKind::BracketIdent(column) = self.peek().clone() {
                    self.next();
                    Ok(Expr::TableColumnRef {
                        table: name,
                        column,
                    })
                } else {
                    Ok(Expr::Identifier(name))
                }
            }
            TokenKind::BracketIdent(name) => Ok(Expr::BracketRef(name)),
            TokenKind::StringLiteral(s) => Ok(Expr::String(s)),
            TokenKind::Number(n) => Ok(Expr::Number(n)),
            TokenKind::Plus => {
                let expr = self.parse_expr_bp(8)?;
                Ok(Expr::Unary {
                    op: UnaryOp::Pos,
                    expr: Box::new(expr),
                })
            }
            TokenKind::Minus => {
                let expr = self.parse_expr_bp(8)?;
                Ok(Expr::Unary {
                    op: UnaryOp::Neg,
                    expr: Box::new(expr),
                })
            }
            TokenKind::LParen => {
                let expr = self.parse_expr_bp(0)?;
                self.consume(&TokenKind::RParen)?;
                Ok(expr)
            }
            other => Err(DaxParseError::new(format!("unexpected token {:?}", other))),
        }
    }

    fn parse_call_args(&mut self) -> Result<Vec<Expr>, DaxParseError> {
        let mut args = Vec::new();
        if matches!(self.peek(), TokenKind::RParen) {
            self.next();
            return Ok(args);
        }
        loop {
            let expr = self.parse_expr_bp(0)?;
            args.push(expr);
            match self.peek() {
                TokenKind::Comma => {
                    self.next();
                }
                TokenKind::RParen => {
                    self.next();
                    break;
                }
                _ => {
                    return Err(DaxParseError::new("expected ',' or ')' in call"));
                }
            }
        }
        Ok(args)
    }

    fn parse_var_block(&mut self) -> Result<Expr, DaxParseError> {
        let mut vars = Vec::new();
        loop {
            let name = match self.next() {
                TokenKind::Ident(name) => name.clone(),
                other => {
                    return Err(DaxParseError::new(format!(
                        "expected identifier after VAR, got {:?}",
                        other
                    )));
                }
            };
            self.consume(&TokenKind::Operator(BinaryOp::Eq))?;
            let expr = self.parse_expr_bp(0)?;
            vars.push((name, expr));

            match self.peek() {
                TokenKind::Ident(next) if next == "var" => {
                    self.next();
                    continue;
                }
                TokenKind::Ident(next) if next == "return" => {
                    self.next();
                    break;
                }
                _ => {
                    return Err(DaxParseError::new("expected VAR or RETURN"));
                }
            }
        }
        let body = self.parse_expr_bp(0)?;
        Ok(Expr::VarBlock {
            vars,
            body: Box::new(body),
        })
    }
}

fn normalize_ident(s: &str) -> String {
    s.to_lowercase()
}

pub(crate) fn semantic_hash(expr: &str) -> Result<u64, DaxParseError> {
    let mut lexer = Lexer::new(expr);
    let mut tokens = Vec::new();
    loop {
        let token = lexer.next_token()?;
        let end = matches!(token.kind, TokenKind::End);
        tokens.push(token);
        if end {
            break;
        }
    }

    let mut parser = Parser::new(tokens);
    let expr = parser.parse()?;
    let mut h = Xxh64::new(XXH64_SEED);
    expr.hash(&mut h);
    Ok(h.finish())
}

#[cfg(test)]
mod tests {
    use super::*;

    fn hash(expr: &str) -> Result<u64, DaxParseError> {
        semantic_hash(expr)
    }

    #[test]
    fn dax_semantic_hash_ignores_whitespace_and_case() {
        let a = "SUM ( Sales[Amount] )";
        let b = "sum(sales[amount])";
        assert_eq!(hash(a).unwrap(), hash(b).unwrap());
    }

    #[test]
    fn dax_semantic_hash_var_block() {
        let a = "VAR x = 1 RETURN x + 2";
        let b = "var X=1 return X+2";
        assert_eq!(hash(a).unwrap(), hash(b).unwrap());
    }

    #[test]
    fn dax_semantic_hash_detects_semantic_change() {
        let a = "SUM(Sales[Amount])";
        let b = "SUM(Sales[Net])";
        assert_ne!(hash(a).unwrap(), hash(b).unwrap());
    }

    #[test]
    fn dax_semantic_hash_handles_binary_plus_minus() {
        let a = "1+2-3";
        let b = "1 + 2 - 3";
        assert_eq!(hash(a).unwrap(), hash(b).unwrap());
    }

    #[test]
    fn dax_semantic_hash_handles_comments() {
        let a = "SUM(Sales[Amount]) // comment";
        let b = "SUM(Sales[Amount])";
        assert_eq!(hash(a).unwrap(), hash(b).unwrap());
    }
}

```

---

### File: `core\src\diff.rs`

```rust
//! Diff operations and reports for workbook comparison.
//!
//! This module defines the types used to represent differences between two workbooks:
//! - [`DiffOp`]: Individual operations representing a single change (cell edit, row/column add/remove, etc.)
//! - [`DiffReport`]: A versioned collection of diff operations
//! - [`DiffError`]: Errors that can occur during the diff process

use crate::error_codes;
use crate::string_pool::StringId;
use crate::workbook::{CellAddress, CellSnapshot, CellValue, ColSignature, RowSignature};
use thiserror::Error;

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum QueryChangeKind {
    /// A semantic change (meaningfully different after canonicalization).
    Semantic,
    /// Only formatting changed (whitespace/comments); meaning is unchanged.
    FormattingOnly,
    /// The query was renamed (definition may be unchanged).
    Renamed,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ExpressionChangeKind {
    Semantic,
    FormattingOnly,
    Unknown,
}

impl Default for ExpressionChangeKind {
    fn default() -> Self {
        ExpressionChangeKind::Unknown
    }
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct QuerySemanticDetail {
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub step_diffs: Vec<StepDiff>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub ast_summary: Option<AstDiffSummary>,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum StepDiff {
    StepAdded { step: StepSnapshot },
    StepRemoved { step: StepSnapshot },
    StepReordered {
        name: StringId,
        from_index: u32,
        to_index: u32,
    },
    StepModified {
        before: StepSnapshot,
        after: StepSnapshot,
        #[serde(default, skip_serializing_if = "Vec::is_empty")]
        changes: Vec<StepChange>,
    },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct StepSnapshot {
    pub name: StringId,
    pub index: u32,
    pub step_type: StepType,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub source_refs: Vec<StringId>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub params: Option<StepParams>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub signature: Option<u64>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum StepType {
    TableSelectRows,
    TableRemoveColumns,
    TableRenameColumns,
    TableTransformColumnTypes,
    TableNestedJoin,
    TableJoin,
    Other,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum StepParams {
    TableSelectRows { predicate_hash: u64 },
    TableRemoveColumns { columns: ExtractedStringList },
    TableRenameColumns { renames: ExtractedRenamePairs },
    TableTransformColumnTypes { transforms: ExtractedColumnTypeChanges },
    TableNestedJoin {
        left_keys: ExtractedStringList,
        right_keys: ExtractedStringList,
        new_column: ExtractedString,
        join_kind_hash: Option<u64>,
    },
    TableJoin {
        left_keys: ExtractedStringList,
        right_keys: ExtractedStringList,
        join_kind_hash: Option<u64>,
    },
    Other {
        function_name_hash: Option<u64>,
        arity: Option<u32>,
        expr_hash: u64,
    },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum ExtractedString {
    Known { value: StringId },
    Unknown { hash: u64 },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum ExtractedStringList {
    Known { values: Vec<StringId> },
    Unknown { hash: u64 },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum ExtractedRenamePairs {
    Known { pairs: Vec<RenamePair> },
    Unknown { hash: u64 },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct RenamePair {
    pub from: StringId,
    pub to: StringId,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum ExtractedColumnTypeChanges {
    Known { changes: Vec<ColumnTypeChange> },
    Unknown { hash: u64 },
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct ColumnTypeChange {
    pub column: StringId,
    pub ty_hash: u64,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind", rename_all = "snake_case")]
pub enum StepChange {
    Renamed { from: StringId, to: StringId },
    SourceRefsChanged {
        #[serde(default, skip_serializing_if = "Vec::is_empty")]
        removed: Vec<StringId>,
        #[serde(default, skip_serializing_if = "Vec::is_empty")]
        added: Vec<StringId>,
    },
    ParamsChanged,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AstDiffMode {
    SmallExact,
    LargeHeuristic,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct AstDiffSummary {
    pub mode: AstDiffMode,
    pub node_count_old: u32,
    pub node_count_new: u32,
    pub inserted: u32,
    pub deleted: u32,
    pub updated: u32,
    pub moved: u32,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub move_hints: Vec<AstMoveHint>,
}

#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct AstMoveHint {
    pub subtree_hash: u64,
    pub from_preorder: u32,
    pub to_preorder: u32,
    pub subtree_size: u32,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FormulaDiffResult {
    /// Unknown or not computed.
    Unknown,
    /// Formula/value is unchanged.
    Unchanged,
    /// Formula/value was added.
    Added,
    /// Formula/value was removed.
    Removed,
    /// Only formatting changed (whitespace/casing), semantics unchanged.
    FormattingOnly,
    /// Filled down/across (shift-equivalent).
    Filled,
    /// Semantic change.
    SemanticChange,
    /// Textual change (different text but semantics not computed/unknown).
    TextChange,
}

impl Default for FormulaDiffResult {
    fn default() -> Self {
        FormulaDiffResult::Unknown
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub enum QueryMetadataField {
    /// Whether the query loads to a sheet.
    LoadToSheet,
    /// Whether the query loads to the data model.
    LoadToModel,
    /// Query group path.
    GroupPath,
    /// Whether the query is connection-only.
    ConnectionOnly,
}

#[cfg(feature = "model-diff")]
#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ModelColumnProperty {
    Hidden,
    FormatString,
    SortBy,
    SummarizeBy,
}

#[cfg(feature = "model-diff")]
#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum RelationshipProperty {
    CrossFilteringBehavior,
    Cardinality,
    IsActive,
}

/// Errors produced by diffing APIs.
#[derive(Debug, Error)]
#[non_exhaustive]
pub enum DiffError {
    #[error(
        "[EXDIFF_DIFF_001] alignment limits exceeded for sheet '{sheet}': rows={rows}, cols={cols} (limits: rows={max_rows}, cols={max_cols}). Suggestion: increase `max_align_rows`/`max_align_cols` or change `on_limit_exceeded`."
    )]
    LimitsExceeded {
        sheet: StringId,
        rows: u32,
        cols: u32,
        max_rows: u32,
        max_cols: u32,
    },

    #[error("[EXDIFF_DIFF_002] sink error: {message}. Suggestion: check the output destination and retry.")]
    SinkError { message: String },

    #[error("[EXDIFF_DIFF_003] sheet '{requested}' not found. Available sheets: {}. Suggestion: check the sheet name and casing.", available.join(", "))]
    SheetNotFound {
        requested: String,
        available: Vec<String>,
    },

    #[error("[EXDIFF_DIFF_004] internal error: {message}. Suggestion: report a bug with the input file if possible.")]
    InternalError { message: String },
}

impl DiffError {
    pub fn code(&self) -> &'static str {
        match self {
            DiffError::LimitsExceeded { .. } => error_codes::DIFF_LIMITS_EXCEEDED,
            DiffError::SinkError { .. } => error_codes::DIFF_SINK_ERROR,
            DiffError::SheetNotFound { .. } => error_codes::DIFF_SHEET_NOT_FOUND,
            DiffError::InternalError { .. } => error_codes::DIFF_INTERNAL_ERROR,
        }
    }
}

pub type SheetId = StringId;

/// Summary metadata about a diff run emitted alongside streamed ops.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct DiffSummary {
    /// Whether the diff completed without early aborts/fallbacks.
    pub complete: bool,
    /// Warnings explaining why results are incomplete (when `complete == false`).
    pub warnings: Vec<String>,
    /// Total number of ops emitted.
    pub op_count: usize,
    #[cfg(feature = "perf-metrics")]
    /// Optional performance metrics when the `perf-metrics` feature is enabled.
    pub metrics: Option<crate::perf::DiffMetrics>,
}

/// A single diff operation representing one logical change between workbooks.
///
/// Operations are emitted by the diff engine and collected into a [`DiffReport`].
/// The enum is marked `#[non_exhaustive]` to allow future additions.
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(tag = "kind")]
#[non_exhaustive]
pub enum DiffOp {
    SheetAdded {
        sheet: SheetId,
    },
    SheetRemoved {
        sheet: SheetId,
    },
    SheetRenamed {
        sheet: SheetId,
        from: SheetId,
        to: SheetId,
    },
    RowAdded {
        sheet: SheetId,
        row_idx: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        row_signature: Option<RowSignature>,
    },
    RowRemoved {
        sheet: SheetId,
        row_idx: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        row_signature: Option<RowSignature>,
    },
    DuplicateKeyCluster {
        sheet: SheetId,
        key: Vec<Option<CellValue>>,
        left_rows: Vec<u32>,
        right_rows: Vec<u32>,
    },
    RowReplaced {
        sheet: SheetId,
        row_idx: u32,
    },
    ColumnAdded {
        sheet: SheetId,
        col_idx: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        col_signature: Option<ColSignature>,
    },
    ColumnRemoved {
        sheet: SheetId,
        col_idx: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        col_signature: Option<ColSignature>,
    },
    BlockMovedRows {
        sheet: SheetId,
        src_start_row: u32,
        row_count: u32,
        dst_start_row: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        block_hash: Option<u64>,
    },
    BlockMovedColumns {
        sheet: SheetId,
        src_start_col: u32,
        col_count: u32,
        dst_start_col: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        block_hash: Option<u64>,
    },
    BlockMovedRect {
        sheet: SheetId,
        src_start_row: u32,
        src_row_count: u32,
        src_start_col: u32,
        src_col_count: u32,
        dst_start_row: u32,
        dst_start_col: u32,
        #[serde(skip_serializing_if = "Option::is_none")]
        block_hash: Option<u64>,
    },
    RectReplaced {
        sheet: SheetId,
        start_row: u32,
        row_count: u32,
        start_col: u32,
        col_count: u32,
    },
    /// Logical change to a single cell.
    ///
    /// Invariants (maintained by producers and tests, not by the type system):
    /// - `addr` is the canonical location for the edit.
    /// - `from.addr` and `to.addr` must both equal `addr`.
    /// - `CellSnapshot` equality intentionally ignores `addr` and compares only
    ///   `(value, formula)`, so `DiffOp::CellEdited` equality does not by itself
    ///   enforce the address invariants; callers must respect them when
    ///   constructing ops.
    CellEdited {
        sheet: SheetId,
        addr: CellAddress,
        from: CellSnapshot,
        to: CellSnapshot,
        #[serde(default)]
        formula_diff: FormulaDiffResult,
    },

    VbaModuleAdded {
        name: StringId,
    },
    VbaModuleRemoved {
        name: StringId,
    },
    VbaModuleChanged {
        name: StringId,
    },

    NamedRangeAdded {
        name: StringId,
    },
    NamedRangeRemoved {
        name: StringId,
    },
    NamedRangeChanged {
        name: StringId,
        old_ref: StringId,
        new_ref: StringId,
    },

    ChartAdded {
        sheet: StringId,
        name: StringId,
    },
    ChartRemoved {
        sheet: StringId,
        name: StringId,
    },
    ChartChanged {
        sheet: StringId,
        name: StringId,
    },

    QueryAdded {
        name: StringId,
    },
    QueryRemoved {
        name: StringId,
    },
    QueryRenamed {
        from: StringId,
        to: StringId,
    },
    QueryDefinitionChanged {
        name: StringId,
        change_kind: QueryChangeKind,
        old_hash: u64,
        new_hash: u64,
        #[serde(skip_serializing_if = "Option::is_none")]
        semantic_detail: Option<QuerySemanticDetail>,
    },
    QueryMetadataChanged {
        name: StringId,
        field: QueryMetadataField,
        old: Option<StringId>,
        new: Option<StringId>,
    },
    #[cfg(feature = "model-diff")]
    TableAdded {
        name: StringId,
    },
    #[cfg(feature = "model-diff")]
    TableRemoved {
        name: StringId,
    },
    #[cfg(feature = "model-diff")]
    ModelColumnAdded {
        table: StringId,
        name: StringId,
        #[serde(skip_serializing_if = "Option::is_none")]
        data_type: Option<StringId>,
    },
    #[cfg(feature = "model-diff")]
    ModelColumnRemoved {
        table: StringId,
        name: StringId,
    },
    #[cfg(feature = "model-diff")]
    ModelColumnTypeChanged {
        table: StringId,
        name: StringId,
        #[serde(skip_serializing_if = "Option::is_none")]
        old_type: Option<StringId>,
        #[serde(skip_serializing_if = "Option::is_none")]
        new_type: Option<StringId>,
    },
    #[cfg(feature = "model-diff")]
    ModelColumnPropertyChanged {
        table: StringId,
        name: StringId,
        field: ModelColumnProperty,
        #[serde(skip_serializing_if = "Option::is_none")]
        old: Option<StringId>,
        #[serde(skip_serializing_if = "Option::is_none")]
        new: Option<StringId>,
    },
    #[cfg(feature = "model-diff")]
    CalculatedColumnDefinitionChanged {
        table: StringId,
        name: StringId,
        #[serde(default)]
        change_kind: ExpressionChangeKind,
        old_hash: u64,
        new_hash: u64,
    },
    #[cfg(feature = "model-diff")]
    RelationshipAdded {
        from_table: StringId,
        from_column: StringId,
        to_table: StringId,
        to_column: StringId,
    },
    #[cfg(feature = "model-diff")]
    RelationshipRemoved {
        from_table: StringId,
        from_column: StringId,
        to_table: StringId,
        to_column: StringId,
    },
    #[cfg(feature = "model-diff")]
    RelationshipPropertyChanged {
        from_table: StringId,
        from_column: StringId,
        to_table: StringId,
        to_column: StringId,
        field: RelationshipProperty,
        #[serde(skip_serializing_if = "Option::is_none")]
        old: Option<StringId>,
        #[serde(skip_serializing_if = "Option::is_none")]
        new: Option<StringId>,
    },
    #[cfg(feature = "model-diff")]
    MeasureAdded {
        name: StringId,
    },
    #[cfg(feature = "model-diff")]
    MeasureRemoved {
        name: StringId,
    },
    #[cfg(feature = "model-diff")]
    MeasureDefinitionChanged {
        name: StringId,
        #[serde(default)]
        change_kind: ExpressionChangeKind,
        old_hash: u64,
        new_hash: u64,
    },
}

/// A versioned collection of diff operations between two workbooks.
///
/// The `version` field indicates the schema version for forwards compatibility.
///
/// # Incomplete results
///
/// Some safety rails and limit behaviors can produce partial results. In that case:
///
/// - `complete == false`
/// - `warnings` contains at least one human-readable explanation
///
/// The CLI prints warnings to stderr as `Warning: ...`.
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct DiffReport {
    /// Schema version (currently "1").
    pub version: String,
    /// Interned string table used by ids referenced in this report.
    #[serde(default)]
    pub strings: Vec<String>,
    /// The list of diff operations.
    pub ops: Vec<DiffOp>,
    /// Whether the diff result is complete. When `false`, some operations may be missing
    /// due to resource limits being exceeded (e.g., row/column limits).
    #[serde(default = "default_complete")]
    pub complete: bool,
    /// Warnings generated during the diff process. Non-empty when limits were exceeded
    /// or other partial-result conditions occurred.
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub warnings: Vec<String>,
    #[cfg(feature = "perf-metrics")]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metrics: Option<crate::perf::DiffMetrics>,
}

fn default_complete() -> bool {
    true
}

impl DiffReport {
    pub const SCHEMA_VERSION: &'static str = "1";

    pub fn new(ops: Vec<DiffOp>) -> DiffReport {
        DiffReport {
            version: Self::SCHEMA_VERSION.to_string(),
            strings: Vec::new(),
            ops,
            complete: true,
            warnings: Vec::new(),
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        }
    }

    pub fn from_ops_and_summary(
        ops: Vec<DiffOp>,
        summary: DiffSummary,
        strings: Vec<String>,
    ) -> DiffReport {
        let mut report = DiffReport::new(ops);
        report.complete = summary.complete;
        report.warnings = summary.warnings;
        #[cfg(feature = "perf-metrics")]
        {
            report.metrics = summary.metrics;
        }
        report.strings = strings;
        report
    }

    pub fn with_partial_result(ops: Vec<DiffOp>, warning: String) -> DiffReport {
        DiffReport {
            version: Self::SCHEMA_VERSION.to_string(),
            strings: Vec::new(),
            ops,
            complete: false,
            warnings: vec![warning],
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        }
    }

    pub fn add_warning(&mut self, warning: String) {
        self.warnings.push(warning);
        self.complete = false;
    }

    /// Resolve an interned [`StringId`] into a string slice using this report's `strings` table.
    ///
    /// Many fields in [`DiffOp`] are `StringId`s (sheet names, query names, etc.). The returned
    /// string is owned by the report.
    ///
    /// ```
    /// # use excel_diff::{DiffReport, StringId};
    /// # fn demo(report: &DiffReport, id: StringId) {
    /// let text = report.resolve(id).unwrap_or("<unknown>");
    /// # let _ = text;
    /// # }
    /// ```
    pub fn resolve(&self, id: StringId) -> Option<&str> {
        self.strings.get(id.0 as usize).map(|s| s.as_str())
    }

    pub fn grid_ops(&self) -> impl Iterator<Item = &DiffOp> {
        self.ops
            .iter()
            .filter(|op| !op.is_m_op() && !op.is_model_op())
    }

    pub fn m_ops(&self) -> impl Iterator<Item = &DiffOp> {
        self.ops.iter().filter(|op| op.is_m_op())
    }
}

impl DiffOp {
    pub fn is_m_op(&self) -> bool {
        matches!(
            self,
            DiffOp::QueryAdded { .. }
                | DiffOp::QueryRemoved { .. }
                | DiffOp::QueryRenamed { .. }
                | DiffOp::QueryDefinitionChanged { .. }
                | DiffOp::QueryMetadataChanged { .. }
        )
    }

    pub fn is_model_op(&self) -> bool {
        #[cfg(feature = "model-diff")]
        {
            matches!(
                self,
                DiffOp::TableAdded { .. }
                    | DiffOp::TableRemoved { .. }
                    | DiffOp::ModelColumnAdded { .. }
                    | DiffOp::ModelColumnRemoved { .. }
                    | DiffOp::ModelColumnTypeChanged { .. }
                    | DiffOp::ModelColumnPropertyChanged { .. }
                    | DiffOp::CalculatedColumnDefinitionChanged { .. }
                    | DiffOp::RelationshipAdded { .. }
                    | DiffOp::RelationshipRemoved { .. }
                    | DiffOp::RelationshipPropertyChanged { .. }
                    | DiffOp::MeasureAdded { .. }
                    | DiffOp::MeasureRemoved { .. }
                    | DiffOp::MeasureDefinitionChanged { .. }
            )
        }
        #[cfg(not(feature = "model-diff"))]
        {
            false
        }
    }

    pub fn cell_edited(
        sheet: SheetId,
        addr: CellAddress,
        from: CellSnapshot,
        to: CellSnapshot,
        formula_diff: FormulaDiffResult,
    ) -> DiffOp {
        debug_assert_eq!(from.addr, addr, "from.addr must match canonical addr");
        debug_assert_eq!(to.addr, addr, "to.addr must match canonical addr");
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            formula_diff,
        }
    }

    pub fn row_added(sheet: SheetId, row_idx: u32, row_signature: Option<RowSignature>) -> DiffOp {
        DiffOp::RowAdded {
            sheet,
            row_idx,
            row_signature,
        }
    }

    pub fn row_removed(
        sheet: SheetId,
        row_idx: u32,
        row_signature: Option<RowSignature>,
    ) -> DiffOp {
        DiffOp::RowRemoved {
            sheet,
            row_idx,
            row_signature,
        }
    }

    pub fn row_replaced(sheet: SheetId, row_idx: u32) -> DiffOp {
        DiffOp::RowReplaced { sheet, row_idx }
    }

    pub fn column_added(
        sheet: SheetId,
        col_idx: u32,
        col_signature: Option<ColSignature>,
    ) -> DiffOp {
        DiffOp::ColumnAdded {
            sheet,
            col_idx,
            col_signature,
        }
    }

    pub fn column_removed(
        sheet: SheetId,
        col_idx: u32,
        col_signature: Option<ColSignature>,
    ) -> DiffOp {
        DiffOp::ColumnRemoved {
            sheet,
            col_idx,
            col_signature,
        }
    }

    pub fn block_moved_rows(
        sheet: SheetId,
        src_start_row: u32,
        row_count: u32,
        dst_start_row: u32,
        block_hash: Option<u64>,
    ) -> DiffOp {
        DiffOp::BlockMovedRows {
            sheet,
            src_start_row,
            row_count,
            dst_start_row,
            block_hash,
        }
    }

    pub fn block_moved_columns(
        sheet: SheetId,
        src_start_col: u32,
        col_count: u32,
        dst_start_col: u32,
        block_hash: Option<u64>,
    ) -> DiffOp {
        DiffOp::BlockMovedColumns {
            sheet,
            src_start_col,
            col_count,
            dst_start_col,
            block_hash,
        }
    }

    #[allow(clippy::too_many_arguments)]
    pub fn block_moved_rect(
        sheet: SheetId,
        src_start_row: u32,
        src_row_count: u32,
        src_start_col: u32,
        src_col_count: u32,
        dst_start_row: u32,
        dst_start_col: u32,
        block_hash: Option<u64>,
    ) -> DiffOp {
        DiffOp::BlockMovedRect {
            sheet,
            src_start_row,
            src_row_count,
            src_start_col,
            src_col_count,
            dst_start_row,
            dst_start_col,
            block_hash,
        }
    }

    pub fn rect_replaced(
        sheet: SheetId,
        start_row: u32,
        row_count: u32,
        start_col: u32,
        col_count: u32,
    ) -> DiffOp {
        DiffOp::RectReplaced {
            sheet,
            start_row,
            row_count,
            start_col,
            col_count,
        }
    }
}

```

---

### File: `core\src\diffable.rs`

```rust
//! Lightweight Diffable trait for component-level diffs.
//!
//! This exposes a config-aware entry point so callers can diff grids, sheets, or
//! workbooks without standing up a full engine session.

use crate::config::DiffConfig;
use crate::diff::DiffReport;
use crate::string_pool::StringPool;
use crate::workbook::{Grid, Sheet, Workbook};

/// Shared context for Diffable implementations.
pub struct DiffContext<'a> {
    pub config: &'a DiffConfig,
    pub pool: &'a mut StringPool,
}

impl<'a> DiffContext<'a> {
    pub fn new(pool: &'a mut StringPool, config: &'a DiffConfig) -> Self {
        Self { config, pool }
    }
}

/// Component-level diff trait.
pub trait Diffable {
    type Output;

    /// Diff `self` against `other` using the supplied context.
    fn diff(&self, other: &Self, ctx: &mut DiffContext<'_>) -> Self::Output;
}

impl Diffable for Workbook {
    type Output = DiffReport;

    fn diff(&self, other: &Self, ctx: &mut DiffContext<'_>) -> DiffReport {
        crate::engine::diff_workbooks(self, other, ctx.pool, ctx.config)
    }
}

impl Diffable for Sheet {
    type Output = DiffReport;

    fn diff(&self, other: &Self, ctx: &mut DiffContext<'_>) -> DiffReport {
        crate::engine::diff_sheets(self, other, ctx.pool, ctx.config)
    }
}

impl Diffable for Grid {
    type Output = DiffReport;

    fn diff(&self, other: &Self, ctx: &mut DiffContext<'_>) -> DiffReport {
        crate::engine::diff_grids(self, other, ctx.pool, ctx.config)
    }
}

```

---

### File: `core\src\engine\amr.rs`

```rust
use crate::alignment::align_rows_amr_with_signatures_from_views;
#[cfg(feature = "perf-metrics")]
use crate::alignment::align_rows_amr_with_signatures_from_views_with_metrics;
use crate::alignment::move_extraction::moves_from_matched_pairs;
use crate::alignment_types::RowAlignment;
use crate::column_alignment::{
    align_columns_amr_from_views, align_single_column_change_from_views,
};
use crate::config::DiffConfig;
use crate::diff::DiffError;
use crate::grid_view::GridView;
#[cfg(feature = "perf-metrics")]
use crate::perf::Phase;
use crate::sink::DiffSink;
use crate::workbook::{Grid, RowSignature};

use std::collections::{HashMap, HashSet};

use super::context::EmitCtx;
use super::grid_primitives::{
    emit_column_aligned_diffs, emit_row_aligned_diffs, run_positional_diff_from_views_with_metrics,
};
use super::move_mask::row_signature_at;

pub(crate) fn row_signature_multiset_equal(a: &Grid, b: &Grid) -> bool {
    if a.nrows != b.nrows {
        return false;
    }

    let mut a_sigs: Vec<RowSignature> = (0..a.nrows)
        .filter_map(|row| row_signature_at(a, row))
        .collect();
    let mut b_sigs: Vec<RowSignature> = (0..b.nrows)
        .filter_map(|row| row_signature_at(b, row))
        .collect();

    a_sigs.sort_unstable_by_key(|s| s.hash);
    b_sigs.sort_unstable_by_key(|s| s.hash);

    a_sigs == b_sigs
}

fn amr_strip_moves_policy(old: &Grid, new: &Grid, alignment: &mut RowAlignment) {
    let mut deleted_from_moves = Vec::new();
    let mut inserted_from_moves = Vec::new();
    for mv in &alignment.moves {
        deleted_from_moves.extend(mv.src_start_row..mv.src_start_row.saturating_add(mv.row_count));
        inserted_from_moves.extend(mv.dst_start_row..mv.dst_start_row.saturating_add(mv.row_count));
    }

    let multiset_equal = row_signature_multiset_equal(old, new);
    if multiset_equal {
        for (a, b) in &alignment.matched {
            if row_signature_at(old, *a) != row_signature_at(new, *b) {
                deleted_from_moves.push(*a);
                inserted_from_moves.push(*b);
            }
        }
    }

    if !deleted_from_moves.is_empty() || !inserted_from_moves.is_empty() {
        let deleted_set: HashSet<u32> = deleted_from_moves.iter().copied().collect();
        let inserted_set: HashSet<u32> = inserted_from_moves.iter().copied().collect();

        alignment
            .matched
            .retain(|(a, b)| !deleted_set.contains(a) && !inserted_set.contains(b));

        alignment.deleted.extend(deleted_set);
        alignment.inserted.extend(inserted_set);
        alignment.deleted.sort_unstable();
        alignment.deleted.dedup();
        alignment.inserted.sort_unstable();
        alignment.inserted.dedup();
    }

    alignment.moves.clear();
}

fn amr_should_fallback_no_matched_rows(alignment: &RowAlignment) -> bool {
    let has_structural = !alignment.inserted.is_empty() || !alignment.deleted.is_empty();
    has_structural && alignment.matched.is_empty()
}

fn amr_should_fallback_row_edits_with_structural(
    old: &Grid,
    new: &Grid,
    alignment: &RowAlignment,
    config: &DiffConfig,
) -> bool {
    let has_structural = !alignment.inserted.is_empty() || !alignment.deleted.is_empty();
    if !has_structural {
        return false;
    }

    let has_row_edits = alignment
        .matched
        .iter()
        .any(|(a, b)| row_signature_at(old, *a) != row_signature_at(new, *b));

    has_row_edits && config.moves.max_move_iterations > 0
}

fn amr_alignment_is_trivial_identity(old: &Grid, new: &Grid, alignment: &RowAlignment) -> bool {
    alignment.moves.is_empty()
        && alignment.inserted.is_empty()
        && alignment.deleted.is_empty()
        && old.nrows == new.nrows
        && alignment.matched.len() as u32 == old.nrows
        && alignment.matched.iter().all(|(a, b)| a == b)
}

fn amr_should_fallback_multiset_reorder(
    old: &Grid,
    new: &Grid,
    alignment: &RowAlignment,
    config: &DiffConfig,
) -> bool {
    let is_trivial = amr_alignment_is_trivial_identity(old, new, alignment);
    !is_trivial
        && alignment.moves.is_empty()
        && row_signature_multiset_equal(old, new)
        && config.moves.max_move_iterations > 0
}

fn amr_can_try_column_alignment(alignment: &RowAlignment) -> bool {
    alignment.moves.is_empty() && alignment.inserted.is_empty() && alignment.deleted.is_empty()
}

fn inject_moves_from_insert_delete(
    old: &Grid,
    new: &Grid,
    alignment: &mut RowAlignment,
    row_signatures_old: &[RowSignature],
    row_signatures_new: &[RowSignature],
) {
    if alignment.inserted.is_empty() || alignment.deleted.is_empty() {
        return;
    }

    let mut deleted_by_sig: HashMap<RowSignature, Vec<u32>> = HashMap::new();
    for row in &alignment.deleted {
        let sig = row_signatures_old
            .get(*row as usize)
            .copied()
            .or_else(|| row_signature_at(old, *row));
        if let Some(sig) = sig {
            deleted_by_sig.entry(sig).or_default().push(*row);
        }
    }

    let mut inserted_by_sig: HashMap<RowSignature, Vec<u32>> = HashMap::new();
    for row in &alignment.inserted {
        let sig = row_signatures_new
            .get(*row as usize)
            .copied()
            .or_else(|| row_signature_at(new, *row));
        if let Some(sig) = sig {
            inserted_by_sig.entry(sig).or_default().push(*row);
        }
    }

    let mut matched_pairs = Vec::new();
    for (sig, deleted_rows) in deleted_by_sig.iter() {
        if deleted_rows.len() != 1 {
            continue;
        }
        if let Some(insert_rows) = inserted_by_sig.get(sig) {
            if insert_rows.len() != 1 {
                continue;
            }
            matched_pairs.push((deleted_rows[0], insert_rows[0]));
        }
    }

    if matched_pairs.is_empty() {
        return;
    }

    let new_moves = moves_from_matched_pairs(&matched_pairs);
    if new_moves.is_empty() {
        return;
    }

    let mut moved_src = HashSet::new();
    let mut moved_dst = HashSet::new();
    for mv in &new_moves {
        for r in mv.src_start_row..mv.src_start_row.saturating_add(mv.row_count) {
            moved_src.insert(r);
        }
        for r in mv.dst_start_row..mv.dst_start_row.saturating_add(mv.row_count) {
            moved_dst.insert(r);
        }
    }

    alignment.deleted.retain(|r| !moved_src.contains(r));
    alignment.inserted.retain(|r| !moved_dst.contains(r));
    alignment.moves.extend(new_moves);
    alignment
        .moves
        .sort_by_key(|m| (m.src_start_row, m.dst_start_row, m.row_count));
}

pub(super) fn try_diff_with_amr<S: DiffSink>(
    emit_ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<bool, DiffError> {
    #[cfg(feature = "perf-metrics")]
    let amr_result = {
        if let Some(m) = emit_ctx.metrics.as_deref_mut() {
            align_rows_amr_with_signatures_from_views_with_metrics(
                old_view,
                new_view,
                emit_ctx.config,
                m,
            )
        } else {
            align_rows_amr_with_signatures_from_views(old_view, new_view, emit_ctx.config)
        }
    };
    #[cfg(not(feature = "perf-metrics"))]
    let amr_result = align_rows_amr_with_signatures_from_views(old_view, new_view, emit_ctx.config);

    let Some(amr_result) = amr_result else {
        return Ok(false);
    };

    let mut alignment = amr_result.alignment;

    if emit_ctx.config.moves.max_move_iterations > 0 {
        inject_moves_from_insert_delete(
            old,
            new,
            &mut alignment,
            &amr_result.row_signatures_a,
            &amr_result.row_signatures_b,
        );
    } else {
        amr_strip_moves_policy(old, new, &mut alignment);
    }

    if amr_should_fallback_no_matched_rows(&alignment) {
        run_positional_diff_from_views_with_metrics(emit_ctx, old, new, old_view, new_view)?;
        return Ok(true);
    }

    if amr_should_fallback_row_edits_with_structural(old, new, &alignment, emit_ctx.config) {
        run_positional_diff_from_views_with_metrics(emit_ctx, old, new, old_view, new_view)?;
        return Ok(true);
    }

    if amr_can_try_column_alignment(&alignment) {
        let col_alignment = align_columns_amr_from_views(old_view, new_view, emit_ctx.config)
            .or_else(|| align_single_column_change_from_views(old_view, new_view, emit_ctx.config));

        if let Some(col_alignment) = col_alignment {
            emit_ctx.hardening.progress("cell_diff", 0.0);
            #[cfg(feature = "perf-metrics")]
            if let Some(m) = emit_ctx.metrics.as_deref_mut() {
                m.start_phase(Phase::CellDiff);
            }
            emit_column_aligned_diffs(emit_ctx, old, new, &col_alignment)?;
            emit_ctx.hardening.progress("cell_diff", 1.0);
            #[cfg(feature = "perf-metrics")]
            if let Some(m) = emit_ctx.metrics.as_deref_mut() {
                let overlap_rows = old.nrows.min(new.nrows) as u64;
                m.add_cells_compared(
                    overlap_rows.saturating_mul(col_alignment.matched.len() as u64),
                );
                m.end_phase(Phase::CellDiff);
            }
            return Ok(true);
        }
    }

    if amr_should_fallback_multiset_reorder(old, new, &alignment, emit_ctx.config) {
        run_positional_diff_from_views_with_metrics(emit_ctx, old, new, old_view, new_view)?;
        return Ok(true);
    }

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::CellDiff);
    }

    emit_ctx.hardening.progress("cell_diff", 0.0);
    let compared = emit_row_aligned_diffs(emit_ctx, old_view, new_view, &alignment)?;
    emit_ctx.hardening.progress("cell_diff", 1.0);
    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        m.add_cells_compared(compared);
        m.anchors_found = m
            .anchors_found
            .saturating_add(alignment.matched.len() as u32);
        m.moves_detected = m
            .moves_detected
            .saturating_add(alignment.moves.len() as u32);
        m.end_phase(Phase::CellDiff);
    }
    #[cfg(not(feature = "perf-metrics"))]
    let _ = compared;

    Ok(true)
}

```

---

### File: `core\src\engine\context.rs`

```rust
use crate::config::DiffConfig;
use crate::diff::{DiffError, DiffOp};
use crate::formula_diff::FormulaParseCache;
#[cfg(feature = "perf-metrics")]
use crate::perf::{DiffMetrics, Phase};
use crate::sink::DiffSink;
use crate::string_pool::StringPool;

use crate::diff::SheetId;
use super::hardening::HardeningController;

#[derive(Debug, Default)]
pub(super) struct DiffContext {
    pub(super) warnings: Vec<String>,
    pub(super) formula_cache: FormulaParseCache,
}

pub(super) fn emit_op<S: DiffSink>(
    sink: &mut S,
    op_count: &mut usize,
    op: DiffOp,
) -> Result<(), DiffError> {
    sink.emit(op)?;
    *op_count = op_count.saturating_add(1);
    Ok(())
}

pub(super) struct EmitCtx<'a, 'p, S: DiffSink> {
    pub(super) sheet_id: SheetId,
    pub(super) pool: &'a StringPool,
    pub(super) config: &'a DiffConfig,
    pub(super) cache: &'a mut FormulaParseCache,
    pub(super) sink: &'a mut S,
    pub(super) op_count: &'a mut usize,
    pub(super) warnings: &'a mut Vec<String>,
    pub(super) hardening: &'a mut HardeningController<'p>,
    #[cfg(feature = "perf-metrics")]
    pub(super) metrics: Option<&'a mut DiffMetrics>,
}

impl<'a, 'p, S: DiffSink> EmitCtx<'a, 'p, S> {
    pub(super) fn new(
        sheet_id: SheetId,
        pool: &'a StringPool,
        config: &'a DiffConfig,
        cache: &'a mut FormulaParseCache,
        sink: &'a mut S,
        op_count: &'a mut usize,
        warnings: &'a mut Vec<String>,
        hardening: &'a mut HardeningController<'p>,
        #[cfg(feature = "perf-metrics")] metrics: Option<&'a mut DiffMetrics>,
    ) -> Self {
        Self {
            sheet_id,
            pool,
            config,
            cache,
            sink,
            op_count,
            warnings,
            hardening,
            #[cfg(feature = "perf-metrics")]
            metrics,
        }
    }

    pub(super) fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        if self.hardening.check_op_limit(*self.op_count, self.warnings) {
            return Ok(());
        }
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = self.metrics.as_deref_mut() {
            let _guard = m.phase_guard(Phase::OpEmit);
            return emit_op(self.sink, self.op_count, op);
        }
        emit_op(self.sink, self.op_count, op)
    }
}

```

---

### File: `core\src\engine\grid_diff.rs`

```rust
use crate::config::{DiffConfig, LimitBehavior};
use crate::diff::{DiffError, DiffOp, DiffReport, DiffSummary};
use crate::grid_view::GridView;
#[cfg(feature = "perf-metrics")]
use crate::perf::{DiffMetrics, Phase};
use crate::progress::ProgressCallback;
use crate::sink::{DiffSink, SinkFinishGuard, VecSink};
use crate::string_pool::StringPool;
use crate::workbook::{CellAddress, CellValue, Grid, RowSignature};
use std::collections::{HashMap, HashSet};

use crate::diff::SheetId;
use super::context::{DiffContext, EmitCtx};
use super::grid_primitives::{
    cells_content_equal, emit_cell_edit, positional_diff_for_rows,
    run_positional_diff_with_metrics,
};
use super::move_mask::SheetGridDiffer;

use crate::database_alignment::diff_table_by_key;
use crate::matching::hungarian;

const GRID_MODE_SHEET_ID: &str = "<grid>";
const DATABASE_MODE_SHEET_ID: &str = "<database>";
const DUPLICATE_CLUSTER_EXACT_MAX: usize = 16;
const DUPLICATE_MATCH_THRESHOLD: f64 = 0.5;
const TABLE_COLUMN_FILL_RATIO: f64 = 0.5;

pub fn diff_grids(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> DiffReport {
    let mut sink = VecSink::new();
    match try_diff_grids_streaming(old, new, pool, config, &mut sink) {
        Ok(summary) => {
            let strings = pool.strings().to_vec();
            DiffReport::from_ops_and_summary(sink.into_ops(), summary, strings)
        }
        Err(e) => {
            let strings = pool.strings().to_vec();
            DiffReport {
                version: DiffReport::SCHEMA_VERSION.to_string(),
                strings,
                ops: sink.into_ops(),
                complete: false,
                warnings: vec![e.to_string()],
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            }
        }
    }
}

pub fn try_diff_grids(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Result<DiffReport, DiffError> {
    let mut sink = VecSink::new();
    let summary = try_diff_grids_streaming(old, new, pool, config, &mut sink)?;
    let strings = pool.strings().to_vec();
    Ok(DiffReport::from_ops_and_summary(
        sink.into_ops(),
        summary,
        strings,
    ))
}

/// Stream a grid diff into `sink`.
///
/// Streaming output follows the contract in `docs/streaming_contract.md`.
pub fn diff_grids_streaming<S: DiffSink>(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> DiffSummary {
    match try_diff_grids_streaming(old, new, pool, config, sink) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

pub fn diff_grids_streaming_with_progress<S: DiffSink>(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> DiffSummary {
    match try_diff_grids_streaming_with_progress(old, new, pool, config, sink, progress) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

/// Like [`diff_grids_streaming`], but returns errors instead of embedding them in the summary.
pub fn try_diff_grids_streaming<S: DiffSink>(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> Result<DiffSummary, DiffError> {
    let mut op_count = 0usize;
    try_diff_grids_streaming_with_op_count(old, new, pool, config, sink, &mut op_count, None)
}

pub fn try_diff_grids_streaming_with_progress<S: DiffSink>(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> Result<DiffSummary, DiffError> {
    let mut op_count = 0usize;
    try_diff_grids_streaming_with_op_count(
        old,
        new,
        pool,
        config,
        sink,
        &mut op_count,
        Some(progress),
    )
}

#[allow(clippy::too_many_arguments)]
fn try_diff_grids_streaming_with_op_count<'p, S: DiffSink>(
    old: &Grid,
    new: &Grid,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    op_count: &mut usize,
    progress: Option<&'p dyn ProgressCallback>,
) -> Result<DiffSummary, DiffError> {
    let sheet_id: SheetId = pool.intern(GRID_MODE_SHEET_ID);

    sink.begin(pool)?;
    let mut finish_guard = SinkFinishGuard::new(sink);

    let mut ctx = DiffContext::default();
    let mut hardening = super::hardening::HardeningController::new(config, progress);

    if hardening.check_timeout(&mut ctx.warnings) {
        finish_guard.finish_and_disarm()?;
        return Ok(DiffSummary {
            complete: false,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    }

    try_diff_grids_internal(
        sheet_id,
        old,
        new,
        config,
        pool,
        sink,
        op_count,
        &mut ctx,
        &mut hardening,
        #[cfg(feature = "perf-metrics")]
        None,
    )?;

    finish_guard.finish_and_disarm()?;
    let complete = ctx.warnings.is_empty();
    Ok(DiffSummary {
        complete,
        warnings: ctx.warnings,
        op_count: *op_count,
        #[cfg(feature = "perf-metrics")]
        metrics: None,
    })
}

#[allow(clippy::too_many_arguments)]
pub(super) fn try_diff_grids_internal<'p, S: DiffSink>(
    sheet_id: SheetId,
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
    pool: &StringPool,
    sink: &mut S,
    op_count: &mut usize,
    ctx: &mut DiffContext,
    hardening: &mut super::hardening::HardeningController<'p>,
    #[cfg(feature = "perf-metrics")] mut metrics: Option<&mut DiffMetrics>,
) -> Result<(), DiffError> {
    if old.nrows == 0 && new.nrows == 0 {
        return Ok(());
    }

    if hardening.check_timeout(&mut ctx.warnings) {
        return Ok(());
    }

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = metrics.as_mut() {
        m.rows_processed = m
            .rows_processed
            .saturating_add(old.nrows as u64)
            .saturating_add(new.nrows as u64);
    }

    let exceeds_limits = old.nrows.max(new.nrows) > config.alignment.max_align_rows
        || old.ncols.max(new.ncols) > config.alignment.max_align_cols;

    if exceeds_limits {
        let warning = format!(
            "Sheet '{}': alignment limits exceeded (rows={}, cols={}; limits: rows={}, cols={})",
            pool.resolve(sheet_id),
            old.nrows.max(new.nrows),
            old.ncols.max(new.ncols),
            config.alignment.max_align_rows,
            config.alignment.max_align_cols
        );

        match config.hardening.on_limit_exceeded {
            LimitBehavior::ReturnError => {
                return Err(DiffError::LimitsExceeded {
                    sheet: sheet_id,
                    rows: old.nrows.max(new.nrows),
                    cols: old.ncols.max(new.ncols),
                    max_rows: config.alignment.max_align_rows,
                    max_cols: config.alignment.max_align_cols,
                });
            }
            behavior => {
                if matches!(behavior, LimitBehavior::ReturnPartialResult) {
                    ctx.warnings.push(warning);
                }

                let mut emit_ctx = EmitCtx::new(
                    sheet_id,
                    pool,
                    config,
                    &mut ctx.formula_cache,
                    sink,
                    op_count,
                    &mut ctx.warnings,
                    hardening,
                    #[cfg(feature = "perf-metrics")]
                    metrics.as_deref_mut(),
                );

                run_positional_diff_with_metrics(&mut emit_ctx, old, new)?;

                return Ok(());
            }
        }
    }

    diff_grids_core(
        sheet_id,
        old,
        new,
        config,
        pool,
        sink,
        op_count,
        ctx,
        hardening,
        #[cfg(feature = "perf-metrics")]
        metrics,
    )?;

    Ok(())
}

#[allow(clippy::too_many_arguments)]
fn diff_grids_core<'p, S: DiffSink>(
    sheet_id: SheetId,
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
    pool: &StringPool,
    sink: &mut S,
    op_count: &mut usize,
    ctx: &mut DiffContext,
    hardening: &mut super::hardening::HardeningController<'p>,
    #[cfg(feature = "perf-metrics")] mut metrics: Option<&mut DiffMetrics>,
) -> Result<(), DiffError> {
    if old.nrows == new.nrows && old.ncols == new.ncols && grids_non_blank_cells_equal(old, new) {
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = metrics.as_mut() {
            m.add_cells_compared(cells_in_overlap(old, new));
        }
        return Ok(());
    }

    if hardening.check_timeout(&mut ctx.warnings) {
        return Ok(());
    }

    let sheet_name = pool.resolve(sheet_id);
    let context = format!("sheet '{sheet_name}'");
    if hardening.memory_guard_or_warn(
        crate::memory_estimate::estimate_advanced_sheet_diff_peak(old, new),
        &mut ctx.warnings,
        &context,
    ) {
        let mut emit_ctx = EmitCtx::new(
            sheet_id,
            pool,
            config,
            &mut ctx.formula_cache,
            sink,
            op_count,
            &mut ctx.warnings,
            hardening,
            #[cfg(feature = "perf-metrics")]
            metrics.as_deref_mut(),
        );
        run_positional_diff_with_metrics(&mut emit_ctx, old, new)?;
        return Ok(());
    }

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = metrics.as_mut() {
        m.start_phase(Phase::SignatureBuild);
    }
    let preflight = preflight_decision_from_grids(old, new, config);

    if matches!(
        preflight.decision,
        PreflightDecision::ShortCircuitNearIdentical | PreflightDecision::ShortCircuitDissimilar
    ) {
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = metrics.as_mut() {
            m.end_phase(Phase::SignatureBuild);
        }
        let mut emit_ctx = EmitCtx::new(
            sheet_id,
            pool,
            config,
            &mut ctx.formula_cache,
            sink,
            op_count,
            &mut ctx.warnings,
            hardening,
            #[cfg(feature = "perf-metrics")]
            metrics.as_deref_mut(),
        );

        if preflight.decision == PreflightDecision::ShortCircuitNearIdentical
            && old.nrows == new.nrows
            && old.ncols == new.ncols
        {
            let rows = rows_with_context(
                &preflight.mismatched_rows,
                config.preflight.max_context_rows,
                old.nrows,
            );
            #[cfg(feature = "perf-metrics")]
            if let Some(m) = emit_ctx.metrics.as_deref_mut() {
                m.start_phase(Phase::CellDiff);
            }
            let compared = positional_diff_for_rows(&mut emit_ctx, old, new, &rows)?;
            #[cfg(feature = "perf-metrics")]
            if let Some(m) = emit_ctx.metrics.as_deref_mut() {
                m.add_cells_compared(compared);
                m.end_phase(Phase::CellDiff);
            }
            #[cfg(not(feature = "perf-metrics"))]
            let _ = compared;
        } else {
            run_positional_diff_with_metrics(&mut emit_ctx, old, new)?;
        }

        return Ok(());
    }

    let old_view = GridView::from_grid_with_config(old, config);
    let new_view = GridView::from_grid_with_config(new, config);
    #[cfg(feature = "perf-metrics")]
    if let Some(m) = metrics.as_mut() {
        let lookups = old.cell_count() as u64 + new.cell_count() as u64;
        let allocations = old.nrows as u64
            + old.ncols as u64
            + new.nrows as u64
            + new.ncols as u64;
        m.add_hash_lookups_est(lookups);
        m.add_allocations_est(allocations);
        m.end_phase(Phase::SignatureBuild);
    }

    let emit_ctx = EmitCtx::new(
        sheet_id,
        pool,
        config,
        &mut ctx.formula_cache,
        sink,
        op_count,
        &mut ctx.warnings,
        hardening,
        #[cfg(feature = "perf-metrics")]
        metrics.as_deref_mut(),
    );

    let mut differ = SheetGridDiffer::from_views(
        emit_ctx,
        old,
        new,
        old_view,
        new_view,
    );

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::MoveDetection);
    }

    differ.emit_ctx.hardening.progress("move_detection", 0.0);
    if differ.emit_ctx.hardening.check_timeout(differ.emit_ctx.warnings) {
        return Ok(());
    }
    differ.detect_moves()?;
    differ.emit_ctx.hardening.progress("move_detection", 1.0);

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
        m.end_phase(Phase::MoveDetection);
    }

    if differ.emit_ctx.hardening.should_abort() {
        return Ok(());
    }

    if differ.has_mask_exclusions() {
        differ.emit_ctx.hardening.progress("alignment", 1.0);
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
            m.start_phase(Phase::CellDiff);
        }
        differ.diff_with_masks()?;
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
            m.end_phase(Phase::CellDiff);
        }
        return Ok(());
    }

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::Alignment);
    }

    differ.emit_ctx.hardening.progress("alignment", 0.0);
    if differ.emit_ctx.hardening.check_timeout(differ.emit_ctx.warnings) {
        return Ok(());
    }
    if differ.try_amr()? {
        differ.emit_ctx.hardening.progress("alignment", 1.0);
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
            m.end_phase(Phase::Alignment);
        }
        return Ok(());
    }

    if differ.try_row_alignment()? {
        differ.emit_ctx.hardening.progress("alignment", 1.0);
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
            m.end_phase(Phase::Alignment);
        }
        return Ok(());
    }

    if differ.try_single_column_alignment()? {
        differ.emit_ctx.hardening.progress("alignment", 1.0);
        #[cfg(feature = "perf-metrics")]
        if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
            m.end_phase(Phase::Alignment);
        }
        return Ok(());
    }

    differ.positional()?;

    differ.emit_ctx.hardening.progress("alignment", 1.0);

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = differ.emit_ctx.metrics.as_deref_mut() {
        m.end_phase(Phase::Alignment);
    }

    Ok(())
}

pub fn diff_grids_database_mode(
    old: &Grid,
    new: &Grid,
    key_columns: &[u32],
    pool: &mut StringPool,
    config: &DiffConfig,
) -> DiffReport {
    let sheet_id: SheetId = pool.intern(DATABASE_MODE_SHEET_ID);
    let mut sink = VecSink::new();
    let mut op_count = 0usize;
    match try_diff_grids_database_mode_streaming(
        sheet_id,
        old,
        new,
        key_columns,
        pool,
        config,
        &mut sink,
        &mut op_count,
    ) {
        Ok(summary) => {
            let strings = pool.strings().to_vec();
            DiffReport::from_ops_and_summary(sink.into_ops(), summary, strings)
        }
        Err(e) => {
            let strings = pool.strings().to_vec();
            DiffReport {
                version: DiffReport::SCHEMA_VERSION.to_string(),
                strings,
                ops: sink.into_ops(),
                complete: false,
                warnings: vec![e.to_string()],
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            }
        }
    }
}

/// Stream a database-mode grid diff into `sink`.
///
/// Streaming output follows the contract in `docs/streaming_contract.md`.
pub fn try_diff_grids_database_mode_streaming<S: DiffSink>(
    sheet_id: SheetId,
    old: &Grid,
    new: &Grid,
    key_columns: &[u32],
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    op_count: &mut usize,
) -> Result<DiffSummary, DiffError> {
    let mut ctx = DiffContext::default();
    let mut hardening = super::hardening::HardeningController::new(config, None);

    sink.begin(pool)?;
    let mut finish_guard = SinkFinishGuard::new(sink);
    if hardening.check_timeout(&mut ctx.warnings) {
        finish_guard.finish_and_disarm()?;
        return Ok(DiffSummary {
            complete: false,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    }

    if key_columns.is_empty() {
        ctx.warnings.push(
            "database-mode: no key columns provided; falling back to spreadsheet mode"
                .to_string(),
        );
        try_diff_grids_internal(
            sheet_id,
            old,
            new,
            config,
            pool,
            sink,
            op_count,
            &mut ctx,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        )?;
        finish_guard.finish_and_disarm()?;
        let complete = ctx.warnings.is_empty();
        return Ok(DiffSummary {
            complete,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    }

    if key_columns
        .iter()
        .any(|&col| col >= old.ncols || col >= new.ncols)
    {
        ctx.warnings.push(
            "database-mode: invalid key columns; falling back to spreadsheet mode".to_string(),
        );
        try_diff_grids_internal(
            sheet_id,
            old,
            new,
            config,
            pool,
            sink,
            op_count,
            &mut ctx,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        )?;
        finish_guard.finish_and_disarm()?;
        let complete = ctx.warnings.is_empty();
        return Ok(DiffSummary {
            complete,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    }

    let Some(table_scope) = build_table_scope(old, new, key_columns) else {
        ctx.warnings.push(
            "database-mode: no non-empty keys found; falling back to spreadsheet mode"
                .to_string(),
        );
        try_diff_grids_internal(
            sheet_id,
            old,
            new,
            config,
            pool,
            sink,
            op_count,
            &mut ctx,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        )?;
        finish_guard.finish_and_disarm()?;
        let complete = ctx.warnings.is_empty();
        return Ok(DiffSummary {
            complete,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    };

    let table_rows =
        table_scope.rows_old.len().max(table_scope.rows_new.len()) as u32;
    let table_cols = table_scope.cols_union.len() as u32;
    let exceeds_limits = table_rows > config.alignment.max_align_rows
        || table_cols > config.alignment.max_align_cols;

    if exceeds_limits {
        let warning = format!(
            "Sheet '{}': alignment limits exceeded (rows={}, cols={}; limits: rows={}, cols={})",
            pool.resolve(sheet_id),
            table_rows,
            table_cols,
            config.alignment.max_align_rows,
            config.alignment.max_align_cols
        );

        match config.hardening.on_limit_exceeded {
            LimitBehavior::ReturnError => {
                return Err(DiffError::LimitsExceeded {
                    sheet: sheet_id,
                    rows: table_rows,
                    cols: table_cols,
                    max_rows: config.alignment.max_align_rows,
                    max_cols: config.alignment.max_align_cols,
                });
            }
            behavior => {
                if matches!(behavior, LimitBehavior::ReturnPartialResult) {
                    ctx.warnings.push(warning);
                }

                let mut emit_ctx = EmitCtx::new(
                    sheet_id,
                    pool,
                    config,
                    &mut ctx.formula_cache,
                    sink,
                    op_count,
                    &mut ctx.warnings,
                    &mut hardening,
                    #[cfg(feature = "perf-metrics")]
                    None,
                );
                run_positional_diff_with_metrics(&mut emit_ctx, old, new)?;
                finish_guard.finish_and_disarm()?;
                let complete = ctx.warnings.is_empty();
                return Ok(DiffSummary {
                    complete,
                    warnings: ctx.warnings,
                    op_count: *op_count,
                    #[cfg(feature = "perf-metrics")]
                    metrics: None,
                });
            }
        }
    }

    let (table_old, row_map_old) =
        build_table_grid(old, &table_scope.rows_old, &table_scope.cols_union);
    let (table_new, row_map_new) =
        build_table_grid(new, &table_scope.rows_new, &table_scope.cols_union);

    let Some(table_key_cols) = map_key_columns(key_columns, &table_scope.cols_union) else {
        ctx.warnings.push(
            "database-mode: invalid key columns; falling back to spreadsheet mode".to_string(),
        );
        try_diff_grids_internal(
            sheet_id,
            old,
            new,
            config,
            pool,
            sink,
            op_count,
            &mut ctx,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        )?;
        finish_guard.finish_and_disarm()?;
        let complete = ctx.warnings.is_empty();
        return Ok(DiffSummary {
            complete,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    };

    let key_col_set: HashSet<u32> = table_key_cols.iter().copied().collect();
    let max_cols = table_old.ncols.max(table_new.ncols);
    let compare_cols: Vec<u32> = (0..max_cols)
        .filter(|col| !key_col_set.contains(col))
        .collect();

    let alignment = diff_table_by_key(&table_old, &table_new, &table_key_cols);

    {
        let mut emit_ctx = EmitCtx::new(
            sheet_id,
            pool,
            config,
            &mut ctx.formula_cache,
            sink,
            op_count,
            &mut ctx.warnings,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        );
        let should_abort = |emit_ctx: &mut EmitCtx<'_, '_, S>| {
            let hardening = &mut *emit_ctx.hardening;
            let warnings = &mut *emit_ctx.warnings;
            hardening.check_timeout(warnings) || hardening.should_abort()
        };

        for row_idx in &alignment.left_only_rows {
            if should_abort(&mut emit_ctx) {
                break;
            }
            if let Some(row) = row_map_old.get(*row_idx as usize).copied() {
                emit_ctx.emit(DiffOp::row_removed(sheet_id, row, None))?;
            }
        }

        for row_idx in &alignment.right_only_rows {
            if should_abort(&mut emit_ctx) {
                break;
            }
            if let Some(row) = row_map_new.get(*row_idx as usize).copied() {
                emit_ctx.emit(DiffOp::row_added(sheet_id, row, None))?;
            }
        }

        for (row_a, row_b) in &alignment.matched_rows {
            if should_abort(&mut emit_ctx) {
                break;
            }
            let Some(row_a_orig) = row_map_old.get(*row_a as usize).copied() else {
                continue;
            };
            let Some(row_b_orig) = row_map_new.get(*row_b as usize).copied() else {
                continue;
            };
            let row_shift = row_b_orig as i32 - row_a_orig as i32;

            for col in 0..max_cols {
                if key_col_set.contains(&col) {
                    continue;
                }

                let old_cell = table_old.get(*row_a, col);
                let new_cell = table_new.get(*row_b, col);

                if cells_content_equal(old_cell, new_cell) {
                    continue;
                }

                let Some(col_orig) = table_scope.cols_union.get(col as usize).copied() else {
                    continue;
                };
                let addr = CellAddress::from_indices(row_b_orig, col_orig);
                emit_cell_edit(&mut emit_ctx, addr, old_cell, new_cell, row_shift, 0)?;
            }
        }

        for cluster in &alignment.duplicate_clusters {
            if should_abort(&mut emit_ctx) {
                break;
            }

            let left_rows: Vec<u32> = cluster
                .left_rows
                .iter()
                .filter_map(|idx| row_map_old.get(*idx as usize).copied())
                .collect();
            let right_rows: Vec<u32> = cluster
                .right_rows
                .iter()
                .filter_map(|idx| row_map_new.get(*idx as usize).copied())
                .collect();

            emit_ctx.emit(DiffOp::DuplicateKeyCluster {
                sheet: sheet_id,
                key: cluster.key.as_cell_values(),
                left_rows: left_rows.clone(),
                right_rows: right_rows.clone(),
            })?;

            let cluster_match = match_duplicate_cluster(
                &table_old,
                &table_new,
                &cluster.left_rows,
                &cluster.right_rows,
                &compare_cols,
            );

            for (row_a, row_b) in cluster_match.matched {
                if should_abort(&mut emit_ctx) {
                    break;
                }
                let Some(row_a_orig) = row_map_old.get(row_a as usize).copied() else {
                    continue;
                };
                let Some(row_b_orig) = row_map_new.get(row_b as usize).copied() else {
                    continue;
                };
                let row_shift = row_b_orig as i32 - row_a_orig as i32;

                for col in 0..max_cols {
                    if key_col_set.contains(&col) {
                        continue;
                    }

                    let old_cell = table_old.get(row_a, col);
                    let new_cell = table_new.get(row_b, col);
                    if cells_content_equal(old_cell, new_cell) {
                        continue;
                    }
                    let Some(col_orig) = table_scope.cols_union.get(col as usize).copied() else {
                        continue;
                    };
                    let addr = CellAddress::from_indices(row_b_orig, col_orig);
                    emit_cell_edit(&mut emit_ctx, addr, old_cell, new_cell, row_shift, 0)?;
                }
            }

            for row_idx in cluster_match.left_unmatched {
                if should_abort(&mut emit_ctx) {
                    break;
                }
                if let Some(row) = row_map_old.get(row_idx as usize).copied() {
                    emit_ctx.emit(DiffOp::row_removed(sheet_id, row, None))?;
                }
            }

            for row_idx in cluster_match.right_unmatched {
                if should_abort(&mut emit_ctx) {
                    break;
                }
                if let Some(row) = row_map_new.get(row_idx as usize).copied() {
                    emit_ctx.emit(DiffOp::row_added(sheet_id, row, None))?;
                }
            }
        }
    }

    if !hardening.should_abort()
        && (has_cells_outside_rect(
            old,
            table_scope.row_start,
            table_scope.row_end,
            table_scope.col_start,
            table_scope.col_end,
        ) || has_cells_outside_rect(
            new,
            table_scope.row_start,
            table_scope.row_end,
            table_scope.col_start,
            table_scope.col_end,
        ))
    {
        let old_free = mask_grid_excluding_rect(
            old,
            table_scope.row_start,
            table_scope.row_end,
            table_scope.col_start,
            table_scope.col_end,
        );
        let new_free = mask_grid_excluding_rect(
            new,
            table_scope.row_start,
            table_scope.row_end,
            table_scope.col_start,
            table_scope.col_end,
        );

        if !hardening.check_timeout(&mut ctx.warnings) {
            let mut emit_ctx = EmitCtx::new(
                sheet_id,
                pool,
                config,
                &mut ctx.formula_cache,
                sink,
                op_count,
                &mut ctx.warnings,
                &mut hardening,
                #[cfg(feature = "perf-metrics")]
                None,
            );
            run_positional_diff_with_metrics(&mut emit_ctx, &old_free, &new_free)?;
        }
    }

    finish_guard.finish_and_disarm()?;
    Ok(DiffSummary {
        complete: ctx.warnings.is_empty(),
        warnings: ctx.warnings,
        op_count: *op_count,
        #[cfg(feature = "perf-metrics")]
        metrics: None,
    })
}

#[derive(Debug, Clone)]
struct TableScope {
    rows_old: Vec<u32>,
    rows_new: Vec<u32>,
    cols_union: Vec<u32>,
    row_start: u32,
    row_end: u32,
    col_start: u32,
    col_end: u32,
}

fn build_table_scope(old: &Grid, new: &Grid, key_columns: &[u32]) -> Option<TableScope> {
    let rows_old = table_rows_for_grid(old, key_columns);
    let rows_new = table_rows_for_grid(new, key_columns);
    let rows_union = union_sorted(&rows_old, &rows_new);
    if rows_union.is_empty() {
        return None;
    }

    let cols_old = table_cols_for_grid(old, &rows_old, key_columns);
    let cols_new = table_cols_for_grid(new, &rows_new, key_columns);
    let cols_union = union_sorted(&cols_old, &cols_new);
    if cols_union.is_empty() {
        return None;
    }

    let row_start = *rows_union.first()?;
    let row_end = *rows_union.last()?;
    let col_start = *cols_union.first()?;
    let col_end = *cols_union.last()?;

    Some(TableScope {
        rows_old,
        rows_new,
        cols_union,
        row_start,
        row_end,
        col_start,
        col_end,
    })
}

fn table_rows_for_grid(grid: &Grid, key_columns: &[u32]) -> Vec<u32> {
    if grid.nrows == 0 || grid.ncols == 0 || key_columns.is_empty() {
        return Vec::new();
    }
    if key_columns.iter().any(|&col| col >= grid.ncols) {
        return Vec::new();
    }

    let mut rows = Vec::new();
    'row: for row in 0..grid.nrows {
        for &col in key_columns {
            if !cell_value_is_non_empty(grid.get(row, col)) {
                continue 'row;
            }
        }
        rows.push(row);
    }
    rows
}

fn table_cols_for_grid(grid: &Grid, table_rows: &[u32], key_columns: &[u32]) -> Vec<u32> {
    if grid.ncols == 0 {
        return Vec::new();
    }

    let mut cols: HashSet<u32> = HashSet::new();
    for &col in key_columns {
        if col < grid.ncols {
            cols.insert(col);
        }
    }

    let row_count = table_rows.len();
    if row_count == 0 {
        let mut out: Vec<u32> = cols.into_iter().collect();
        out.sort_unstable();
        return out;
    }

    for col in 0..grid.ncols {
        let mut filled = 0usize;
        for &row in table_rows {
            if grid.get(row, col).is_some() {
                filled += 1;
            }
        }
        let ratio = filled as f64 / row_count as f64;
        if ratio >= TABLE_COLUMN_FILL_RATIO {
            cols.insert(col);
        }
    }

    let mut out: Vec<u32> = cols.into_iter().collect();
    out.sort_unstable();
    out
}

fn union_sorted(left: &[u32], right: &[u32]) -> Vec<u32> {
    let mut out: Vec<u32> = left.iter().copied().chain(right.iter().copied()).collect();
    out.sort_unstable();
    out.dedup();
    out
}

fn cell_value_is_non_empty(cell: Option<&crate::workbook::Cell>) -> bool {
    match cell.and_then(|cell| cell.value.as_ref()) {
        Some(CellValue::Blank) | None => false,
        Some(_) => true,
    }
}

fn map_key_columns(key_columns: &[u32], cols_union: &[u32]) -> Option<Vec<u32>> {
    let mut mapped = Vec::with_capacity(key_columns.len());
    for &col in key_columns {
        let idx = cols_union.iter().position(|&c| c == col)? as u32;
        mapped.push(idx);
    }
    Some(mapped)
}

fn build_table_grid(grid: &Grid, rows: &[u32], cols: &[u32]) -> (Grid, Vec<u32>) {
    let mut table = Grid::new(rows.len() as u32, cols.len() as u32);
    for (row_idx, &row) in rows.iter().enumerate() {
        for (col_idx, &col) in cols.iter().enumerate() {
            if let Some(cell) = grid.get(row, col) {
                table.insert_cell(
                    row_idx as u32,
                    col_idx as u32,
                    cell.value.clone(),
                    cell.formula,
                );
            }
        }
    }
    (table, rows.to_vec())
}

fn has_cells_outside_rect(
    grid: &Grid,
    row_start: u32,
    row_end: u32,
    col_start: u32,
    col_end: u32,
) -> bool {
    for ((row, col), _) in grid.iter_cells() {
        if row < row_start || row > row_end || col < col_start || col > col_end {
            return true;
        }
    }
    false
}

fn mask_grid_excluding_rect(
    grid: &Grid,
    row_start: u32,
    row_end: u32,
    col_start: u32,
    col_end: u32,
) -> Grid {
    let mut out = Grid::new(grid.nrows, grid.ncols);
    for ((row, col), cell) in grid.iter_cells() {
        if row < row_start || row > row_end || col < col_start || col > col_end {
            out.insert_cell(row, col, cell.value.clone(), cell.formula);
        }
    }
    out
}

#[derive(Debug, Default)]
struct ClusterMatch {
    matched: Vec<(u32, u32)>,
    left_unmatched: Vec<u32>,
    right_unmatched: Vec<u32>,
}

fn match_duplicate_cluster(
    old: &Grid,
    new: &Grid,
    left_rows: &[u32],
    right_rows: &[u32],
    compare_cols: &[u32],
) -> ClusterMatch {
    if left_rows.is_empty() && right_rows.is_empty() {
        return ClusterMatch::default();
    }
    if left_rows.is_empty() {
        return ClusterMatch {
            matched: Vec::new(),
            left_unmatched: Vec::new(),
            right_unmatched: right_rows.to_vec(),
        };
    }
    if right_rows.is_empty() {
        return ClusterMatch {
            matched: Vec::new(),
            left_unmatched: left_rows.to_vec(),
            right_unmatched: Vec::new(),
        };
    }

    let unmatched_cost = duplicate_unmatched_cost(compare_cols.len());

    let mut costs: Vec<Vec<i64>> = Vec::with_capacity(left_rows.len());
    for &left_row in left_rows {
        let mut row_costs = Vec::with_capacity(right_rows.len());
        for &right_row in right_rows {
            row_costs.push(duplicate_pair_cost(old, new, left_row, right_row, compare_cols));
        }
        costs.push(row_costs);
    }

    if left_rows.len().max(right_rows.len()) <= DUPLICATE_CLUSTER_EXACT_MAX {
        let assignment = hungarian::solve_rect(&costs, unmatched_cost);
        let mut right_used = vec![false; right_rows.len()];
        let mut matched = Vec::new();
        let mut left_unmatched = Vec::new();

        for (row_idx, &col_idx) in assignment.iter().take(left_rows.len()).enumerate() {
            if col_idx >= right_rows.len() {
                left_unmatched.push(left_rows[row_idx]);
                continue;
            }
            let cost = costs
                .get(row_idx)
                .and_then(|row| row.get(col_idx))
                .copied()
                .unwrap_or(unmatched_cost);
            if cost >= unmatched_cost {
                left_unmatched.push(left_rows[row_idx]);
                continue;
            }
            if !right_used[col_idx] {
                matched.push((left_rows[row_idx], right_rows[col_idx]));
                right_used[col_idx] = true;
            }
        }

        let mut right_unmatched = Vec::new();
        for (idx, &row) in right_rows.iter().enumerate() {
            if !right_used[idx] {
                right_unmatched.push(row);
            }
        }

        return ClusterMatch {
            matched,
            left_unmatched,
            right_unmatched,
        };
    }

    let mut candidates = Vec::new();
    for (left_idx, _) in left_rows.iter().enumerate() {
        for (right_idx, _) in right_rows.iter().enumerate() {
            let cost = costs[left_idx][right_idx];
            if cost < unmatched_cost {
                candidates.push((cost, left_idx, right_idx));
            }
        }
    }
    candidates.sort_by(|a, b| a.cmp(b));

    let mut left_used = vec![false; left_rows.len()];
    let mut right_used = vec![false; right_rows.len()];
    let mut matched = Vec::new();

    for (_, left_idx, right_idx) in candidates {
        if left_used[left_idx] || right_used[right_idx] {
            continue;
        }
        left_used[left_idx] = true;
        right_used[right_idx] = true;
        matched.push((left_rows[left_idx], right_rows[right_idx]));
    }

    let mut left_unmatched = Vec::new();
    for (idx, &row) in left_rows.iter().enumerate() {
        if !left_used[idx] {
            left_unmatched.push(row);
        }
    }
    let mut right_unmatched = Vec::new();
    for (idx, &row) in right_rows.iter().enumerate() {
        if !right_used[idx] {
            right_unmatched.push(row);
        }
    }

    ClusterMatch {
        matched,
        left_unmatched,
        right_unmatched,
    }
}

fn duplicate_unmatched_cost(compare_cols_len: usize) -> i64 {
    if compare_cols_len == 0 {
        return 1;
    }
    let threshold = DUPLICATE_MATCH_THRESHOLD.clamp(0.0, 1.0);
    let max_mismatches =
        ((compare_cols_len as f64) * (1.0 - threshold)).floor() as i64;
    (max_mismatches + 1).max(1)
}

fn duplicate_pair_cost(
    old: &Grid,
    new: &Grid,
    left_row: u32,
    right_row: u32,
    compare_cols: &[u32],
) -> i64 {
    let mut cost = 0i64;
    for &col in compare_cols {
        let old_cell = old.get(left_row, col);
        let new_cell = new.get(right_row, col);
        if !cells_content_equal(old_cell, new_cell) {
            cost += 1;
        }
    }
    cost
}

fn grids_non_blank_cells_equal(old: &Grid, new: &Grid) -> bool {
    if old.cell_count() != new.cell_count() {
        return false;
    }

    if old.cell_count() == 0 {
        return true;
    }

    if let (Some(old_sigs), Some(new_sigs)) = (&old.row_signatures, &new.row_signatures) {
        if old_sigs != new_sigs {
            return false;
        }
        if let (Some(old_col_sigs), Some(new_col_sigs)) = (&old.col_signatures, &new.col_signatures)
        {
            if old_col_sigs != new_col_sigs {
                return false;
            }
            return true;
        }
    }

    old.cells_equal(&new.cells)
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub(super) enum PreflightDecision {
    RunFullPipeline,
    ShortCircuitNearIdentical,
    ShortCircuitDissimilar,
}

#[derive(Debug)]
struct PreflightLite {
    decision: PreflightDecision,
    mismatched_rows: Vec<u32>,
}

fn preflight_decision_from_grids(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> PreflightLite {
    let nrows_old = old.nrows as usize;
    let nrows_new = new.nrows as usize;
    let ncols_old = old.ncols as usize;
    let ncols_new = new.ncols as usize;

    if nrows_old != nrows_new || ncols_old != ncols_new {
        return PreflightLite {
            decision: PreflightDecision::RunFullPipeline,
            mismatched_rows: Vec::new(),
        };
    }

    let nrows = nrows_old;
    if nrows < config.preflight.preflight_min_rows as usize {
        return PreflightLite {
            decision: PreflightDecision::RunFullPipeline,
            mismatched_rows: Vec::new(),
        };
    }

    let old_signatures = row_signatures_for_grid(old);
    let new_signatures = row_signatures_for_grid(new);

    let (in_order_matches, old_sig_set, new_sig_set) =
        compute_row_signature_stats(&old_signatures, &new_signatures);

    let in_order_mismatches = nrows.saturating_sub(in_order_matches);
    let in_order_match_ratio = if nrows > 0 {
        in_order_matches as f64 / nrows as f64
    } else {
        1.0
    };

    let intersection_size = old_sig_set.intersection(&new_sig_set).count();
    let union_size = old_sig_set.union(&new_sig_set).count();
    let jaccard = if union_size > 0 {
        intersection_size as f64 / union_size as f64
    } else {
        1.0
    };

    if jaccard < config.preflight.bailout_similarity_threshold {
        return PreflightLite {
            decision: PreflightDecision::ShortCircuitDissimilar,
            mismatched_rows: Vec::new(),
        };
    }

    let (multiset_equal, multiset_edit_distance_rows) =
        multiset_equal_and_edit_distance(&old_signatures, &new_signatures);

    let reorder_suspected = (in_order_mismatches as u64) > multiset_edit_distance_rows;

    let near_identical = in_order_mismatches
        <= config.preflight.preflight_in_order_mismatch_max as usize
        && in_order_match_ratio >= config.preflight.preflight_in_order_match_ratio_min
        && !multiset_equal
        && !reorder_suspected;

    if near_identical {
        return PreflightLite {
            decision: PreflightDecision::ShortCircuitNearIdentical,
            mismatched_rows: mismatched_rows_from_signatures(&old_signatures, &new_signatures),
        };
    }

    PreflightLite {
        decision: PreflightDecision::RunFullPipeline,
        mismatched_rows: Vec::new(),
    }
}

fn compute_row_signature_stats(
    old_signatures: &[RowSignature],
    new_signatures: &[RowSignature],
) -> (usize, HashSet<RowSignature>, HashSet<RowSignature>) {
    let mut in_order_matches = 0usize;
    let mut old_sig_set = HashSet::with_capacity(old_signatures.len());
    let mut new_sig_set = HashSet::with_capacity(new_signatures.len());

    for (old_sig, new_sig) in old_signatures.iter().zip(new_signatures.iter()) {
        if old_sig == new_sig {
            in_order_matches += 1;
        }
        old_sig_set.insert(*old_sig);
        new_sig_set.insert(*new_sig);
    }

    (in_order_matches, old_sig_set, new_sig_set)
}

fn multiset_equal_and_edit_distance(
    old_signatures: &[RowSignature],
    new_signatures: &[RowSignature],
) -> (bool, u64) {
    let mut delta: HashMap<RowSignature, i32> = HashMap::new();
    for sig in old_signatures {
        *delta.entry(*sig).or_insert(0) += 1;
    }
    for sig in new_signatures {
        *delta.entry(*sig).or_insert(0) -= 1;
    }

    let mut equal = true;
    let mut sum_abs: u64 = 0;
    for (_sig, d) in delta {
        if d != 0 {
            equal = false;
            sum_abs = sum_abs.saturating_add(d.unsigned_abs() as u64);
        }
    }

    (equal, sum_abs / 2)
}

fn mismatched_rows_from_signatures(
    old_signatures: &[RowSignature],
    new_signatures: &[RowSignature],
) -> Vec<u32> {
    let mut rows = Vec::new();
    let count = old_signatures.len().min(new_signatures.len());
    for idx in 0..count {
        let a = old_signatures[idx];
        let b = new_signatures[idx];
        if a != b {
            rows.push(idx as u32);
        }
    }
    rows
}

fn row_signatures_for_grid(grid: &Grid) -> Vec<RowSignature> {
    if let Some(sigs) = &grid.row_signatures {
        return sigs.clone();
    }
    let mut out = Vec::with_capacity(grid.nrows as usize);
    for row in 0..grid.nrows {
        out.push(grid.compute_row_signature(row));
    }
    out
}

fn rows_with_context(rows: &[u32], context: u32, max_rows: u32) -> Vec<u32> {
    if rows.is_empty() || context == 0 {
        return rows.to_vec();
    }

    let mut out = Vec::new();
    for &row in rows {
        let start = row.saturating_sub(context);
        let end = row.saturating_add(context).min(max_rows.saturating_sub(1));
        for r in start..=end {
            out.push(r);
        }
    }

    out.sort_unstable();
    out.dedup();
    out
}

#[cfg(feature = "perf-metrics")]
fn cells_in_overlap(old: &Grid, new: &Grid) -> u64 {
    let overlap_rows = old.nrows.min(new.nrows) as u64;
    let overlap_cols = old.ncols.min(new.ncols) as u64;
    overlap_rows.saturating_mul(overlap_cols)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::sink::VecSink;
    use crate::string_pool::StringPool;
    use crate::workbook::{Cell, CellValue};

    fn numbered_cell(value: f64) -> Cell {
        Cell {
            value: Some(CellValue::Number(value)),
            formula: None,
        }
    }

    #[test]
    fn grids_non_blank_cells_equal_requires_matching_entries() {
        let base_cell = Cell {
            value: Some(CellValue::Number(1.0)),
            formula: None,
        };

        let mut grid_a = Grid::new(2, 2);
        let mut grid_b = Grid::new(2, 2);
        grid_a.insert_cell(0, 0, base_cell.value.clone(), base_cell.formula);
        grid_b.insert_cell(0, 0, base_cell.value.clone(), base_cell.formula);

        assert!(grids_non_blank_cells_equal(&grid_a, &grid_b));

        let mut grid_b_changed = grid_b.clone();
        let mut changed_cell = base_cell.clone();
        changed_cell.value = Some(CellValue::Number(2.0));
        grid_b_changed.insert_cell(0, 0, changed_cell.value.clone(), changed_cell.formula);

        assert!(!grids_non_blank_cells_equal(&grid_a, &grid_b_changed));

        grid_a.insert_cell(1, 1, Some(CellValue::Blank), None);

        assert!(!grids_non_blank_cells_equal(&grid_a, &grid_b));
    }

    #[test]
    fn diff_row_pair_sparse_counts_union_columns_not_sum_lengths() {
        use super::super::grid_primitives::diff_row_pair_sparse;
        use crate::formula_diff::FormulaParseCache;

        let mut pool = StringPool::new();
        let sheet_id: SheetId = pool.intern("Sheet1");
        let config = DiffConfig::default();
        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let mut cache = FormulaParseCache::default();
        let mut warnings: Vec<String> = Vec::new();
        let mut hardening = super::super::hardening::HardeningController::new(&config, None);

        let old_cells_storage = [numbered_cell(1.0), numbered_cell(2.0), numbered_cell(3.0)];
        let new_cells_storage = [numbered_cell(1.0), numbered_cell(2.0), numbered_cell(4.0)];

        let old_cells: Vec<(u32, &Cell)> = old_cells_storage
            .iter()
            .enumerate()
            .map(|(idx, cell)| (idx as u32, cell))
            .collect();
        let new_cells: Vec<(u32, &Cell)> = new_cells_storage
            .iter()
            .enumerate()
            .map(|(idx, cell)| (idx as u32, cell))
            .collect();

        let mut emit_ctx = EmitCtx::new(
            sheet_id,
            &pool,
            &config,
            &mut cache,
            &mut sink,
            &mut op_count,
            &mut warnings,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        );
        let result = diff_row_pair_sparse(&mut emit_ctx, 0, 0, 3, &old_cells, &new_cells)
            .expect("diff should succeed");

        assert_eq!(result.compared, 3);
        assert!(!result.replaced);
    }

    #[test]
    fn diff_row_pair_sparse_counts_union_for_sparse_columns() {
        use super::super::grid_primitives::diff_row_pair_sparse;
        use crate::formula_diff::FormulaParseCache;

        let mut pool = StringPool::new();
        let sheet_id: SheetId = pool.intern("Sheet1");
        let config = DiffConfig::default();
        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let mut cache = FormulaParseCache::default();
        let mut warnings: Vec<String> = Vec::new();
        let mut hardening = super::super::hardening::HardeningController::new(&config, None);

        let old_cells_storage = [numbered_cell(1.0)];
        let new_cells_storage = [numbered_cell(2.0)];

        let old_cells: Vec<(u32, &Cell)> = vec![(0, &old_cells_storage[0])];
        let new_cells: Vec<(u32, &Cell)> = vec![(2, &new_cells_storage[0])];

        let mut emit_ctx = EmitCtx::new(
            sheet_id,
            &pool,
            &config,
            &mut cache,
            &mut sink,
            &mut op_count,
            &mut warnings,
            &mut hardening,
            #[cfg(feature = "perf-metrics")]
            None,
        );
        let result = diff_row_pair_sparse(&mut emit_ctx, 0, 0, 3, &old_cells, &new_cells)
            .expect("diff should succeed");

        assert_eq!(result.compared, 2);
        assert!(!result.replaced);
    }

    #[test]
    fn preflight_detects_row_swap_with_edit_as_not_near_identical() {
        let mut grid_a = Grid::new(6000, 10);
        let mut grid_b = Grid::new(6000, 10);

        for row in 0..6000u32 {
            for col in 0..10u32 {
                let value = (row * 1000 + col) as f64;
                grid_a.insert_cell(row, col, Some(CellValue::Number(value)), None);
                grid_b.insert_cell(row, col, Some(CellValue::Number(value)), None);
            }
        }

        let row_0_cells: Vec<_> = (0..10u32).map(|c| c as f64).collect();
        let row_1_cells: Vec<_> = (0..10u32).map(|c| 1000.0 + c as f64).collect();

        for col in 0..10u32 {
            grid_b.insert_cell(0, col, Some(CellValue::Number(row_1_cells[col as usize])), None);
            grid_b.insert_cell(1, col, Some(CellValue::Number(row_0_cells[col as usize])), None);
        }

        grid_b.insert_cell(2999, 5, Some(CellValue::Number(999999.0)), None);

        let config = DiffConfig::default();
        let decision = preflight_decision_from_grids(&grid_a, &grid_b, &config);

        assert_eq!(
            decision.decision,
            PreflightDecision::RunFullPipeline,
            "small row swap + edit should NOT short-circuit to near-identical"
        );
    }

    #[test]
    fn preflight_short_circuit_dissimilar_skips_gridview_build() {
        use crate::grid_view::{gridview_build_count, reset_gridview_build_count};

        let mut grid_a = Grid::new(200, 10);
        let mut grid_b = Grid::new(200, 10);

        for row in 0..200u32 {
            for col in 0..10u32 {
                let value = (row * 1000 + col) as f64;
                grid_a.insert_cell(row, col, Some(CellValue::Number(value)), None);
                grid_b.insert_cell(row, col, Some(CellValue::Number(value + 1.0)), None);
            }
        }

        let config = DiffConfig::builder()
            .preflight_min_rows(0)
            .bailout_similarity_threshold(0.05)
            .build()
            .expect("valid config");

        let mut pool = StringPool::new();
        let sheet_id: SheetId = pool.intern("PreflightTest");
        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let mut ctx = DiffContext::default();
        let mut hardening = super::super::hardening::HardeningController::new(&config, None);

        reset_gridview_build_count();

        #[cfg(feature = "perf-metrics")]
        {
            diff_grids_core(
                sheet_id,
                &grid_a,
                &grid_b,
                &config,
                &pool,
                &mut sink,
                &mut op_count,
                &mut ctx,
                &mut hardening,
                None,
            )
            .expect("diff should succeed");
        }

        #[cfg(not(feature = "perf-metrics"))]
        {
            diff_grids_core(
                sheet_id,
                &grid_a,
                &grid_b,
                &config,
                &pool,
                &mut sink,
                &mut op_count,
                &mut ctx,
                &mut hardening,
            )
            .expect("diff should succeed");
        }

        assert_eq!(
            gridview_build_count(),
            0,
            "low-similarity preflight should avoid GridView construction"
        );
    }

    #[test]
    fn multiset_edit_distance_computes_correctly() {
        let mut grid_a = Grid::new(10, 5);
        let mut grid_b = Grid::new(10, 5);

        for row in 0..10u32 {
            for col in 0..5u32 {
                grid_a.insert_cell(row, col, Some(CellValue::Number((row * 100 + col) as f64)), None);
            }
        }

        for row in 0..10u32 {
            for col in 0..5u32 {
                grid_b.insert_cell(row, col, Some(CellValue::Number((row * 100 + col) as f64)), None);
            }
        }

        let old_signatures = row_signatures_for_grid(&grid_a);
        let new_signatures = row_signatures_for_grid(&grid_b);

        let (equal, edit_distance) =
            multiset_equal_and_edit_distance(&old_signatures, &new_signatures);
        assert!(equal);
        assert_eq!(edit_distance, 0);

        for col in 0..5u32 {
            grid_b.insert_cell(0, col, Some(CellValue::Number(99999.0 + col as f64)), None);
        }

        let new_signatures_edited = row_signatures_for_grid(&grid_b);
        let (equal2, edit_distance2) =
            multiset_equal_and_edit_distance(&old_signatures, &new_signatures_edited);
        assert!(!equal2);
        assert_eq!(edit_distance2, 1);
    }
}

```

---

### File: `core\src\engine\grid_primitives.rs`

```rust
use crate::alignment_types::{RowAlignment, RowBlockMove};
use crate::column_alignment::{
    ColumnAlignment, ColumnBlockMove, align_single_column_change_from_views,
};
use crate::config::DiffConfig;
use crate::diff::{DiffError, DiffOp, FormulaDiffResult};
use crate::formula_diff::{FormulaParseCache, diff_cell_formulas_ids};
use crate::grid_view::GridView;
#[cfg(feature = "perf-metrics")]
use crate::perf::Phase;
use crate::rect_block_move::RectBlockMove;
use crate::row_alignment::align_row_changes_from_views;
use crate::sink::DiffSink;
use crate::string_pool::StringPool;
use crate::workbook::{Cell, CellAddress, CellSnapshot, Grid};

use super::context::EmitCtx;

struct PendingCell<'a> {
    col: u32,
    old_cell: Option<&'a Cell>,
    new_cell: Option<&'a Cell>,
}

pub(super) struct RowDiffResult<'a> {
    pub(super) compared: u64,
    pub(super) replaced: bool,
    pending: Vec<PendingCell<'a>>,
}

struct PendingRect {
    start_old: u32,
    start_new: u32,
    row_count: u32,
}

struct RowPairPlan<'a> {
    row_a: u32,
    row_b: u32,
    skipped: bool,
    replaced: bool,
    compared: u64,
    pending: Vec<PendingCell<'a>>,
}

pub(super) fn compute_formula_diff(
    pool: &StringPool,
    cache: &mut FormulaParseCache,
    old_cell: Option<&Cell>,
    new_cell: Option<&Cell>,
    row_shift: i32,
    col_shift: i32,
    config: &DiffConfig,
) -> FormulaDiffResult {
    let old_f = old_cell.and_then(|c| c.formula);
    let new_f = new_cell.and_then(|c| c.formula);
    diff_cell_formulas_ids(pool, cache, old_f, new_f, row_shift, col_shift, config)
}

pub(super) fn emit_cell_edit<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    addr: CellAddress,
    old_cell: Option<&Cell>,
    new_cell: Option<&Cell>,
    row_shift: i32,
    col_shift: i32,
) -> Result<(), DiffError> {
    let from = snapshot_with_addr(old_cell, addr);
    let to = snapshot_with_addr(new_cell, addr);
    let formula_diff = compute_formula_diff(
        ctx.pool, ctx.cache, old_cell, new_cell, row_shift, col_shift, ctx.config,
    );
    ctx.emit(DiffOp::cell_edited(
        ctx.sheet_id,
        addr,
        from,
        to,
        formula_diff,
    ))
}

fn dense_row_replace_threshold(config: &DiffConfig, total_cols: u32) -> Option<usize> {
    if config.semantic.include_unchanged_cells || total_cols == 0 {
        return None;
    }

    let ratio = config.semantic.dense_row_replace_ratio;
    if !ratio.is_finite() || ratio <= 0.0 {
        return None;
    }

    if config.semantic.dense_row_replace_min_cols > 0
        && total_cols < config.semantic.dense_row_replace_min_cols
    {
        return None;
    }

    let threshold = (ratio * total_cols as f64).ceil() as usize;
    Some(threshold.max(1))
}

fn flush_pending_rect<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    pending: &mut Option<PendingRect>,
    overlap_cols: u32,
) -> Result<(), DiffError> {
    let Some(rect) = pending.take() else {
        return Ok(());
    };

    if overlap_cols == 0 {
        return Ok(());
    }

    let min_rows = ctx.config.semantic.dense_rect_replace_min_rows;
    if min_rows > 0 && rect.row_count >= min_rows {
        ctx.emit(DiffOp::RectReplaced {
            sheet: ctx.sheet_id,
            start_row: rect.start_new,
            row_count: rect.row_count,
            start_col: 0,
            col_count: overlap_cols,
        })?;
    } else {
        for offset in 0..rect.row_count {
            ctx.emit(DiffOp::RowReplaced {
                sheet: ctx.sheet_id,
                row_idx: rect.start_new + offset,
            })?;
        }
    }

    Ok(())
}

fn emit_pending_cells<'a, S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    row_a: u32,
    row_b: u32,
    pending: Vec<PendingCell<'a>>,
) -> Result<(), DiffError> {
    let row_shift = row_b as i32 - row_a as i32;
    for cell in pending {
        let addr = CellAddress::from_indices(row_b, cell.col);
        emit_cell_edit(ctx, addr, cell.old_cell, cell.new_cell, row_shift, 0)?;
    }
    Ok(())
}

pub(super) fn cells_content_equal(a: Option<&Cell>, b: Option<&Cell>) -> bool {
    match (a, b) {
        (None, None) => true,
        (Some(cell_a), None) | (None, Some(cell_a)) => {
            cell_a.value.is_none() && cell_a.formula.is_none()
        }
        (Some(cell_a), Some(cell_b)) => {
            cell_a.value == cell_b.value && cell_a.formula == cell_b.formula
        }
    }
}

pub(super) fn snapshot_with_addr(cell: Option<&Cell>, addr: CellAddress) -> CellSnapshot {
    match cell {
        Some(cell) => CellSnapshot {
            addr,
            value: cell.value,
            formula: cell.formula,
        },
        None => CellSnapshot::empty(addr),
    }
}

pub(super) fn emit_row_block_move<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    mv: RowBlockMove,
) -> Result<(), DiffError> {
    ctx.emit(DiffOp::BlockMovedRows {
        sheet: ctx.sheet_id,
        src_start_row: mv.src_start_row,
        row_count: mv.row_count,
        dst_start_row: mv.dst_start_row,
        block_hash: None,
    })
}

pub(super) fn emit_column_block_move<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    mv: ColumnBlockMove,
) -> Result<(), DiffError> {
    ctx.emit(DiffOp::BlockMovedColumns {
        sheet: ctx.sheet_id,
        src_start_col: mv.src_start_col,
        col_count: mv.col_count,
        dst_start_col: mv.dst_start_col,
        block_hash: None,
    })
}

pub(super) fn emit_rect_block_move<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    mv: RectBlockMove,
) -> Result<(), DiffError> {
    ctx.emit(DiffOp::BlockMovedRect {
        sheet: ctx.sheet_id,
        src_start_row: mv.src_start_row,
        src_row_count: mv.src_row_count,
        src_start_col: mv.src_start_col,
        src_col_count: mv.src_col_count,
        dst_start_row: mv.dst_start_row,
        dst_start_col: mv.dst_start_col,
        block_hash: mv.block_hash,
    })
}

pub(super) fn emit_moved_row_block_edits<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old_view: &GridView,
    new_view: &GridView,
    mv: RowBlockMove,
) -> Result<(), DiffError> {
    let overlap_cols = old_view.source.ncols.min(new_view.source.ncols);
    let mut offset = 0u32;

    while offset < mv.row_count {
        if ctx.hardening.should_abort() {
            break;
        }

        let chunk_len = cell_diff_chunk_len(overlap_cols)
            .min((mv.row_count - offset) as usize);
        let mut pairs: Vec<(u32, u32)> = Vec::with_capacity(chunk_len);
        for idx in 0..chunk_len {
            let row_a = mv.src_start_row + offset + idx as u32;
            let row_b = mv.dst_start_row + offset + idx as u32;
            pairs.push((row_a, row_b));
        }

        let plans = plan_row_pair_chunk(old_view, new_view, &pairs, overlap_cols, ctx.config);

        for plan in plans {
            if plan.skipped {
                continue;
            }

            if plan.replaced {
                ctx.emit(DiffOp::RowReplaced {
                    sheet: ctx.sheet_id,
                    row_idx: plan.row_b,
                })?;
            } else {
                emit_pending_cells(ctx, plan.row_a, plan.row_b, plan.pending)?;
            }
        }

        offset = offset.saturating_add(chunk_len as u32);

        if ctx.hardening.check_timeout(ctx.warnings) {
            break;
        }
    }
    Ok(())
}

fn diff_row_pair_sparse_plan<'a>(
    config: &DiffConfig,
    overlap_cols: u32,
    old_cells: &[(u32, &'a Cell)],
    new_cells: &[(u32, &'a Cell)],
) -> RowDiffResult<'a> {
    let Some(threshold) = dense_row_replace_threshold(config, overlap_cols) else {
        return diff_row_pair_sparse_thresholdless(config, overlap_cols, old_cells, new_cells);
    };

    let mut compared = 0u64;
    let mut changed_cells = 0usize;
    let mut pending: Vec<PendingCell<'a>> = Vec::new();

    let mut i = 0usize;
    let mut j = 0usize;

    while i < old_cells.len() || j < new_cells.len() {
        let col = match (old_cells.get(i), new_cells.get(j)) {
            (Some((ca, _)), Some((cb, _))) => (*ca).min(*cb),
            (Some((ca, _)), None) => *ca,
            (None, Some((cb, _))) => *cb,
            (None, None) => break,
        };

        if col >= overlap_cols {
            break;
        }

        let mut old_cell = None;
        let mut new_cell = None;

        if let Some((c, cell)) = old_cells.get(i) {
            if *c == col {
                old_cell = Some(*cell);
                i += 1;
            }
        }
        if let Some((c, cell)) = new_cells.get(j) {
            if *c == col {
                new_cell = Some(*cell);
                j += 1;
            }
        }

        compared = compared.saturating_add(1);

        if cells_content_equal(old_cell, new_cell) {
            if config.semantic.include_unchanged_cells {
                pending.push(PendingCell {
                    col,
                    old_cell,
                    new_cell,
                });
            }
            continue;
        }

        changed_cells = changed_cells.saturating_add(1);
        if changed_cells >= threshold {
            return RowDiffResult {
                compared,
                replaced: true,
                pending: Vec::new(),
            };
        }

        pending.push(PendingCell {
            col,
            old_cell,
            new_cell,
        });
    }

    RowDiffResult {
        compared,
        replaced: false,
        pending,
    }
}

fn diff_row_pair_sparse_thresholdless<'a>(
    config: &DiffConfig,
    overlap_cols: u32,
    old_cells: &[(u32, &'a Cell)],
    new_cells: &[(u32, &'a Cell)],
) -> RowDiffResult<'a> {
    let mut compared = 0u64;
    let mut pending: Vec<PendingCell<'a>> = Vec::new();

    let mut i = 0usize;
    let mut j = 0usize;

    while i < old_cells.len() || j < new_cells.len() {
        let col = match (old_cells.get(i), new_cells.get(j)) {
            (Some((ca, _)), Some((cb, _))) => (*ca).min(*cb),
            (Some((ca, _)), None) => *ca,
            (None, Some((cb, _))) => *cb,
            (None, None) => break,
        };

        if col >= overlap_cols {
            break;
        }

        let mut old_cell = None;
        let mut new_cell = None;

        if let Some((c, cell)) = old_cells.get(i) {
            if *c == col {
                old_cell = Some(*cell);
                i += 1;
            }
        }
        if let Some((c, cell)) = new_cells.get(j) {
            if *c == col {
                new_cell = Some(*cell);
                j += 1;
            }
        }

        compared = compared.saturating_add(1);

        if config.semantic.include_unchanged_cells || !cells_content_equal(old_cell, new_cell) {
            pending.push(PendingCell {
                col,
                old_cell,
                new_cell,
            });
        }
    }

    RowDiffResult {
        compared,
        replaced: false,
        pending,
    }
}

pub(super) fn diff_row_pair_sparse<'a, S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    _row_a: u32,
    _row_b: u32,
    overlap_cols: u32,
    old_cells: &[(u32, &'a Cell)],
    new_cells: &[(u32, &'a Cell)],
) -> Result<RowDiffResult<'a>, DiffError> {
    Ok(diff_row_pair_sparse_plan(
        ctx.config,
        overlap_cols,
        old_cells,
        new_cells,
    ))
}

pub(super) fn diff_row_pair<'a, S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &'a Grid,
    new: &'a Grid,
    row_a: u32,
    row_b: u32,
    overlap_cols: u32,
) -> Result<RowDiffResult<'a>, DiffError> {
    let mut compared = 0u64;
    let mut changed_cells = 0usize;
    let mut pending: Vec<PendingCell<'a>> = Vec::new();
    let threshold = dense_row_replace_threshold(ctx.config, overlap_cols);

    for col in 0..overlap_cols {
        let old_cell = old.get(row_a, col);
        let new_cell = new.get(row_b, col);
        compared = compared.saturating_add(1);

        let changed = !cells_content_equal(old_cell, new_cell);
        if changed {
            changed_cells = changed_cells.saturating_add(1);
            if let Some(limit) = threshold {
                if changed_cells >= limit {
                    return Ok(RowDiffResult {
                        compared,
                        replaced: true,
                        pending: Vec::new(),
                    });
                }
            }
        }

        if changed || ctx.config.semantic.include_unchanged_cells {
            pending.push(PendingCell {
                col,
                old_cell,
                new_cell,
            });
        }
    }

    Ok(RowDiffResult {
        compared,
        replaced: false,
        pending,
    })
}

fn cell_diff_chunk_len(overlap_cols: u32) -> usize {
    if overlap_cols >= 2048 {
        64
    } else if overlap_cols >= 512 {
        256
    } else {
        1024
    }
}

fn plan_row_pair_chunk<'a>(
    old_view: &'a GridView<'a>,
    new_view: &'a GridView<'a>,
    chunk: &[(u32, u32)],
    overlap_cols: u32,
    config: &DiffConfig,
) -> Vec<RowPairPlan<'a>> {
    #[cfg(feature = "parallel")]
    {
        use rayon::prelude::*;
        chunk
            .par_iter()
            .map(|(row_a, row_b)| {
                plan_one_row_pair(old_view, new_view, *row_a, *row_b, overlap_cols, config)
            })
            .collect()
    }

    #[cfg(not(feature = "parallel"))]
    chunk
        .iter()
        .map(|(row_a, row_b)| {
            plan_one_row_pair(old_view, new_view, *row_a, *row_b, overlap_cols, config)
        })
        .collect()
}

fn plan_one_row_pair<'a>(
    old_view: &'a GridView<'a>,
    new_view: &'a GridView<'a>,
    row_a: u32,
    row_b: u32,
    overlap_cols: u32,
    config: &DiffConfig,
) -> RowPairPlan<'a> {
    let Some(row_view_a) = old_view.rows.get(row_a as usize) else {
        return RowPairPlan {
            row_a,
            row_b,
            skipped: true,
            replaced: false,
            compared: 0,
            pending: Vec::new(),
        };
    };
    let Some(row_view_b) = new_view.rows.get(row_b as usize) else {
        return RowPairPlan {
            row_a,
            row_b,
            skipped: true,
            replaced: false,
            compared: 0,
            pending: Vec::new(),
        };
    };

    if !config.semantic.include_unchanged_cells {
        let sig_a = old_view.row_meta.get(row_a as usize).map(|m| m.signature);
        let sig_b = new_view.row_meta.get(row_b as usize).map(|m| m.signature);
        if let (Some(a), Some(b)) = (sig_a, sig_b) {
            if a == b {
                return RowPairPlan {
                    row_a,
                    row_b,
                    skipped: true,
                    replaced: false,
                    compared: 0,
                    pending: Vec::new(),
                };
            }
        }
    }

    let r = diff_row_pair_sparse_plan(
        config,
        overlap_cols,
        &row_view_a.cells,
        &row_view_b.cells,
    );

    RowPairPlan {
        row_a,
        row_b,
        skipped: false,
        replaced: r.replaced,
        compared: r.compared,
        pending: r.pending,
    }
}

pub(super) fn emit_row_aligned_diffs<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old_view: &GridView,
    new_view: &GridView,
    alignment: &RowAlignment,
) -> Result<u64, DiffError> {
    let overlap_cols = old_view.source.ncols.min(new_view.source.ncols);
    let mut compared = 0u64;
    let mut pending_rect: Option<PendingRect> = None;
    let matched = &alignment.matched;
    let mut idx = 0usize;

    while idx < matched.len() {
        if ctx.hardening.should_abort() {
            break;
        }

        let chunk_len = cell_diff_chunk_len(overlap_cols);
        let end = (idx + chunk_len).min(matched.len());
        let chunk = &matched[idx..end];

        let plans = plan_row_pair_chunk(old_view, new_view, chunk, overlap_cols, ctx.config);

        for plan in plans {
            if plan.skipped {
                flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                continue;
            }

            compared = compared.saturating_add(plan.compared);

            if plan.replaced {
                if let Some(existing) = pending_rect.as_mut() {
                    let expected_old = existing.start_old.saturating_add(existing.row_count);
                    let expected_new = existing.start_new.saturating_add(existing.row_count);
                    if plan.row_a == expected_old && plan.row_b == expected_new {
                        existing.row_count = existing.row_count.saturating_add(1);
                    } else {
                        flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                        pending_rect = Some(PendingRect {
                            start_old: plan.row_a,
                            start_new: plan.row_b,
                            row_count: 1,
                        });
                    }
                } else {
                    pending_rect = Some(PendingRect {
                        start_old: plan.row_a,
                        start_new: plan.row_b,
                        row_count: 1,
                    });
                }
            } else {
                flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                emit_pending_cells(ctx, plan.row_a, plan.row_b, plan.pending)?;
            }
        }

        idx = end;

        if ctx.hardening.check_timeout(ctx.warnings) {
            break;
        }
    }

    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;

    for row_idx in &alignment.inserted {
        ctx.emit(DiffOp::row_added(ctx.sheet_id, *row_idx, None))?;
    }

    for row_idx in &alignment.deleted {
        ctx.emit(DiffOp::row_removed(ctx.sheet_id, *row_idx, None))?;
    }

    for mv in &alignment.moves {
        emit_row_block_move(ctx, *mv)?;
    }

    if new_view.source.ncols > old_view.source.ncols {
        for col_idx in old_view.source.ncols..new_view.source.ncols {
            ctx.emit(DiffOp::column_added(ctx.sheet_id, col_idx, None))?;
        }
    } else if old_view.source.ncols > new_view.source.ncols {
        for col_idx in new_view.source.ncols..old_view.source.ncols {
            ctx.emit(DiffOp::column_removed(ctx.sheet_id, col_idx, None))?;
        }
    }

    Ok(compared)
}

pub(super) fn emit_column_aligned_diffs<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    alignment: &ColumnAlignment,
) -> Result<(), DiffError> {
    let overlap_rows = old.nrows.min(new.nrows);

    for row in 0..overlap_rows {
        for (col_a, col_b) in &alignment.matched {
            let old_cell = old.get(row, *col_a);
            let new_cell = new.get(row, *col_b);

            if cells_content_equal(old_cell, new_cell) {
                continue;
            }

            let addr = CellAddress::from_indices(row, *col_b);
            let col_shift = *col_b as i32 - *col_a as i32;
            emit_cell_edit(ctx, addr, old_cell, new_cell, 0, col_shift)?;
        }
    }

    for col_idx in &alignment.inserted {
        ctx.emit(DiffOp::column_added(ctx.sheet_id, *col_idx, None))?;
    }

    for col_idx in &alignment.deleted {
        ctx.emit(DiffOp::column_removed(ctx.sheet_id, *col_idx, None))?;
    }

    for mv in &alignment.moves {
        emit_column_block_move(ctx, *mv)?;
    }

    Ok(())
}

pub(super) fn positional_diff<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
) -> Result<(), DiffError> {
    let overlap_rows = old.nrows.min(new.nrows);
    let overlap_cols = old.ncols.min(new.ncols);
    let mut pending_rect: Option<PendingRect> = None;

    ctx.hardening.progress("cell_diff", 0.0);

    for row in 0..overlap_rows {
        if ctx.hardening.check_timeout(ctx.warnings) {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            return Ok(());
        }

        if overlap_rows > 0 && row % 256 == 0 {
            ctx.hardening
                .progress("cell_diff", row as f32 / overlap_rows as f32);
        }

        let result = diff_row_pair(ctx, old, new, row, row, overlap_cols)?;
        if result.replaced {
            if let Some(existing) = pending_rect.as_mut() {
                let expected_row = existing.start_new + existing.row_count;
                if row == expected_row {
                    existing.row_count = existing.row_count.saturating_add(1);
                } else {
                    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                    pending_rect = Some(PendingRect {
                        start_old: row,
                        start_new: row,
                        row_count: 1,
                    });
                }
            } else {
                pending_rect = Some(PendingRect {
                    start_old: row,
                    start_new: row,
                    row_count: 1,
                });
            }
        } else {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            emit_pending_cells(ctx, row, row, result.pending)?;
        }
    }

    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;

    if overlap_rows > 0 {
        ctx.hardening.progress("cell_diff", 1.0);
    }

    if ctx.hardening.check_timeout(ctx.warnings) {
        return Ok(());
    }

    if new.nrows > old.nrows {
        for row_idx in old.nrows..new.nrows {
            if row_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            ctx.emit(DiffOp::row_added(ctx.sheet_id, row_idx, None))?;
        }
    } else if old.nrows > new.nrows {
        for row_idx in new.nrows..old.nrows {
            if row_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            ctx.emit(DiffOp::row_removed(ctx.sheet_id, row_idx, None))?;
        }
    }

    if new.ncols > old.ncols {
        for col_idx in old.ncols..new.ncols {
            if col_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            ctx.emit(DiffOp::column_added(ctx.sheet_id, col_idx, None))?;
        }
    } else if old.ncols > new.ncols {
        for col_idx in new.ncols..old.ncols {
            if col_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            ctx.emit(DiffOp::column_removed(ctx.sheet_id, col_idx, None))?;
        }
    }

    Ok(())
}

pub(super) fn positional_diff_from_views<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<u64, DiffError> {
    let overlap_rows = old.nrows.min(new.nrows);
    let overlap_cols = old.ncols.min(new.ncols);

    ctx.hardening.progress("cell_diff", 0.0);

    let mut compared: u64 = 0;
    let mut pending_rect: Option<PendingRect> = None;

    for row in 0..overlap_rows {
        if ctx.hardening.check_timeout(ctx.warnings) {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            break;
        }
        if overlap_rows > 0 {
            ctx.hardening
                .progress("cell_diff", (row as f32) / (overlap_rows as f32));
        }

        if !ctx.config.semantic.include_unchanged_cells {
            let old_sig = old_view.row_meta.get(row as usize).map(|m| m.signature);
            let new_sig = new_view.row_meta.get(row as usize).map(|m| m.signature);
            if let (Some(a), Some(b)) = (old_sig, new_sig) {
                if a == b {
                    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                    continue;
                }
            }
        }

        let old_cells = old_view
            .rows
            .get(row as usize)
            .map(|r| r.cells.as_slice())
            .unwrap_or(&[]);
        let new_cells = new_view
            .rows
            .get(row as usize)
            .map(|r| r.cells.as_slice())
            .unwrap_or(&[]);

        let result = diff_row_pair_sparse(
            ctx,
            row,
            row,
            overlap_cols,
            old_cells,
            new_cells,
        )?;
        compared = compared.saturating_add(result.compared);
        if result.replaced {
            if let Some(existing) = pending_rect.as_mut() {
                let expected_row = existing.start_new + existing.row_count;
                if row == expected_row {
                    existing.row_count = existing.row_count.saturating_add(1);
                } else {
                    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                    pending_rect = Some(PendingRect {
                        start_old: row,
                        start_new: row,
                        row_count: 1,
                    });
                }
            } else {
                pending_rect = Some(PendingRect {
                    start_old: row,
                    start_new: row,
                    row_count: 1,
                });
            }
        } else {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            emit_pending_cells(ctx, row, row, result.pending)?;
        }
    }

    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;

    if old.nrows > new.nrows {
        for row in new.nrows..old.nrows {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::row_removed(ctx.sheet_id, row, None))?;
        }
    } else if new.nrows > old.nrows {
        for row in old.nrows..new.nrows {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::row_added(ctx.sheet_id, row, None))?;
        }
    }

    if old.ncols > new.ncols {
        for col in new.ncols..old.ncols {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::column_removed(ctx.sheet_id, col, None))?;
        }
    } else if new.ncols > old.ncols {
        for col in old.ncols..new.ncols {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::column_added(ctx.sheet_id, col, None))?;
        }
    }

    ctx.hardening.progress("cell_diff", 1.0);

    Ok(compared)
}

pub(super) fn positional_diff_for_rows<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    rows: &[u32],
) -> Result<u64, DiffError> {
    let overlap_rows = old.nrows.min(new.nrows);
    let overlap_cols = old.ncols.min(new.ncols);
    let mut compared: u64 = 0;
    let mut pending_rect: Option<PendingRect> = None;

    ctx.hardening.progress("cell_diff", 0.0);

    let mut rows_sorted: Vec<u32> = rows.to_vec();
    rows_sorted.sort_unstable();
    rows_sorted.dedup();

    let total_rows = rows_sorted.len();
    for (idx, &row) in rows_sorted.iter().enumerate() {
        if ctx.hardening.check_timeout(ctx.warnings) {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            break;
        }
        if total_rows > 0 && idx % 64 == 0 {
            ctx.hardening
                .progress("cell_diff", idx as f32 / total_rows as f32);
        }

        if row >= overlap_rows {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            continue;
        }

        let result = diff_row_pair(ctx, old, new, row, row, overlap_cols)?;
        compared = compared.saturating_add(result.compared);
        if result.replaced {
            if let Some(existing) = pending_rect.as_mut() {
                let expected_row = existing.start_new + existing.row_count;
                if row == expected_row {
                    existing.row_count = existing.row_count.saturating_add(1);
                } else {
                    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
                    pending_rect = Some(PendingRect {
                        start_old: row,
                        start_new: row,
                        row_count: 1,
                    });
                }
            } else {
                pending_rect = Some(PendingRect {
                    start_old: row,
                    start_new: row,
                    row_count: 1,
                });
            }
        } else {
            flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;
            emit_pending_cells(ctx, row, row, result.pending)?;
        }
    }

    flush_pending_rect(ctx, &mut pending_rect, overlap_cols)?;

    if old.nrows > new.nrows {
        for row in new.nrows..old.nrows {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::row_removed(ctx.sheet_id, row, None))?;
        }
    } else if new.nrows > old.nrows {
        for row in old.nrows..new.nrows {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::row_added(ctx.sheet_id, row, None))?;
        }
    }

    if old.ncols > new.ncols {
        for col in new.ncols..old.ncols {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::column_removed(ctx.sheet_id, col, None))?;
        }
    } else if new.ncols > old.ncols {
        for col in old.ncols..new.ncols {
            if ctx.hardening.check_timeout(ctx.warnings) {
                break;
            }
            ctx.emit(DiffOp::column_added(ctx.sheet_id, col, None))?;
        }
    }

    ctx.hardening.progress("cell_diff", 1.0);

    Ok(compared)
}

#[cfg(feature = "perf-metrics")]
pub(super) fn cells_in_overlap(old: &Grid, new: &Grid) -> u64 {
    let overlap_rows = old.nrows.min(new.nrows) as u64;
    let overlap_cols = old.ncols.min(new.ncols) as u64;
    overlap_rows.saturating_mul(overlap_cols)
}

#[cfg(feature = "perf-metrics")]
pub(super) fn run_positional_diff_from_views_with_metrics<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<(), DiffError> {
    if let Some(m) = ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::CellDiff);
    }
    let compared = positional_diff_from_views(ctx, old, new, old_view, new_view)?;
    if let Some(m) = ctx.metrics.as_deref_mut() {
        m.add_cells_compared(compared);
        m.end_phase(Phase::CellDiff);
    }
    Ok(())
}

#[cfg(not(feature = "perf-metrics"))]
pub(super) fn run_positional_diff_from_views_with_metrics<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<(), DiffError> {
    let _ = positional_diff_from_views(ctx, old, new, old_view, new_view)?;
    Ok(())
}

#[cfg(feature = "perf-metrics")]
pub(super) fn run_positional_diff_with_metrics<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
) -> Result<(), DiffError> {
    if let Some(m) = ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::CellDiff);
    }
    positional_diff(ctx, old, new)?;
    if let Some(m) = ctx.metrics.as_deref_mut() {
        m.add_cells_compared(cells_in_overlap(old, new));
        m.end_phase(Phase::CellDiff);
    }

    Ok(())
}

#[cfg(not(feature = "perf-metrics"))]
pub(super) fn run_positional_diff_with_metrics<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
) -> Result<(), DiffError> {
    positional_diff(ctx, old, new)
}

pub(super) fn try_row_alignment_internal<S: DiffSink>(
    emit_ctx: &mut EmitCtx<'_, '_, S>,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<bool, DiffError> {
    let Some(alignment) = align_row_changes_from_views(old_view, new_view, emit_ctx.config) else {
        return Ok(false);
    };

    emit_ctx.hardening.progress("cell_diff", 0.0);

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::CellDiff);
    }
    let compared = emit_row_aligned_diffs(emit_ctx, old_view, new_view, &alignment)?;
    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        m.add_cells_compared(compared);
        m.end_phase(Phase::CellDiff);
    }

    emit_ctx.hardening.progress("cell_diff", 1.0);

    #[cfg(not(feature = "perf-metrics"))]
    let _ = compared;

    Ok(true)
}

pub(super) fn try_single_column_alignment_internal<S: DiffSink>(
    emit_ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_view: &GridView,
    new_view: &GridView,
) -> Result<bool, DiffError> {
    let Some(alignment) =
        align_single_column_change_from_views(old_view, new_view, emit_ctx.config)
    else {
        return Ok(false);
    };

    emit_ctx.hardening.progress("cell_diff", 0.0);

    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        m.start_phase(Phase::CellDiff);
    }
    emit_column_aligned_diffs(emit_ctx, old, new, &alignment)?;
    #[cfg(feature = "perf-metrics")]
    if let Some(m) = emit_ctx.metrics.as_deref_mut() {
        let overlap_rows = old.nrows.min(new.nrows) as u64;
        m.add_cells_compared(overlap_rows.saturating_mul(alignment.matched.len() as u64));
        m.end_phase(Phase::CellDiff);
    }

    emit_ctx.hardening.progress("cell_diff", 1.0);

    Ok(true)
}

```

---

### File: `core\src\engine\hardening.rs`

```rust
use crate::config::DiffConfig;
use crate::progress::ProgressCallback;

#[cfg(not(target_arch = "wasm32"))]
use std::time::{Duration, Instant};

const BYTES_PER_MB: u64 = 1024 * 1024;

const PROGRESS_MIN_DELTA: f32 = 0.01;
#[cfg(not(target_arch = "wasm32"))]
const TIMEOUT_CHECK_EVERY_TICKS: u64 = 256;

pub(super) struct HardeningController<'a> {
    #[cfg(not(target_arch = "wasm32"))]
    start: Instant,
    #[cfg(not(target_arch = "wasm32"))]
    timeout: Option<Duration>,
    max_memory_bytes: Option<u64>,
    max_ops: Option<usize>,
    aborted: bool,
    #[cfg(not(target_arch = "wasm32"))]
    warned_timeout: bool,
    warned_memory: bool,
    warned_ops: bool,
    progress: Option<&'a dyn ProgressCallback>,
    last_progress_phase: Option<&'static str>,
    last_progress_percent: Option<f32>,
    #[cfg(not(target_arch = "wasm32"))]
    timeout_tick: u64,
}

impl<'a> HardeningController<'a> {
    pub(super) fn new(config: &DiffConfig, progress: Option<&'a dyn ProgressCallback>) -> Self {
        Self {
            #[cfg(not(target_arch = "wasm32"))]
            start: Instant::now(),
            #[cfg(not(target_arch = "wasm32"))]
            timeout: config
                .hardening
                .timeout_seconds
                .map(|secs| Duration::from_secs(secs as u64)),
            max_memory_bytes: config
                .hardening
                .max_memory_mb
                .map(|mb| (mb as u64).saturating_mul(BYTES_PER_MB)),
            max_ops: config.hardening.max_ops,
            aborted: false,
            #[cfg(not(target_arch = "wasm32"))]
            warned_timeout: false,
            warned_memory: false,
            warned_ops: false,
            progress,
            last_progress_phase: None,
            last_progress_percent: None,
            #[cfg(not(target_arch = "wasm32"))]
            timeout_tick: 0,
        }
    }

    pub(super) fn should_abort(&self) -> bool {
        self.aborted
    }

    #[cfg(not(target_arch = "wasm32"))]
    pub(super) fn check_timeout(&mut self, warnings: &mut Vec<String>) -> bool {
        if self.aborted {
            return true;
        }
        let Some(timeout) = self.timeout else {
            return false;
        };

        self.timeout_tick = self.timeout_tick.saturating_add(1);
        let should_check = self.timeout_tick == 1 || self.timeout_tick % TIMEOUT_CHECK_EVERY_TICKS == 0;
        if !should_check {
            return false;
        }

        if self.start.elapsed() < timeout {
            return false;
        }

        self.aborted = true;
        if !self.warned_timeout {
            self.warned_timeout = true;
            warnings.push(format!(
                "timeout after {} seconds; diff aborted early; results may be incomplete",
                timeout.as_secs()
            ));
        }
        true
    }

    #[cfg(target_arch = "wasm32")]
    pub(super) fn check_timeout(&mut self, _warnings: &mut Vec<String>) -> bool {
        self.aborted
    }

    pub(super) fn check_op_limit(&mut self, current_op_count: usize, warnings: &mut Vec<String>) -> bool {
        if self.aborted {
            return true;
        }
        let Some(limit) = self.max_ops else {
            return false;
        };

        if current_op_count < limit {
            return false;
        }

        self.aborted = true;
        if !self.warned_ops {
            self.warned_ops = true;
            warnings.push(format!(
                "op limit reached ({} ops); diff stopped early; results may be incomplete",
                limit
            ));
        }
        true
    }

    pub(super) fn memory_guard_or_warn(
        &mut self,
        estimated_extra_bytes: u64,
        warnings: &mut Vec<String>,
        context: &str,
    ) -> bool {
        let Some(limit) = self.max_memory_bytes else {
            return false;
        };

        if estimated_extra_bytes <= limit {
            return false;
        }

        if !self.warned_memory {
            self.warned_memory = true;
            warnings.push(format!(
                "memory budget exceeded in {context} (estimated ~{} MB > limit {} MB); falling back to positional diff; results may be incomplete",
                bytes_to_mb_ceil(estimated_extra_bytes),
                bytes_to_mb_ceil(limit),
            ));
        }

        true
    }

    pub(super) fn progress(&mut self, phase: &'static str, percent: f32) {
        let Some(callback) = self.progress else {
            return;
        };

        let mut clamped = if percent.is_finite() { percent } else { 0.0 };
        if clamped < 0.0 {
            clamped = 0.0;
        } else if clamped > 1.0 {
            clamped = 1.0;
        }

        let should_emit = match (self.last_progress_phase, self.last_progress_percent) {
            (Some(last_phase), Some(last_percent)) if last_phase == phase => {
                clamped == 0.0
                    || clamped == 1.0
                    || clamped < last_percent
                    || (clamped - last_percent) >= PROGRESS_MIN_DELTA
            }
            _ => true,
        };

        if !should_emit {
            return;
        }

        self.last_progress_phase = Some(phase);
        self.last_progress_percent = Some(clamped);
        callback.on_progress(phase, clamped);
    }
}

fn bytes_to_mb_ceil(bytes: u64) -> u64 {
    bytes
        .saturating_add(BYTES_PER_MB.saturating_sub(1))
        .saturating_div(BYTES_PER_MB)
}


```

---

### File: `core\src\engine\mod.rs`

```rust
//! Core diffing engine for workbook comparison.
//!
//! Provides the main entry point [`diff_workbooks`] for comparing two workbooks
//! and generating a [`DiffReport`] of all changes.
//!
//! ## Module Structure
//!
//! - `workbook_diff`: Workbook-level diff orchestration and sheet enumeration
//! - `grid_diff`: Grid diffing pipeline, cell comparison, and positional diff
//! - `move_mask`: Move detection with region masks and SheetGridDiffer
//! - `sheet_diff`: Sheet-level leaf diff entry points
//! - `amr`: AMR (Adaptive Move Recognition) alignment and decision helpers
//! - `context`: Shared types for diff context and emission

mod amr;
mod context;
mod grid_diff;
mod grid_primitives;
mod hardening;
mod move_mask;
mod sheet_diff;
mod workbook_diff;

pub use grid_diff::{
    diff_grids, diff_grids_database_mode, diff_grids_streaming, diff_grids_streaming_with_progress,
    try_diff_grids, try_diff_grids_database_mode_streaming, try_diff_grids_streaming,
    try_diff_grids_streaming_with_progress,
};
pub use sheet_diff::{
    diff_sheets, diff_sheets_streaming, diff_sheets_streaming_with_progress, try_diff_sheets,
    try_diff_sheets_streaming, try_diff_sheets_streaming_with_progress,
};
pub use workbook_diff::{
    diff_workbooks, diff_workbooks_streaming, diff_workbooks_streaming_with_progress,
    diff_workbooks_with_progress, try_diff_workbooks, try_diff_workbooks_streaming,
    try_diff_workbooks_streaming_with_progress, try_diff_workbooks_with_progress,
};

```

---

### File: `core\src\engine\move_mask.rs`

```rust
use crate::alignment_types::RowBlockMove;
use crate::column_alignment::{ColumnBlockMove, detect_exact_column_block_move};
use crate::config::DiffConfig;
use crate::diff::DiffError;
use crate::grid_view::GridView;
use crate::rect_block_move::{RectBlockMove, detect_exact_rect_block_move};
use crate::region_mask::RegionMask;
use crate::row_alignment::{detect_exact_row_block_move, detect_fuzzy_row_block_move};
use crate::sink::DiffSink;
use crate::workbook::{CellAddress, ColSignature, Grid, RowSignature};

use std::collections::{BTreeMap, HashSet};

use super::amr::try_diff_with_amr;
use super::context::EmitCtx;
use super::grid_primitives::{
    cells_content_equal, emit_cell_edit, emit_column_block_move, emit_moved_row_block_edits,
    emit_rect_block_move, emit_row_block_move, run_positional_diff_from_views_with_metrics,
    try_row_alignment_internal, try_single_column_alignment_internal,
};

pub(super) struct SheetGridDiffer<'a, 'p, 'b, S: DiffSink> {
    pub(super) emit_ctx: EmitCtx<'a, 'p, S>,
    pub(super) old: &'b Grid,
    pub(super) new: &'b Grid,
    pub(super) old_view: GridView<'b>,
    pub(super) new_view: GridView<'b>,
    pub(super) old_mask: RegionMask,
    pub(super) new_mask: RegionMask,
}

impl<'a, 'p, 'b, S: DiffSink> SheetGridDiffer<'a, 'p, 'b, S> {
    pub(super) fn from_views(
        emit_ctx: EmitCtx<'a, 'p, S>,
        old: &'b Grid,
        new: &'b Grid,
        old_view: GridView<'b>,
        new_view: GridView<'b>,
    ) -> Self {
        let old_mask = RegionMask::all_active(old.nrows, old.ncols);
        let new_mask = RegionMask::all_active(new.nrows, new.ncols);

        Self {
            emit_ctx,
            old,
            new,
            old_view,
            new_view,
            old_mask,
            new_mask,
        }
    }

    fn move_detection_enabled(&self) -> bool {
        self.old.nrows.max(self.new.nrows) <= self.emit_ctx.config.moves.max_move_detection_rows
            && self.old.ncols.max(self.new.ncols) <= self.emit_ctx.config.moves.max_move_detection_cols
    }

    pub(super) fn detect_moves(&mut self) -> Result<u32, DiffError> {
        if !self.move_detection_enabled() {
            return Ok(0);
        }

        let mut iteration = 0u32;
        let config = self.emit_ctx.config;

        loop {
            if iteration >= config.moves.max_move_iterations {
                break;
            }

            if !self.old_mask.has_active_cells() || !self.new_mask.has_active_cells() {
                break;
            }

            let mut found_move = false;

            if let Some(mv) = detect_exact_rect_block_move_masked(
                self.old,
                self.new,
                &self.old_mask,
                &self.new_mask,
                config,
            ) {
                emit_rect_block_move(&mut self.emit_ctx, mv)?;
                #[cfg(feature = "perf-metrics")]
                if let Some(m) = self.emit_ctx.metrics.as_deref_mut() {
                    m.moves_detected = m.moves_detected.saturating_add(1);
                }
                self.old_mask.exclude_rect_cells(
                    mv.src_start_row,
                    mv.src_row_count,
                    mv.src_start_col,
                    mv.src_col_count,
                );
                self.new_mask.exclude_rect_cells(
                    mv.dst_start_row,
                    mv.src_row_count,
                    mv.dst_start_col,
                    mv.src_col_count,
                );
                self.old_mask.exclude_rect_cells(
                    mv.dst_start_row,
                    mv.src_row_count,
                    mv.dst_start_col,
                    mv.src_col_count,
                );
                self.new_mask.exclude_rect_cells(
                    mv.src_start_row,
                    mv.src_row_count,
                    mv.src_start_col,
                    mv.src_col_count,
                );
                iteration += 1;
                found_move = true;
            }

            if !found_move
                && let Some(mv) = detect_exact_row_block_move_masked(
                    self.old,
                    self.new,
                    &self.old_mask,
                    &self.new_mask,
                    config,
                )
            {
                emit_row_block_move(&mut self.emit_ctx, mv)?;
                #[cfg(feature = "perf-metrics")]
                if let Some(m) = self.emit_ctx.metrics.as_deref_mut() {
                    m.moves_detected = m.moves_detected.saturating_add(1);
                }
                self.old_mask.exclude_rows(mv.src_start_row, mv.row_count);
                self.new_mask.exclude_rows(mv.dst_start_row, mv.row_count);
                iteration += 1;
                found_move = true;
            }

            if !found_move
                && let Some(mv) = detect_exact_column_block_move_masked(
                    self.old,
                    self.new,
                    &self.old_mask,
                    &self.new_mask,
                    config,
                )
            {
                emit_column_block_move(&mut self.emit_ctx, mv)?;
                #[cfg(feature = "perf-metrics")]
                if let Some(m) = self.emit_ctx.metrics.as_deref_mut() {
                    m.moves_detected = m.moves_detected.saturating_add(1);
                }
                self.old_mask.exclude_cols(mv.src_start_col, mv.col_count);
                self.new_mask.exclude_cols(mv.dst_start_col, mv.col_count);
                iteration += 1;
                found_move = true;
            }

            if !found_move
                && config.moves.enable_fuzzy_moves
                && let Some(mv) = detect_fuzzy_row_block_move_masked(
                    self.old,
                    self.new,
                    &self.old_mask,
                    &self.new_mask,
                    config,
                )
            {
                emit_row_block_move(&mut self.emit_ctx, mv)?;
                emit_moved_row_block_edits(&mut self.emit_ctx, &self.old_view, &self.new_view, mv)?;
                #[cfg(feature = "perf-metrics")]
                if let Some(m) = self.emit_ctx.metrics.as_deref_mut() {
                    m.moves_detected = m.moves_detected.saturating_add(1);
                }
                self.old_mask.exclude_rows(mv.src_start_row, mv.row_count);
                self.new_mask.exclude_rows(mv.dst_start_row, mv.row_count);
                iteration += 1;
                found_move = true;
            }

            if !found_move {
                break;
            }

            if self.old.nrows != self.new.nrows || self.old.ncols != self.new.ncols {
                break;
            }
        }

        Ok(iteration)
    }

    pub(super) fn has_mask_exclusions(&self) -> bool {
        self.old_mask.has_exclusions() || self.new_mask.has_exclusions()
    }

    pub(super) fn diff_with_masks(&mut self) -> Result<(), DiffError> {
        if self.old.nrows != self.new.nrows || self.old.ncols != self.new.ncols {
            if diff_aligned_with_masks(
                &mut self.emit_ctx,
                self.old,
                self.new,
                &self.old_mask,
                &self.new_mask,
            )? {
                return Ok(());
            }
            positional_diff_with_masks(
                &mut self.emit_ctx,
                self.old,
                self.new,
                &self.old_mask,
                &self.new_mask,
            )?;
            return Ok(());
        }

        positional_diff_masked_equal_size(
            &mut self.emit_ctx,
            self.old,
            self.new,
            &self.old_mask,
            &self.new_mask,
        )?;

        Ok(())
    }

    pub(super) fn try_amr(&mut self) -> Result<bool, DiffError> {
        let handled =
            try_diff_with_amr(&mut self.emit_ctx, self.old, self.new, &self.old_view, &self.new_view)?;
        Ok(handled)
    }

    pub(super) fn try_row_alignment(&mut self) -> Result<bool, DiffError> {
        return try_row_alignment_internal(&mut self.emit_ctx, &self.old_view, &self.new_view);
    }

    pub(super) fn try_single_column_alignment(&mut self) -> Result<bool, DiffError> {
        return try_single_column_alignment_internal(
            &mut self.emit_ctx,
            self.old,
            self.new,
            &self.old_view,
            &self.new_view,
        );
    }

    pub(super) fn positional(&mut self) -> Result<(), DiffError> {
        run_positional_diff_from_views_with_metrics(
            &mut self.emit_ctx,
            self.old,
            self.new,
            &self.old_view,
            &self.new_view,
        )?;
        Ok(())
    }
}

pub(super) fn row_signature_at(grid: &Grid, row: u32) -> Option<RowSignature> {
    if let Some(sig) = grid
        .row_signatures
        .as_ref()
        .and_then(|rows| rows.get(row as usize))
    {
        return Some(*sig);
    }
    Some(grid.compute_row_signature(row))
}

pub(super) fn col_signature_at(grid: &Grid, col: u32) -> Option<ColSignature> {
    if let Some(sig) = grid
        .col_signatures
        .as_ref()
        .and_then(|cols| cols.get(col as usize))
    {
        return Some(*sig);
    }
    Some(grid.compute_col_signature(col))
}

fn align_indices_by_signature<T: Copy + Eq>(
    idx_a: &[u32],
    idx_b: &[u32],
    sig_a: impl Fn(u32) -> Option<T>,
    sig_b: impl Fn(u32) -> Option<T>,
) -> Option<(Vec<u32>, Vec<u32>)> {
    if idx_a.is_empty() || idx_b.is_empty() {
        return None;
    }

    if idx_a.len() == idx_b.len() {
        return Some((idx_a.to_vec(), idx_b.to_vec()));
    }

    let (short, long, short_is_a) = if idx_a.len() <= idx_b.len() {
        (idx_a, idx_b, true)
    } else {
        (idx_b, idx_a, false)
    };

    let diff = long.len() - short.len();
    let mut best_offset = 0usize;
    let mut best_matches = 0usize;

    for offset in 0..=diff {
        let mut matches = 0usize;
        for (i, &short_idx) in short.iter().enumerate() {
            let long_idx = long[offset + i];
            let (sig_short, sig_long) = if short_is_a {
                (sig_a(short_idx), sig_b(long_idx))
            } else {
                (sig_b(short_idx), sig_a(long_idx))
            };
            if let (Some(sa), Some(sb)) = (sig_short, sig_long)
                && sa == sb
            {
                matches += 1;
            }
        }
        if matches > best_matches {
            best_matches = matches;
            best_offset = offset;
        }
    }

    if short_is_a {
        let aligned_b = long[best_offset..best_offset + short.len()].to_vec();
        Some((idx_a.to_vec(), aligned_b))
    } else {
        let aligned_a = long[best_offset..best_offset + short.len()].to_vec();
        Some((aligned_a, idx_b.to_vec()))
    }
}

fn collect_differences_in_grid(old: &Grid, new: &Grid) -> Vec<(u32, u32)> {
    let mut diffs = Vec::new();

    for row in 0..old.nrows {
        for col in 0..old.ncols {
            if !cells_content_equal(old.get(row, col), new.get(row, col)) {
                diffs.push((row, col));
            }
        }
    }

    diffs
}

fn contiguous_ranges<I>(indices: I) -> Vec<(u32, u32)>
where
    I: IntoIterator<Item = u32>,
{
    let mut values: Vec<u32> = indices.into_iter().collect();
    if values.is_empty() {
        return Vec::new();
    }

    values.sort_unstable();
    values.dedup();

    let mut ranges: Vec<(u32, u32)> = Vec::new();
    let mut start = values[0];
    let mut prev = values[0];

    for &val in values.iter().skip(1) {
        if val == prev + 1 {
            prev = val;
            continue;
        }

        ranges.push((start, prev));
        start = val;
        prev = val;
    }
    ranges.push((start, prev));

    ranges
}

fn group_rows_by_column_patterns(diffs: &[(u32, u32)]) -> Vec<(u32, u32)> {
    if diffs.is_empty() {
        return Vec::new();
    }

    let mut row_to_cols: BTreeMap<u32, Vec<u32>> = BTreeMap::new();
    for (row, col) in diffs {
        row_to_cols.entry(*row).or_default().push(*col);
    }

    for cols in row_to_cols.values_mut() {
        cols.sort_unstable();
        cols.dedup();
    }

    let mut rows: Vec<u32> = row_to_cols.keys().copied().collect();
    rows.sort_unstable();

    let mut groups: Vec<(u32, u32)> = Vec::new();
    if let Some(&first_row) = rows.first() {
        let mut start = first_row;
        let mut prev = first_row;
        let mut current_cols = row_to_cols.get(&first_row).cloned().unwrap_or_default();

        for row in rows.into_iter().skip(1) {
            let cols = row_to_cols.get(&row).cloned().unwrap_or_default();
            if row == prev + 1 && cols == current_cols {
                prev = row;
            } else {
                groups.push((start, prev));
                start = row;
                prev = row;
                current_cols = cols;
            }
        }
        groups.push((start, prev));
    }

    groups
}

fn build_projected_grid_from_maps(
    source: &Grid,
    mask: &RegionMask,
    row_map: &[u32],
    col_map: &[u32],
) -> (Grid, Vec<u32>, Vec<u32>) {
    let nrows = row_map.len() as u32;
    let ncols = col_map.len() as u32;

    let mut row_lookup: Vec<Option<u32>> = vec![None; source.nrows as usize];
    for (new_idx, old_row) in row_map.iter().enumerate() {
        row_lookup[*old_row as usize] = Some(new_idx as u32);
    }

    let mut col_lookup: Vec<Option<u32>> = vec![None; source.ncols as usize];
    for (new_idx, old_col) in col_map.iter().enumerate() {
        col_lookup[*old_col as usize] = Some(new_idx as u32);
    }

    let mut projected = Grid::new(nrows, ncols);

    for ((row, col), cell) in source.iter_cells() {
        if !mask.is_cell_active(row, col) {
            continue;
        }
        let Some(new_row) = row_lookup.get(row as usize).and_then(|v| *v) else {
            continue;
        };
        let Some(new_col) = col_lookup.get(col as usize).and_then(|v| *v) else {
            continue;
        };

        projected.insert_cell(new_row, new_col, cell.value, cell.formula);
    }

    (projected, row_map.to_vec(), col_map.to_vec())
}

fn build_masked_grid(source: &Grid, mask: &RegionMask) -> (Grid, Vec<u32>, Vec<u32>) {
    let row_map: Vec<u32> = mask.active_rows().collect();
    let col_map: Vec<u32> = mask.active_cols().collect();

    let nrows = row_map.len() as u32;
    let ncols = col_map.len() as u32;

    let mut row_lookup: Vec<Option<u32>> = vec![None; source.nrows as usize];
    for (new_idx, old_row) in row_map.iter().enumerate() {
        row_lookup[*old_row as usize] = Some(new_idx as u32);
    }

    let mut col_lookup: Vec<Option<u32>> = vec![None; source.ncols as usize];
    for (new_idx, old_col) in col_map.iter().enumerate() {
        col_lookup[*old_col as usize] = Some(new_idx as u32);
    }

    let mut projected = Grid::new(nrows, ncols);

    for ((row, col), cell) in source.iter_cells() {
        if !mask.is_cell_active(row, col) {
            continue;
        }

        let Some(new_row) = row_lookup.get(row as usize).and_then(|v| *v) else {
            continue;
        };
        let Some(new_col) = col_lookup.get(col as usize).and_then(|v| *v) else {
            continue;
        };

        projected.insert_cell(new_row, new_col, cell.value, cell.formula);
    }

    (projected, row_map, col_map)
}

fn detect_exact_row_block_move_masked(
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
    config: &DiffConfig,
) -> Option<RowBlockMove> {
    if !old_mask.has_active_cells() || !new_mask.has_active_cells() {
        return None;
    }

    if !old_mask.has_exclusions() && !new_mask.has_exclusions() {
        return detect_exact_row_block_move(old, new, config);
    }

    let (old_proj, old_rows, _) = build_masked_grid(old, old_mask);
    let (new_proj, new_rows, _) = build_masked_grid(new, new_mask);

    if old_proj.nrows != new_proj.nrows || old_proj.ncols != new_proj.ncols {
        return None;
    }

    let mv_local = detect_exact_row_block_move(&old_proj, &new_proj, config)?;
    let src_start_row = *old_rows.get(mv_local.src_start_row as usize)?;
    let dst_start_row = *new_rows.get(mv_local.dst_start_row as usize)?;

    Some(RowBlockMove {
        src_start_row,
        dst_start_row,
        row_count: mv_local.row_count,
    })
}

fn detect_exact_column_block_move_masked(
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
    config: &DiffConfig,
) -> Option<ColumnBlockMove> {
    if !old_mask.has_active_cells() || !new_mask.has_active_cells() {
        return None;
    }

    if !old_mask.has_exclusions() && !new_mask.has_exclusions() {
        return detect_exact_column_block_move(old, new, config);
    }

    let (old_proj, _, old_cols) = build_masked_grid(old, old_mask);
    let (new_proj, _, new_cols) = build_masked_grid(new, new_mask);

    if old_proj.nrows != new_proj.nrows || old_proj.ncols != new_proj.ncols {
        return None;
    }

    let mv_local = detect_exact_column_block_move(&old_proj, &new_proj, config)?;
    let src_start_col = *old_cols.get(mv_local.src_start_col as usize)?;
    let dst_start_col = *new_cols.get(mv_local.dst_start_col as usize)?;

    Some(ColumnBlockMove {
        src_start_col,
        dst_start_col,
        col_count: mv_local.col_count,
    })
}

fn detect_exact_rect_block_move_masked(
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
    config: &DiffConfig,
) -> Option<RectBlockMove> {
    if !old_mask.has_active_cells() || !new_mask.has_active_cells() {
        return None;
    }

    if !old_mask.has_exclusions()
        && !new_mask.has_exclusions()
        && old.nrows == new.nrows
        && old.ncols == new.ncols
        && let Some(mv) = detect_exact_rect_block_move(old, new, config)
    {
        return Some(mv);
    }

    let aligned_rows = align_indices_by_signature(
        &old_mask.active_rows().collect::<Vec<_>>(),
        &new_mask.active_rows().collect::<Vec<_>>(),
        |r| row_signature_at(old, r),
        |r| row_signature_at(new, r),
    )?;
    let aligned_cols = align_indices_by_signature(
        &old_mask.active_cols().collect::<Vec<_>>(),
        &new_mask.active_cols().collect::<Vec<_>>(),
        |c| col_signature_at(old, c),
        |c| col_signature_at(new, c),
    )?;
    let (old_proj, old_rows, old_cols) =
        build_projected_grid_from_maps(old, old_mask, &aligned_rows.0, &aligned_cols.0);
    let (new_proj, new_rows, new_cols) =
        build_projected_grid_from_maps(new, new_mask, &aligned_rows.1, &aligned_cols.1);

    let map_move = |mv_local: RectBlockMove,
                    row_map_old: &[u32],
                    row_map_new: &[u32],
                    col_map_old: &[u32],
                    col_map_new: &[u32]|
     -> Option<RectBlockMove> {
        let src_start_row = *row_map_old.get(mv_local.src_start_row as usize)?;
        let dst_start_row = *row_map_new.get(mv_local.dst_start_row as usize)?;
        let src_start_col = *col_map_old.get(mv_local.src_start_col as usize)?;
        let dst_start_col = *col_map_new.get(mv_local.dst_start_col as usize)?;

        Some(RectBlockMove {
            src_start_row,
            dst_start_row,
            src_start_col,
            dst_start_col,
            src_row_count: mv_local.src_row_count,
            src_col_count: mv_local.src_col_count,
            block_hash: mv_local.block_hash,
        })
    };

    if let Some(mv_local) = detect_exact_rect_block_move(&old_proj, &new_proj, config)
        && let Some(mapped) = map_move(mv_local, &old_rows, &new_rows, &old_cols, &new_cols)
    {
        return Some(mapped);
    }

    let diff_positions = collect_differences_in_grid(&old_proj, &new_proj);
    if diff_positions.is_empty() {
        return None;
    }

    let row_ranges = group_rows_by_column_patterns(&diff_positions);
    let col_ranges_full = contiguous_ranges(diff_positions.iter().map(|(_, c)| *c));
    let has_prior_exclusions = old_mask.has_exclusions() || new_mask.has_exclusions();
    if !has_prior_exclusions && row_ranges.len() <= 2 && col_ranges_full.len() <= 2 {
        return None;
    }

    let range_len = |range: (u32, u32)| range.1.saturating_sub(range.0).saturating_add(1);
    let in_range = |idx: u32, range: (u32, u32)| idx >= range.0 && idx <= range.1;
    let rectangles_match = |src_rows: (u32, u32),
                            src_cols: (u32, u32),
                            dst_rows: (u32, u32),
                            dst_cols: (u32, u32)|
     -> bool {
        let row_count = range_len(src_rows);
        let col_count = range_len(src_cols);

        for dr in 0..row_count {
            for dc in 0..col_count {
                let src_row = src_rows.0 + dr;
                let src_col = src_cols.0 + dc;
                let dst_row = dst_rows.0 + dr;
                let dst_col = dst_cols.0 + dc;

                if !cells_content_equal(
                    old_proj.get(src_row, src_col),
                    new_proj.get(dst_row, dst_col),
                ) {
                    return false;
                }
            }
        }

        true
    };

    for (row_idx, &row_a) in row_ranges.iter().enumerate() {
        for &row_b in row_ranges.iter().skip(row_idx + 1) {
            if range_len(row_a) != range_len(row_b) {
                continue;
            }

            let cols_row_a: Vec<u32> = diff_positions
                .iter()
                .filter_map(|(r, c)| if in_range(*r, row_a) { Some(*c) } else { None })
                .collect();
            let cols_row_b: Vec<u32> = diff_positions
                .iter()
                .filter_map(|(r, c)| if in_range(*r, row_b) { Some(*c) } else { None })
                .collect();
            let col_ranges_a = contiguous_ranges(cols_row_a);
            let col_ranges_b = contiguous_ranges(cols_row_b);
            let mut col_pairs: Vec<((u32, u32), (u32, u32))> = Vec::new();

            for &col_a in &col_ranges_a {
                for &col_b in &col_ranges_b {
                    if range_len(col_a) != range_len(col_b) {
                        continue;
                    }
                    col_pairs.push((col_a, col_b));
                }
            }

            if col_pairs.is_empty() {
                continue;
            }

            for (col_a, col_b) in col_pairs {
                let mut scoped_old_mask = RegionMask::all_active(old_proj.nrows, old_proj.ncols);
                let mut scoped_new_mask = RegionMask::all_active(new_proj.nrows, new_proj.ncols);

                for row in 0..old_proj.nrows {
                    if !in_range(row, row_a) && !in_range(row, row_b) {
                        scoped_old_mask.exclude_row(row);
                        scoped_new_mask.exclude_row(row);
                    }
                }

                for col in 0..old_proj.ncols {
                    if !in_range(col, col_a) && !in_range(col, col_b) {
                        scoped_old_mask.exclude_col(col);
                        scoped_new_mask.exclude_col(col);
                    }
                }

                let (old_scoped, scoped_old_rows, scoped_old_cols) =
                    build_masked_grid(&old_proj, &scoped_old_mask);
                let (new_scoped, scoped_new_rows, scoped_new_cols) =
                    build_masked_grid(&new_proj, &scoped_new_mask);

                if old_scoped.nrows != new_scoped.nrows || old_scoped.ncols != new_scoped.ncols {
                    continue;
                }

                if let Some(candidate) =
                    detect_exact_rect_block_move(&old_scoped, &new_scoped, config)
                {
                    let scoped_row_map_old: Option<Vec<u32>> = scoped_old_rows
                        .iter()
                        .map(|idx| old_rows.get(*idx as usize).copied())
                        .collect();
                    let scoped_row_map_new: Option<Vec<u32>> = scoped_new_rows
                        .iter()
                        .map(|idx| new_rows.get(*idx as usize).copied())
                        .collect();
                    let scoped_col_map_old: Option<Vec<u32>> = scoped_old_cols
                        .iter()
                        .map(|idx| old_cols.get(*idx as usize).copied())
                        .collect();
                    let scoped_col_map_new: Option<Vec<u32>> = scoped_new_cols
                        .iter()
                        .map(|idx| new_cols.get(*idx as usize).copied())
                        .collect();

                    if let (
                        Some(row_map_old),
                        Some(row_map_new),
                        Some(col_map_old),
                        Some(col_map_new),
                    ) = (
                        scoped_row_map_old,
                        scoped_row_map_new,
                        scoped_col_map_old,
                        scoped_col_map_new,
                    ) && let Some(mapped) = map_move(
                        candidate,
                        &row_map_old,
                        &row_map_new,
                        &col_map_old,
                        &col_map_new,
                    ) {
                        return Some(mapped);
                    }
                }

                let row_len = range_len(row_a);
                let col_len = range_len(col_a);
                if row_len == 0 || col_len == 0 {
                    continue;
                }

                let candidates = [
                    (row_a, col_a, row_b, col_b),
                    (row_a, col_b, row_b, col_a),
                    (row_b, col_a, row_a, col_b),
                    (row_b, col_b, row_a, col_a),
                ];

                for (src_rows, src_cols, dst_rows, dst_cols) in candidates {
                    if range_len(src_rows) != range_len(dst_rows)
                        || range_len(src_cols) != range_len(dst_cols)
                    {
                        continue;
                    }

                    if rectangles_match(src_rows, src_cols, dst_rows, dst_cols) {
                        let mapped = RectBlockMove {
                            src_start_row: *old_rows.get(src_rows.0 as usize)?,
                            dst_start_row: *new_rows.get(dst_rows.0 as usize)?,
                            src_start_col: *old_cols.get(src_cols.0 as usize)?,
                            dst_start_col: *new_cols.get(dst_cols.0 as usize)?,
                            src_row_count: range_len(src_rows),
                            src_col_count: range_len(src_cols),
                            block_hash: None,
                        };
                        return Some(mapped);
                    }
                }
            }
        }
    }

    None
}

fn detect_fuzzy_row_block_move_masked(
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
    config: &DiffConfig,
) -> Option<RowBlockMove> {
    if !old_mask.has_active_cells() || !new_mask.has_active_cells() {
        return None;
    }

    if !old_mask.has_exclusions() && !new_mask.has_exclusions() {
        return detect_fuzzy_row_block_move(old, new, config);
    }

    let (old_proj, old_rows, _) = build_masked_grid(old, old_mask);
    let (new_proj, new_rows, _) = build_masked_grid(new, new_mask);

    if old_proj.nrows != new_proj.nrows || old_proj.ncols != new_proj.ncols {
        return None;
    }

    let mv_local = detect_fuzzy_row_block_move(&old_proj, &new_proj, config)?;
    let src_start_row = *old_rows.get(mv_local.src_start_row as usize)?;
    let dst_start_row = *new_rows.get(mv_local.dst_start_row as usize)?;

    Some(RowBlockMove {
        src_start_row,
        dst_start_row,
        row_count: mv_local.row_count,
    })
}

fn diff_aligned_with_masks<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
) -> Result<bool, DiffError> {
    let old_rows: Vec<u32> = old_mask.active_rows().collect();
    let new_rows: Vec<u32> = new_mask.active_rows().collect();
    let old_cols: Vec<u32> = old_mask.active_cols().collect();
    let new_cols: Vec<u32> = new_mask.active_cols().collect();

    let Some((rows_a, rows_b)) = align_indices_by_signature(
        &old_rows,
        &new_rows,
        |r| row_signature_at(old, r),
        |r| row_signature_at(new, r),
    ) else {
        return Ok(false);
    };

    let (cols_a, cols_b) = align_indices_by_signature(
        &old_cols,
        &new_cols,
        |c| col_signature_at(old, c),
        |c| col_signature_at(new, c),
    )
    .unwrap_or((old_cols.clone(), new_cols.clone()));

    if rows_a.len() != rows_b.len() || cols_a.len() != cols_b.len() {
        return Ok(false);
    }

    ctx.hardening.progress("cell_diff", 0.0);

    let total_rows = rows_a.len();
    for (idx, (row_a, row_b)) in rows_a.iter().zip(rows_b.iter()).enumerate() {
        if ctx.hardening.check_timeout(ctx.warnings) {
            return Ok(true);
        }

        if total_rows > 0 && idx % 64 == 0 {
            ctx.hardening
                .progress("cell_diff", idx as f32 / total_rows as f32);
        }

        for (col_a, col_b) in cols_a.iter().zip(cols_b.iter()) {
            if !old_mask.is_cell_active(*row_a, *col_a) || !new_mask.is_cell_active(*row_b, *col_b)
            {
                continue;
            }
            let old_cell = old.get(*row_a, *col_a);
            let new_cell = new.get(*row_b, *col_b);

            if cells_content_equal(old_cell, new_cell) {
                continue;
            }

            let addr = CellAddress::from_indices(*row_b, *col_b);
            let row_shift = *row_b as i32 - *row_a as i32;
            let col_shift = *col_b as i32 - *col_a as i32;
            emit_cell_edit(ctx, addr, old_cell, new_cell, row_shift, col_shift)?;
        }
    }

    ctx.hardening.progress("cell_diff", 1.0);

    let rows_a_set: HashSet<u32> = rows_a.iter().copied().collect();
    let rows_b_set: HashSet<u32> = rows_b.iter().copied().collect();

    for row_idx in new_rows.iter().filter(|r| !rows_b_set.contains(r)) {
        if new_mask.is_row_active(*row_idx) {
            ctx.emit(crate::diff::DiffOp::row_added(ctx.sheet_id, *row_idx, None))?;
        }
    }

    for row_idx in old_rows.iter().filter(|r| !rows_a_set.contains(r)) {
        if old_mask.is_row_active(*row_idx) {
            ctx.emit(crate::diff::DiffOp::row_removed(
                ctx.sheet_id,
                *row_idx,
                None,
            ))?;
        }
    }

    let cols_a_set: HashSet<u32> = cols_a.iter().copied().collect();
    let cols_b_set: HashSet<u32> = cols_b.iter().copied().collect();

    for col_idx in new_cols.iter().filter(|c| !cols_b_set.contains(c)) {
        if new_mask.is_col_active(*col_idx) {
            ctx.emit(crate::diff::DiffOp::column_added(
                ctx.sheet_id,
                *col_idx,
                None,
            ))?;
        }
    }

    for col_idx in old_cols.iter().filter(|c| !cols_a_set.contains(c)) {
        if old_mask.is_col_active(*col_idx) {
            ctx.emit(crate::diff::DiffOp::column_removed(
                ctx.sheet_id,
                *col_idx,
                None,
            ))?;
        }
    }

    Ok(true)
}

fn positional_diff_with_masks<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
) -> Result<(), DiffError> {
    let overlap_rows = old.nrows.min(new.nrows);
    let overlap_cols = old.ncols.min(new.ncols);

    ctx.hardening.progress("cell_diff", 0.0);

    for row in 0..overlap_rows {
        if ctx.hardening.check_timeout(ctx.warnings) {
            return Ok(());
        }
        if overlap_rows > 0 && row % 256 == 0 {
            ctx.hardening
                .progress("cell_diff", row as f32 / overlap_rows as f32);
        }
        for col in 0..overlap_cols {
            if !old_mask.is_cell_active(row, col) || !new_mask.is_cell_active(row, col) {
                continue;
            }
            let old_cell = old.get(row, col);
            let new_cell = new.get(row, col);

            if cells_content_equal(old_cell, new_cell) {
                continue;
            }

            let addr = CellAddress::from_indices(row, col);
            emit_cell_edit(ctx, addr, old_cell, new_cell, 0, 0)?;
        }
    }

    if overlap_rows > 0 {
        ctx.hardening.progress("cell_diff", 1.0);
    }

    if ctx.hardening.check_timeout(ctx.warnings) {
        return Ok(());
    }

    if new.nrows > old.nrows {
        for row_idx in old.nrows..new.nrows {
            if row_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            if new_mask.is_row_active(row_idx) {
                ctx.emit(crate::diff::DiffOp::row_added(ctx.sheet_id, row_idx, None))?;
            }
        }
    } else if old.nrows > new.nrows {
        for row_idx in new.nrows..old.nrows {
            if row_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            if old_mask.is_row_active(row_idx) {
                ctx.emit(crate::diff::DiffOp::row_removed(
                    ctx.sheet_id,
                    row_idx,
                    None,
                ))?;
            }
        }
    }

    if new.ncols > old.ncols {
        for col_idx in old.ncols..new.ncols {
            if col_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            if new_mask.is_col_active(col_idx) {
                ctx.emit(crate::diff::DiffOp::column_added(
                    ctx.sheet_id,
                    col_idx,
                    None,
                ))?;
            }
        }
    } else if old.ncols > new.ncols {
        for col_idx in new.ncols..old.ncols {
            if col_idx % 4096 == 0 && ctx.hardening.check_timeout(ctx.warnings) {
                return Ok(());
            }
            if old_mask.is_col_active(col_idx) {
                ctx.emit(crate::diff::DiffOp::column_removed(
                    ctx.sheet_id,
                    col_idx,
                    None,
                ))?;
            }
        }
    }

    Ok(())
}

fn positional_diff_masked_equal_size<S: DiffSink>(
    ctx: &mut EmitCtx<'_, '_, S>,
    old: &Grid,
    new: &Grid,
    old_mask: &RegionMask,
    new_mask: &RegionMask,
) -> Result<(), DiffError> {
    let row_shift_zone =
        compute_combined_shift_zone(old_mask.row_shift_bounds(), new_mask.row_shift_bounds());
    let col_shift_zone =
        compute_combined_shift_zone(old_mask.col_shift_bounds(), new_mask.col_shift_bounds());

    let stable_rows: Vec<u32> = (0..old.nrows)
        .filter(|&r| !is_in_zone(r, &row_shift_zone))
        .collect();
    let stable_cols: Vec<u32> = (0..old.ncols)
        .filter(|&c| !is_in_zone(c, &col_shift_zone))
        .collect();

    ctx.hardening.progress("cell_diff", 0.0);

    let total_rows = stable_rows.len();
    for (idx, &row) in stable_rows.iter().enumerate() {
        if ctx.hardening.check_timeout(ctx.warnings) {
            return Ok(());
        }
        if total_rows > 0 && idx % 64 == 0 {
            ctx.hardening
                .progress("cell_diff", idx as f32 / total_rows as f32);
        }
        for &col in &stable_cols {
            if !old_mask.is_cell_active(row, col) || !new_mask.is_cell_active(row, col) {
                continue;
            }
            let old_cell = old.get(row, col);
            let new_cell = new.get(row, col);

            if cells_content_equal(old_cell, new_cell) {
                continue;
            }

            let addr = CellAddress::from_indices(row, col);
            emit_cell_edit(ctx, addr, old_cell, new_cell, 0, 0)?;
        }
    }

    ctx.hardening.progress("cell_diff", 1.0);

    Ok(())
}

fn compute_combined_shift_zone(a: Option<(u32, u32)>, b: Option<(u32, u32)>) -> Option<(u32, u32)> {
    match (a, b) {
        (Some((a_min, a_max)), Some((b_min, b_max))) => Some((a_min.min(b_min), a_max.max(b_max))),
        (Some(bounds), None) | (None, Some(bounds)) => Some(bounds),
        (None, None) => None,
    }
}

fn is_in_zone(idx: u32, zone: &Option<(u32, u32)>) -> bool {
    match zone {
        Some((min, max)) => idx >= *min && idx <= *max,
        None => false,
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::workbook::CellValue;

    fn grid_from_matrix(values: &[Vec<i32>]) -> Grid {
        let nrows = values.len() as u32;
        let ncols = if nrows == 0 {
            0
        } else {
            values[0].len() as u32
        };
        let mut grid = Grid::new(nrows, ncols);
        for (r, row) in values.iter().enumerate() {
            for (c, val) in row.iter().enumerate() {
                grid.insert_cell(
                    r as u32,
                    c as u32,
                    Some(CellValue::Number(*val as f64)),
                    None,
                );
            }
        }
        grid
    }

    #[test]
    fn rect_move_masked_falls_back_when_outside_edit_exists() {
        let rows = 12usize;
        let cols = 12usize;
        let base: Vec<Vec<i32>> = (0..rows)
            .map(|r| {
                (0..cols)
                    .map(|c| 10_000 + (r as i32) * 100 + c as i32)
                    .collect()
            })
            .collect();
        let mut changed = base.clone();

        let src = (2usize, 2usize);
        let dst = (8usize, 6usize);
        let size = (2usize, 3usize);

        for dr in 0..size.0 {
            for dc in 0..size.1 {
                let src_r = src.0 + dr;
                let src_c = src.1 + dc;
                let dst_r = dst.0 + dr;
                let dst_c = dst.1 + dc;

                let src_val = base[src_r][src_c];
                let dst_val = base[dst_r][dst_c];

                changed[dst_r][dst_c] = src_val;
                changed[src_r][src_c] = dst_val;
            }
        }

        changed[0][0] = 77_777;

        let old = grid_from_matrix(&base);
        let new = grid_from_matrix(&changed);
        let old_mask = RegionMask::all_active(old.nrows, old.ncols);
        let new_mask = RegionMask::all_active(new.nrows, new.ncols);

        let mv = detect_exact_rect_block_move_masked(
            &old,
            &new,
            &old_mask,
            &new_mask,
            &DiffConfig::default(),
        )
        .expect("masked detector should fall back and still detect the move");

        assert_eq!(mv.src_start_row, src.0 as u32);
        assert_eq!(mv.src_start_col, src.1 as u32);
        assert_eq!(mv.src_row_count, size.0 as u32);
        assert_eq!(mv.src_col_count, size.1 as u32);
        assert_eq!(mv.dst_start_row, dst.0 as u32);
        assert_eq!(mv.dst_start_col, dst.1 as u32);
    }
}

```

---

### File: `core\src\engine\sheet_diff.rs`

```rust
use crate::config::DiffConfig;
use crate::diff::{DiffError, DiffReport, DiffSummary};
use crate::progress::ProgressCallback;
use crate::sink::{DiffSink, SinkFinishGuard, VecSink};
use crate::string_pool::StringPool;
use crate::workbook::Sheet;

use crate::diff::SheetId;
use super::context::DiffContext;
use super::grid_diff::try_diff_grids_internal;
use super::hardening::HardeningController;

pub fn diff_sheets(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> DiffReport {
    let mut sink = VecSink::new();
    match try_diff_sheets_streaming(old, new, pool, config, &mut sink) {
        Ok(summary) => {
            let strings = pool.strings().to_vec();
            DiffReport::from_ops_and_summary(sink.into_ops(), summary, strings)
        }
        Err(e) => {
            let strings = pool.strings().to_vec();
            DiffReport {
                version: DiffReport::SCHEMA_VERSION.to_string(),
                strings,
                ops: sink.into_ops(),
                complete: false,
                warnings: vec![e.to_string()],
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            }
        }
    }
}

pub fn try_diff_sheets(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Result<DiffReport, DiffError> {
    let mut sink = VecSink::new();
    let summary = try_diff_sheets_streaming(old, new, pool, config, &mut sink)?;
    let strings = pool.strings().to_vec();
    Ok(DiffReport::from_ops_and_summary(
        sink.into_ops(),
        summary,
        strings,
    ))
}

/// Stream a sheet diff into `sink`.
///
/// Streaming output follows the contract in `docs/streaming_contract.md`.
pub fn diff_sheets_streaming<S: DiffSink>(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> DiffSummary {
    match try_diff_sheets_streaming(old, new, pool, config, sink) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

pub fn diff_sheets_streaming_with_progress<S: DiffSink>(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> DiffSummary {
    match try_diff_sheets_streaming_with_progress(old, new, pool, config, sink, progress) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

/// Like [`diff_sheets_streaming`], but returns errors instead of embedding them in the summary.
pub fn try_diff_sheets_streaming<S: DiffSink>(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> Result<DiffSummary, DiffError> {
    let mut op_count = 0usize;
    try_diff_sheets_streaming_with_op_count(old, new, pool, config, sink, &mut op_count, None)
}

pub fn try_diff_sheets_streaming_with_progress<S: DiffSink>(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> Result<DiffSummary, DiffError> {
    let mut op_count = 0usize;
    try_diff_sheets_streaming_with_op_count(
        old,
        new,
        pool,
        config,
        sink,
        &mut op_count,
        Some(progress),
    )
}

#[allow(clippy::too_many_arguments)]
fn try_diff_sheets_streaming_with_op_count<'p, S: DiffSink>(
    old: &Sheet,
    new: &Sheet,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    op_count: &mut usize,
    progress: Option<&'p dyn ProgressCallback>,
) -> Result<DiffSummary, DiffError> {
    let sheet_id: SheetId = old.name;

    sink.begin(pool)?;
    let mut finish_guard = SinkFinishGuard::new(sink);

    let mut ctx = DiffContext::default();
    let mut hardening = HardeningController::new(config, progress);

    if hardening.check_timeout(&mut ctx.warnings) {
        finish_guard.finish_and_disarm()?;
        return Ok(DiffSummary {
            complete: false,
            warnings: ctx.warnings,
            op_count: *op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        });
    }

    try_diff_grids_internal(
        sheet_id,
        &old.grid,
        &new.grid,
        config,
        pool,
        sink,
        op_count,
        &mut ctx,
        &mut hardening,
        #[cfg(feature = "perf-metrics")]
        None,
    )?;

    finish_guard.finish_and_disarm()?;
    let complete = ctx.warnings.is_empty();
    Ok(DiffSummary {
        complete,
        warnings: ctx.warnings,
        op_count: *op_count,
        #[cfg(feature = "perf-metrics")]
        metrics: None,
    })
}

```

---

### File: `core\src\engine\workbook_diff.rs`

```rust
use crate::config::DiffConfig;
use crate::diff::{DiffError, DiffOp, DiffReport, DiffSummary};
#[cfg(feature = "perf-metrics")]
use crate::perf::{DiffMetrics, Phase};
use crate::sink::{DiffSink, SinkFinishGuard, VecSink};
use crate::string_pool::StringPool;
use crate::workbook::{Sheet, SheetKind, Workbook};
use crate::progress::ProgressCallback;

use std::collections::{HashMap, HashSet};
#[cfg(feature = "perf-metrics")]
use std::mem::size_of;

use super::context::{DiffContext, emit_op};
use super::grid_diff::try_diff_grids_internal;
use super::hardening::HardeningController;
use crate::diff::SheetId;

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct SheetKey {
    name_lower: String,
    kind: SheetKind,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct SheetIdKey {
    id: u32,
    kind: SheetKind,
}

fn make_sheet_key(sheet: &Sheet, pool: &StringPool) -> SheetKey {
    SheetKey {
        name_lower: pool.resolve(sheet.name).to_lowercase(),
        kind: sheet.kind.clone(),
    }
}

fn sheet_name_lower(sheet: &Sheet, pool: &StringPool) -> String {
    pool.resolve(sheet.name).to_lowercase()
}

fn sheet_kind_order(kind: &SheetKind) -> u8 {
    match kind {
        SheetKind::Worksheet => 0,
        SheetKind::Chart => 1,
        SheetKind::Macro => 2,
        SheetKind::Other => 3,
    }
}

pub fn diff_workbooks(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> DiffReport {
    match try_diff_workbooks(old, new, pool, config) {
        Ok(report) => report,
        Err(e) => {
            let strings = pool.strings().to_vec();
            DiffReport {
                version: DiffReport::SCHEMA_VERSION.to_string(),
                strings,
                ops: Vec::new(),
                complete: false,
                warnings: vec![e.to_string()],
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            }
        }
    }
}

pub fn diff_workbooks_with_progress(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    progress: &dyn ProgressCallback,
) -> DiffReport {
    match try_diff_workbooks_with_progress(old, new, pool, config, progress) {
        Ok(report) => report,
        Err(e) => {
            let strings = pool.strings().to_vec();
            DiffReport {
                version: DiffReport::SCHEMA_VERSION.to_string(),
                strings,
                ops: Vec::new(),
                complete: false,
                warnings: vec![e.to_string()],
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            }
        }
    }
}

/// Stream a workbook diff into `sink`.
///
/// Streaming output follows the contract in `docs/streaming_contract.md`.
pub fn diff_workbooks_streaming<S: DiffSink>(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> DiffSummary {
    match try_diff_workbooks_streaming(old, new, pool, config, sink) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

pub fn diff_workbooks_streaming_with_progress<S: DiffSink>(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> DiffSummary {
    match try_diff_workbooks_streaming_with_progress(old, new, pool, config, sink, progress) {
        Ok(summary) => summary,
        Err(e) => DiffSummary {
            complete: false,
            warnings: vec![e.to_string()],
            op_count: 0,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        },
    }
}

pub fn try_diff_workbooks(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Result<DiffReport, DiffError> {
    let mut sink = VecSink::new();
    #[allow(unused_mut)]
    let mut summary = try_diff_workbooks_streaming(old, new, pool, config, &mut sink)?;
    #[cfg(feature = "perf-metrics")]
    if let Some(metrics) = summary.metrics.as_mut() {
        metrics.op_buffer_bytes = estimate_op_buffer_bytes(&sink);
    }
    let strings = pool.strings().to_vec();
    Ok(DiffReport::from_ops_and_summary(
        sink.into_ops(),
        summary,
        strings,
    ))
}

pub fn try_diff_workbooks_with_progress(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    progress: &dyn ProgressCallback,
) -> Result<DiffReport, DiffError> {
    let mut sink = VecSink::new();
    #[allow(unused_mut)]
    let mut summary =
        try_diff_workbooks_streaming_with_progress(old, new, pool, config, &mut sink, progress)?;
    #[cfg(feature = "perf-metrics")]
    if let Some(metrics) = summary.metrics.as_mut() {
        metrics.op_buffer_bytes = estimate_op_buffer_bytes(&sink);
    }
    let strings = pool.strings().to_vec();
    Ok(DiffReport::from_ops_and_summary(
        sink.into_ops(),
        summary,
        strings,
    ))
}

/// Like [`diff_workbooks_streaming`], but returns errors instead of embedding them in the summary.
pub fn try_diff_workbooks_streaming<S: DiffSink>(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
) -> Result<DiffSummary, DiffError> {
    try_diff_workbooks_streaming_impl(old, new, pool, config, sink, None)
}

pub fn try_diff_workbooks_streaming_with_progress<S: DiffSink>(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: &dyn ProgressCallback,
) -> Result<DiffSummary, DiffError> {
    try_diff_workbooks_streaming_impl(old, new, pool, config, sink, Some(progress))
}

fn try_diff_workbooks_streaming_impl<'p, S: DiffSink>(
    old: &Workbook,
    new: &Workbook,
    pool: &mut StringPool,
    config: &DiffConfig,
    sink: &mut S,
    progress: Option<&'p dyn ProgressCallback>,
) -> Result<DiffSummary, DiffError> {
    let mut hardening = HardeningController::new(config, progress);
    #[cfg(feature = "perf-metrics")]
    let mut metrics = {
        let mut m = DiffMetrics::default();
        m.start_phase(Phase::Total);
        m.start_phase(Phase::Parse);
        m
    };
    hardening.progress("parse", 0.0);

    sink.begin(pool)?;
    let mut finish_guard = SinkFinishGuard::new(sink);

    let mut ctx = DiffContext::default();
    let mut op_count = 0usize;

    if hardening.check_timeout(&mut ctx.warnings) {
        #[cfg(feature = "perf-metrics")]
        {
            metrics.end_phase(Phase::Parse);
            metrics.end_phase(Phase::Total);
            apply_accounted_peak(&mut metrics, old, new, pool);
        }
        finish_guard.finish_and_disarm()?;
        return Ok(DiffSummary {
            complete: false,
            warnings: ctx.warnings,
            op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: Some(metrics),
        });
    }

    let mut old_sheets_by_name: HashMap<SheetKey, &Sheet> = HashMap::new();
    for sheet in &old.sheets {
        let key = make_sheet_key(sheet, pool);
        if let Some(previous) = old_sheets_by_name.insert(key.clone(), sheet) {
            ctx.warnings.push(format!(
                "duplicate sheet identity in old workbook: '{}' ({:?}); \
                 later definition '{}' overwrites earlier one '{}'. The file may be corrupt.",
                key.name_lower,
                key.kind,
                pool.resolve(sheet.name),
                pool.resolve(previous.name)
            ));
        }
    }

    let mut new_sheets_by_name: HashMap<SheetKey, &Sheet> = HashMap::new();
    for sheet in &new.sheets {
        let key = make_sheet_key(sheet, pool);
        if let Some(previous) = new_sheets_by_name.insert(key.clone(), sheet) {
            ctx.warnings.push(format!(
                "duplicate sheet identity in new workbook: '{}' ({:?}); \
                 later definition '{}' overwrites earlier one '{}'. The file may be corrupt.",
                key.name_lower,
                key.kind,
                pool.resolve(sheet.name),
                pool.resolve(previous.name)
            ));
        }
    }

    let mut id_counts_old: HashMap<u32, usize> = HashMap::new();
    for sheet in &old.sheets {
        if let Some(id) = sheet.workbook_sheet_id {
            *id_counts_old.entry(id).or_insert(0) += 1;
        }
    }
    for (id, count) in id_counts_old.iter() {
        if *count > 1 {
            ctx.warnings.push(format!(
                "duplicate workbook sheetId in old workbook: id={}, falling back to name-based matching for those sheets.",
                id
            ));
        }
    }

    let mut id_counts_new: HashMap<u32, usize> = HashMap::new();
    for sheet in &new.sheets {
        if let Some(id) = sheet.workbook_sheet_id {
            *id_counts_new.entry(id).or_insert(0) += 1;
        }
    }
    for (id, count) in id_counts_new.iter() {
        if *count > 1 {
            ctx.warnings.push(format!(
                "duplicate workbook sheetId in new workbook: id={}, falling back to name-based matching for those sheets.",
                id
            ));
        }
    }

    let mut old_by_id: HashMap<SheetIdKey, &Sheet> = HashMap::new();
    for sheet in &old.sheets {
        let Some(id) = sheet.workbook_sheet_id else {
            continue;
        };
        if id_counts_old.get(&id) != Some(&1) {
            continue;
        }
        let key = SheetIdKey {
            id,
            kind: sheet.kind.clone(),
        };
        old_by_id.insert(key, sheet);
    }

    let mut new_by_id: HashMap<SheetIdKey, &Sheet> = HashMap::new();
    for sheet in &new.sheets {
        let Some(id) = sheet.workbook_sheet_id else {
            continue;
        };
        if id_counts_new.get(&id) != Some(&1) {
            continue;
        }
        let key = SheetIdKey {
            id,
            kind: sheet.kind.clone(),
        };
        new_by_id.insert(key, sheet);
    }

    struct SheetEntry<'a> {
        old: Option<&'a Sheet>,
        new: Option<&'a Sheet>,
        by_id: bool,
        sort_name_lower: String,
        kind: SheetKind,
        id: Option<u32>,
    }

    let mut entries: Vec<SheetEntry<'_>> = Vec::new();
    let mut consumed_old: HashSet<*const Sheet> = HashSet::new();
    let mut consumed_new: HashSet<*const Sheet> = HashSet::new();

    let mut id_keys: HashSet<SheetIdKey> = HashSet::new();
    id_keys.extend(old_by_id.keys().cloned());
    id_keys.extend(new_by_id.keys().cloned());

    for key in id_keys {
        let old_sheet = old_by_id.get(&key).copied();
        let new_sheet = new_by_id.get(&key).copied();
        if let Some(sheet) = old_sheet {
            consumed_old.insert(sheet as *const Sheet);
        }
        if let Some(sheet) = new_sheet {
            consumed_new.insert(sheet as *const Sheet);
        }

        let sort_name_lower = if let Some(new_sheet) = new_sheet {
            sheet_name_lower(new_sheet, pool)
        } else {
            sheet_name_lower(
                old_sheet.expect("id entry must have old or new sheet"),
                pool,
            )
        };
        let kind = new_sheet
            .map(|sheet| sheet.kind.clone())
            .unwrap_or_else(|| old_sheet.expect("entry has sheet").kind.clone());
        entries.push(SheetEntry {
            old: old_sheet,
            new: new_sheet,
            by_id: true,
            sort_name_lower,
            kind,
            id: Some(key.id),
        });
    }

    let mut name_keys: Vec<SheetKey> = old_sheets_by_name
        .keys()
        .chain(new_sheets_by_name.keys())
        .cloned()
        .collect();
    name_keys.sort_by(|a, b| match a.name_lower.cmp(&b.name_lower) {
        std::cmp::Ordering::Equal => sheet_kind_order(&a.kind).cmp(&sheet_kind_order(&b.kind)),
        other => other,
    });
    name_keys.dedup();

    for key in name_keys {
        let old_sheet = old_sheets_by_name
            .get(&key)
            .copied()
            .filter(|sheet| !consumed_old.contains(&(*sheet as *const Sheet)));
        let new_sheet = new_sheets_by_name
            .get(&key)
            .copied()
            .filter(|sheet| !consumed_new.contains(&(*sheet as *const Sheet)));
        if old_sheet.is_none() && new_sheet.is_none() {
            continue;
        }
        let sort_name_lower = if let Some(new_sheet) = new_sheet {
            sheet_name_lower(new_sheet, pool)
        } else {
            sheet_name_lower(
                old_sheet.expect("name entry must have old or new sheet"),
                pool,
            )
        };
        let kind = new_sheet
            .map(|sheet| sheet.kind.clone())
            .unwrap_or_else(|| old_sheet.expect("entry has sheet").kind.clone());
        entries.push(SheetEntry {
            old: old_sheet,
            new: new_sheet,
            by_id: false,
            sort_name_lower,
            kind,
            id: None,
        });
    }

    entries.sort_by(|a, b| match a.sort_name_lower.cmp(&b.sort_name_lower) {
        std::cmp::Ordering::Equal => {
            let kind_cmp = sheet_kind_order(&a.kind).cmp(&sheet_kind_order(&b.kind));
            if kind_cmp != std::cmp::Ordering::Equal {
                return kind_cmp;
            }
            match (a.by_id, b.by_id) {
                (true, true) => a.id.cmp(&b.id),
                (false, false) => std::cmp::Ordering::Equal,
                (true, false) => std::cmp::Ordering::Less,
                (false, true) => std::cmp::Ordering::Greater,
            }
        }
        other => other,
    });

    hardening.progress("parse", 1.0);
    #[cfg(feature = "perf-metrics")]
    {
        metrics.end_phase(Phase::Parse);
    }

    for entry in entries {
        if hardening.check_timeout(&mut ctx.warnings) {
            break;
        }

        match (entry.old, entry.new) {
            (None, Some(new_sheet)) => {
                emit_op(
                    sink,
                    &mut op_count,
                    DiffOp::SheetAdded {
                        sheet: new_sheet.name,
                    },
                )?;
            }
            (Some(old_sheet), None) => {
                emit_op(
                    sink,
                    &mut op_count,
                    DiffOp::SheetRemoved {
                        sheet: old_sheet.name,
                    },
                )?;
            }
            (Some(old_sheet), Some(new_sheet)) => {
                if entry.by_id {
                    let old_lower = sheet_name_lower(old_sheet, pool);
                    let new_lower = sheet_name_lower(new_sheet, pool);
                    if old_lower != new_lower {
                        emit_op(
                            sink,
                            &mut op_count,
                            DiffOp::SheetRenamed {
                                sheet: new_sheet.name,
                                from: old_sheet.name,
                                to: new_sheet.name,
                            },
                        )?;
                    }
                }

                let sheet_id: SheetId = if entry.by_id {
                    new_sheet.name
                } else {
                    old_sheet.name
                };
                try_diff_grids_internal(
                    sheet_id,
                    &old_sheet.grid,
                    &new_sheet.grid,
                    config,
                    pool,
                    sink,
                    &mut op_count,
                    &mut ctx,
                    &mut hardening,
                    #[cfg(feature = "perf-metrics")]
                    Some(&mut metrics),
                )?;
                if hardening.should_abort() {
                    break;
                }
            }
            (None, None) => {
                debug_assert!(false, "entry without old or new sheet");
                continue;
            }
        }
    }

    #[cfg(feature = "perf-metrics")]
    {
        metrics.end_phase(Phase::Total);
        apply_accounted_peak(&mut metrics, old, new, pool);
    }
    finish_guard.finish_and_disarm()?;
    let complete = ctx.warnings.is_empty();
    Ok(DiffSummary {
        complete,
        warnings: ctx.warnings,
        op_count,
        #[cfg(feature = "perf-metrics")]
        metrics: Some(metrics),
    })
}

#[cfg(feature = "perf-metrics")]
fn estimate_workbook_bytes(workbook: &Workbook) -> u64 {
    let sheet_bytes: u64 = workbook
        .sheets
        .iter()
        .map(|sheet| sheet.grid.estimated_bytes())
        .sum();
    let named_ranges = workbook.named_ranges.len() as u64 * size_of::<crate::workbook::NamedRange>() as u64;
    let charts = workbook.charts.len() as u64 * size_of::<crate::workbook::ChartObject>() as u64;
    sheet_bytes.saturating_add(named_ranges).saturating_add(charts)
}

#[cfg(feature = "perf-metrics")]
fn estimate_grid_storage_bytes(workbook: &Workbook) -> u64 {
    workbook
        .sheets
        .iter()
        .map(|sheet| sheet.grid.estimated_bytes())
        .sum()
}

#[cfg(feature = "perf-metrics")]
fn estimate_alignment_buffer_bytes(
    old: &Workbook,
    new: &Workbook,
    pool: &StringPool,
) -> u64 {
    let mut old_sheets: HashMap<SheetKey, &Sheet> = HashMap::new();
    for sheet in &old.sheets {
        old_sheets.insert(make_sheet_key(sheet, pool), sheet);
    }

    let mut new_sheets: HashMap<SheetKey, &Sheet> = HashMap::new();
    for sheet in &new.sheets {
        new_sheets.insert(make_sheet_key(sheet, pool), sheet);
    }

    let mut max_estimate = 0u64;
    for (key, old_sheet) in &old_sheets {
        if let Some(new_sheet) = new_sheets.get(key) {
            let estimate = crate::memory_estimate::estimate_advanced_sheet_diff_peak(
                &old_sheet.grid,
                &new_sheet.grid,
            );
            max_estimate = max_estimate.max(estimate);
        }
    }

    max_estimate
}

#[cfg(feature = "perf-metrics")]
fn estimate_op_buffer_bytes(sink: &VecSink) -> u64 {
    (sink.op_capacity() as u64).saturating_mul(size_of::<DiffOp>() as u64)
}

#[cfg(feature = "perf-metrics")]
fn apply_accounted_peak(
    metrics: &mut DiffMetrics,
    old: &Workbook,
    new: &Workbook,
    pool: &StringPool,
) {
    let grid_storage_bytes = estimate_grid_storage_bytes(old)
        .saturating_add(estimate_grid_storage_bytes(new));
    let string_pool_bytes = pool.estimated_bytes();
    let alignment_buffer_bytes = estimate_alignment_buffer_bytes(old, new, pool);

    metrics.grid_storage_bytes = grid_storage_bytes;
    metrics.string_pool_bytes = string_pool_bytes;
    metrics.alignment_buffer_bytes = alignment_buffer_bytes;

    let estimated = estimate_workbook_bytes(old)
        .saturating_add(estimate_workbook_bytes(new))
        .saturating_add(string_pool_bytes);
    if estimated > metrics.peak_memory_bytes {
        metrics.peak_memory_bytes = estimated;
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn sheet_kind_order_ranking_includes_macro_and_other() {
        assert!(
            sheet_kind_order(&SheetKind::Worksheet) < sheet_kind_order(&SheetKind::Chart),
            "Worksheet should rank before Chart"
        );
        assert!(
            sheet_kind_order(&SheetKind::Chart) < sheet_kind_order(&SheetKind::Macro),
            "Chart should rank before Macro"
        );
        assert!(
            sheet_kind_order(&SheetKind::Macro) < sheet_kind_order(&SheetKind::Other),
            "Macro should rank before Other"
        );
    }
}

```

---

### File: `core\src\error_codes.rs`

```rust
pub const PKG_NOT_ZIP: &str = "EXDIFF_PKG_001";
pub const PKG_NOT_OPC: &str = "EXDIFF_PKG_002";
pub const PKG_MISSING_PART: &str = "EXDIFF_PKG_003";
pub const PKG_INVALID_XML: &str = "EXDIFF_PKG_004";
pub const PKG_ZIP_PART_TOO_LARGE: &str = "EXDIFF_PKG_005";
pub const PKG_ZIP_TOO_MANY_ENTRIES: &str = "EXDIFF_PKG_006";
pub const PKG_ZIP_TOTAL_TOO_LARGE: &str = "EXDIFF_PKG_007";
pub const PKG_ZIP_READ: &str = "EXDIFF_PKG_008";
pub const PKG_UNSUPPORTED_FORMAT: &str = "EXDIFF_PKG_009";
pub const PKG_NO_DATAMASHUP_USE_TABULAR_MODEL: &str = "EXDIFF_PKG_010";

pub const GRID_XML_ERROR: &str = "EXDIFF_GRID_001";
pub const GRID_INVALID_ADDRESS: &str = "EXDIFF_GRID_002";
pub const GRID_SHARED_STRING_OOB: &str = "EXDIFF_GRID_003";

pub const CONTAINER_IO: &str = "EXDIFF_CTR_001";
pub const CONTAINER_ZIP: &str = "EXDIFF_CTR_002";
pub const CONTAINER_NOT_ZIP: &str = "EXDIFF_CTR_003";
pub const CONTAINER_NOT_OPC: &str = "EXDIFF_CTR_004";
pub const CONTAINER_TOO_MANY_ENTRIES: &str = "EXDIFF_CTR_005";
pub const CONTAINER_PART_TOO_LARGE: &str = "EXDIFF_CTR_006";
pub const CONTAINER_TOTAL_TOO_LARGE: &str = "EXDIFF_CTR_007";

pub const DM_BASE64_INVALID: &str = "EXDIFF_DM_001";
pub const DM_UNSUPPORTED_VERSION: &str = "EXDIFF_DM_002";
pub const DM_FRAMING_INVALID: &str = "EXDIFF_DM_003";
pub const DM_XML_ERROR: &str = "EXDIFF_DM_004";
pub const DM_INNER_PART_TOO_LARGE: &str = "EXDIFF_DM_005";
pub const DM_INVALID_HEADER: &str = "EXDIFF_DM_006";
pub const DM_INNER_TOO_MANY_ENTRIES: &str = "EXDIFF_DM_007";
pub const DM_INNER_TOTAL_TOO_LARGE: &str = "EXDIFF_DM_008";
pub const DM_PERMISSION_BINDINGS_UNVERIFIED: &str = "EXDIFF_DM_009";

pub const DIFF_LIMITS_EXCEEDED: &str = "EXDIFF_DIFF_001";
pub const DIFF_SINK_ERROR: &str = "EXDIFF_DIFF_002";
pub const DIFF_SHEET_NOT_FOUND: &str = "EXDIFF_DIFF_003";
pub const DIFF_INTERNAL_ERROR: &str = "EXDIFF_DIFF_004";


```

---

### File: `core\src\excel_open_xml.rs`

```rust
//! Excel Open XML file parsing.
//!
//! Provides functions for opening `.xlsx` files and parsing their contents into
//! the internal representation used for diffing.

use crate::container::{ContainerError, OpcContainer};
use crate::datamashup_framing::{
    DataMashupError, RawDataMashup, decode_datamashup_base64, parse_data_mashup,
    read_datamashup_text,
};
use crate::error_codes;
use crate::grid_parser::{
    GridParseError, parse_defined_names, parse_relationships, parse_relationships_all,
    parse_shared_strings, parse_sheet_xml, parse_workbook_xml, resolve_sheet_target,
};
#[cfg(feature = "vba")]
use crate::vba::VbaModuleType;
use crate::string_pool::StringId;
use crate::string_pool::StringPool;
use crate::vba::VbaModule;
use crate::workbook::{ChartInfo, ChartObject, Sheet, SheetKind, Workbook};
use std::collections::HashMap;
#[cfg(feature = "std-fs")]
use std::path::Path;
use thiserror::Error;
use xxhash_rust::xxh3::Xxh3;

#[derive(Debug, Error)]
#[non_exhaustive]
pub enum PackageError {
    #[error("{0}")]
    Container(#[from] ContainerError),
    #[error("{0}")]
    GridParse(#[from] GridParseError),
    #[error("{0}")]
    DataMashup(#[from] DataMashupError),
    #[error("[EXDIFF_PKG_003] workbook.xml missing or unreadable. Suggestion: re-save the file in Excel or verify it is a valid .xlsx.")]
    WorkbookXmlMissing,
    #[error("[EXDIFF_PKG_003] worksheet XML missing for sheet {sheet_name}. Suggestion: re-save the file in Excel or verify it is a valid .xlsx.")]
    WorksheetXmlMissing { sheet_name: String },
    #[error("[EXDIFF_PKG_009] serialization error: {0}. Suggestion: verify the workbook is a standard .xlsx saved by Excel.")]
    SerializationError(String),

    #[error("[EXDIFF_PKG_001] not a valid ZIP file: {message}. Suggestion: verify the input is a ZIP-based file and not corrupt.")]
    NotAZip { message: String },

    #[error("[EXDIFF_PKG_010] PBIX/PBIT does not contain DataMashup.\nSuggestion: if this is an enhanced-metadata PBIX, export as PBIT to expose DataModelSchema. If this is a legacy PBIX, ensure DataMashup is present.")]
    NoDataMashupUseTabularModel,

    #[error("[EXDIFF_PKG_003] missing required part: {path}. Suggestion: the workbook may be corrupt; re-save the file in Excel.")]
    MissingPart { path: String },

    #[error("[EXDIFF_PKG_004] invalid XML in '{part}' at line {line}, column {column}: {message}. Suggestion: re-save the file in Excel.")]
    InvalidXml {
        part: String,
        line: usize,
        column: usize,
        message: String,
    },

    #[error("[EXDIFF_PKG_009] unsupported format: {message}. Suggestion: verify the workbook is a standard .xlsx saved by Excel.")]
    UnsupportedFormat { message: String },

    #[error("[EXDIFF_PKG_008] failed to read part '{part}': {message}")]
    ReadPartFailed { part: String, message: String },

    #[error("{source} (in part '{part}')")]
    DataMashupPartError { part: String, source: DataMashupError },

    #[error("[{path}] {source}")]
    WithPath {
        path: String,
        #[source]
        source: Box<PackageError>,
    },
}

impl PackageError {
    pub fn code(&self) -> &'static str {
        match self {
            PackageError::Container(e) => e.code(),
            PackageError::GridParse(e) => e.code(),
            PackageError::DataMashup(e) => e.code(),
            PackageError::WorkbookXmlMissing => error_codes::PKG_MISSING_PART,
            PackageError::WorksheetXmlMissing { .. } => error_codes::PKG_MISSING_PART,
            PackageError::SerializationError(_) => error_codes::PKG_UNSUPPORTED_FORMAT,
            PackageError::NotAZip { .. } => error_codes::PKG_NOT_ZIP,
            PackageError::NoDataMashupUseTabularModel => {
                error_codes::PKG_NO_DATAMASHUP_USE_TABULAR_MODEL
            }
            PackageError::MissingPart { .. } => error_codes::PKG_MISSING_PART,
            PackageError::InvalidXml { .. } => error_codes::PKG_INVALID_XML,
            PackageError::UnsupportedFormat { .. } => error_codes::PKG_UNSUPPORTED_FORMAT,
            PackageError::ReadPartFailed { .. } => error_codes::PKG_ZIP_READ,
            PackageError::DataMashupPartError { source, .. } => source.code(),
            PackageError::WithPath { source, .. } => source.code(),
        }
    }

    pub fn with_path(self, path: impl Into<String>) -> Self {
        PackageError::WithPath {
            path: path.into(),
            source: Box::new(self),
        }
    }
}

#[deprecated(note = "use PackageError")]
pub type ExcelOpenError = PackageError;

pub(crate) fn open_workbook_from_container(
    container: &mut OpcContainer,
    pool: &mut StringPool,
) -> Result<Workbook, PackageError> {
    let shared_strings = match container.read_file_optional_checked("xl/sharedStrings.xml")? {
        Some(bytes) => parse_shared_strings(&bytes, pool).map_err(|e| {
            wrap_grid_parse_error(e, "xl/sharedStrings.xml")
        })?,
        None => Vec::new(),
    };

    let workbook_bytes = container
        .read_file_checked("xl/workbook.xml")
        .map_err(|e| match e {
            ContainerError::FileNotFound { .. } => {
                if container.file_names().any(|name| name == "xl/workbook.bin") {
                    PackageError::UnsupportedFormat {
                        message: "XLSB detected (xl/workbook.bin present); convert to .xlsx/.xlsm"
                            .to_string(),
                    }
                } else {
                    PackageError::MissingPart {
                        path: "xl/workbook.xml".to_string(),
                    }
                }
            }
            other => PackageError::ReadPartFailed {
                part: "xl/workbook.xml".to_string(),
                message: other.to_string(),
            },
        })?;

    let sheets = parse_workbook_xml(&workbook_bytes)
        .map_err(|e| wrap_grid_parse_error(e, "xl/workbook.xml"))?;

    let named_ranges = parse_defined_names(&workbook_bytes, &sheets, pool)
        .map_err(|e| wrap_grid_parse_error(e, "xl/workbook.xml"))?;

    let relationships = match container.read_file_optional_checked("xl/_rels/workbook.xml.rels")? {
        Some(bytes) => parse_relationships(&bytes)
            .map_err(|e| wrap_grid_parse_error(e, "xl/_rels/workbook.xml.rels"))?,
        None => HashMap::new(),
    };

    let mut charts: Vec<ChartObject> = Vec::new();
    let mut chart_parts: HashMap<String, ChartPartCacheEntry> = HashMap::new();

    let mut sheet_ir = Vec::with_capacity(sheets.len());
    for (idx, sheet) in sheets.iter().enumerate() {
        let target = resolve_sheet_target(sheet, &relationships, idx);
        let sheet_bytes = container.read_file_checked(&target).map_err(|e| match e {
            ContainerError::FileNotFound { .. } => PackageError::MissingPart {
                path: target.clone(),
            },
            other => PackageError::ReadPartFailed {
                part: target.clone(),
                message: other.to_string(),
            },
        })?;

        let sheet_name_id = pool.intern(&sheet.name);

        let grid = parse_sheet_xml(&sheet_bytes, &shared_strings, pool)
            .map_err(|e| wrap_grid_parse_error(e, &target))?;
        sheet_ir.push(Sheet {
            name: sheet_name_id,
            workbook_sheet_id: sheet.sheet_id,
            kind: SheetKind::Worksheet,
            grid,
        });

        let drawing_rids = match parse_worksheet_drawing_rids(&sheet_bytes) {
            Ok(rids) => rids,
            Err(_) => continue,
        };
        for drawing_rid in drawing_rids {
            let sheet_rels_path = rels_part_path(&target);
            let sheet_rels_bytes = match read_optional_part(container, &sheet_rels_path)? {
                Some(bytes) => bytes,
                None => continue,
            };
            let sheet_rels = parse_relationships_all(&sheet_rels_bytes)
                .map_err(|e| wrap_grid_parse_error(e, &sheet_rels_path))?;
            let Some(drawing_target) = sheet_rels.get(&drawing_rid) else {
                continue;
            };
            let drawing_part_path = resolve_target_against_part(&target, drawing_target);

            let drawing_bytes = match read_optional_part(container, &drawing_part_path)? {
                Some(bytes) => bytes,
                None => continue,
            };
            let drawing_chart_refs = parse_drawing_chart_refs(&drawing_bytes)
                .map_err(|e| wrap_grid_parse_error(e, &drawing_part_path))?;
            if drawing_chart_refs.is_empty() {
                continue;
            }

            let drawing_rels_path = rels_part_path(&drawing_part_path);
            let drawing_rels_bytes = match read_optional_part(container, &drawing_rels_path)? {
                Some(bytes) => bytes,
                None => continue,
            };
            let drawing_rels = parse_relationships_all(&drawing_rels_bytes)
                .map_err(|e| wrap_grid_parse_error(e, &drawing_rels_path))?;

            for chart_ref in drawing_chart_refs {
                let Some(chart_target) = drawing_rels.get(&chart_ref.rel_id) else {
                    continue;
                };
                let chart_part_path = resolve_target_against_part(&drawing_part_path, chart_target);
                let chart_bytes = match read_optional_part(container, &chart_part_path)? {
                    Some(bytes) => bytes,
                    None => continue,
                };

                let entry = match chart_parts.get(&chart_part_path) {
                    Some(entry) => entry.clone(),
                    None => {
                        let xml_hash = hash_xml_part(&chart_bytes);
                        let (chart_type, data_range) =
                            parse_chart_part_metadata(&chart_bytes, pool)
                                .map_err(|e| wrap_grid_parse_error(e, &chart_part_path))?;
                        let entry = ChartPartCacheEntry {
                            xml_hash,
                            chart_type,
                            data_range,
                        };
                        chart_parts.insert(chart_part_path.clone(), entry.clone());
                        entry
                    }
                };

                let name = match chart_ref.name {
                    Some(name) => name,
                    None => fallback_chart_name_from_path(&chart_part_path),
                };

                charts.push(ChartObject {
                    sheet: sheet_name_id,
                    workbook_sheet_id: sheet.sheet_id,
                    info: ChartInfo {
                        name: pool.intern(&name),
                        chart_type: entry.chart_type,
                        data_range: entry.data_range,
                    },
                    xml_hash: entry.xml_hash,
                });
            }
        }
    }

    Ok(Workbook {
        sheets: sheet_ir,
        named_ranges,
        charts,
    })
}

#[cfg(feature = "vba")]
pub(crate) fn open_vba_modules_from_container(
    container: &mut OpcContainer,
    pool: &mut StringPool,
) -> Result<Option<Vec<VbaModule>>, PackageError> {
    let bytes = match container.read_file_optional_checked("xl/vbaProject.bin")? {
        Some(bytes) => bytes,
        None => return Ok(None),
    };

    let project = ovba::open_project(bytes).map_err(|e| PackageError::UnsupportedFormat {
        message: format!("failed to parse xl/vbaProject.bin: {e}"),
    })?;

    let mut modules = Vec::with_capacity(project.modules.len());
    for module in &project.modules {
        let name = pool.intern(&module.name);
        let module_type = match module.module_type {
            ovba::ModuleType::Procedural => VbaModuleType::Standard,
            ovba::ModuleType::DocClsDesigner => VbaModuleType::Document,
        };

        let code = match project.module_source(&module.name) {
            Ok(code) => code,
            Err(_) => match project.module_source_raw(&module.name) {
                Ok(raw) => String::from_utf8_lossy(&raw).into_owned(),
                Err(_) => String::new(),
            },
        };

        modules.push(VbaModule {
            name,
            module_type,
            code,
        });
    }

    Ok(Some(modules))
}

#[cfg(not(feature = "vba"))]
pub(crate) fn open_vba_modules_from_container(
    _container: &mut OpcContainer,
    _pool: &mut StringPool,
) -> Result<Option<Vec<VbaModule>>, PackageError> {
    Ok(None)
}

#[derive(Debug, Clone)]
struct ChartPartCacheEntry {
    xml_hash: u128,
    chart_type: StringId,
    data_range: Option<StringId>,
}

#[derive(Debug, Clone)]
struct DrawingChartRef {
    rel_id: String,
    name: Option<String>,
}

fn read_optional_part(
    container: &mut OpcContainer,
    path: &str,
) -> Result<Option<Vec<u8>>, PackageError> {
    container
        .read_file_optional_checked(path)
        .map_err(|e| PackageError::ReadPartFailed {
            part: path.to_string(),
            message: e.to_string(),
        })
}

fn rels_part_path(part_path: &str) -> String {
    let part_path = part_path.trim_start_matches('/');
    let (dir, file) = match part_path.rsplit_once('/') {
        Some((dir, file)) => (dir, file),
        None => ("", part_path),
    };

    if dir.is_empty() {
        format!("_rels/{file}.rels")
    } else {
        format!("{dir}/_rels/{file}.rels")
    }
}

fn resolve_target_against_part(base_part: &str, target: &str) -> String {
    let target = target.trim();
    if let Some(rest) = target.strip_prefix('/') {
        return normalize_part_path(rest);
    }
    if target.starts_with("xl/") {
        return normalize_part_path(target);
    }

    let base_part = base_part.trim_start_matches('/');
    let base_dir = base_part.rsplit_once('/').map(|(dir, _)| dir).unwrap_or("");
    if base_dir.is_empty() {
        normalize_part_path(target)
    } else {
        normalize_part_path(&format!("{base_dir}/{target}"))
    }
}

fn normalize_part_path(path: &str) -> String {
    let mut stack: Vec<&str> = Vec::new();
    for seg in path.split('/') {
        match seg {
            "" | "." => {}
            ".." => {
                let _ = stack.pop();
            }
            other => stack.push(other),
        }
    }
    stack.join("/")
}

fn local_name(name: &[u8]) -> &[u8] {
    name.rsplit(|&b| b == b':').next().unwrap_or(name)
}

fn parse_worksheet_drawing_rids(xml: &[u8]) -> Result<Vec<String>, GridParseError> {
    let mut reader = quick_xml::Reader::from_reader(xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut rids = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(quick_xml::events::Event::Start(e))
            | Ok(quick_xml::events::Event::Empty(e))
                if local_name(e.name().as_ref()) == b"drawing" =>
            {
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| GridParseError::XmlError(e.to_string()))?;
                    if attr.key.as_ref() == b"r:id" {
                        let rid = attr
                            .unescape_value()
                            .map_err(|e| GridParseError::XmlError(e.to_string()))?
                            .into_owned();
                        rids.push(rid);
                    }
                }
            }
            Ok(quick_xml::events::Event::Eof) => break,
            Err(e) => return Err(GridParseError::XmlError(e.to_string())),
            _ => {}
        }
        buf.clear();
    }

    Ok(rids)
}

fn parse_drawing_chart_refs(drawing_xml: &[u8]) -> Result<Vec<DrawingChartRef>, GridParseError> {
    let mut reader = quick_xml::Reader::from_reader(drawing_xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut refs = Vec::new();
    let mut pending_name: Option<String> = None;

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(quick_xml::events::Event::Start(e))
            | Ok(quick_xml::events::Event::Empty(e)) => {
                let name = e.name();
                let tag = local_name(name.as_ref());
                if tag == b"cNvPr" {
                    for attr in e.attributes() {
                        let attr = attr.map_err(|e| GridParseError::XmlError(e.to_string()))?;
                        if attr.key.as_ref() == b"name" {
                            pending_name = Some(
                                attr.unescape_value()
                                    .map_err(|e| GridParseError::XmlError(e.to_string()))?
                                    .into_owned(),
                            );
                        }
                    }
                } else if tag == b"chart" {
                    let mut rel_id = None;
                    for attr in e.attributes() {
                        let attr = attr.map_err(|e| GridParseError::XmlError(e.to_string()))?;
                        if attr.key.as_ref() == b"r:id" {
                            rel_id = Some(
                                attr.unescape_value()
                                    .map_err(|e| GridParseError::XmlError(e.to_string()))?
                                    .into_owned(),
                            );
                        }
                    }
                    if let Some(rel_id) = rel_id {
                        refs.push(DrawingChartRef {
                            rel_id,
                            name: pending_name.take(),
                        });
                    }
                }
            }
            Ok(quick_xml::events::Event::Eof) => break,
            Err(e) => return Err(GridParseError::XmlError(e.to_string())),
            _ => {}
        }
        buf.clear();
    }

    Ok(refs)
}

fn parse_chart_part_metadata(
    chart_xml: &[u8],
    pool: &mut StringPool,
) -> Result<(StringId, Option<StringId>), GridParseError> {
    let mut reader = quick_xml::Reader::from_reader(chart_xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();

    let mut chart_type: Option<String> = None;
    let mut data_range: Option<String> = None;

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(quick_xml::events::Event::Start(e))
                if local_name(e.name().as_ref()) == b"f" && data_range.is_none() =>
            {
                let text = reader
                    .read_text(e.name())
                    .map_err(|e| GridParseError::XmlError(e.to_string()))?
                    .into_owned();
                let trimmed = text.trim();
                if !trimmed.is_empty() {
                    data_range = Some(trimmed.to_string());
                }
            }
            Ok(quick_xml::events::Event::Start(e)) | Ok(quick_xml::events::Event::Empty(e)) => {
                let name = e.name();
                let tag = local_name(name.as_ref());
                if chart_type.is_none() && tag.ends_with(b"Chart") {
                    chart_type = Some(String::from_utf8_lossy(tag).to_string());
                }
            }
            Ok(quick_xml::events::Event::Eof) => break,
            Err(e) => return Err(GridParseError::XmlError(e.to_string())),
            _ => {}
        }
        buf.clear();
    }

    let chart_type_id = pool.intern(chart_type.as_deref().unwrap_or("unknown"));
    let data_range_id = data_range.as_deref().map(|s| pool.intern(s));
    Ok((chart_type_id, data_range_id))
}

fn fallback_chart_name_from_path(chart_part_path: &str) -> String {
    let file = chart_part_path
        .rsplit('/')
        .next()
        .unwrap_or(chart_part_path);
    file.strip_suffix(".xml").unwrap_or(file).to_string()
}

fn hash_xml_part(xml: &[u8]) -> u128 {
    let mut hasher = Xxh3::new();
    hasher.update(xml);
    hasher.digest128()
}

fn wrap_grid_parse_error(err: GridParseError, part: &str) -> PackageError {
    match err {
        GridParseError::XmlErrorAt { line, column, message } => PackageError::InvalidXml {
            part: part.to_string(),
            line,
            column,
            message,
        },
        GridParseError::XmlError(msg) => PackageError::InvalidXml {
            part: part.to_string(),
            line: 0,
            column: 0,
            message: msg,
        },
        GridParseError::InvalidAddress(addr) => PackageError::UnsupportedFormat {
            message: format!("invalid cell address '{}' in {}", addr, part),
        },
        GridParseError::SharedStringOutOfBounds(idx) => PackageError::UnsupportedFormat {
            message: format!(
                "shared string index {} out of bounds while parsing {}",
                idx, part
            ),
        },
    }
}

#[cfg(feature = "std-fs")]
#[allow(deprecated)]
pub fn open_workbook(
    path: impl AsRef<Path>,
    pool: &mut StringPool,
) -> Result<Workbook, PackageError> {
    let path_str = path.as_ref().display().to_string();
    let mut container = OpcContainer::open_from_path(path.as_ref())
        .map_err(|e| PackageError::from(e).with_path(&path_str))?;
    open_workbook_from_container(&mut container, pool)
        .map_err(|e| e.with_path(&path_str))
}

#[cfg(feature = "std-fs")]
#[allow(deprecated)]
pub fn open_vba_modules(
    path: impl AsRef<Path>,
    pool: &mut StringPool,
) -> Result<Option<Vec<VbaModule>>, PackageError> {
    let path_str = path.as_ref().display().to_string();
    let mut container = OpcContainer::open_from_path(path.as_ref())
        .map_err(|e| PackageError::from(e).with_path(&path_str))?;
    open_vba_modules_from_container(&mut container, pool).map_err(|e| e.with_path(&path_str))
}

pub(crate) fn open_data_mashup_from_container(
    container: &mut OpcContainer,
) -> Result<Option<RawDataMashup>, PackageError> {
    let mut found: Option<RawDataMashup> = None;
    let names: Vec<String> = container.file_names().map(|s| s.to_string()).collect();

    for name in names {
        if !(name.starts_with("customXml/") && name.ends_with(".xml") && name.contains("item")) {
            continue;
        }

        let bytes = container
            .read_file_checked(&name)
            .map_err(|e| PackageError::ReadPartFailed {
                part: name.clone(),
                message: e.to_string(),
            })?;

        match read_datamashup_text(&bytes) {
            Ok(Some(text)) => {
                let decoded = decode_datamashup_base64(&text).map_err(|e| {
                    PackageError::DataMashupPartError {
                        part: name.clone(),
                        source: e,
                    }
                })?;
                let parsed = parse_data_mashup(&decoded).map_err(|e| {
                    PackageError::DataMashupPartError {
                        part: name.clone(),
                        source: e,
                    }
                })?;
                if found.is_some() {
                    return Err(PackageError::DataMashupPartError {
                        part: name,
                        source: DataMashupError::FramingInvalid,
                    });
                }
                found = Some(parsed);
            }
            Ok(None) => {}
            Err(e) => {
                return Err(PackageError::DataMashupPartError {
                    part: name,
                    source: e,
                });
            }
        }
    }

    Ok(found)
}

#[cfg(feature = "std-fs")]
#[allow(deprecated)]
pub fn open_data_mashup(path: impl AsRef<Path>) -> Result<Option<RawDataMashup>, PackageError> {
    let path_str = path.as_ref().display().to_string();
    let mut container = OpcContainer::open_from_path(path.as_ref())
        .map_err(|e| PackageError::from(e).with_path(&path_str))?;
    open_data_mashup_from_container(&mut container)
        .map_err(|e| e.with_path(&path_str))
}

```

---

### File: `core\src\formula.rs`

```rust
use std::fmt;

#[derive(Debug, Clone, PartialEq)]
pub enum FormulaExpr {
    Number(f64),
    Text(String),
    Boolean(bool),
    Error(ExcelError),

    CellRef(CellReference),
    RangeRef(RangeReference),

    NamedRef(String),

    FunctionCall {
        name: String,
        args: Vec<FormulaExpr>,
    },

    UnaryOp {
        op: UnaryOperator,
        operand: Box<FormulaExpr>,
    },

    BinaryOp {
        op: BinaryOperator,
        left: Box<FormulaExpr>,
        right: Box<FormulaExpr>,
    },

    Array(Vec<Vec<FormulaExpr>>),
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ExcelError {
    Null,
    Div0,
    Value,
    Ref,
    Name,
    Num,
    NA,
    Spill,
    Calc,
    GettingData,
    Unknown(String),
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum RowRef {
    Absolute(u32),
    Relative(u32),
    Offset(i32),
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum ColRef {
    Absolute(u32),
    Relative(u32),
    Offset(i32),
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct CellReference {
    pub sheet: Option<String>,
    pub row: RowRef,
    pub col: ColRef,
    pub spill: bool,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct RangeReference {
    pub sheet: Option<String>,
    pub start: CellReference,
    pub end: CellReference,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum UnaryOperator {
    Plus,
    Minus,
    Percent,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum BinaryOperator {
    Add,
    Sub,
    Mul,
    Div,
    Pow,
    Concat,
    Eq,
    Ne,
    Lt,
    Le,
    Gt,
    Ge,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct FormulaParseError {
    pub pos: usize,
    pub message: String,
}

impl fmt::Display for FormulaParseError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "formula parse error at {}: {}", self.pos, self.message)
    }
}

impl std::error::Error for FormulaParseError {}

pub fn parse_formula(formula: &str) -> Result<FormulaExpr, FormulaParseError> {
    let s = formula.trim();
    let s = s.strip_prefix('=').unwrap_or(s);
    let mut p = Parser::new(s);
    let expr = p.parse_expr(0)?;
    p.skip_ws();
    if !p.eof() {
        return Err(p.err("trailing characters"));
    }
    Ok(expr)
}

#[derive(Clone, Copy)]
pub(crate) enum ShiftMode {
    RelativeOnly,
    #[cfg(any(test, feature = "dev-apis"))]
    #[allow(dead_code)]
    All,
}

impl FormulaExpr {
    pub fn canonicalize(&self) -> FormulaExpr {
        let mut e = self.clone();
        e.canonicalize_in_place();
        e
    }

    pub(crate) fn shifted(&self, row_shift: i32, col_shift: i32, mode: ShiftMode) -> FormulaExpr {
        shift_expr(self, row_shift, col_shift, mode)
    }

    fn canonicalize_in_place(&mut self) {
        match self {
            FormulaExpr::FunctionCall { name, args } => {
                *name = name.to_ascii_uppercase();
                for a in args.iter_mut() {
                    a.canonicalize_in_place();
                }
                if is_commutative_function(name) {
                    args.sort_by_key(|a| canonical_sort_key(a));
                }
            }
            FormulaExpr::NamedRef(name) => {
                *name = name.to_ascii_uppercase();
            }
            FormulaExpr::CellRef(r) => {
                if let Some(s) = &mut r.sheet {
                    *s = s.to_ascii_uppercase();
                }
            }
            FormulaExpr::RangeRef(r) => {
                if let Some(s) = &mut r.sheet {
                    *s = s.to_ascii_uppercase();
                }
                if let Some(s) = &mut r.start.sheet {
                    *s = s.to_ascii_uppercase();
                }
                if let Some(s) = &mut r.end.sheet {
                    *s = s.to_ascii_uppercase();
                }
                let a = ref_sort_key(&r.start);
                let b = ref_sort_key(&r.end);
                if b < a {
                    std::mem::swap(&mut r.start, &mut r.end);
                }
            }
            FormulaExpr::UnaryOp { operand, .. } => {
                operand.canonicalize_in_place();
            }
            FormulaExpr::BinaryOp { op, left, right } => {
                left.canonicalize_in_place();
                right.canonicalize_in_place();
                if is_commutative_binary(*op) {
                    let lk = canonical_sort_key(left);
                    let rk = canonical_sort_key(right);
                    if rk < lk {
                        std::mem::swap(left, right);
                    }
                }
            }
            FormulaExpr::Array(rows) => {
                for row in rows.iter_mut() {
                    for cell in row.iter_mut() {
                        cell.canonicalize_in_place();
                    }
                }
            }
            _ => {}
        }
    }
}

fn is_commutative_function(name: &str) -> bool {
    matches!(name, "SUM" | "PRODUCT" | "MIN" | "MAX" | "AND" | "OR")
}

fn is_commutative_binary(op: BinaryOperator) -> bool {
    matches!(
        op,
        BinaryOperator::Add | BinaryOperator::Mul | BinaryOperator::Eq | BinaryOperator::Ne
    )
}

fn canonical_sort_key(e: &FormulaExpr) -> String {
    format!("{:?}", e.canonicalize())
}

fn ref_sort_key(r: &CellReference) -> (i64, i64, u8, u8) {
    (
        row_key(r.row),
        col_key(r.col),
        abs_key_row(r.row),
        abs_key_col(r.col),
    )
}

fn row_key(r: RowRef) -> i64 {
    match r {
        RowRef::Absolute(n) | RowRef::Relative(n) => n as i64,
        RowRef::Offset(n) => n as i64,
    }
}

fn col_key(c: ColRef) -> i64 {
    match c {
        ColRef::Absolute(n) | ColRef::Relative(n) => n as i64,
        ColRef::Offset(n) => n as i64,
    }
}

fn abs_key_row(r: RowRef) -> u8 {
    match r {
        RowRef::Absolute(_) => 0,
        RowRef::Relative(_) => 1,
        RowRef::Offset(_) => 2,
    }
}

fn abs_key_col(c: ColRef) -> u8 {
    match c {
        ColRef::Absolute(_) => 0,
        ColRef::Relative(_) => 1,
        ColRef::Offset(_) => 2,
    }
}

fn shift_expr(e: &FormulaExpr, row_shift: i32, col_shift: i32, mode: ShiftMode) -> FormulaExpr {
    match e {
        FormulaExpr::CellRef(r) => {
            FormulaExpr::CellRef(shift_cell_ref(r, row_shift, col_shift, mode))
        }
        FormulaExpr::RangeRef(r) => {
            let mut rr = r.clone();
            rr.start = shift_cell_ref(&rr.start, row_shift, col_shift, mode);
            rr.end = shift_cell_ref(&rr.end, row_shift, col_shift, mode);
            FormulaExpr::RangeRef(rr)
        }
        FormulaExpr::FunctionCall { name, args } => FormulaExpr::FunctionCall {
            name: name.clone(),
            args: args
                .iter()
                .map(|a| shift_expr(a, row_shift, col_shift, mode))
                .collect(),
        },
        FormulaExpr::UnaryOp { op, operand } => FormulaExpr::UnaryOp {
            op: *op,
            operand: Box::new(shift_expr(operand, row_shift, col_shift, mode)),
        },
        FormulaExpr::BinaryOp { op, left, right } => FormulaExpr::BinaryOp {
            op: *op,
            left: Box::new(shift_expr(left, row_shift, col_shift, mode)),
            right: Box::new(shift_expr(right, row_shift, col_shift, mode)),
        },
        FormulaExpr::Array(rows) => FormulaExpr::Array(
            rows.iter()
                .map(|row| {
                    row.iter()
                        .map(|x| shift_expr(x, row_shift, col_shift, mode))
                        .collect()
                })
                .collect(),
        ),
        _ => e.clone(),
    }
}

fn shift_cell_ref(
    r: &CellReference,
    row_shift: i32,
    col_shift: i32,
    mode: ShiftMode,
) -> CellReference {
    let mut out = r.clone();
    out.row = shift_row_ref(r.row, row_shift, mode);
    out.col = shift_col_ref(r.col, col_shift, mode);
    out
}

fn shift_row_ref(r: RowRef, delta: i32, mode: ShiftMode) -> RowRef {
    match r {
        RowRef::Relative(n) => RowRef::Relative(shift_u32(n, delta)),
        RowRef::Absolute(n) => match mode {
            ShiftMode::RelativeOnly => RowRef::Absolute(n),
            #[cfg(any(test, feature = "dev-apis"))]
            ShiftMode::All => RowRef::Absolute(shift_u32(n, delta)),
        },
        RowRef::Offset(n) => RowRef::Offset(n),
    }
}

fn shift_col_ref(c: ColRef, delta: i32, mode: ShiftMode) -> ColRef {
    match c {
        ColRef::Relative(n) => ColRef::Relative(shift_u32(n, delta)),
        ColRef::Absolute(n) => match mode {
            ShiftMode::RelativeOnly => ColRef::Absolute(n),
            #[cfg(any(test, feature = "dev-apis"))]
            ShiftMode::All => ColRef::Absolute(shift_u32(n, delta)),
        },
        ColRef::Offset(n) => ColRef::Offset(n),
    }
}

fn shift_u32(n: u32, delta: i32) -> u32 {
    let v = n as i64 + delta as i64;
    if v <= 0 {
        0
    } else if v >= u32::MAX as i64 {
        u32::MAX
    } else {
        v as u32
    }
}

pub fn formulas_equivalent_modulo_shift(
    a: &FormulaExpr,
    b: &FormulaExpr,
    row_shift: i32,
    col_shift: i32,
) -> bool {
    let a_shifted = a
        .shifted(row_shift, col_shift, ShiftMode::RelativeOnly)
        .canonicalize();
    let b_canon = b.canonicalize();
    a_shifted == b_canon
}

struct Parser<'a> {
    s: &'a [u8],
    pos: usize,
}

impl<'a> Parser<'a> {
    fn new(input: &'a str) -> Self {
        Self {
            s: input.as_bytes(),
            pos: 0,
        }
    }

    fn eof(&self) -> bool {
        self.pos >= self.s.len()
    }

    fn peek(&self) -> Option<u8> {
        self.s.get(self.pos).copied()
    }

    fn bump(&mut self) -> Option<u8> {
        let b = self.peek()?;
        self.pos += 1;
        Some(b)
    }

    fn skip_ws(&mut self) {
        while matches!(self.peek(), Some(b' ' | b'\t' | b'\r' | b'\n')) {
            self.pos += 1;
        }
    }

    fn err(&self, msg: &str) -> FormulaParseError {
        FormulaParseError {
            pos: self.pos,
            message: msg.to_string(),
        }
    }

    fn parse_expr(&mut self, min_bp: u8) -> Result<FormulaExpr, FormulaParseError> {
        self.skip_ws();

        let mut lhs = if matches!(self.peek(), Some(b'+' | b'-')) {
            let op_byte = self
                .bump()
                .ok_or_else(|| self.err("unexpected EOF after unary op"))?;
            let op = match op_byte {
                b'+' => UnaryOperator::Plus,
                b'-' => UnaryOperator::Minus,
                _ => return Err(self.err("invalid unary op")),
            };
            let rhs = self.parse_expr(90)?;
            FormulaExpr::UnaryOp {
                op,
                operand: Box::new(rhs),
            }
        } else {
            self.parse_primary()?
        };

        loop {
            self.skip_ws();

            while matches!(self.peek(), Some(b'%')) {
                self.bump();
                lhs = FormulaExpr::UnaryOp {
                    op: UnaryOperator::Percent,
                    operand: Box::new(lhs),
                };
                self.skip_ws();
            }

            let (op, l_bp, r_bp) = match self.peek_infix_op() {
                Some(x) => x,
                None => break,
            };

            if l_bp < min_bp {
                break;
            }

            self.consume_infix_op(op)?;
            let rhs = self.parse_expr(r_bp)?;
            lhs = FormulaExpr::BinaryOp {
                op,
                left: Box::new(lhs),
                right: Box::new(rhs),
            };
        }

        Ok(lhs)
    }

    fn peek_infix_op(&self) -> Option<(BinaryOperator, u8, u8)> {
        let b = self.peek()?;
        match b {
            b'+' => Some((BinaryOperator::Add, 50, 51)),
            b'-' => Some((BinaryOperator::Sub, 50, 51)),
            b'*' => Some((BinaryOperator::Mul, 60, 61)),
            b'/' => Some((BinaryOperator::Div, 60, 61)),
            b'^' => Some((BinaryOperator::Pow, 70, 70)),
            b'&' => Some((BinaryOperator::Concat, 40, 41)),
            b'=' => Some((BinaryOperator::Eq, 30, 31)),
            b'<' => {
                if self.s.get(self.pos + 1) == Some(&b'=') {
                    Some((BinaryOperator::Le, 30, 31))
                } else if self.s.get(self.pos + 1) == Some(&b'>') {
                    Some((BinaryOperator::Ne, 30, 31))
                } else {
                    Some((BinaryOperator::Lt, 30, 31))
                }
            }
            b'>' => {
                if self.s.get(self.pos + 1) == Some(&b'=') {
                    Some((BinaryOperator::Ge, 30, 31))
                } else {
                    Some((BinaryOperator::Gt, 30, 31))
                }
            }
            _ => None,
        }
    }

    fn consume_infix_op(&mut self, op: BinaryOperator) -> Result<(), FormulaParseError> {
        match op {
            BinaryOperator::Le | BinaryOperator::Ge => {
                self.bump();
                if self.bump() != Some(b'=') {
                    return Err(self.err("expected '='"));
                }
            }
            BinaryOperator::Ne => {
                self.bump();
                if self.bump() != Some(b'>') {
                    return Err(self.err("expected '>'"));
                }
            }
            BinaryOperator::Lt | BinaryOperator::Gt => {
                self.bump();
            }
            _ => {
                self.bump();
            }
        }
        Ok(())
    }

    fn parse_primary(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        self.skip_ws();
        match self.peek() {
            Some(b'(') => {
                self.bump();
                let e = self.parse_expr(0)?;
                self.skip_ws();
                if self.bump() != Some(b')') {
                    return Err(self.err("expected ')'"));
                }
                Ok(e)
            }
            Some(b'{') => self.parse_array(),
            Some(b'"') => self.parse_string(),
            Some(b'#') => self.parse_error(),
            Some(b'0'..=b'9') => {
                if self.looks_like_row_range() {
                    return self.parse_row_range(None);
                }
                self.parse_number()
            }
            Some(b'\'' | b'[') => self.parse_ref_or_name_with_optional_sheet(),
            Some(b'$' | b'A'..=b'Z' | b'a'..=b'z' | b'_') => {
                self.parse_ref_or_name_with_optional_sheet()
            }
            _ => Err(self.err("unexpected token")),
        }
    }

    fn parse_ref_or_name_with_optional_sheet(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        let start = self.pos;
        if let Some(sheet) = self.try_parse_sheet_prefix()? {
            return self.parse_ref_or_name(Some(sheet));
        }
        self.pos = start;
        self.parse_ref_or_name(None)
    }

    fn try_parse_sheet_prefix(&mut self) -> Result<Option<String>, FormulaParseError> {
        self.skip_ws();
        match self.peek() {
            Some(b'\'') => {
                let sheet = self.parse_quoted_sheet_name()?;
                if self.peek() == Some(b'!') {
                    self.bump();
                    return Ok(Some(sheet));
                }
                Ok(None)
            }
            Some(b'[') => {
                let start = self.pos;
                while let Some(b) = self.peek() {
                    if b == b'!' {
                        let sheet = std::str::from_utf8(&self.s[start..self.pos])
                            .map_err(|_| self.err("invalid utf-8 in sheet name"))?
                            .to_string();
                        self.bump();
                        return Ok(Some(sheet));
                    }
                    self.pos += 1;
                }
                self.pos = start;
                Ok(None)
            }
            _ => {
                let start = self.pos;
                let Some(b) = self.peek() else {
                    return Ok(None);
                };
                if !is_ident_start(b) {
                    return Ok(None);
                }
                let ident = self.parse_identifier()?;
                self.skip_ws();
                if self.peek() == Some(b'!') {
                    self.bump();
                    return Ok(Some(ident));
                }
                self.pos = start;
                Ok(None)
            }
        }
    }

    fn parse_ref_or_name(
        &mut self,
        sheet: Option<String>,
    ) -> Result<FormulaExpr, FormulaParseError> {
        self.skip_ws();

        if matches!(self.peek(), Some(b'0'..=b'9')) && self.looks_like_row_range() {
            return self.parse_row_range(sheet);
        }

        if matches!(self.peek(), Some(b'R' | b'r')) {
            let start = self.pos;
            if let Ok(r) = self.try_parse_r1c1(sheet.clone()) {
                return Ok(r);
            }
            self.pos = start;
        }

        if matches!(self.peek(), Some(b'$' | b'A'..=b'Z' | b'a'..=b'z')) {
            let start = self.pos;
                if let Some(r) = self.try_parse_a1_cell_ref(sheet.clone())? {
                    let start_ref = r.clone();
                    let mut expr = FormulaExpr::CellRef(r);
                    self.skip_ws();
                    if self.peek() == Some(b':') {
                        self.bump();
                        let rhs = self.try_parse_a1_cell_ref(None)?;
                        if let Some(end) = rhs {
                            expr = FormulaExpr::RangeRef(RangeReference {
                                sheet,
                                start: start_ref,
                                end,
                            });
                        }
                    }
                    return Ok(expr);
            }
            self.pos = start;
        }

        let ident = self.parse_identifier()?;
        self.skip_ws();

        if sheet.is_none()
            && ident.eq_ignore_ascii_case("TRUE")
            && !matches!(self.peek(), Some(b'(' | b'['))
        {
            return Ok(FormulaExpr::Boolean(true));
        }
        if sheet.is_none()
            && ident.eq_ignore_ascii_case("FALSE")
            && !matches!(self.peek(), Some(b'(' | b'['))
        {
            return Ok(FormulaExpr::Boolean(false));
        }

        if self.peek() == Some(b'[') {
            let structured = self.parse_bracket_blob()?;
            let full = match sheet {
                Some(s) => format!("{}!{}{}", s, ident, structured),
                None => format!("{}{}", ident, structured),
            };
            return Ok(FormulaExpr::NamedRef(full));
        }

        if self.peek() == Some(b'(') {
            self.bump();
            let mut args = Vec::new();
            loop {
                self.skip_ws();
                if self.peek() == Some(b')') {
                    self.bump();
                    break;
                }
                let arg = self.parse_expr(0)?;
                args.push(arg);
                self.skip_ws();
                match self.peek() {
                    Some(b',' | b';') => {
                        self.bump();
                    }
                    Some(b')') => {
                        self.bump();
                        break;
                    }
                    _ => return Err(self.err("expected ',' or ')'")),
                }
            }

            let name = match sheet {
                Some(s) => format!("{}!{}", s, ident),
                None => ident,
            };

            return Ok(FormulaExpr::FunctionCall { name, args });
        }

        let name = match sheet {
            Some(s) => format!("{}!{}", s, ident),
            None => ident,
        };

        Ok(FormulaExpr::NamedRef(name))
    }

    fn parse_array(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        self.bump();
        let mut rows = Vec::new();
        let mut current_row = Vec::new();
        loop {
            self.skip_ws();
            if self.peek() == Some(b'}') {
                self.bump();
                if !current_row.is_empty() {
                    rows.push(current_row);
                }
                break;
            }
            let elem = self.parse_expr(0)?;
            current_row.push(elem);
            self.skip_ws();
            match self.peek() {
                Some(b',') => {
                    self.bump();
                }
                Some(b';') => {
                    self.bump();
                    rows.push(current_row);
                    current_row = Vec::new();
                }
                Some(b'}') => {
                    self.bump();
                    rows.push(current_row);
                    break;
                }
                _ => return Err(self.err("expected ',', ';', or '}'")),
            }
        }
        Ok(FormulaExpr::Array(rows))
    }

    fn parse_string(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        if self.bump() != Some(b'"') {
            return Err(self.err("expected '\"'"));
        }

        let mut out = Vec::new();
        loop {
            match self.bump() {
                Some(b'"') => {
                    if self.peek() == Some(b'"') {
                        self.bump();
                        out.push(b'"');
                        continue;
                    }
                    break;
                }
                Some(b) => out.push(b),
                None => return Err(self.err("unterminated string literal")),
            }
        }

        let s = String::from_utf8(out).map_err(|_| self.err("invalid utf-8 in string"))?;
        Ok(FormulaExpr::Text(s))
    }

    fn parse_error(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        let start = self.pos;

        if self.bump() != Some(b'#') {
            return Err(self.err("expected '#'"));
        }

        while let Some(b) = self.peek() {
            if b.is_ascii_alphanumeric() || matches!(b, b'/' | b'!' | b'?' | b'_') {
                self.pos += 1;
            } else {
                break;
            }
        }

        let txt = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in error literal"))?
            .to_string();

        let upper = txt.to_ascii_uppercase();
        let err = match upper.as_str() {
            "#NULL!" => ExcelError::Null,
            "#DIV/0!" => ExcelError::Div0,
            "#VALUE!" => ExcelError::Value,
            "#REF!" => ExcelError::Ref,
            "#NAME?" => ExcelError::Name,
            "#NUM!" => ExcelError::Num,
            "#N/A" => ExcelError::NA,
            "#SPILL!" => ExcelError::Spill,
            "#CALC!" => ExcelError::Calc,
            "#GETTING_DATA" => ExcelError::GettingData,
            _ => ExcelError::Unknown(txt),
        };
        Ok(FormulaExpr::Error(err))
    }

    fn parse_bracket_blob(&mut self) -> Result<String, FormulaParseError> {
        self.skip_ws();
        if self.peek() != Some(b'[') {
            return Err(self.err("expected '['"));
        }

        let start = self.pos;
        let mut depth: i32 = 0;

        while let Some(b) = self.bump() {
            match b {
                b'[' => depth += 1,
                b']' => {
                    depth -= 1;
                    if depth == 0 {
                        break;
                    }
                }
                _ => {}
            }
        }

        if depth != 0 {
            return Err(self.err("unterminated structured reference"));
        }

        let txt = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in structured reference"))?
            .to_string();

        Ok(txt)
    }

    fn parse_number(&mut self) -> Result<FormulaExpr, FormulaParseError> {
        let start = self.pos;
        while matches!(self.peek(), Some(b'0'..=b'9')) {
            self.pos += 1;
        }
        if self.peek() == Some(b'.') {
            self.pos += 1;
            while matches!(self.peek(), Some(b'0'..=b'9')) {
                self.pos += 1;
            }
        }
        if matches!(self.peek(), Some(b'e' | b'E')) {
            self.pos += 1;
            if matches!(self.peek(), Some(b'+' | b'-')) {
                self.pos += 1;
            }
            while matches!(self.peek(), Some(b'0'..=b'9')) {
                self.pos += 1;
            }
        }
        let txt = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in number"))?;
        let n: f64 = txt.parse().map_err(|_| self.err("invalid number"))?;
        Ok(FormulaExpr::Number(n))
    }

    fn parse_identifier(&mut self) -> Result<String, FormulaParseError> {
        self.skip_ws();
        let start = self.pos;
        if let Some(b) = self.peek() {
            if !is_ident_start(b) {
                return Err(self.err("expected identifier"));
            }
        } else {
            return Err(self.err("expected identifier"));
        }
        self.pos += 1;
        while let Some(b) = self.peek() {
            if is_ident_continue(b) {
                self.pos += 1;
            } else {
                break;
            }
        }
        let ident = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in identifier"))?
            .to_string();
        Ok(ident)
    }

    fn parse_quoted_sheet_name(&mut self) -> Result<String, FormulaParseError> {
        debug_assert_eq!(self.peek(), Some(b'\''));
        self.bump();
        let start = self.pos;
        while let Some(b) = self.peek() {
            self.pos += 1;
            if b == b'\'' {
                if self.peek() == Some(b'\'') {
                    self.pos += 1;
                    continue;
                }
                let name = std::str::from_utf8(&self.s[start..self.pos - 1])
                    .map_err(|_| self.err("invalid utf-8 in sheet name"))?
                    .replace("''", "'");
                return Ok(name);
            }
        }
        Err(self.err("unterminated sheet name"))
    }

    fn try_parse_r1c1(&mut self, sheet: Option<String>) -> Result<FormulaExpr, FormulaParseError> {
        let start = self.pos;
        self.skip_ws();
        if !matches!(self.peek(), Some(b'R' | b'r')) {
            self.pos = start;
            return Err(self.err("expected R1C1 ref"));
        }
        self.bump();
        let row = if self.peek() == Some(b'[') {
            self.bump();
            let offset = self.parse_i32()?;
            if self.bump() != Some(b']') {
                return Err(self.err("expected ']'"));
            }
            RowRef::Offset(offset)
        } else if matches!(self.peek(), Some(b'0'..=b'9')) {
            RowRef::Absolute(self.parse_u32()?)
        } else {
            RowRef::Relative(0)
        };

        if !matches!(self.peek(), Some(b'C' | b'c')) {
            self.pos = start;
            return Err(self.err("expected 'C'"));
        }
        self.bump();

        let col = if self.peek() == Some(b'[') {
            self.bump();
            let offset = self.parse_i32()?;
            if self.bump() != Some(b']') {
                return Err(self.err("expected ']'"));
            }
            ColRef::Offset(offset)
        } else if matches!(self.peek(), Some(b'0'..=b'9')) {
            ColRef::Absolute(self.parse_u32()?)
        } else {
            ColRef::Relative(0)
        };

        Ok(FormulaExpr::CellRef(CellReference {
            sheet,
            row,
            col,
            spill: false,
        }))
    }

    fn parse_i32(&mut self) -> Result<i32, FormulaParseError> {
        self.skip_ws();
        let start = self.pos;
        if self.peek() == Some(b'-') || self.peek() == Some(b'+') {
            self.pos += 1;
        }
        while matches!(self.peek(), Some(b'0'..=b'9')) {
            self.pos += 1;
        }
        let txt = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in signed int"))?;
        txt.parse::<i32>()
            .map_err(|_| self.err("invalid signed int"))
    }

    fn parse_u32(&mut self) -> Result<u32, FormulaParseError> {
        self.skip_ws();
        let start = self.pos;
        while matches!(self.peek(), Some(b'0'..=b'9')) {
            self.pos += 1;
        }
        let txt = std::str::from_utf8(&self.s[start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in number"))?;
        txt.parse::<u32>().map_err(|_| self.err("invalid number"))
    }

    fn try_parse_a1_cell_ref(
        &mut self,
        sheet: Option<String>,
    ) -> Result<Option<CellReference>, FormulaParseError> {
        self.skip_ws();
        let start = self.pos;

        let col_abs = self.consume_if(b'$');
        if matches!(self.peek(), Some(b'R' | b'r')) {
            if self.looks_like_r1c1() {
                self.pos = start;
                return Ok(None);
            }
        }

        let col_start = self.pos;
        while matches!(self.peek(), Some(b'A'..=b'Z' | b'a'..=b'z')) {
            self.pos += 1;
        }
        if self.pos == col_start {
            self.pos = start;
            return Ok(None);
        }

        let col_txt = std::str::from_utf8(&self.s[col_start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in column"))?;
        if col_txt.len() > 3 {
            self.pos = start;
            return Ok(None);
        }

        let col_num = col_letters_to_u32(col_txt).ok_or_else(|| self.err("invalid column"))?;
        let row_abs = self.consume_if(b'$');

        let row_start = self.pos;
        while matches!(self.peek(), Some(b'0'..=b'9')) {
            self.pos += 1;
        }
        if self.pos == row_start {
            self.pos = start;
            return Ok(None);
        }

        let row_txt = std::str::from_utf8(&self.s[row_start..self.pos])
            .map_err(|_| self.err("invalid utf-8 in row"))?;
        let row_num = row_txt
            .parse::<u32>()
            .map_err(|_| self.err("invalid row"))?;

        let mut spill = false;
        if self.peek() == Some(b'#') {
            self.bump();
            spill = true;
        }

        Ok(Some(CellReference {
            sheet,
            row: if row_abs {
                RowRef::Absolute(row_num)
            } else {
                RowRef::Relative(row_num)
            },
            col: if col_abs {
                ColRef::Absolute(col_num)
            } else {
                ColRef::Relative(col_num)
            },
            spill,
        }))
    }

    fn consume_if(&mut self, b: u8) -> bool {
        if self.peek() == Some(b) {
            self.pos += 1;
            true
        } else {
            false
        }
    }

    fn looks_like_r1c1(&self) -> bool {
        let mut i = self.pos;
        if i >= self.s.len() {
            return false;
        }
        let b = self.s[i];
        if b != b'R' && b != b'r' {
            return false;
        }
        i += 1;
        while i < self.s.len() && matches!(self.s[i], b'0'..=b'9') {
            i += 1;
        }
        if i < self.s.len() && self.s[i] == b'[' {
            return true;
        }
        i < self.s.len() && (self.s[i] == b'C' || self.s[i] == b'c')
    }

    fn looks_like_row_range(&self) -> bool {
        let mut i = self.pos;
        while i < self.s.len() && matches!(self.s[i], b'0'..=b'9') {
            i += 1;
        }
        if i == self.pos {
            return false;
        }
        while i < self.s.len() && matches!(self.s[i], b' ' | b'\t') {
            i += 1;
        }
        if i >= self.s.len() || self.s[i] != b':' {
            return false;
        }
        i += 1;
        while i < self.s.len() && matches!(self.s[i], b' ' | b'\t') {
            i += 1;
        }
        let j = i;
        while i < self.s.len() && matches!(self.s[i], b'0'..=b'9') {
            i += 1;
        }
        i > j
    }

    fn parse_row_range(&mut self, sheet: Option<String>) -> Result<FormulaExpr, FormulaParseError> {
        let start_row = self.parse_u32()?;
        self.skip_ws();
        if self.bump() != Some(b':') {
            return Err(self.err("expected ':' in row range"));
        }
        self.skip_ws();
        let end_row = self.parse_u32()?;
        Ok(FormulaExpr::NamedRef(format!(
            "{}{}:{}",
            match sheet {
                Some(s) => format!("{}!", s),
                None => "".to_string(),
            },
            start_row,
            end_row
        )))
    }
}

fn is_ident_start(b: u8) -> bool {
    matches!(b, b'A'..=b'Z' | b'a'..=b'z' | b'_' | b'\\')
}

fn is_ident_continue(b: u8) -> bool {
    is_ident_start(b) || matches!(b, b'0'..=b'9' | b'.')
}

fn col_letters_to_u32(s: &str) -> Option<u32> {
    let mut col: u32 = 0;
    for b in s.bytes() {
        let c = b.to_ascii_uppercase();
        if !(b'A'..=b'Z').contains(&c) {
            return None;
        }
        col = col * 26 + (c - b'A' + 1) as u32;
    }
    Some(col)
}

```

---

### File: `core\src\formula_diff.rs`

```rust
use rustc_hash::FxHashMap;

use crate::config::DiffConfig;
use crate::diff::FormulaDiffResult;
use crate::formula::{FormulaExpr, formulas_equivalent_modulo_shift, parse_formula};
use crate::string_pool::{StringId, StringPool};

#[derive(Debug, Default)]
pub(crate) struct FormulaParseCache {
    parsed: FxHashMap<StringId, Option<FormulaExpr>>,
    canonical: FxHashMap<StringId, Option<FormulaExpr>>,
}

impl FormulaParseCache {
    fn parsed(&mut self, pool: &StringPool, id: StringId) -> Option<&FormulaExpr> {
        if !self.parsed.contains_key(&id) {
            let s = pool.resolve(id);
            self.parsed.insert(id, parse_formula(s).ok());
        }
        self.parsed.get(&id).and_then(|x| x.as_ref())
    }

    fn canonical(&mut self, pool: &StringPool, id: StringId) -> Option<FormulaExpr> {
        if !self.canonical.contains_key(&id) {
            let canon = self.parsed(pool, id).map(|e| e.canonicalize());
            self.canonical.insert(id, canon);
        }
        self.canonical.get(&id).and_then(|x| x.clone())
    }
}

pub(crate) fn diff_cell_formulas_ids(
    pool: &StringPool,
    cache: &mut FormulaParseCache,
    old: Option<StringId>,
    new: Option<StringId>,
    row_shift: i32,
    col_shift: i32,
    config: &DiffConfig,
) -> FormulaDiffResult {
    if old == new {
        return FormulaDiffResult::Unchanged;
    }

    match (old, new) {
        (None, Some(_)) => return FormulaDiffResult::Added,
        (Some(_), None) => return FormulaDiffResult::Removed,
        (None, None) => return FormulaDiffResult::Unchanged,
        _ => {}
    }

    if !config.semantic.enable_formula_semantic_diff {
        return FormulaDiffResult::TextChange;
    }

    let (Some(old_id), Some(new_id)) = (old, new) else {
        return FormulaDiffResult::TextChange;
    };

    let old_ast = match cache.parsed(pool, old_id) {
        Some(a) => a.clone(),
        None => return FormulaDiffResult::TextChange,
    };
    let new_ast = match cache.parsed(pool, new_id) {
        Some(a) => a.clone(),
        None => return FormulaDiffResult::TextChange,
    };

    let old_c = old_ast.canonicalize();
    let new_c = match cache.canonical(pool, new_id) {
        Some(c) => c,
        None => new_ast.canonicalize(),
    };

    if old_c == new_c {
        return FormulaDiffResult::FormattingOnly;
    }

    if row_shift != 0 || col_shift != 0 {
        if formulas_equivalent_modulo_shift(&old_ast, &new_ast, row_shift, col_shift) {
            return FormulaDiffResult::Filled;
        }
    }

    FormulaDiffResult::SemanticChange
}

```

---

### File: `core\src\grid_metadata.rs`

```rust
//! Grid row metadata and frequency classification.
//!
//! This module is the canonical home for row metadata shared across the grid view
//! layer and alignment algorithms.

use std::collections::HashMap;

use crate::config::DiffConfig;
use crate::workbook::RowSignature;

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum FrequencyClass {
    Unique,
    Rare,
    Common,
    LowInfo,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub struct RowMeta {
    pub row_idx: u32,
    pub signature: RowSignature,
    pub non_blank_count: u16,
    pub first_non_blank_col: u16,
    pub frequency_class: FrequencyClass,
    pub is_low_info: bool,
}

impl RowMeta {
    pub fn is_low_info(&self) -> bool {
        self.is_low_info || matches!(self.frequency_class, FrequencyClass::LowInfo)
    }
}

pub fn frequency_map(row_meta: &[RowMeta]) -> HashMap<RowSignature, u32> {
    let mut map = HashMap::new();
    for meta in row_meta {
        *map.entry(meta.signature).or_insert(0) += 1;
    }
    map
}

pub fn classify_row_frequencies(row_meta: &mut [RowMeta], config: &DiffConfig) {
    let freq_map = frequency_map(row_meta);
    for meta in row_meta.iter_mut() {
        if meta.frequency_class == FrequencyClass::LowInfo {
            continue;
        }

        let count = freq_map.get(&meta.signature).copied().unwrap_or(0);
        let mut class = match count {
            1 => FrequencyClass::Unique,
            0 => FrequencyClass::Common,
            c if c <= config.alignment.rare_threshold => FrequencyClass::Rare,
            _ => FrequencyClass::Common,
        };

        if (meta.non_blank_count as u32) < config.alignment.low_info_threshold || meta.is_low_info {
            class = FrequencyClass::LowInfo;
            meta.is_low_info = true;
        }

        meta.frequency_class = class;
    }
}

#[cfg(test)]
#[allow(clippy::field_reassign_with_default)]
mod tests {
    use super::*;

    fn make_meta(row_idx: u32, hash: u128, non_blank: u16) -> RowMeta {
        let signature = RowSignature { hash };
        RowMeta {
            row_idx,
            signature,
            non_blank_count: non_blank,
            first_non_blank_col: 0,
            frequency_class: FrequencyClass::Common,
            is_low_info: false,
        }
    }

    #[test]
    fn classifies_unique_and_rare_and_low_info() {
        let mut meta = vec![make_meta(0, 1, 3), make_meta(1, 1, 3), make_meta(2, 2, 1)];

        let mut config = DiffConfig::default();
        config.alignment.rare_threshold = 2;
        config.alignment.low_info_threshold = 2;

        classify_row_frequencies(&mut meta, &config);

        assert_eq!(meta[0].frequency_class, FrequencyClass::Rare);
        assert_eq!(meta[1].frequency_class, FrequencyClass::Rare);
        assert_eq!(meta[2].frequency_class, FrequencyClass::LowInfo);
    }
}

```

---

### File: `core\src\grid_parser.rs`

```rust
//! XML parsing for Excel worksheet grids.
//!
//! Handles parsing of worksheet XML, shared strings, workbook structure, and
//! relationship files to construct [`Grid`] representations of sheet data.

use crate::addressing::address_to_index;
use crate::error_codes;
use crate::string_pool::{StringId, StringPool};
use crate::workbook::{CellValue, Grid, NamedRange};
use quick_xml::Reader;
use quick_xml::events::{BytesStart, Event};
use std::collections::HashMap;
use thiserror::Error;

#[derive(Debug, Error)]
#[non_exhaustive]
pub enum GridParseError {
    #[error("[EXDIFF_GRID_001] XML parse error: {0}. Suggestion: re-save the file in Excel or verify it is valid XML.")]
    XmlError(String),
    #[error("[EXDIFF_GRID_001] XML parse error at line {line}, column {column}: {message}. Suggestion: re-save the file in Excel or verify it is valid XML.")]
    XmlErrorAt {
        line: usize,
        column: usize,
        message: String,
    },
    #[error("[EXDIFF_GRID_002] invalid cell address: {0}. Suggestion: the workbook may be corrupt.")]
    InvalidAddress(String),
    #[error("[EXDIFF_GRID_003] shared string index {0} out of bounds. Suggestion: the workbook may be corrupt.")]
    SharedStringOutOfBounds(usize),
}

impl GridParseError {
    pub fn code(&self) -> &'static str {
        match self {
            GridParseError::XmlError(_) => error_codes::GRID_XML_ERROR,
            GridParseError::XmlErrorAt { .. } => error_codes::GRID_XML_ERROR,
            GridParseError::InvalidAddress(_) => error_codes::GRID_INVALID_ADDRESS,
            GridParseError::SharedStringOutOfBounds(_) => error_codes::GRID_SHARED_STRING_OOB,
        }
    }
}

pub struct SheetDescriptor {
    pub name: String,
    pub rel_id: Option<String>,
    pub sheet_id: Option<u32>,
}

pub fn parse_shared_strings(
    xml: &[u8],
    pool: &mut StringPool,
) -> Result<Vec<StringId>, GridParseError> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(false);
    let mut buf = Vec::new();
    let mut strings = Vec::new();
    let mut current = String::new();
    let mut in_si = false;

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if e.name().as_ref() == b"si" => {
                current.clear();
                in_si = true;
            }
            Ok(Event::Start(e)) if e.name().as_ref() == b"t" && in_si => {
                let text = reader
                    .read_text(e.name())
                    .map_err(|e| xml_err(&reader, xml, e))?
                    .into_owned();
                current.push_str(&text);
            }
            Ok(Event::End(e)) if e.name().as_ref() == b"si" => {
                let id = pool.intern(&current);
                strings.push(id);
                in_si = false;
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    Ok(strings)
}

pub fn parse_workbook_xml(xml: &[u8]) -> Result<Vec<SheetDescriptor>, GridParseError> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut sheets = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) | Ok(Event::Empty(e)) if e.name().as_ref() == b"sheet" => {
                let mut name = None;
                let mut rel_id = None;
                let mut sheet_id = None;
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| xml_msg_err(&reader, xml, e.to_string()))?;
                    match attr.key.as_ref() {
                        b"name" => {
                            name = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        b"sheetId" => {
                            let parsed = attr
                                .unescape_value()
                                .map_err(|e| xml_err(&reader, xml, e))?;
                            sheet_id = parsed.into_owned().parse::<u32>().ok();
                        }
                        b"r:id" => {
                            rel_id = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        _ => {}
                    }
                }
                if let Some(name) = name {
                    sheets.push(SheetDescriptor {
                        name,
                        rel_id,
                        sheet_id,
                    });
                }
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    Ok(sheets)
}

pub fn parse_defined_names(
    workbook_xml: &[u8],
    sheets_in_order: &[SheetDescriptor],
    pool: &mut StringPool,
) -> Result<Vec<NamedRange>, GridParseError> {
    fn local_name(name: &[u8]) -> &[u8] {
        name.rsplit(|&b| b == b':').next().unwrap_or(name)
    }

    fn quote_sheet_name(sheet: &str) -> String {
        let needs_quotes = sheet
            .chars()
            .any(|c| matches!(c, ' ' | '\'' | '!' | ',' | ';' | '[' | ']' | '(' | ')'));
        if !needs_quotes {
            return sheet.to_string();
        }
        let escaped = sheet.replace('\'', "''");
        format!("'{escaped}'")
    }

    let mut reader = Reader::from_reader(workbook_xml);
    reader.config_mut().trim_text(false);
    let mut buf = Vec::new();
    let mut named_ranges = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if local_name(e.name().as_ref()) == b"definedName" => {
                let mut name = None;
                let mut local_sheet_id = None;
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| xml_msg_err(&reader, workbook_xml, e.to_string()))?;
                    match attr.key.as_ref() {
                        b"name" => {
                            name = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, workbook_xml, e))?
                                    .into_owned(),
                            );
                        }
                        b"localSheetId" => {
                            let value = attr
                                .unescape_value()
                                .map_err(|e| xml_err(&reader, workbook_xml, e))?
                                .into_owned();
                            local_sheet_id = value.parse::<usize>().ok();
                        }
                        _ => {}
                    }
                }

                let name = match name {
                    Some(name) => name,
                    None => {
                        return Err(xml_msg_err(
                            &reader,
                            workbook_xml,
                            "definedName missing required 'name' attribute",
                        ));
                    }
                };

                let refers_to = reader
                    .read_text(e.name())
                    .map_err(|e| xml_err(&reader, workbook_xml, e))?
                    .into_owned();
                let refers_to = refers_to.trim();

                let (qualified_name, scope) = match local_sheet_id {
                    None => (name.clone(), None),
                    Some(idx) => {
                        let sheet_name = sheets_in_order.get(idx).map(|s| s.name.as_str());
                        let sheet_name = match sheet_name {
                            Some(sheet_name) => sheet_name,
                            None => {
                                return Err(xml_msg_err(
                                    &reader,
                                    workbook_xml,
                                    format!(
                                        "definedName localSheetId {idx} out of bounds (sheets={})",
                                        sheets_in_order.len()
                                    ),
                                ));
                            }
                        };
                        let sheet_name_id = pool.intern(sheet_name);
                        let qualified = format!("{}!{}", quote_sheet_name(sheet_name), name);
                        (qualified, Some(sheet_name_id))
                    }
                };

                named_ranges.push(NamedRange {
                    name: pool.intern(&qualified_name),
                    refers_to: pool.intern(refers_to),
                    scope,
                });
            }
            Ok(Event::Empty(e)) if local_name(e.name().as_ref()) == b"definedName" => {
                let mut name = None;
                let mut local_sheet_id = None;
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| xml_msg_err(&reader, workbook_xml, e.to_string()))?;
                    match attr.key.as_ref() {
                        b"name" => {
                            name = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, workbook_xml, e))?
                                    .into_owned(),
                            );
                        }
                        b"localSheetId" => {
                            let value = attr
                                .unescape_value()
                                .map_err(|e| xml_err(&reader, workbook_xml, e))?
                                .into_owned();
                            local_sheet_id = value.parse::<usize>().ok();
                        }
                        _ => {}
                    }
                }

                let name = match name {
                    Some(name) => name,
                    None => {
                        return Err(xml_msg_err(
                            &reader,
                            workbook_xml,
                            "definedName missing required 'name' attribute",
                        ));
                    }
                };

                let (qualified_name, scope) = match local_sheet_id {
                    None => (name.clone(), None),
                    Some(idx) => {
                        let sheet_name = sheets_in_order.get(idx).map(|s| s.name.as_str());
                        let sheet_name = match sheet_name {
                            Some(sheet_name) => sheet_name,
                            None => {
                                return Err(xml_msg_err(
                                    &reader,
                                    workbook_xml,
                                    format!(
                                        "definedName localSheetId {idx} out of bounds (sheets={})",
                                        sheets_in_order.len()
                                    ),
                                ));
                            }
                        };
                        let sheet_name_id = pool.intern(sheet_name);
                        let qualified = format!("{}!{}", quote_sheet_name(sheet_name), name);
                        (qualified, Some(sheet_name_id))
                    }
                };

                named_ranges.push(NamedRange {
                    name: pool.intern(&qualified_name),
                    refers_to: pool.intern(""),
                    scope,
                });
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, workbook_xml, e)),
            _ => {}
        }
        buf.clear();
    }

    Ok(named_ranges)
}

pub fn parse_relationships(xml: &[u8]) -> Result<HashMap<String, String>, GridParseError> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut map = HashMap::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) | Ok(Event::Empty(e)) if e.name().as_ref() == b"Relationship" => {
                let mut id = None;
                let mut target = None;
                let mut rel_type = None;
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| xml_msg_err(&reader, xml, e.to_string()))?;
                    match attr.key.as_ref() {
                        b"Id" => {
                            id = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        b"Target" => {
                            target = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        b"Type" => {
                            rel_type = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        _ => {}
                    }
                }

                if let (Some(id), Some(target), Some(rel_type)) = (id, target, rel_type)
                    && rel_type.contains("worksheet")
                {
                    map.insert(id, target);
                }
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    Ok(map)
}

pub fn parse_relationships_all(xml: &[u8]) -> Result<HashMap<String, String>, GridParseError> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(true);
    let mut buf = Vec::new();
    let mut map = HashMap::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) | Ok(Event::Empty(e)) if e.name().as_ref() == b"Relationship" => {
                let mut id = None;
                let mut target = None;
                for attr in e.attributes() {
                    let attr = attr.map_err(|e| xml_msg_err(&reader, xml, e.to_string()))?;
                    match attr.key.as_ref() {
                        b"Id" => {
                            id = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        b"Target" => {
                            target = Some(
                                attr.unescape_value()
                                    .map_err(|e| xml_err(&reader, xml, e))?
                                    .into_owned(),
                            )
                        }
                        _ => {}
                    }
                }

                if let (Some(id), Some(target)) = (id, target) {
                    map.insert(id, target);
                }
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    Ok(map)
}

pub fn resolve_sheet_target(
    sheet: &SheetDescriptor,
    relationships: &HashMap<String, String>,
    index: usize,
) -> String {
    if let Some(rel_id) = &sheet.rel_id
        && let Some(target) = relationships.get(rel_id)
    {
        return normalize_target(target);
    }

    let guessed = sheet
        .sheet_id
        .map(|id| format!("xl/worksheets/sheet{id}.xml"))
        .unwrap_or_else(|| format!("xl/worksheets/sheet{}.xml", index + 1));
    normalize_target(&guessed)
}

fn normalize_target(target: &str) -> String {
    let trimmed = target.trim_start_matches('/');
    if trimmed.starts_with("xl/") {
        trimmed.to_string()
    } else {
        format!("xl/{trimmed}")
    }
}

pub fn parse_sheet_xml(
    xml: &[u8],
    shared_strings: &[StringId],
    pool: &mut StringPool,
) -> Result<Grid, GridParseError> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(false);
    let mut buf = Vec::new();

    let mut dimension_hint: Option<(u32, u32)> = None;
    let mut parsed_cells: Vec<ParsedCell> = Vec::new();
    let mut max_row: Option<u32> = None;
    let mut max_col: Option<u32> = None;

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) | Ok(Event::Empty(e)) if e.name().as_ref() == b"dimension" => {
                if let Some(r) = get_attr_value(&reader, xml, &e, b"ref")? {
                    dimension_hint = dimension_from_ref(&r);
                }
            }
            Ok(Event::Start(e)) if e.name().as_ref() == b"c" => {
                let cell = parse_cell(&mut reader, xml, e, shared_strings, pool)?;
                max_row = Some(max_row.map_or(cell.row, |r| r.max(cell.row)));
                max_col = Some(max_col.map_or(cell.col, |c| c.max(cell.col)));
                parsed_cells.push(cell);
            }
            Ok(Event::Eof) => break,
            Err(e) => return Err(xml_err(&reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    if parsed_cells.is_empty() {
        return Ok(Grid::new(0, 0));
    }

    let mut nrows = dimension_hint.map(|(r, _)| r).unwrap_or(0);
    let mut ncols = dimension_hint.map(|(_, c)| c).unwrap_or(0);

    if let Some(max_r) = max_row {
        nrows = nrows.max(max_r + 1);
    }
    if let Some(max_c) = max_col {
        ncols = ncols.max(max_c + 1);
    }

    build_grid(nrows, ncols, parsed_cells)
}

fn parse_cell(
    reader: &mut Reader<&[u8]>,
    xml: &[u8],
    start: BytesStart,
    shared_strings: &[StringId],
    pool: &mut StringPool,
) -> Result<ParsedCell, GridParseError> {
    let address_raw =
        get_attr_value(reader, xml, &start, b"r")?.ok_or_else(|| {
            xml_msg_err(reader, xml, "cell missing address")
        })?;
    let (row, col) = address_to_index(&address_raw)
        .ok_or_else(|| GridParseError::InvalidAddress(address_raw.clone()))?;

    let cell_type = get_attr_value(reader, xml, &start, b"t")?;

    let mut value_text: Option<String> = None;
    let mut formula_text: Option<String> = None;
    let mut inline_text: Option<String> = None;
    let mut buf = Vec::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if e.name().as_ref() == b"v" => {
                let text = reader
                    .read_text(e.name())
                    .map_err(|e| xml_err(reader, xml, e))?
                    .into_owned();
                value_text = Some(text);
            }
            Ok(Event::Start(e)) if e.name().as_ref() == b"f" => {
                let text = reader
                    .read_text(e.name())
                    .map_err(|e| xml_err(reader, xml, e))?
                    .into_owned();
                let unescaped = quick_xml::escape::unescape(&text)
                    .map_err(|e| xml_msg_err(reader, xml, e.to_string()))?
                    .into_owned();
                formula_text = Some(unescaped);
            }
            Ok(Event::Start(e)) if e.name().as_ref() == b"is" => {
                inline_text = Some(read_inline_string(reader, xml)?);
            }
            Ok(Event::End(e)) if e.name().as_ref() == start.name().as_ref() => break,
            Ok(Event::Eof) => {
                return Err(xml_msg_err(reader, xml, "unexpected EOF inside cell"));
            }
            Err(e) => return Err(xml_err(reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }

    let value = match inline_text {
        Some(text) => Some(CellValue::Text(pool.intern(&text))),
        None => convert_value(
            value_text.as_deref(),
            cell_type.as_deref(),
            shared_strings,
            pool,
            reader,
            xml,
        )?,
    };

    Ok(ParsedCell {
        row,
        col,
        value,
        formula: formula_text.map(|f| pool.intern(&f)),
    })
}

fn read_inline_string(reader: &mut Reader<&[u8]>, xml: &[u8]) -> Result<String, GridParseError> {
    let mut buf = Vec::new();
    let mut value = String::new();
    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if e.name().as_ref() == b"t" => {
                let text = reader
                    .read_text(e.name())
                    .map_err(|e| xml_err(reader, xml, e))?
                    .into_owned();
                value.push_str(&text);
            }
            Ok(Event::End(e)) if e.name().as_ref() == b"is" => break,
            Ok(Event::Eof) => {
                return Err(xml_msg_err(
                    reader,
                    xml,
                    "unexpected EOF inside inline string",
                ));
            }
            Err(e) => return Err(xml_err(reader, xml, e)),
            _ => {}
        }
        buf.clear();
    }
    Ok(value)
}

fn convert_value(
    value_text: Option<&str>,
    cell_type: Option<&str>,
    shared_strings: &[StringId],
    pool: &mut StringPool,
    reader: &Reader<&[u8]>,
    xml: &[u8],
) -> Result<Option<CellValue>, GridParseError> {
    let raw = match value_text {
        Some(t) => t,
        None => return Ok(None),
    };

    let trimmed = raw.trim();
    if raw.is_empty() || trimmed.is_empty() {
        return Ok(Some(CellValue::Text(pool.intern(""))));
    }

    match cell_type {
        Some("s") => {
            let idx = trimmed
                .parse::<usize>()
                .map_err(|e| xml_msg_err(reader, xml, e.to_string()))?;
            let text_id = *shared_strings
                .get(idx)
                .ok_or(GridParseError::SharedStringOutOfBounds(idx))?;
            Ok(Some(CellValue::Text(text_id)))
        }
        Some("b") => Ok(match trimmed {
            "1" => Some(CellValue::Bool(true)),
            "0" => Some(CellValue::Bool(false)),
            _ => None,
        }),
        Some("e") => Ok(Some(CellValue::Error(pool.intern(trimmed)))),
        Some("str") | Some("inlineStr") => Ok(Some(CellValue::Text(pool.intern(raw)))),
        _ => {
            if let Ok(n) = trimmed.parse::<f64>() {
                Ok(Some(CellValue::Number(n)))
            } else {
                Ok(Some(CellValue::Text(pool.intern(trimmed))))
            }
        }
    }
}

fn dimension_from_ref(reference: &str) -> Option<(u32, u32)> {
    let mut parts = reference.split(':');
    let start = parts.next()?;
    let end = parts.next().unwrap_or(start);
    let (start_row, start_col) = address_to_index(start)?;
    let (end_row, end_col) = address_to_index(end)?;
    let height = end_row.checked_sub(start_row)?.checked_add(1)?;
    let width = end_col.checked_sub(start_col)?.checked_add(1)?;
    Some((height, width))
}

fn build_grid(nrows: u32, ncols: u32, cells: Vec<ParsedCell>) -> Result<Grid, GridParseError> {
    let filled = cells.len();
    let mut grid = if Grid::should_use_dense(nrows, ncols, filled) {
        Grid::new_dense(nrows, ncols)
    } else {
        Grid::new(nrows, ncols)
    };

    for parsed in cells {
        grid.insert_cell(parsed.row, parsed.col, parsed.value, parsed.formula);
    }

    Ok(grid)
}

fn get_attr_value(
    reader: &Reader<&[u8]>,
    xml: &[u8],
    element: &BytesStart<'_>,
    key: &[u8],
) -> Result<Option<String>, GridParseError> {
    for attr in element.attributes() {
        let attr = attr.map_err(|e| xml_msg_err(reader, xml, e.to_string()))?;
        if attr.key.as_ref() == key {
            return Ok(Some(
                attr.unescape_value()
                    .map_err(|e| xml_err(reader, xml, e))?
                    .into_owned(),
            ));
        }
    }
    Ok(None)
}

fn xml_err(reader: &Reader<&[u8]>, xml: &[u8], err: quick_xml::Error) -> GridParseError {
    xml_error_with_position(err, xml, reader.buffer_position())
}

fn xml_msg_err(reader: &Reader<&[u8]>, xml: &[u8], message: impl Into<String>) -> GridParseError {
    let (line, column) = compute_line_col(xml, reader.buffer_position());
    GridParseError::XmlErrorAt {
        line,
        column,
        message: message.into(),
    }
}

fn xml_error_with_position(
    err: quick_xml::Error,
    xml: &[u8],
    byte_offset: usize,
) -> GridParseError {
    let (line, column) = compute_line_col(xml, byte_offset);
    GridParseError::XmlErrorAt {
        line,
        column,
        message: err.to_string(),
    }
}

fn compute_line_col(data: &[u8], offset: usize) -> (usize, usize) {
    let safe_offset = offset.min(data.len());
    let slice = &data[..safe_offset];
    let line = slice.iter().filter(|&&b| b == b'\n').count() + 1;
    let last_newline = slice.iter().rposition(|&b| b == b'\n');
    let column = match last_newline {
        Some(pos) => safe_offset - pos,
        None => safe_offset + 1,
    };
    (line, column)
}

struct ParsedCell {
    row: u32,
    col: u32,
    value: Option<CellValue>,
    formula: Option<StringId>,
}

#[cfg(test)]
mod tests {
    use super::{GridParseError, convert_value, parse_shared_strings, read_inline_string};
    use crate::string_pool::StringPool;
    use crate::workbook::CellValue;
    use quick_xml::Reader;

    #[test]
    fn parse_shared_strings_rich_text_flattens_runs() {
        let xml = br#"<?xml version="1.0"?>
<sst>
  <si>
    <r><t>Hello</t></r>
    <r><t xml:space="preserve"> World</t></r>
  </si>
</sst>"#;
        let mut pool = StringPool::new();
        let strings = parse_shared_strings(xml, &mut pool).expect("shared strings should parse");
        let first = strings.first().copied().unwrap();
        assert_eq!(pool.resolve(first), "Hello World");
    }

    #[test]
    fn read_inline_string_preserves_xml_space_preserve() {
        let xml = br#"<is><t xml:space="preserve"> hello</t></is>"#;
        let mut reader = Reader::from_reader(xml.as_ref());
        reader.config_mut().trim_text(false);
        let value = read_inline_string(&mut reader, xml).expect("inline string should parse");
        assert_eq!(value, " hello");

        let mut pool = StringPool::new();
        let dummy_xml: &[u8] = b"";
        let dummy_reader = Reader::from_reader(dummy_xml);
        let converted = convert_value(
            Some(value.as_str()),
            Some("inlineStr"),
            &[],
            &mut pool,
            &dummy_reader,
            dummy_xml,
        )
            .expect("inlineStr conversion should succeed");
        let text_id = converted
            .as_ref()
            .and_then(CellValue::as_text_id)
            .expect("text id");
        assert_eq!(pool.resolve(text_id), " hello");
    }

    #[test]
    fn convert_value_bool_0_1_and_other() {
        let dummy_xml: &[u8] = b"";
        let dummy_reader = Reader::from_reader(dummy_xml);

        let mut pool = StringPool::new();
        let false_val = convert_value(Some("0"), Some("b"), &[], &mut pool, &dummy_reader, dummy_xml)
            .expect("bool cell conversion should succeed");
        assert_eq!(false_val, Some(CellValue::Bool(false)));

        let mut pool = StringPool::new();
        let true_val = convert_value(Some("1"), Some("b"), &[], &mut pool, &dummy_reader, dummy_xml)
            .expect("bool cell conversion should succeed");
        assert_eq!(true_val, Some(CellValue::Bool(true)));

        let none_val = convert_value(Some("2"), Some("b"), &[], &mut pool, &dummy_reader, dummy_xml)
            .expect("unexpected bool tokens should still parse");
        assert!(none_val.is_none());
    }

    #[test]
    fn convert_value_shared_string_index_out_of_bounds_errors() {
        let dummy_xml: &[u8] = b"";
        let dummy_reader = Reader::from_reader(dummy_xml);

        let mut pool = StringPool::new();
        let only_id = pool.intern("only");
        let err = convert_value(Some("5"), Some("s"), &[only_id], &mut pool, &dummy_reader, dummy_xml)
            .expect_err("invalid shared string index should error");
        assert!(matches!(err, GridParseError::SharedStringOutOfBounds(5)));
    }

    #[test]
    fn convert_value_error_cell_as_text() {
        let dummy_xml: &[u8] = b"";
        let dummy_reader = Reader::from_reader(dummy_xml);

        let mut pool = StringPool::new();
        let value = convert_value(Some("#DIV/0!"), Some("e"), &[], &mut pool, &dummy_reader, dummy_xml)
            .expect("error cell should convert");
        let err_id = value
            .and_then(|v| {
                if let CellValue::Error(id) = v {
                    Some(id)
                } else {
                    None
                }
            })
            .expect("error id");
        assert_eq!(pool.resolve(err_id), "#DIV/0!");
    }
}

```

---

### File: `core\src\grid_view.rs`

```rust
use std::collections::HashMap;
use std::hash::Hash;
#[cfg(test)]
use std::cell::Cell as ThreadLocalCell;

use crate::config::DiffConfig;
use crate::grid_metadata::classify_row_frequencies;
use crate::hashing::{hash_cell_value, hash_row_content_128};
use crate::memory_estimate::estimate_gridview_bytes;
use crate::workbook::{Cell, CellValue, ColSignature, Grid, RowSignature};
use xxhash_rust::xxh3::Xxh3;

pub use crate::grid_metadata::{FrequencyClass, RowMeta};

pub type RowHash = RowSignature;
pub type ColHash = ColSignature;

#[cfg(test)]
thread_local! {
    static GRIDVIEW_BUILD_COUNT: ThreadLocalCell<usize> = ThreadLocalCell::new(0);
}

#[cfg(test)]
pub(crate) fn reset_gridview_build_count() {
    GRIDVIEW_BUILD_COUNT.with(|count| count.set(0));
}

#[cfg(test)]
pub(crate) fn gridview_build_count() -> usize {
    GRIDVIEW_BUILD_COUNT.with(|count| count.get())
}

#[derive(Debug)]
pub struct RowView<'a> {
    pub cells: Vec<(u32, &'a Cell)>, // sorted by column index
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub struct ColMeta {
    pub col_idx: u32,
    pub hash: ColHash,
    pub non_blank_count: u16,
    pub first_non_blank_row: u16,
}

#[derive(Debug)]
pub struct GridView<'a> {
    pub rows: Vec<RowView<'a>>,
    pub row_meta: Vec<RowMeta>,
    pub col_meta: Vec<ColMeta>,
    pub source: &'a Grid,
}

impl<'a> GridView<'a> {
    pub fn from_grid(grid: &'a Grid) -> GridView<'a> {
        let default_config = DiffConfig::default();
        Self::from_grid_with_config(grid, &default_config)
    }

    pub fn from_grid_with_config(grid: &'a Grid, config: &DiffConfig) -> GridView<'a> {
        #[cfg(test)]
        {
            GRIDVIEW_BUILD_COUNT.with(|count| count.set(count.get().saturating_add(1)));
        }
        let nrows = grid.nrows as usize;
        let ncols = grid.ncols as usize;

        let mut row_counts = vec![0u32; nrows];
        let mut row_first_non_blank: Vec<Option<u32>> = vec![None; nrows];

        let mut col_counts = vec![0u32; ncols];
        let mut col_first_non_blank: Vec<Option<u32>> = vec![None; ncols];
        let mut total_cells: usize = 0;

        for ((row, col), cell) in grid.iter_cells() {
            let r = row as usize;
            let c = col as usize;

            debug_assert!(
                r < nrows && c < ncols,
                "cell coordinates must lie within the grid bounds"
            );

            total_cells = total_cells.saturating_add(1);

            if is_non_blank(cell) {
                row_counts[r] = row_counts[r].saturating_add(1);
                col_counts[c] = col_counts[c].saturating_add(1);

                row_first_non_blank[r] =
                    Some(row_first_non_blank[r].map_or(col, |cur| cur.min(col)));
                col_first_non_blank[c] =
                    Some(col_first_non_blank[c].map_or(row, |cur| cur.min(row)));
            }
        }

        let mut rows: Vec<RowView<'a>> = (0..nrows)
            .map(|idx| RowView {
                cells: Vec::with_capacity(row_counts[idx] as usize),
            })
            .collect();

        for ((row, col), cell) in grid.iter_cells() {
            let r = row as usize;
            debug_assert!(
                r < nrows && (col as usize) < ncols,
                "cell coordinates must lie within the grid bounds"
            );
            rows[r].cells.push((col, cell));
        }

        sort_row_cells(&mut rows, total_cells);

        let mut row_meta =
            build_row_meta(&rows, &row_counts, &row_first_non_blank, config, total_cells);

        classify_row_frequencies(&mut row_meta, config);

        let allow_parallel_cols = !should_force_sequential_col_meta(grid, config);
        let col_meta = build_col_meta(
            &rows,
            &col_counts,
            &col_first_non_blank,
            total_cells,
            allow_parallel_cols,
        );

        GridView {
            rows,
            row_meta,
            col_meta,
            source: grid,
        }
    }

    pub fn is_low_info_dominated(&self) -> bool {
        if self.row_meta.is_empty() {
            return false;
        }
        let low = self.row_meta.iter().filter(|m| m.is_low_info()).count();
        low * 2 > self.row_meta.len()
    }

    pub fn is_blank_dominated(&self) -> bool {
        if self.col_meta.is_empty() {
            return false;
        }
        let blank = self
            .col_meta
            .iter()
            .filter(|m| m.non_blank_count == 0)
            .count();
        blank * 2 > self.col_meta.len()
    }
}

#[derive(Debug, Default)]
pub struct HashStats<H> {
    pub freq_a: HashMap<H, u32>,
    pub freq_b: HashMap<H, u32>,
    pub hash_to_positions_b: HashMap<H, Vec<u32>>,
}

impl HashStats<RowHash> {
    pub fn from_row_meta(rows_a: &[RowMeta], rows_b: &[RowMeta]) -> HashStats<RowHash> {
        let mut stats = HashStats::default();

        for meta in rows_a {
            *stats.freq_a.entry(meta.signature).or_insert(0) += 1;
        }

        for meta in rows_b {
            *stats.freq_b.entry(meta.signature).or_insert(0) += 1;
            stats
                .hash_to_positions_b
                .entry(meta.signature)
                .or_insert_with(Vec::new)
                .push(meta.row_idx);
        }

        stats
    }
}

impl HashStats<ColHash> {
    pub fn from_col_meta(cols_a: &[ColMeta], cols_b: &[ColMeta]) -> HashStats<ColHash> {
        let mut stats = HashStats::default();

        for meta in cols_a {
            *stats.freq_a.entry(meta.hash).or_insert(0) += 1;
        }

        for meta in cols_b {
            *stats.freq_b.entry(meta.hash).or_insert(0) += 1;
            stats
                .hash_to_positions_b
                .entry(meta.hash)
                .or_insert_with(Vec::new)
                .push(meta.col_idx);
        }

        stats
    }
}

impl<H> HashStats<H>
where
    H: Eq + Hash + Copy,
{
    pub fn is_unique(&self, hash: H) -> bool {
        self.freq_a.get(&hash).copied().unwrap_or(0) == 1
            && self.freq_b.get(&hash).copied().unwrap_or(0) == 1
    }

    pub fn is_unique_to_a(&self, hash: H) -> bool {
        self.freq_a.get(&hash).copied().unwrap_or(0) == 1
            && self.freq_b.get(&hash).copied().unwrap_or(0) == 0
    }

    pub fn is_unique_to_b(&self, hash: H) -> bool {
        self.freq_a.get(&hash).copied().unwrap_or(0) == 0
            && self.freq_b.get(&hash).copied().unwrap_or(0) == 1
    }

    pub fn max_frequency(&self) -> u32 {
        self.freq_a
            .values()
            .chain(self.freq_b.values())
            .copied()
            .max()
            .unwrap_or(0)
    }

    pub fn has_heavy_repetition(&self, max_repeat: u32) -> bool {
        self.max_frequency() > max_repeat
    }

    pub fn is_rare(&self, hash: H, threshold: u32) -> bool {
        let freq_a = self.freq_a.get(&hash).copied().unwrap_or(0);
        let freq_b = self.freq_b.get(&hash).copied().unwrap_or(0);

        if freq_a == 0 || freq_b == 0 || self.is_unique(hash) {
            return false;
        }

        freq_a <= threshold && freq_b <= threshold
    }

    pub fn is_common(&self, hash: H, threshold: u32) -> bool {
        let freq_a = self.freq_a.get(&hash).copied().unwrap_or(0);
        let freq_b = self.freq_b.get(&hash).copied().unwrap_or(0);

        if freq_a == 0 && freq_b == 0 {
            return false;
        }

        freq_a > threshold || freq_b > threshold
    }

    pub fn appears_in_both(&self, hash: H) -> bool {
        self.freq_a.get(&hash).copied().unwrap_or(0) > 0
            && self.freq_b.get(&hash).copied().unwrap_or(0) > 0
    }
}

#[cfg(feature = "parallel")]
const PAR_MIN_ROWS: usize = 2048;
#[cfg(feature = "parallel")]
const PAR_MIN_CELLS: usize = 200_000;
#[cfg(feature = "parallel")]
const PAR_MIN_COLS: usize = 8;

const BYTES_PER_MB: u64 = 1024 * 1024;
const COL_META_BUDGET_RATIO_NUMERATOR: u64 = 4;
const COL_META_BUDGET_RATIO_DENOMINATOR: u64 = 5;

#[cfg(feature = "parallel")]
fn should_parallelize_rows(row_len: usize, total_cells: usize) -> bool {
    row_len >= PAR_MIN_ROWS && total_cells >= PAR_MIN_CELLS
}

#[cfg(feature = "parallel")]
fn should_parallelize_cols(col_len: usize, total_cells: usize) -> bool {
    col_len >= PAR_MIN_COLS && total_cells >= PAR_MIN_CELLS
}

fn should_force_sequential_col_meta(grid: &Grid, config: &DiffConfig) -> bool {
    let Some(max_mb) = config.hardening.max_memory_mb else {
        return false;
    };
    let max_bytes = (max_mb as u64).saturating_mul(BYTES_PER_MB);
    let estimate = estimate_gridview_bytes(grid);
    estimate.saturating_mul(COL_META_BUDGET_RATIO_DENOMINATOR)
        >= max_bytes.saturating_mul(COL_META_BUDGET_RATIO_NUMERATOR)
}

#[cfg(feature = "parallel")]
fn sort_row_cells(rows: &mut [RowView<'_>], total_cells: usize) {
    if should_parallelize_rows(rows.len(), total_cells) {
        use rayon::prelude::*;
        rows.par_iter_mut()
            .for_each(|r| r.cells.sort_unstable_by_key(|(c, _)| *c));
        return;
    }

    for r in rows.iter_mut() {
        r.cells.sort_unstable_by_key(|(c, _)| *c);
    }
}

#[cfg(not(feature = "parallel"))]
fn sort_row_cells(rows: &mut [RowView<'_>], _total_cells: usize) {
    for r in rows.iter_mut() {
        r.cells.sort_unstable_by_key(|(c, _)| *c);
    }
}

#[cfg(feature = "parallel")]
fn build_row_meta<'a>(
    rows: &[RowView<'a>],
    row_counts: &[u32],
    row_first_non_blank: &[Option<u32>],
    _config: &DiffConfig,
    total_cells: usize,
) -> Vec<RowMeta> {
    if should_parallelize_rows(rows.len(), total_cells) {
        use rayon::prelude::*;
        return rows
            .par_iter()
            .enumerate()
            .map(|(idx, row_view)| {
                row_meta_for_row(idx, row_view, row_counts, row_first_non_blank)
            })
            .collect();
    }

    rows.iter()
        .enumerate()
        .map(|(idx, row_view)| row_meta_for_row(idx, row_view, row_counts, row_first_non_blank))
        .collect()
}

#[cfg(not(feature = "parallel"))]
fn build_row_meta<'a>(
    rows: &[RowView<'a>],
    row_counts: &[u32],
    row_first_non_blank: &[Option<u32>],
    _config: &DiffConfig,
    _total_cells: usize,
) -> Vec<RowMeta> {
    rows.iter()
        .enumerate()
        .map(|(idx, row_view)| row_meta_for_row(idx, row_view, row_counts, row_first_non_blank))
        .collect()
}

fn build_col_meta_sequential<'a>(
    rows: &[RowView<'a>],
    col_counts: &[u32],
    col_first_non_blank: &[Option<u32>],
) -> Vec<ColMeta> {
    let ncols = col_counts.len();
    let mut col_hashers: Vec<Xxh3> = (0..ncols).map(|_| Xxh3::new()).collect();

    for row_view in rows {
        for (col, cell) in &row_view.cells {
            let idx = *col as usize;
            if idx >= ncols {
                continue;
            }
            let hasher = &mut col_hashers[idx];
            hash_cell_value(&cell.value, hasher);
            cell.formula.hash(hasher);
        }
    }

    (0..ncols)
        .map(|col_idx| ColMeta {
            col_idx: col_idx as u32,
            hash: ColSignature {
                hash: col_hashers[col_idx].digest128(),
            },
            non_blank_count: to_u16(col_counts[col_idx]),
            first_non_blank_row: col_first_non_blank[col_idx].map(to_u16).unwrap_or(0),
        })
        .collect()
}

#[cfg(feature = "parallel")]
fn build_col_meta<'a>(
    rows: &[RowView<'a>],
    col_counts: &[u32],
    col_first_non_blank: &[Option<u32>],
    total_cells: usize,
    allow_parallel: bool,
) -> Vec<ColMeta> {
    let ncols = col_counts.len();
    if !allow_parallel || !should_parallelize_cols(ncols, total_cells) {
        return build_col_meta_sequential(rows, col_counts, col_first_non_blank);
    }

    let mut col_cells: Vec<Vec<&'a Cell>> = (0..ncols)
        .map(|i| Vec::with_capacity(col_counts[i] as usize))
        .collect();

    for row_view in rows {
        for (col, cell) in &row_view.cells {
            let idx = *col as usize;
            if idx < ncols {
                col_cells[idx].push(*cell);
            }
        }
    }

    use rayon::prelude::*;
    let mut out: Vec<ColMeta> = col_cells
        .par_iter()
        .enumerate()
        .map(|(col_idx, cells)| {
            let mut hasher = Xxh3::new();
            for &cell in cells {
                hash_cell_value(&cell.value, &mut hasher);
                cell.formula.hash(&mut hasher);
            }
            ColMeta {
                col_idx: col_idx as u32,
                hash: ColSignature {
                    hash: hasher.digest128(),
                },
                non_blank_count: to_u16(col_counts[col_idx]),
                first_non_blank_row: col_first_non_blank[col_idx].map(to_u16).unwrap_or(0),
            }
        })
        .collect();

    out.sort_unstable_by_key(|m| m.col_idx);
    out
}

#[cfg(not(feature = "parallel"))]
fn build_col_meta<'a>(
    rows: &[RowView<'a>],
    col_counts: &[u32],
    col_first_non_blank: &[Option<u32>],
    _total_cells: usize,
    _allow_parallel: bool,
) -> Vec<ColMeta> {
    build_col_meta_sequential(rows, col_counts, col_first_non_blank)
}

fn row_meta_for_row<'a>(
    idx: usize,
    row_view: &RowView<'a>,
    row_counts: &[u32],
    row_first_non_blank: &[Option<u32>],
) -> RowMeta {
    let count = row_counts.get(idx).copied().unwrap_or(0);
    let non_blank_count = to_u16(count);
    let first_non_blank_col = row_first_non_blank
        .get(idx)
        .and_then(|c| c.map(to_u16))
        .unwrap_or(0);
    let is_low_info = compute_is_low_info(non_blank_count, row_view);

    let signature = RowSignature {
        hash: hash_row_content_128(&row_view.cells),
    };

    let frequency_class = if is_low_info {
        FrequencyClass::LowInfo
    } else {
        FrequencyClass::Common
    };

    RowMeta {
        row_idx: idx as u32,
        signature,
        non_blank_count,
        first_non_blank_col,
        frequency_class,
        is_low_info,
    }
}

fn is_non_blank(cell: &Cell) -> bool {
    cell.value.is_some() || cell.formula.is_some()
}

fn compute_is_low_info(non_blank_count: u16, row_view: &RowView<'_>) -> bool {
    if non_blank_count == 0 {
        return true;
    }

    if non_blank_count > 1 {
        return false;
    }

    let cell = row_view
        .cells
        .iter()
        .find_map(|(_, cell)| is_non_blank(cell).then_some(*cell));

    match cell {
        None => true,
        Some(cell) => match (&cell.value, &cell.formula) {
            (_, Some(_)) => false,
            (Some(CellValue::Text(id)), None) => id.0 == 0,
            (Some(CellValue::Number(_)), _) => false,
            (Some(CellValue::Bool(_)), _) => false,
            (Some(CellValue::Error(_)), _) => false,
            (Some(CellValue::Blank), _) => true,
            (None, None) => true,
        },
    }
}

fn to_u16(value: u32) -> u16 {
    u16::try_from(value).unwrap_or(u16::MAX)
}

```

---

### File: `core\src\hashing.rs`

```rust
//! Hash utilities for row/column signature computation.
//!
//! Provides consistent hashing functions used for computing structural
//! signatures that enable efficient alignment during diffing.
//!
//! # Position Independence
//!
//! Row signatures are computed by hashing cell content in column-sorted order
//! *without* including column indices. This ensures that inserting or deleting
//! columns does not invalidate row alignment.
//!
//! Column signatures similarly hash content in row-sorted order without row indices.
//!
//! # Collision Probability
//!
//! Using 128-bit xxHash3 signatures, the collision probability is ~10^-38 per pair.
//! At 50K rows, the birthday-bound collision probability is ~10^-29, which is
//! negligible for practical purposes.

use std::hash::{Hash, Hasher};
use xxhash_rust::xxh3::Xxh3;
#[cfg(any(test, feature = "dev-apis"))]
use xxhash_rust::xxh64::Xxh64;

use crate::workbook::{CellContent, CellValue};
#[cfg(any(test, feature = "dev-apis"))]
use crate::workbook::{ColSignature, RowSignature};

pub(crate) const XXH64_SEED: u64 = 0;
#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
const HASH_MIX_CONSTANT: u64 = 0x9e3779b97f4a7c15;
const CANONICAL_NAN_BITS: u64 = 0x7FF8_0000_0000_0000;

pub(crate) fn normalize_float_for_hash(n: f64) -> u64 {
    if n.is_nan() {
        return CANONICAL_NAN_BITS;
    }
    if n == 0.0 {
        return 0u64;
    }
    let magnitude = n.abs().log10().floor() as i32;
    let scale = 10f64.powi(14 - magnitude);
    let normalized = (n * scale).round() / scale;
    normalized.to_bits()
}

pub(crate) fn hash_cell_value<H: Hasher>(value: &Option<CellValue>, state: &mut H) {
    match value {
        None => {
            3u8.hash(state);
        }
        Some(CellValue::Blank) => {
            4u8.hash(state);
        }
        Some(CellValue::Number(n)) => {
            0u8.hash(state);
            normalize_float_for_hash(*n).hash(state);
        }
        Some(CellValue::Text(s)) => {
            1u8.hash(state);
            s.hash(state);
        }
        Some(CellValue::Bool(b)) => {
            2u8.hash(state);
            b.hash(state);
        }
        Some(CellValue::Error(id)) => {
            5u8.hash(state);
            id.hash(state);
        }
    }
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn hash_cell_content(cell: &CellContent) -> u64 {
    let mut hasher = Xxh64::new(XXH64_SEED);
    hash_cell_value(&cell.value, &mut hasher);
    cell.formula.hash(&mut hasher);
    hasher.finish()
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn hash_cell_content_128(cell: &CellContent) -> u128 {
    let mut hasher = Xxh3::new();
    hash_cell_value(&cell.value, &mut hasher);
    cell.formula.hash(&mut hasher);
    hasher.digest128()
}

pub(crate) fn hash_row_content_128(cells: &[(u32, &CellContent)]) -> u128 {
    let mut hasher = Xxh3::new();
    for (_, cell) in cells.iter() {
        hash_cell_value(&cell.value, &mut hasher);
        cell.formula.hash(&mut hasher);
    }
    hasher.digest128()
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn hash_col_content_128(cells: &[&CellContent]) -> u128 {
    let mut hasher = Xxh3::new();
    for cell in cells.iter() {
        hash_cell_value(&cell.value, &mut hasher);
        cell.formula.hash(&mut hasher);
    }
    hasher.digest128()
}

pub(crate) fn hash_col_content_unordered_128(cells: &[&CellContent]) -> u128 {
    if cells.is_empty() {
        return Xxh3::new().digest128();
    }

    let mut cell_hashes: Vec<u128> = cells
        .iter()
        .map(|cell| {
            let mut h = Xxh3::new();
            hash_cell_value(&cell.value, &mut h);
            cell.formula.hash(&mut h);
            h.digest128()
        })
        .collect();

    cell_hashes.sort_unstable();

    let mut combined = Xxh3::new();
    for h in cell_hashes {
        combined.update(&h.to_le_bytes());
    }
    combined.digest128()
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn mix_hash(hash: u64) -> u64 {
    hash.rotate_left(13) ^ HASH_MIX_CONSTANT
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn mix_hash_128(hash: u128) -> u128 {
    hash.rotate_left(47) ^ (HASH_MIX_CONSTANT as u128)
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn combine_hashes(current: u64, contribution: u64) -> u64 {
    current.wrapping_add(mix_hash(contribution))
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn combine_hashes_128(current: u128, contribution: u128) -> u128 {
    current.wrapping_add(mix_hash_128(contribution))
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn compute_row_signature<'a>(
    cells: impl Iterator<Item = ((u32, u32), &'a CellContent)>,
    row: u32,
) -> RowSignature {
    let mut row_cells: Vec<(u32, &CellContent)> = cells
        .filter_map(|((r, c), cell)| (r == row).then_some((c, cell)))
        .collect();
    row_cells.sort_by_key(|(col, _)| *col);

    let hash = hash_row_content_128(&row_cells);
    RowSignature { hash }
}

#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub(crate) fn compute_col_signature<'a>(
    cells: impl Iterator<Item = ((u32, u32), &'a CellContent)>,
    col: u32,
) -> ColSignature {
    let mut col_cells: Vec<(u32, &CellContent)> = cells
        .filter_map(|((r, c), cell)| (c == col).then_some((r, cell)))
        .collect();
    col_cells.sort_by_key(|(r, _)| *r);
    let ordered: Vec<&CellContent> = col_cells.into_iter().map(|(_, cell)| cell).collect();
    let hash = hash_col_content_128(&ordered);
    ColSignature { hash }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn normalize_zero_values() {
        assert_eq!(
            normalize_float_for_hash(0.0),
            normalize_float_for_hash(-0.0)
        );
        assert_eq!(normalize_float_for_hash(0.0), 0u64);
    }

    #[test]
    fn normalize_nan_values() {
        let nan1 = f64::NAN;
        let nan2 = f64::from_bits(0x7FF8_0000_0000_0001);
        assert_eq!(
            normalize_float_for_hash(nan1),
            normalize_float_for_hash(nan2)
        );
        assert_eq!(normalize_float_for_hash(nan1), CANONICAL_NAN_BITS);
    }

    #[test]
    fn normalize_ulp_drift() {
        let a = 1.0;
        let b = 1.0000000000000002;
        assert_eq!(normalize_float_for_hash(a), normalize_float_for_hash(b));
    }

    #[test]
    fn normalize_meaningful_difference() {
        let a = 1.0;
        let b = 1.0001;
        assert_ne!(normalize_float_for_hash(a), normalize_float_for_hash(b));
    }

    #[test]
    fn normalize_preserves_large_numbers() {
        let a = 1e15;
        let b = 1e15 + 1.0;
        assert_eq!(normalize_float_for_hash(a), normalize_float_for_hash(b));
    }

    #[test]
    fn normalize_distinguishes_different_magnitudes() {
        let a = 1.0;
        let b = 2.0;
        assert_ne!(normalize_float_for_hash(a), normalize_float_for_hash(b));
    }
}

```

---

### File: `core\src\lib.rs`

```rust
//! Excel Diff: a library for comparing Excel workbooks.
//!
//! The main entry point is [`WorkbookPackage`], which can parse a workbook (when the
//! `excel-open-xml` feature is enabled) and then diff it against another workbook.
//!
//! The diff includes:
//! - sheet/grid ops (cell edits, row/column adds/removes, block moves)
//! - object ops (named ranges, charts, VBA modules)
//! - Power Query ops (M query add/remove/rename and definition/metadata changes)
//!
//! # Architecture overview
//!
//! The pipeline is Parse -> IR -> Diff -> Output. For the detailed narrative and entry-point map,
//! see `docs/maintainers/architecture.md` and `docs/maintainers/entrypoints.md`.
//!
//! # Quick start
//!
//! ```no_run
//! use excel_diff::{DiffConfig, WorkbookPackage};
//! use std::fs::File;
//!
//! # fn main() -> Result<(), Box<dyn std::error::Error>> {
//! let old_pkg = WorkbookPackage::open(File::open("old.xlsx")?)?;
//! let new_pkg = WorkbookPackage::open(File::open("new.xlsx")?)?;
//!
//! let report = old_pkg.diff(&new_pkg, &DiffConfig::default());
//! println!("ops={}", report.ops.len());
//! # Ok(())
//! # }
//! ```
//!
//! # Streaming (JSON Lines)
//!
//! ```no_run
//! use excel_diff::{DiffConfig, JsonLinesSink, WorkbookPackage};
//! use std::fs::File;
//! use std::io::{self, BufWriter};
//!
//! # fn main() -> Result<(), Box<dyn std::error::Error>> {
//! let old_pkg = WorkbookPackage::open(File::open("old.xlsx")?)?;
//! let new_pkg = WorkbookPackage::open(File::open("new.xlsx")?)?;
//!
//! let stdout = io::stdout();
//! let mut sink = JsonLinesSink::new(BufWriter::new(stdout.lock()));
//! let summary = old_pkg.diff_streaming(&new_pkg, &DiffConfig::default(), &mut sink)?;
//! eprintln!("complete={} ops={}", summary.complete, summary.op_count);
//! # Ok(())
//! # }
//! ```
//!
//! # Database mode (key-based diff)
//!
//! ```no_run
//! use excel_diff::{DiffConfig, WorkbookPackage};
//! use std::fs::File;
//!
//! # fn main() -> Result<(), Box<dyn std::error::Error>> {
//! let old_pkg = WorkbookPackage::open(File::open("old.xlsx")?)?;
//! let new_pkg = WorkbookPackage::open(File::open("new.xlsx")?)?;
//!
//! // Key columns are 0-based indices (A=0, B=1, ...).
//! let keys: Vec<u32> = vec![0, 2]; // A,C
//! let report = old_pkg.diff_database_mode(&new_pkg, "Data", &keys, &DiffConfig::default())?;
//! println!("ops={}", report.ops.len());
//! # Ok(())
//! # }
//! ```

#![cfg_attr(not(test), deny(clippy::unwrap_used))]
#![cfg_attr(not(test), deny(clippy::expect_used))]

#[cfg(all(feature = "parallel", target_arch = "wasm32"))]
compile_error!("feature \"parallel\" is not supported on wasm32");

use std::cell::RefCell;

mod addressing;
pub(crate) mod alignment;
mod alignment_types;
mod capabilities;
pub(crate) mod column_alignment;
mod config;
mod container;
mod database_alignment;
mod datamashup;
mod datamashup_framing;
mod datamashup_package;
mod permission_bindings;
pub mod error_codes;
mod diff;
mod diffable;
mod engine;
#[cfg(feature = "excel-open-xml")]
mod excel_open_xml;
mod formula;
mod formula_diff;
mod grid_metadata;
mod grid_parser;
mod grid_view;
mod memory_estimate;
pub(crate) mod hashing;
mod matching;
mod m_ast;
mod m_ast_diff;
mod m_diff;
mod m_section;
mod m_semantic_detail;
#[cfg(feature = "model-diff")]
mod dax;
#[cfg(feature = "model-diff")]
mod model;
#[cfg(feature = "model-diff")]
mod model_diff;
#[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
mod tabular_schema;
mod object_diff;
mod output;
mod package;
#[cfg(all(feature = "perf-metrics", not(target_arch = "wasm32")))]
mod memory_metrics;
mod progress;
#[cfg(feature = "perf-metrics")]
#[doc(hidden)]
pub mod perf;
mod policy;
pub(crate) mod rect_block_move;
pub(crate) mod region_mask;
pub(crate) mod row_alignment;
mod session;
mod sink;
mod string_pool;
mod vba;
mod workbook;

#[cfg(all(feature = "perf-metrics", not(target_arch = "wasm32")))]
#[global_allocator]
static GLOBAL_ALLOC: memory_metrics::CountingAllocator<std::alloc::System> =
    memory_metrics::CountingAllocator::new(std::alloc::System);

thread_local! {
    static DEFAULT_SESSION: RefCell<DiffSession> = RefCell::new(DiffSession::new());
}

#[doc(hidden)]
pub fn with_default_session<T>(f: impl FnOnce(&mut DiffSession) -> T) -> T {
    DEFAULT_SESSION.with(|session| {
        let mut session = session.borrow_mut();
        f(&mut session)
    })
}

#[cfg(feature = "legacy-api")]
#[deprecated(note = "use WorkbookPackage::diff")]
#[doc(hidden)]
pub fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    DEFAULT_SESSION.with(|session| {
        let mut session = session.borrow_mut();
        engine::diff_workbooks(old, new, &mut session.strings, config)
    })
}

#[cfg(feature = "legacy-api")]
#[deprecated(note = "use WorkbookPackage::diff")]
#[doc(hidden)]
pub fn try_diff_workbooks(
    old: &Workbook,
    new: &Workbook,
    config: &DiffConfig,
) -> Result<DiffReport, DiffError> {
    DEFAULT_SESSION.with(|session| {
        let mut session = session.borrow_mut();
        engine::try_diff_workbooks(old, new, &mut session.strings, config)
    })
}

#[cfg(all(feature = "legacy-api", feature = "excel-open-xml", feature = "std-fs"))]
#[deprecated(note = "use WorkbookPackage::open")]
#[allow(deprecated)]
#[doc(hidden)]
pub fn open_workbook(path: impl AsRef<std::path::Path>) -> Result<Workbook, ExcelOpenError> {
    DEFAULT_SESSION.with(|session| {
        let mut session = session.borrow_mut();
        excel_open_xml::open_workbook(path, &mut session.strings)
    })
}

/// Advanced APIs for power users.
///
/// The recommended entry point for most callers is [`WorkbookPackage`]. This module exposes
/// lower-level functions and types for callers who want to manage their own sessions/pools or
/// stream ops directly.
///
/// ## Leaf diffs
///
/// Leaf diffs compare individual grids or sheets without workbook orchestration. Grid diffs
/// use a default sheet id of "<grid>".
///
/// ```no_run
/// use excel_diff::{DiffConfig, Grid, StringPool};
///
/// # fn main() -> Result<(), Box<dyn std::error::Error>> {
/// let mut pool = StringPool::new();
/// let old = Grid::new(1, 1);
/// let new = Grid::new(1, 1);
/// let report =
///     excel_diff::advanced::diff_grids_with_pool(&old, &new, &mut pool, &DiffConfig::default());
/// println!("ops={}", report.ops.len());
/// # Ok(())
/// # }
/// ```
///
/// ```no_run
/// use excel_diff::{DiffConfig, Grid, Sheet, SheetKind, StringPool};
///
/// # fn main() -> Result<(), Box<dyn std::error::Error>> {
/// let mut pool = StringPool::new();
/// let sheet_id = pool.intern("Sheet1");
/// let old = Sheet {
///     name: sheet_id,
///     workbook_sheet_id: None,
///     kind: SheetKind::Worksheet,
///     grid: Grid::new(1, 1),
/// };
/// let new = Sheet {
///     name: sheet_id,
///     workbook_sheet_id: None,
///     kind: SheetKind::Worksheet,
///     grid: Grid::new(1, 1),
/// };
/// let report =
///     excel_diff::advanced::diff_sheets_with_pool(&old, &new, &mut pool, &DiffConfig::default());
/// println!("ops={}", report.ops.len());
/// # Ok(())
/// # }
/// ```
///
/// When streaming leaf diffs, all strings referenced by emitted ops must be interned before
/// `begin()` is called. See `docs/streaming_contract.md` for the full contract.
pub mod advanced {
    pub use crate::engine::{
        diff_grids as diff_grids_with_pool, diff_grids_database_mode, diff_grids_streaming,
        diff_grids_streaming_with_progress, diff_sheets as diff_sheets_with_pool,
        diff_sheets_streaming, diff_sheets_streaming_with_progress,
        diff_workbooks as diff_workbooks_with_pool, diff_workbooks_streaming,
        diff_workbooks_streaming_with_progress, diff_workbooks_with_progress,
        try_diff_grids as try_diff_grids_with_pool, try_diff_grids_database_mode_streaming,
        try_diff_grids_streaming, try_diff_grids_streaming_with_progress,
        try_diff_sheets as try_diff_sheets_with_pool, try_diff_sheets_streaming,
        try_diff_sheets_streaming_with_progress, try_diff_workbooks as try_diff_workbooks_with_pool,
        try_diff_workbooks_streaming, try_diff_workbooks_streaming_with_progress,
        try_diff_workbooks_with_progress,
    };
    pub use crate::session::DiffSession;
    pub use crate::sink::{CallbackSink, DiffSink, VecSink};
    pub use crate::string_pool::{StringId, StringPool};
}

pub use addressing::{AddressParseError, address_to_index, index_to_address};
pub use capabilities::{EngineFeatures, engine_features};
pub use config::{DiffConfig, DiffConfigBuilder, LimitBehavior, SemanticNoisePolicy};
pub use container::{ContainerError, ContainerLimits, OpcContainer, ZipContainer};
#[doc(hidden)]
pub use datamashup::parse_metadata;
pub use datamashup::{
    DataMashup, Metadata, Permissions, Query, QueryMetadata, build_data_mashup,
    build_data_mashup_with_decryptor, build_embedded_queries, build_queries,
};
pub use datamashup_framing::{DataMashupError, RawDataMashup, parse_data_mashup};
pub use datamashup_package::{
    DataMashupLimits, EmbeddedContent, PackageParts, PackageXml, SectionDocument,
    parse_package_parts, parse_package_parts_with_limits,
};
pub use permission_bindings::{
    DpapiDecryptError, DpapiDecryptor, PermissionBindingsKind, PermissionBindingsStatus,
};
pub use diff::{
    AstDiffMode, AstDiffSummary, AstMoveHint, ColumnTypeChange, DiffError, DiffOp, DiffReport,
    DiffSummary, ExtractedColumnTypeChanges, ExtractedRenamePairs, ExtractedString,
    ExtractedStringList, FormulaDiffResult, QueryChangeKind, QueryMetadataField,
    QuerySemanticDetail, RenamePair, SheetId, StepChange, StepDiff, StepParams, StepSnapshot,
    StepType, ExpressionChangeKind,
};
#[cfg(feature = "model-diff")]
pub use diff::{ModelColumnProperty, RelationshipProperty};
pub use diffable::{DiffContext, Diffable};
#[doc(hidden)]
pub use engine::{
    diff_grids as diff_grids_with_pool, diff_grids_database_mode, diff_grids_streaming,
    diff_grids_streaming_with_progress, diff_sheets as diff_sheets_with_pool,
    diff_sheets_streaming, diff_sheets_streaming_with_progress,
    diff_workbooks as diff_workbooks_with_pool, diff_workbooks_streaming,
    diff_workbooks_streaming_with_progress, diff_workbooks_with_progress,
    try_diff_grids as try_diff_grids_with_pool, try_diff_grids_database_mode_streaming,
    try_diff_grids_streaming, try_diff_grids_streaming_with_progress,
    try_diff_sheets as try_diff_sheets_with_pool, try_diff_sheets_streaming,
    try_diff_sheets_streaming_with_progress, try_diff_workbooks as try_diff_workbooks_with_pool,
    try_diff_workbooks_streaming, try_diff_workbooks_streaming_with_progress,
    try_diff_workbooks_with_progress,
};
#[cfg(feature = "excel-open-xml")]
#[allow(deprecated)]
#[doc(hidden)]
pub use excel_open_xml::{ExcelOpenError, PackageError};
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
#[allow(deprecated)]
#[doc(hidden)]
pub use excel_open_xml::{open_data_mashup, open_workbook as open_workbook_with_pool};
pub use formula::{
    BinaryOperator, CellReference, ColRef, ExcelError, FormulaExpr, FormulaParseError,
    RangeReference, RowRef, UnaryOperator, formulas_equivalent_modulo_shift, parse_formula,
};
pub use grid_parser::{GridParseError, SheetDescriptor};
pub use grid_view::{
    ColHash, ColMeta, FrequencyClass, GridView, HashStats, RowHash, RowMeta, RowView,
};
#[doc(hidden)]
pub use m_ast::{MAstAccessKind, MAstKind, MTokenDebug, tokenize_for_testing};
pub use m_ast::{
    MModuleAst, MParseError, ast_semantically_equal, canonicalize_m_ast, parse_m_expression,
};
pub use m_section::{SectionMember, SectionParseError, parse_section_members};
#[cfg(feature = "model-diff")]
pub use model::{Measure, Model, ModelColumn, ModelRelationship, ModelTable};
#[cfg(feature = "model-diff")]
pub use model_diff::{diff_models, ModelDiffResult};
#[doc(hidden)]
pub use output::json::diff_report_to_cell_diffs;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
#[doc(hidden)]
pub use output::json::diff_workbooks_to_json;
pub use output::json::{CellDiff, serialize_cell_diffs, serialize_diff_report};
pub use output::json_lines::JsonLinesSink;
pub use package::{PbixPackage, WorkbookPackage};
pub use progress::{NoProgress, ProgressCallback};
pub use policy::{AUTO_STREAM_CELL_THRESHOLD, should_use_large_mode};
pub use session::DiffSession;
pub use sink::{CallbackSink, DiffSink, VecSink};
pub use string_pool::{StringId, StringPool};
pub use vba::{VbaModule, VbaModuleType};
pub use workbook::{
    Cell, CellAddress, CellSnapshot, CellValue, ChartInfo, ChartObject, ColSignature, Grid,
    NamedRange, RowSignature, Sheet, SheetKind, Workbook,
};
pub use database_alignment::suggest_key_columns;

```

---

### File: `core\src\m_ast\step_model.rs`

```rust
use std::collections::{BTreeSet, HashMap};
use std::hash::{Hash, Hasher};

use crate::hashing::XXH64_SEED;

use super::{MExpr, MPrimitive, MToken};

#[derive(Clone, Debug, PartialEq, Eq)]
pub(crate) struct StepPipeline {
    pub(crate) steps: Vec<MStep>,
    pub(crate) output_ref: Option<String>,
    pub(crate) output_signature: u64,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub(crate) struct MStep {
    pub(crate) name: String,
    pub(crate) kind: StepKind,
    pub(crate) source_refs: Vec<String>,
    pub(crate) signature: u64,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum StepKind {
    TableSelectRows {
        predicate_hash: u64,
        extras: Vec<u64>,
    },
    TableRemoveColumns {
        columns: Extracted<Vec<String>>,
        extras: Vec<u64>,
    },
    TableRenameColumns {
        renames: Extracted<Vec<RenamePair>>,
        extras: Vec<u64>,
    },
    TableTransformColumnTypes {
        transforms: Extracted<Vec<ColumnTypeChange>>,
        extras: Vec<u64>,
    },
    TableNestedJoin {
        left_keys: Extracted<Vec<String>>,
        right_keys: Extracted<Vec<String>>,
        new_column: Extracted<String>,
        join_kind_hash: Option<u64>,
        extras: Vec<u64>,
    },
    TableJoin {
        left_keys: Extracted<Vec<String>>,
        right_keys: Extracted<Vec<String>>,
        join_kind_hash: Option<u64>,
        extras: Vec<u64>,
    },
    Other {
        function_name_hash: Option<u64>,
        arity: Option<usize>,
        expr_hash: u64,
    },
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum Extracted<T> {
    Known(T),
    Unknown { hash: u64 },
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct RenamePair {
    pub(crate) from: String,
    pub(crate) to: String,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct ColumnTypeChange {
    pub(crate) column: String,
    pub(crate) ty_hash: u64,
}

pub(crate) fn extract_steps(expr_m: &str) -> Option<StepPipeline> {
    let mut ast = super::parse_m_expression(expr_m).ok()?;
    super::canonicalize_m_ast(&mut ast);
    extract_steps_from_ast(&ast)
}

fn extract_steps_from_ast(ast: &super::MModuleAst) -> Option<StepPipeline> {
    let MExpr::Let { bindings, body } = &ast.root else {
        return None;
    };

    let step_names: BTreeSet<String> = bindings.iter().map(|b| b.name.clone()).collect();
    let mut name_to_idx: HashMap<String, usize> = HashMap::new();
    for (idx, b) in bindings.iter().enumerate() {
        name_to_idx.insert(b.name.clone(), idx);
    }

    let mut steps = Vec::with_capacity(bindings.len());
    for (idx, b) in bindings.iter().enumerate() {
        let source_refs = collect_step_refs(&b.value, &step_names, &name_to_idx, idx);
        let kind = classify_step(&b.value, &step_names);
        let signature = hash64(&kind);
        steps.push(MStep {
            name: b.name.clone(),
            kind,
            source_refs,
            signature,
        });
    }

    let output_signature = hash_expr_signature(body, &step_names);
    let output_ref = match body.as_ref() {
        MExpr::Ident { name } if step_names.contains(name) => Some(name.clone()),
        _ => None,
    };

    Some(StepPipeline {
        steps,
        output_ref,
        output_signature,
    })
}

fn hash64<T: Hash>(value: &T) -> u64 {
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    value.hash(&mut h);
    h.finish()
}

#[derive(Default)]
struct BoundStack<'a> {
    names: Vec<&'a str>,
}

impl<'a> BoundStack<'a> {
    fn contains(&self, name: &str) -> bool {
        self.names.iter().any(|&n| n == name)
    }

    fn push(&mut self, name: &'a str) {
        self.names.push(name);
    }

    fn pop_n(&mut self, n: usize) {
        for _ in 0..n {
            self.names.pop();
        }
    }
}

fn collect_step_refs(
    expr: &MExpr,
    step_names: &BTreeSet<String>,
    name_to_idx: &HashMap<String, usize>,
    current_step_idx: usize,
) -> Vec<String> {
    let mut out: BTreeSet<String> = BTreeSet::new();
    let mut bound = BoundStack::default();
    collect_step_refs_inner(
        expr,
        step_names,
        name_to_idx,
        current_step_idx,
        &mut bound,
        &mut out,
    );

    let mut v: Vec<String> = out.into_iter().collect();
    v.sort_by_key(|name| name_to_idx.get(name.as_str()).copied().unwrap_or(usize::MAX));
    v
}

fn collect_step_refs_inner<'a>(
    expr: &'a MExpr,
    step_names: &BTreeSet<String>,
    name_to_idx: &HashMap<String, usize>,
    current_step_idx: usize,
    bound: &mut BoundStack<'a>,
    out: &mut BTreeSet<String>,
) {
    match expr {
        MExpr::Ident { name } => {
            if bound.contains(name) {
                return;
            }
            if !step_names.contains(name) {
                return;
            }
            let Some(&idx) = name_to_idx.get(name.as_str()) else {
                return;
            };
            if idx < current_step_idx {
                out.insert(name.clone());
            }
        }
        MExpr::Opaque(tokens) => {
            for t in tokens {
                if let MToken::Identifier(id) = t {
                    if bound.contains(id) {
                        continue;
                    }
                    if !step_names.contains(id) {
                        continue;
                    }
                    if let Some(&idx) = name_to_idx.get(id.as_str()) {
                        if idx < current_step_idx {
                            out.insert(id.clone());
                        }
                    }
                }
            }
        }
        MExpr::Let { bindings, body } => {
            for b in bindings {
                collect_step_refs_inner(
                    &b.value,
                    step_names,
                    name_to_idx,
                    current_step_idx,
                    bound,
                    out,
                );
                bound.push(b.name.as_str());
            }
            collect_step_refs_inner(
                body,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            bound.pop_n(bindings.len());
        }
        MExpr::FunctionLiteral { params, body, .. } => {
            let n = params.len();
            for p in params {
                bound.push(p.name.as_str());
            }
            collect_step_refs_inner(
                body,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            bound.pop_n(n);
        }
        MExpr::Each { body } => {
            bound.push("_");
            collect_step_refs_inner(
                body,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            bound.pop_n(1);
        }
        MExpr::Record { fields } => {
            for f in fields {
                collect_step_refs_inner(
                    &f.value,
                    step_names,
                    name_to_idx,
                    current_step_idx,
                    bound,
                    out,
                );
            }
        }
        MExpr::List { items } => {
            for i in items {
                collect_step_refs_inner(
                    i,
                    step_names,
                    name_to_idx,
                    current_step_idx,
                    bound,
                    out,
                );
            }
        }
        MExpr::FunctionCall { args, .. } => {
            for a in args {
                collect_step_refs_inner(
                    a,
                    step_names,
                    name_to_idx,
                    current_step_idx,
                    bound,
                    out,
                );
            }
        }
        MExpr::Access { base, key, .. } => {
            collect_step_refs_inner(
                base,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            collect_step_refs_inner(
                key,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::If {
            cond,
            then_branch,
            else_branch,
        } => {
            collect_step_refs_inner(
                cond,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            collect_step_refs_inner(
                then_branch,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            collect_step_refs_inner(
                else_branch,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::UnaryOp { expr, .. } => {
            collect_step_refs_inner(
                expr,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::BinaryOp { left, right, .. } => {
            collect_step_refs_inner(
                left,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            collect_step_refs_inner(
                right,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::TypeAscription { expr, .. } => {
            collect_step_refs_inner(
                expr,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::TryOtherwise { expr, otherwise } => {
            collect_step_refs_inner(
                expr,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
            collect_step_refs_inner(
                otherwise,
                step_names,
                name_to_idx,
                current_step_idx,
                bound,
                out,
            );
        }
        MExpr::Primitive(_) => {}
    }
}

fn classify_step(expr: &MExpr, step_names: &BTreeSet<String>) -> StepKind {
    let MExpr::FunctionCall { name, args } = expr else {
        return StepKind::Other {
            function_name_hash: None,
            arity: None,
            expr_hash: hash_expr_signature(expr, step_names),
        };
    };

    let name_lc = name.to_ascii_lowercase();
    match name_lc.as_str() {
        "table.selectrows" => classify_select_rows(args, step_names),
        "table.removecolumns" => classify_remove_columns(args, step_names),
        "table.renamecolumns" => classify_rename_columns(args, step_names),
        "table.transformcolumntypes" => classify_transform_column_types(args, step_names),
        "table.nestedjoin" => classify_nested_join(args, step_names),
        "table.join" => classify_join(args, step_names),
        _ => StepKind::Other {
            function_name_hash: Some(hash64(&name_lc)),
            arity: Some(args.len()),
            expr_hash: hash_expr_signature(expr, step_names),
        },
    }
}

fn extras_hashes(args: &[MExpr], start: usize, step_names: &BTreeSet<String>) -> Vec<u64> {
    if args.len() <= start {
        return Vec::new();
    }
    args[start..]
        .iter()
        .map(|e| hash_expr_signature(e, step_names))
        .collect()
}

fn classify_select_rows(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 2 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.selectrows")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    StepKind::TableSelectRows {
        predicate_hash: hash_expr_signature(&args[1], step_names),
        extras: extras_hashes(args, 2, step_names),
    }
}

fn classify_remove_columns(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 2 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.removecolumns")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    let columns = extract_string_list(&args[1]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[1], step_names),
        }
    });

    StepKind::TableRemoveColumns {
        columns,
        extras: extras_hashes(args, 2, step_names),
    }
}

fn classify_rename_columns(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 2 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.renamecolumns")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    let renames = extract_rename_pairs(&args[1]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[1], step_names),
        }
    });

    StepKind::TableRenameColumns {
        renames,
        extras: extras_hashes(args, 2, step_names),
    }
}

fn classify_transform_column_types(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 2 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.transformcolumntypes")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    let transforms = extract_column_type_changes(&args[1], step_names)
        .map(Extracted::Known)
        .unwrap_or_else(|| Extracted::Unknown {
            hash: hash_expr_signature(&args[1], step_names),
        });

    StepKind::TableTransformColumnTypes {
        transforms,
        extras: extras_hashes(args, 2, step_names),
    }
}

fn classify_nested_join(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 5 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.nestedjoin")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    let left_keys = extract_string_list(&args[1]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[1], step_names),
        }
    });

    let right_keys = if args.len() >= 4 {
        extract_string_list(&args[3]).map(Extracted::Known).unwrap_or_else(|| {
            Extracted::Unknown {
                hash: hash_expr_signature(&args[3], step_names),
            }
        })
    } else {
        Extracted::Unknown { hash: 0 }
    };

    let new_column = extract_string(&args[4]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[4], step_names),
        }
    });

    let join_kind_hash = args.get(5).map(|e| hash_expr_signature(e, step_names));
    let extras = if args.len() > 6 {
        extras_hashes(args, 6, step_names)
    } else {
        Vec::new()
    };

    StepKind::TableNestedJoin {
        left_keys,
        right_keys,
        new_column,
        join_kind_hash,
        extras,
    }
}

fn classify_join(args: &[MExpr], step_names: &BTreeSet<String>) -> StepKind {
    if args.len() < 4 {
        return StepKind::Other {
            function_name_hash: Some(hash64(&"table.join")),
            arity: Some(args.len()),
            expr_hash: hash64(&args.len()),
        };
    }

    let left_keys = extract_string_list(&args[1]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[1], step_names),
        }
    });

    let right_keys = extract_string_list(&args[3]).map(Extracted::Known).unwrap_or_else(|| {
        Extracted::Unknown {
            hash: hash_expr_signature(&args[3], step_names),
        }
    });

    let join_kind_hash = args.get(4).map(|e| hash_expr_signature(e, step_names));
    let extras = if args.len() > 5 {
        extras_hashes(args, 5, step_names)
    } else {
        Vec::new()
    };

    StepKind::TableJoin {
        left_keys,
        right_keys,
        join_kind_hash,
        extras,
    }
}

fn extract_string(expr: &MExpr) -> Option<String> {
    match expr {
        MExpr::Primitive(MPrimitive::String(s)) => Some(s.clone()),
        _ => None,
    }
}

fn extract_string_list(expr: &MExpr) -> Option<Vec<String>> {
    match expr {
        MExpr::Primitive(MPrimitive::String(s)) => Some(vec![s.clone()]),
        MExpr::List { items } => {
            let mut out = Vec::with_capacity(items.len());
            for it in items {
                let MExpr::Primitive(MPrimitive::String(s)) = it else {
                    return None;
                };
                out.push(s.clone());
            }
            Some(out)
        }
        _ => None,
    }
}

fn extract_rename_pairs(expr: &MExpr) -> Option<Vec<RenamePair>> {
    let MExpr::List { items } = expr else {
        return None;
    };

    let mut out = Vec::with_capacity(items.len());
    for it in items {
        let MExpr::List { items: pair } = it else {
            return None;
        };
        if pair.len() < 2 {
            return None;
        }
        let MExpr::Primitive(MPrimitive::String(from)) = &pair[0] else {
            return None;
        };
        let MExpr::Primitive(MPrimitive::String(to)) = &pair[1] else {
            return None;
        };
        out.push(RenamePair {
            from: from.clone(),
            to: to.clone(),
        });
    }

    Some(out)
}

fn extract_column_type_changes(
    expr: &MExpr,
    step_names: &BTreeSet<String>,
) -> Option<Vec<ColumnTypeChange>> {
    let MExpr::List { items } = expr else {
        return None;
    };

    let mut out = Vec::with_capacity(items.len());
    for it in items {
        let MExpr::List { items: pair } = it else {
            return None;
        };
        if pair.len() < 2 {
            return None;
        }
        let MExpr::Primitive(MPrimitive::String(col)) = &pair[0] else {
            return None;
        };
        let ty_hash = hash_expr_signature(&pair[1], step_names);
        out.push(ColumnTypeChange {
            column: col.clone(),
            ty_hash,
        });
    }

    Some(out)
}

fn hash_expr_signature(expr: &MExpr, step_names: &BTreeSet<String>) -> u64 {
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    let mut bound = BoundStack::default();
    hash_expr_signature_inner(expr, step_names, &mut bound, &mut h);
    h.finish()
}

fn hash_expr_signature_inner<'a>(
    expr: &'a MExpr,
    step_names: &BTreeSet<String>,
    bound: &mut BoundStack<'a>,
    h: &mut xxhash_rust::xxh64::Xxh64,
) {
    match expr {
        MExpr::Let { bindings, body } => {
            0u8.hash(h);
            bindings.len().hash(h);
            for b in bindings {
                0u8.hash(h);
                b.name.hash(h);
                hash_expr_signature_inner(&b.value, step_names, bound, h);
                bound.push(b.name.as_str());
            }
            hash_expr_signature_inner(body, step_names, bound, h);
            bound.pop_n(bindings.len());
        }
        MExpr::Record { fields } => {
            1u8.hash(h);
            fields.len().hash(h);
            for f in fields {
                f.name.hash(h);
                hash_expr_signature_inner(&f.value, step_names, bound, h);
            }
        }
        MExpr::List { items } => {
            2u8.hash(h);
            items.len().hash(h);
            for it in items {
                hash_expr_signature_inner(it, step_names, bound, h);
            }
        }
        MExpr::FunctionCall { name, args } => {
            3u8.hash(h);
            name.to_ascii_lowercase().hash(h);
            args.len().hash(h);
            for a in args {
                hash_expr_signature_inner(a, step_names, bound, h);
            }
        }
        MExpr::FunctionLiteral {
            params,
            body,
            return_type,
        } => {
            4u8.hash(h);
            params.len().hash(h);
            for p in params {
                p.name.hash(h);
                if let Some(ty) = &p.ty {
                    ty.name.to_ascii_lowercase().hash(h);
                } else {
                    0u8.hash(h);
                }
                bound.push(p.name.as_str());
            }
            if let Some(rt) = return_type {
                1u8.hash(h);
                rt.name.to_ascii_lowercase().hash(h);
            } else {
                0u8.hash(h);
            }
            hash_expr_signature_inner(body, step_names, bound, h);
            bound.pop_n(params.len());
        }
        MExpr::UnaryOp { op, expr } => {
            5u8.hash(h);
            op.hash(h);
            hash_expr_signature_inner(expr, step_names, bound, h);
        }
        MExpr::BinaryOp { op, left, right } => {
            6u8.hash(h);
            op.hash(h);
            hash_expr_signature_inner(left, step_names, bound, h);
            hash_expr_signature_inner(right, step_names, bound, h);
        }
        MExpr::TypeAscription { expr, ty } => {
            7u8.hash(h);
            hash_expr_signature_inner(expr, step_names, bound, h);
            ty.name.to_ascii_lowercase().hash(h);
        }
        MExpr::TryOtherwise { expr, otherwise } => {
            8u8.hash(h);
            hash_expr_signature_inner(expr, step_names, bound, h);
            hash_expr_signature_inner(otherwise, step_names, bound, h);
        }
        MExpr::Ident { name } => {
            9u8.hash(h);
            if bound.contains(name) {
                0u8.hash(h);
                name.hash(h);
            } else if step_names.contains(name) {
                1u8.hash(h);
            } else {
                2u8.hash(h);
                name.hash(h);
            }
        }
        MExpr::If {
            cond,
            then_branch,
            else_branch,
        } => {
            10u8.hash(h);
            hash_expr_signature_inner(cond, step_names, bound, h);
            hash_expr_signature_inner(then_branch, step_names, bound, h);
            hash_expr_signature_inner(else_branch, step_names, bound, h);
        }
        MExpr::Each { body } => {
            11u8.hash(h);
            bound.push("_");
            hash_expr_signature_inner(body, step_names, bound, h);
            bound.pop_n(1);
        }
        MExpr::Access { base, kind, key } => {
            12u8.hash(h);
            kind.hash(h);
            hash_expr_signature_inner(base, step_names, bound, h);
            hash_expr_signature_inner(key, step_names, bound, h);
        }
        MExpr::Primitive(p) => {
            13u8.hash(h);
            p.hash(h);
        }
        MExpr::Opaque(tokens) => {
            14u8.hash(h);
            tokens.len().hash(h);
            for t in tokens {
                match t {
                    MToken::Identifier(id) => {
                        0u8.hash(h);
                        if bound.contains(id) {
                            0u8.hash(h);
                            id.hash(h);
                        } else if step_names.contains(id) {
                            1u8.hash(h);
                        } else {
                            2u8.hash(h);
                            id.hash(h);
                        }
                    }
                    MToken::StringLiteral(s) => {
                        1u8.hash(h);
                        s.hash(h);
                    }
                    MToken::Number(n) => {
                        2u8.hash(h);
                        n.hash(h);
                    }
                    MToken::Symbol(c) => {
                        3u8.hash(h);
                        c.hash(h);
                    }
                    _ => {
                        4u8.hash(h);
                        t.hash(h);
                    }
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn extracts_filter_step_and_dependency() {
        let expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [Amount] > 0)
            in
                #"Filtered Rows"
        "#;

        let pipeline = extract_steps(expr).expect("pipeline should extract");
        assert_eq!(pipeline.steps.len(), 2);
        assert_eq!(pipeline.output_ref.as_deref(), Some("Filtered Rows"));

        assert_eq!(pipeline.steps[0].name, "Source");
        assert_eq!(pipeline.steps[1].name, "Filtered Rows");

        match &pipeline.steps[1].kind {
            StepKind::TableSelectRows { .. } => {}
            other => panic!("expected TableSelectRows, got {:?}", other),
        }

        assert_eq!(pipeline.steps[1].source_refs, vec!["Source".to_string()]);
    }

    #[test]
    fn extracts_remove_columns_params() {
        let expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"A", "B"})
            in
                #"Removed Columns"
        "#;

        let pipeline = extract_steps(expr).expect("pipeline should extract");
        assert_eq!(pipeline.steps.len(), 2);

        match &pipeline.steps[1].kind {
            StepKind::TableRemoveColumns { columns, .. } => match columns {
                Extracted::Known(cols) => {
                    assert_eq!(cols, &vec!["A".to_string(), "B".to_string()])
                }
                other => panic!("expected Known columns, got {:?}", other),
            },
            other => panic!("expected TableRemoveColumns, got {:?}", other),
        }
    }

    #[test]
    fn signatures_survive_step_rename_and_reference_update() {
        let a = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"A", "B"}),
                #"Changed Type" = Table.TransformColumnTypes(#"Removed Columns", {{"C", type text}})
            in
                #"Changed Type"
        "#;

        let b = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Dropped Columns" = Table.RemoveColumns(Source, {"A", "B"}),
                #"Changed Type" = Table.TransformColumnTypes(#"Dropped Columns", {{"C", type text}})
            in
                #"Changed Type"
        "#;

        let pa = extract_steps(a).expect("pipeline should extract");
        let pb = extract_steps(b).expect("pipeline should extract");

        assert_eq!(pa.steps.len(), 3);
        assert_eq!(pb.steps.len(), 3);

        let removed_a = &pa.steps[1];
        let removed_b = &pb.steps[1];
        assert_eq!(removed_a.signature, removed_b.signature);

        let changed_a = &pa.steps[2];
        let changed_b = &pb.steps[2];
        assert_eq!(changed_a.signature, changed_b.signature);
    }
}

```

---

### File: `core\src\m_ast.rs`

```rust
use std::hash::{Hash, Hasher};
use std::iter::Peekable;
use std::str::Chars;

use thiserror::Error;

mod step_model;
#[allow(unused_imports)]
pub(crate) use step_model::{
    extract_steps, ColumnTypeChange as StepColumnTypeChange, Extracted as StepExtracted, MStep,
    RenamePair as StepRenamePair, StepKind, StepPipeline,
};

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub struct MModuleAst {
    root: MExpr,
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum MAstKind {
    Let { binding_count: usize },
    Record { field_count: usize },
    List { item_count: usize },
    FunctionCall { name: String, arg_count: usize },
    FunctionLiteral { param_count: usize },
    UnaryOp,
    BinaryOp,
    TypeAscription,
    TryOtherwise,
    Primitive,
    Ident { name: String },
    If,
    Each,
    Access { kind: MAstAccessKind, chain_len: usize },
    Opaque { token_count: usize },
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum MAstAccessKind {
    Field,
    Item,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum AccessKind {
    Field,
    Item,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub(crate) enum MUnaryOp {
    Not,
    Plus,
    Minus,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub(crate) enum MBinaryOp {
    Add,
    Sub,
    Mul,
    Div,
    Concat,
    Eq,
    Ne,
    Lt,
    Le,
    Gt,
    Ge,
    And,
    Or,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct MTypeRef {
    name: String,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct MParam {
    name: String,
    ty: Option<MTypeRef>,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum MExpr {
    Let {
        bindings: Vec<LetBinding>,
        body: Box<MExpr>,
    },
    Record {
        fields: Vec<RecordField>,
    },
    List {
        items: Vec<MExpr>,
    },
    FunctionCall {
        name: String,
        args: Vec<MExpr>,
    },
    FunctionLiteral {
        params: Vec<MParam>,
        return_type: Option<MTypeRef>,
        body: Box<MExpr>,
    },
    UnaryOp {
        op: MUnaryOp,
        expr: Box<MExpr>,
    },
    BinaryOp {
        op: MBinaryOp,
        left: Box<MExpr>,
        right: Box<MExpr>,
    },
    TypeAscription {
        expr: Box<MExpr>,
        ty: MTypeRef,
    },
    TryOtherwise {
        expr: Box<MExpr>,
        otherwise: Box<MExpr>,
    },
    Ident {
        name: String,
    },
    If {
        cond: Box<MExpr>,
        then_branch: Box<MExpr>,
        else_branch: Box<MExpr>,
    },
    Each {
        body: Box<MExpr>,
    },
    Access {
        base: Box<MExpr>,
        kind: AccessKind,
        key: Box<MExpr>,
    },
    Primitive(MPrimitive),
    Opaque(Vec<MToken>),
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct LetBinding {
    name: String,
    value: Box<MExpr>,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) struct RecordField {
    name: String,
    value: Box<MExpr>,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum MPrimitive {
    String(String),
    Number(String),
    Boolean(bool),
    Null,
}

#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub(crate) enum MToken {
    KeywordLet,
    KeywordIn,
    KeywordIf,
    KeywordThen,
    KeywordElse,
    KeywordEach,
    Identifier(String),
    StringLiteral(String),
    Number(String),
    Symbol(char),
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub enum MTokenDebug {
    KeywordLet,
    KeywordIn,
    KeywordIf,
    KeywordThen,
    KeywordElse,
    KeywordEach,
    Identifier(String),
    StringLiteral(String),
    Number(String),
    Symbol(char),
}

#[derive(Debug, Error, PartialEq, Eq)]
pub enum MParseError {
    #[error("expression is empty")]
    Empty,
    #[error("unterminated string literal")]
    UnterminatedString,
    #[error("unterminated block comment")]
    UnterminatedBlockComment,
    #[error("unbalanced delimiter")]
    UnbalancedDelimiter,
    #[error("invalid let binding syntax")]
    InvalidLetBinding,
    #[error("missing 'in' clause in let expression")]
    MissingInClause,
}

impl From<&MToken> for MTokenDebug {
    fn from(token: &MToken) -> Self {
        match token {
            MToken::KeywordLet => MTokenDebug::KeywordLet,
            MToken::KeywordIn => MTokenDebug::KeywordIn,
            MToken::KeywordIf => MTokenDebug::KeywordIf,
            MToken::KeywordThen => MTokenDebug::KeywordThen,
            MToken::KeywordElse => MTokenDebug::KeywordElse,
            MToken::KeywordEach => MTokenDebug::KeywordEach,
            MToken::Identifier(v) => MTokenDebug::Identifier(v.clone()),
            MToken::StringLiteral(v) => MTokenDebug::StringLiteral(v.clone()),
            MToken::Number(v) => MTokenDebug::Number(v.clone()),
            MToken::Symbol(v) => MTokenDebug::Symbol(*v),
        }
    }
}

impl MModuleAst {
    /// Returns a minimal view of the root expression kind for tests and debugging.
    ///
    /// This keeps the AST opaque for production consumers while allowing
    /// tests to assert the expected structure.
    pub fn root_kind_for_testing(&self) -> MAstKind {
        fn access_chain_len(expr: &MExpr) -> usize {
            let mut n = 0usize;
            let mut cur = expr;
            while let MExpr::Access { base, .. } = cur {
                n += 1;
                cur = base;
            }
            n
        }

        match &self.root {
            MExpr::Let { bindings, .. } => MAstKind::Let {
                binding_count: bindings.len(),
            },
            MExpr::Record { fields } => MAstKind::Record {
                field_count: fields.len(),
            },
            MExpr::List { items } => MAstKind::List {
                item_count: items.len(),
            },
            MExpr::FunctionCall { name, args } => MAstKind::FunctionCall {
                name: name.clone(),
                arg_count: args.len(),
            },
            MExpr::FunctionLiteral { params, .. } => MAstKind::FunctionLiteral {
                param_count: params.len(),
            },
            MExpr::UnaryOp { .. } => MAstKind::UnaryOp,
            MExpr::BinaryOp { .. } => MAstKind::BinaryOp,
            MExpr::TypeAscription { .. } => MAstKind::TypeAscription,
            MExpr::TryOtherwise { .. } => MAstKind::TryOtherwise,
            MExpr::Primitive(_) => MAstKind::Primitive,
            MExpr::Ident { name } => MAstKind::Ident { name: name.clone() },
            MExpr::If { .. } => MAstKind::If,
            MExpr::Each { .. } => MAstKind::Each,
            MExpr::Access { kind, .. } => {
                let kind = match kind {
                    AccessKind::Field => MAstAccessKind::Field,
                    AccessKind::Item => MAstAccessKind::Item,
                };
                MAstKind::Access {
                    kind,
                    chain_len: access_chain_len(&self.root),
                }
            }
            MExpr::Opaque(tokens) => MAstKind::Opaque {
                token_count: tokens.len(),
            },
        }
    }

    pub(crate) fn root_expr(&self) -> &MExpr {
        &self.root
    }
}

impl MExpr {
    pub(crate) fn diff_label_hash(&self) -> u64 {
        use crate::hashing::XXH64_SEED;
        let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
        match self {
            MExpr::Let { bindings, .. } => {
                0u8.hash(&mut h);
                bindings.len().hash(&mut h);
            }
            MExpr::Record { fields } => {
                1u8.hash(&mut h);
                fields.len().hash(&mut h);
            }
            MExpr::List { items } => {
                2u8.hash(&mut h);
                items.len().hash(&mut h);
            }
            MExpr::FunctionCall { name, args } => {
                3u8.hash(&mut h);
                name.to_ascii_lowercase().hash(&mut h);
                args.len().hash(&mut h);
            }
            MExpr::FunctionLiteral {
                params,
                return_type,
                ..
            } => {
                4u8.hash(&mut h);
                params.len().hash(&mut h);
                if let Some(rt) = return_type {
                    rt.name.to_ascii_lowercase().hash(&mut h);
                }
            }
            MExpr::UnaryOp { op, .. } => {
                5u8.hash(&mut h);
                op.hash(&mut h);
            }
            MExpr::BinaryOp { op, .. } => {
                6u8.hash(&mut h);
                op.hash(&mut h);
            }
            MExpr::TypeAscription { ty, .. } => {
                7u8.hash(&mut h);
                ty.name.to_ascii_lowercase().hash(&mut h);
            }
            MExpr::TryOtherwise { .. } => {
                8u8.hash(&mut h);
            }
            MExpr::Ident { name } => {
                9u8.hash(&mut h);
                name.hash(&mut h);
            }
            MExpr::If { .. } => {
                10u8.hash(&mut h);
            }
            MExpr::Each { .. } => {
                11u8.hash(&mut h);
            }
            MExpr::Access { kind, .. } => {
                12u8.hash(&mut h);
                kind.hash(&mut h);
            }
            MExpr::Primitive(p) => {
                13u8.hash(&mut h);
                p.hash(&mut h);
            }
            MExpr::Opaque(tokens) => {
                14u8.hash(&mut h);
                tokens.hash(&mut h);
            }
        }
        h.finish()
    }

    pub(crate) fn diff_children(&self) -> Vec<&MExpr> {
        let mut out = Vec::new();
        match self {
            MExpr::Let { bindings, body } => {
                for b in bindings {
                    out.push(b.value.as_ref());
                }
                out.push(body.as_ref());
            }
            MExpr::Record { fields } => {
                for f in fields {
                    out.push(&f.value);
                }
            }
            MExpr::List { items } => {
                for it in items {
                    out.push(it);
                }
            }
            MExpr::FunctionCall { args, .. } => {
                for a in args {
                    out.push(a);
                }
            }
            MExpr::FunctionLiteral { body, .. } => {
                out.push(body.as_ref());
            }
            MExpr::UnaryOp { expr, .. } => {
                out.push(expr.as_ref());
            }
            MExpr::BinaryOp { left, right, .. } => {
                out.push(left.as_ref());
                out.push(right.as_ref());
            }
            MExpr::TypeAscription { expr, .. } => {
                out.push(expr.as_ref());
            }
            MExpr::TryOtherwise { expr, otherwise } => {
                out.push(expr.as_ref());
                out.push(otherwise.as_ref());
            }
            MExpr::Ident { .. } => {}
            MExpr::If {
                cond,
                then_branch,
                else_branch,
            } => {
                out.push(cond.as_ref());
                out.push(then_branch.as_ref());
                out.push(else_branch.as_ref());
            }
            MExpr::Each { body } => {
                out.push(body.as_ref());
            }
            MExpr::Access { base, key, .. } => {
                out.push(base.as_ref());
                out.push(key.as_ref());
            }
            MExpr::Primitive(_) => {}
            MExpr::Opaque(_) => {}
        }
        out
    }
}

/// Tokenize an M expression for testing and diagnostics.
///
/// The returned tokens are a debug-friendly mirror of the internal lexer output
/// and are not part of the stable public API.
pub fn tokenize_for_testing(source: &str) -> Result<Vec<MTokenDebug>, MParseError> {
    tokenize(source).map(|tokens| tokens.iter().map(MTokenDebug::from).collect())
}

/// Parse a Power Query M expression into a minimal AST.
///
/// Supports top-level `let ... in ...` expressions, record and list literals,
/// qualified function calls, function literals, unary/binary ops, type
/// ascription, `try ... otherwise ...`, primitive literals, identifier
/// references, `if` expressions, `each` expressions, and access chains. Inputs
/// that do not match those forms are preserved as opaque token sequences. The
/// lexer recognizes `let`/`in`/`if`/`then`/`else`/`each`, quoted identifiers
/// (`#"Foo"`), and hash-prefixed literals like `#date`/`#datetime` as single
/// identifiers; other M constructs are parsed best-effort and may be treated as
/// generic tokens.
pub fn parse_m_expression(source: &str) -> Result<MModuleAst, MParseError> {
    let tokens = tokenize(source)?;
    if tokens.is_empty() {
        return Err(MParseError::Empty);
    }

    let root = parse_expression(&tokens)?;
    Ok(MModuleAst { root })
}

pub fn canonicalize_m_ast(ast: &mut MModuleAst) {
    canonicalize_expr(&mut ast.root);
}

pub fn ast_semantically_equal(a: &MModuleAst, b: &MModuleAst) -> bool {
    a == b
}

fn canonicalize_expr(expr: &mut MExpr) {
    match expr {
        MExpr::Let { bindings, body } => {
            for binding in bindings {
                canonicalize_expr(&mut binding.value);
            }
            canonicalize_expr(body);
        }
        MExpr::Record { fields } => {
            for field in fields.iter_mut() {
                canonicalize_expr(&mut field.value);
            }
            fields.sort_by(|a, b| a.name.cmp(&b.name));
        }
        MExpr::List { items } => {
            for item in items {
                canonicalize_expr(item);
            }
        }
        MExpr::FunctionCall { args, .. } => {
            for arg in args {
                canonicalize_expr(arg);
            }
        }
        MExpr::FunctionLiteral {
            params,
            return_type,
            body,
        } => {
            for param in params.iter_mut() {
                if let Some(ty) = param.ty.as_mut() {
                    ty.name = ty.name.to_ascii_lowercase();
                }
            }
            if let Some(ty) = return_type.as_mut() {
                ty.name = ty.name.to_ascii_lowercase();
            }
            canonicalize_expr(body);
        }
        MExpr::UnaryOp { expr, .. } => {
            canonicalize_expr(expr);
        }
        MExpr::BinaryOp { left, right, .. } => {
            canonicalize_expr(left);
            canonicalize_expr(right);
        }
        MExpr::TypeAscription { expr, ty } => {
            canonicalize_expr(expr);
            ty.name = ty.name.to_ascii_lowercase();
        }
        MExpr::TryOtherwise { expr, otherwise } => {
            canonicalize_expr(expr);
            canonicalize_expr(otherwise);
        }
        MExpr::Ident { .. } => {}
        MExpr::If {
            cond,
            then_branch,
            else_branch,
        } => {
            canonicalize_expr(cond);
            canonicalize_expr(then_branch);
            canonicalize_expr(else_branch);
        }
        MExpr::Each { body } => {
            canonicalize_expr(body);
        }
        MExpr::Access { base, key, .. } => {
            canonicalize_expr(base);
            canonicalize_expr(key);
        }
        MExpr::Primitive(_) => {}
        MExpr::Opaque(tokens) => canonicalize_tokens(tokens),
    }
}

fn canonicalize_tokens(tokens: &mut Vec<MToken>) {
    for token in tokens.iter_mut() {
        let MToken::Identifier(ident) = token else {
            continue;
        };

        if ident.eq_ignore_ascii_case("true") {
            *ident = "true".to_string();
        } else if ident.eq_ignore_ascii_case("false") {
            *ident = "false".to_string();
        } else if ident.eq_ignore_ascii_case("null") {
            *ident = "null".to_string();
        }
    }
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
enum InfixSplit {
    Binary(MBinaryOp),
    Ascription,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
struct SplitPoint {
    idx: usize,
    len: usize,
    prec: u8,
    kind: InfixSplit,
}

const PREC_OR: u8 = 10;
const PREC_AND: u8 = 20;
const PREC_CMP: u8 = 30;
const PREC_CONCAT: u8 = 40;
const PREC_ADD: u8 = 50;
const PREC_MUL: u8 = 60;

fn is_tail_keyword(tok: &MToken) -> bool {
    matches!(
        tok,
        MToken::KeywordIf | MToken::KeywordLet | MToken::KeywordEach
    )
}

fn is_try_head(tok: &MToken) -> bool {
    matches!(tok, MToken::Identifier(v) if v.eq_ignore_ascii_case("try"))
}

fn lambda_starts_at(tokens: &[MToken], i: usize) -> bool {
    if tokens.get(i) != Some(&MToken::Symbol('(')) {
        return false;
    }

    let mut depth = 0i32;
    let mut close: Option<usize> = None;
    for j in i..tokens.len() {
        match &tokens[j] {
            MToken::Symbol('(') => depth += 1,
            MToken::Symbol(')') => {
                depth -= 1;
                if depth == 0 {
                    close = Some(j);
                    break;
                }
            }
            _ => {}
        }
    }

    let Some(close) = close else {
        return false;
    };
    matches!(tokens.get(close + 1), Some(MToken::Symbol('=')))
        && matches!(tokens.get(close + 2), Some(MToken::Symbol('>')))
}

fn scan_cutoff_for_tail_expr(tokens: &[MToken]) -> usize {
    let mut depth = 0i32;

    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            _ if depth == 0 && i > 0 => {
                if is_tail_keyword(tok) || is_try_head(tok) || lambda_starts_at(tokens, i) {
                    return i;
                }
            }
            _ => {}
        }
    }

    tokens.len()
}

fn consider_best(best: &mut Option<SplitPoint>, cand: SplitPoint, full_len: usize) {
    if cand.idx == 0 || cand.idx + cand.len >= full_len {
        return;
    }
    match best {
        None => *best = Some(cand),
        Some(b) => {
            if cand.prec < b.prec || (cand.prec == b.prec && cand.idx > b.idx) {
                *best = Some(cand);
            }
        }
    }
}

fn find_best_infix_split(tokens: &[MToken]) -> Option<SplitPoint> {
    let full_len = tokens.len();
    let cutoff = scan_cutoff_for_tail_expr(tokens);
    let tokens = &tokens[..cutoff];

    let mut best: Option<SplitPoint> = None;
    let mut depth = 0i32;

    let mut i = 0usize;
    while i < tokens.len() {
        match &tokens[i] {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            _ if depth == 0 => {
                if i + 1 < tokens.len() {
                    if tokens[i] == MToken::Symbol('<') && tokens[i + 1] == MToken::Symbol('>') {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 2,
                                prec: PREC_CMP,
                                kind: InfixSplit::Binary(MBinaryOp::Ne),
                            },
                            full_len,
                        );
                        i += 2;
                        continue;
                    }
                    if tokens[i] == MToken::Symbol('<') && tokens[i + 1] == MToken::Symbol('=') {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 2,
                                prec: PREC_CMP,
                                kind: InfixSplit::Binary(MBinaryOp::Le),
                            },
                            full_len,
                        );
                        i += 2;
                        continue;
                    }
                    if tokens[i] == MToken::Symbol('>') && tokens[i + 1] == MToken::Symbol('=') {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 2,
                                prec: PREC_CMP,
                                kind: InfixSplit::Binary(MBinaryOp::Ge),
                            },
                            full_len,
                        );
                        i += 2;
                        continue;
                    }
                }

                match &tokens[i] {
                    MToken::Symbol('+') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_ADD,
                            kind: InfixSplit::Binary(MBinaryOp::Add),
                        },
                        full_len,
                    ),
                    MToken::Symbol('-') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_ADD,
                            kind: InfixSplit::Binary(MBinaryOp::Sub),
                        },
                        full_len,
                    ),
                    MToken::Symbol('*') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_MUL,
                            kind: InfixSplit::Binary(MBinaryOp::Mul),
                        },
                        full_len,
                    ),
                    MToken::Symbol('/') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_MUL,
                            kind: InfixSplit::Binary(MBinaryOp::Div),
                        },
                        full_len,
                    ),
                    MToken::Symbol('&') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_CONCAT,
                            kind: InfixSplit::Binary(MBinaryOp::Concat),
                        },
                        full_len,
                    ),
                    MToken::Symbol('=') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_CMP,
                            kind: InfixSplit::Binary(MBinaryOp::Eq),
                        },
                        full_len,
                    ),
                    MToken::Symbol('<') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_CMP,
                            kind: InfixSplit::Binary(MBinaryOp::Lt),
                        },
                        full_len,
                    ),
                    MToken::Symbol('>') => consider_best(
                        &mut best,
                        SplitPoint {
                            idx: i,
                            len: 1,
                            prec: PREC_CMP,
                            kind: InfixSplit::Binary(MBinaryOp::Gt),
                        },
                        full_len,
                    ),
                    _ => {}
                }

                if let MToken::Identifier(v) = &tokens[i] {
                    if v.eq_ignore_ascii_case("and") {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 1,
                                prec: PREC_AND,
                                kind: InfixSplit::Binary(MBinaryOp::And),
                            },
                            full_len,
                        );
                    } else if v.eq_ignore_ascii_case("or") {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 1,
                                prec: PREC_OR,
                                kind: InfixSplit::Binary(MBinaryOp::Or),
                            },
                            full_len,
                        );
                    } else if v.eq_ignore_ascii_case("as") {
                        consider_best(
                            &mut best,
                            SplitPoint {
                                idx: i,
                                len: 1,
                                prec: PREC_CMP,
                                kind: InfixSplit::Ascription,
                            },
                            full_len,
                        );
                    }
                }
            }
            _ => {}
        }

        i += 1;
    }

    best
}

fn parse_tier2_ops(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if let Some(split) = find_best_infix_split(tokens) {
        let left_tokens = &tokens[..split.idx];
        let right_tokens = &tokens[split.idx + split.len..];

        if !left_tokens.is_empty() && !right_tokens.is_empty() {
            let left = parse_expression(left_tokens)?;
            match split.kind {
                InfixSplit::Binary(op) => {
                    let right = parse_expression(right_tokens)?;
                    return Ok(Some(MExpr::BinaryOp {
                        op,
                        left: Box::new(left),
                        right: Box::new(right),
                    }));
                }
                InfixSplit::Ascription => {
                    let ty = match parse_type_ref(right_tokens) {
                        Some(ty) => ty,
                        None => return Ok(None),
                    };
                    return Ok(Some(MExpr::TypeAscription {
                        expr: Box::new(left),
                        ty,
                    }));
                }
            }
        }
    }

    if tokens.len() >= 2 {
        if matches!(&tokens[0], MToken::Identifier(v) if v.eq_ignore_ascii_case("not")) {
            let inner = parse_expression(&tokens[1..])?;
            return Ok(Some(MExpr::UnaryOp {
                op: MUnaryOp::Not,
                expr: Box::new(inner),
            }));
        }

        if tokens[0] == MToken::Symbol('+') {
            let inner = parse_expression(&tokens[1..])?;
            return Ok(Some(MExpr::UnaryOp {
                op: MUnaryOp::Plus,
                expr: Box::new(inner),
            }));
        }

        if tokens[0] == MToken::Symbol('-') {
            if let Some(prim) = parse_primitive(tokens) {
                return Ok(Some(prim));
            }
            let inner = parse_expression(&tokens[1..])?;
            return Ok(Some(MExpr::UnaryOp {
                op: MUnaryOp::Minus,
                expr: Box::new(inner),
            }));
        }
    }

    Ok(None)
}

fn parse_expression(tokens: &[MToken]) -> Result<MExpr, MParseError> {
    if tokens.is_empty() {
        return Err(MParseError::Empty);
    }

    if let Some(let_ast) = parse_let(tokens)? {
        return Ok(let_ast);
    }

    if let Some(inner) = strip_wrapping_parens(tokens) {
        if !inner.is_empty() {
            return parse_expression(inner);
        }
    }

    if let Some(if_expr) = parse_if_then_else(tokens)? {
        return Ok(if_expr);
    }

    if let Some(each_expr) = parse_each_expr(tokens)? {
        return Ok(each_expr);
    }

    if let Some(try_expr) = parse_try_otherwise(tokens)? {
        return Ok(try_expr);
    }

    if let Some(fn_lit) = parse_function_literal(tokens)? {
        return Ok(fn_lit);
    }

    if let Some(op_expr) = parse_tier2_ops(tokens)? {
        return Ok(op_expr);
    }

    if let Some(rec) = parse_record_literal(tokens)? {
        return Ok(rec);
    }
    if let Some(list) = parse_list_literal(tokens)? {
        return Ok(list);
    }
    if let Some(access) = parse_access_chain(tokens)? {
        return Ok(access);
    }
    if let Some(call) = parse_function_call(tokens)? {
        return Ok(call);
    }
    if let Some(prim) = parse_primitive(tokens) {
        return Ok(prim);
    }
    if let Some(ident) = parse_ident_ref(tokens) {
        return Ok(ident);
    }

    Ok(MExpr::Opaque(tokens.to_vec()))
}

fn strip_wrapping_parens(tokens: &[MToken]) -> Option<&[MToken]> {
    if tokens.len() < 2 {
        return None;
    }
    if !matches!(tokens.first(), Some(MToken::Symbol('('))) {
        return None;
    }
    if !matches!(tokens.last(), Some(MToken::Symbol(')'))) {
        return None;
    }

    let mut depth = 0i32;
    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') => depth += 1,
            MToken::Symbol(')') => {
                depth -= 1;
                if depth == 0 && i != tokens.len() - 1 {
                    return None;
                }
            }
            _ => {}
        }
    }

    if depth != 0 {
        return None;
    }

    Some(&tokens[1..tokens.len() - 1])
}

fn is_wrapped_by(tokens: &[MToken], open: char, close: char) -> bool {
    if tokens.len() < 2 {
        return false;
    }
    if !matches!(tokens.first(), Some(MToken::Symbol(c)) if *c == open) {
        return false;
    }
    if !matches!(tokens.last(), Some(MToken::Symbol(c)) if *c == close) {
        return false;
    }

    let mut depth = 0i32;
    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                depth -= 1;
                if depth == 0 && i != tokens.len() - 1 {
                    return false;
                }
            }
            _ => {}
        }
    }

    depth == 0
}

fn split_top_level(tokens: &[MToken], delimiter: char) -> Vec<&[MToken]> {
    let mut out = Vec::new();
    let mut start = 0usize;
    let mut depth = 0i32;
    let mut let_depth = 0i32;

    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            MToken::KeywordLet => let_depth += 1,
            MToken::KeywordIn => {
                if let_depth > 0 {
                    let_depth -= 1;
                }
            }
            MToken::Symbol(c) if *c == delimiter && depth == 0 && let_depth == 0 => {
                out.push(&tokens[start..i]);
                start = i + 1;
            }
            _ => {}
        }
    }

    out.push(&tokens[start..]);
    out
}

fn token_as_name(tok: &MToken) -> Option<String> {
    match tok {
        MToken::Identifier(v) => Some(v.clone()),
        MToken::KeywordIf => Some("if".to_string()),
        MToken::KeywordThen => Some("then".to_string()),
        MToken::KeywordElse => Some("else".to_string()),
        MToken::KeywordEach => Some("each".to_string()),
        _ => None,
    }
}

fn parse_ident_ref(tokens: &[MToken]) -> Option<MExpr> {
    if tokens.len() != 1 {
        return None;
    }

    match &tokens[0] {
        MToken::Identifier(v) => Some(MExpr::Ident { name: v.clone() }),
        MToken::KeywordIf => Some(MExpr::Ident {
            name: "if".to_string(),
        }),
        MToken::KeywordThen => Some(MExpr::Ident {
            name: "then".to_string(),
        }),
        MToken::KeywordElse => Some(MExpr::Ident {
            name: "else".to_string(),
        }),
        MToken::KeywordEach => Some(MExpr::Ident {
            name: "each".to_string(),
        }),
        _ => None,
    }
}

fn parse_each_expr(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if !matches!(tokens.first(), Some(MToken::KeywordEach)) {
        return Ok(None);
    }
    if tokens.len() < 2 {
        return Ok(None);
    }

    let body = parse_expression(&tokens[1..])?;
    Ok(Some(MExpr::Each {
        body: Box::new(body),
    }))
}

fn parse_if_then_else(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if !matches!(tokens.first(), Some(MToken::KeywordIf)) {
        return Ok(None);
    }

    let mut depth = 0i32;
    let mut let_depth = 0i32;
    let mut if_depth = 0i32;
    let mut then_idx: Option<usize> = None;
    let mut else_idx: Option<usize> = None;

    for i in 1..tokens.len() {
        match &tokens[i] {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            MToken::KeywordLet => let_depth += 1,
            MToken::KeywordIn => {
                if let_depth > 0 {
                    let_depth -= 1;
                }
            }
            MToken::KeywordIf if depth == 0 && let_depth == 0 => {
                if_depth += 1;
            }
            MToken::KeywordThen
                if depth == 0 && let_depth == 0 && if_depth == 0 && then_idx.is_none() =>
            {
                then_idx = Some(i);
            }
            MToken::KeywordElse if depth == 0 && let_depth == 0 => {
                if if_depth > 0 {
                    if_depth -= 1;
                } else {
                    else_idx = Some(i);
                    break;
                }
            }
            _ => {}
        }
    }

    let Some(then_idx) = then_idx else {
        return Ok(None);
    };
    let Some(else_idx) = else_idx else {
        return Ok(None);
    };

    if then_idx <= 1 {
        return Ok(None);
    }
    if else_idx <= then_idx + 1 {
        return Ok(None);
    }
    if else_idx + 1 >= tokens.len() {
        return Ok(None);
    }

    let cond = parse_expression(&tokens[1..then_idx])?;
    let then_branch = parse_expression(&tokens[then_idx + 1..else_idx])?;
    let else_branch = parse_expression(&tokens[else_idx + 1..])?;

    Ok(Some(MExpr::If {
        cond: Box::new(cond),
        then_branch: Box::new(then_branch),
        else_branch: Box::new(else_branch),
    }))
}

fn find_top_level_otherwise(tokens: &[MToken]) -> Option<usize> {
    let mut depth = 0i32;
    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            MToken::Identifier(v) if depth == 0 && v.eq_ignore_ascii_case("otherwise") => {
                return Some(i);
            }
            _ => {}
        }
    }
    None
}

fn parse_try_otherwise(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if tokens.is_empty() {
        return Ok(None);
    }
    if !matches!(&tokens[0], MToken::Identifier(v) if v.eq_ignore_ascii_case("try")) {
        return Ok(None);
    }

    let otherwise_idx = match find_top_level_otherwise(&tokens[1..]) {
        Some(i) => i + 1,
        None => return Ok(None),
    };

    let expr_tokens = &tokens[1..otherwise_idx];
    let otherwise_tokens = &tokens[otherwise_idx + 1..];

    if expr_tokens.is_empty() || otherwise_tokens.is_empty() {
        return Ok(None);
    }

    let expr = parse_expression(expr_tokens)?;
    let otherwise = parse_expression(otherwise_tokens)?;

    Ok(Some(MExpr::TryOtherwise {
        expr: Box::new(expr),
        otherwise: Box::new(otherwise),
    }))
}

fn is_ident_token(tok: &MToken, s: &str) -> bool {
    matches!(tok, MToken::Identifier(v) if v.eq_ignore_ascii_case(s))
}

fn find_top_level_arrow(tokens: &[MToken]) -> Option<usize> {
    let mut depth = 0i32;
    for i in 0..tokens.len().saturating_sub(1) {
        match &tokens[i] {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            MToken::Symbol('=') if depth == 0 => {
                if matches!(tokens.get(i + 1), Some(MToken::Symbol('>'))) {
                    return Some(i);
                }
            }
            _ => {}
        }
    }
    None
}

fn parse_param(tokens: &[MToken]) -> Option<MParam> {
    if tokens.is_empty() {
        return None;
    }

    if tokens.len() == 1 {
        let name = token_as_name(&tokens[0])?;
        return Some(MParam { name, ty: None });
    }

    for i in 1..tokens.len() {
        if is_ident_token(&tokens[i], "as") {
            let name = token_as_name(&tokens[0])?;
            let ty_tokens = &tokens[i + 1..];
            let ty = parse_type_ref(ty_tokens)?;
            return Some(MParam {
                name,
                ty: Some(ty),
            });
        }
    }

    None
}

fn parse_function_literal(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    let arrow_idx = match find_top_level_arrow(tokens) {
        Some(i) => i,
        None => return Ok(None),
    };

    if tokens.first() != Some(&MToken::Symbol('(')) {
        return Ok(None);
    }
    if arrow_idx < 2 {
        return Ok(None);
    }

    let mut depth = 0i32;
    let mut close_paren: Option<usize> = None;
    for i in 0..arrow_idx {
        match &tokens[i] {
            MToken::Symbol('(') => depth += 1,
            MToken::Symbol(')') => {
                depth -= 1;
                if depth == 0 {
                    close_paren = Some(i);
                    break;
                }
            }
            _ => {}
        }
    }
    let close_paren = match close_paren {
        Some(i) => i,
        None => return Ok(None),
    };

    let params_tokens = &tokens[1..close_paren];
    let param_slices = if params_tokens.is_empty() {
        Vec::new()
    } else {
        split_top_level(params_tokens, ',')
    };

    let mut params = Vec::new();
    for slice in param_slices {
        if slice.is_empty() {
            return Ok(None);
        }
        let p = match parse_param(slice) {
            Some(p) => p,
            None => return Ok(None),
        };
        params.push(p);
    }

    let mut return_type: Option<MTypeRef> = None;
    let between = &tokens[close_paren + 1..arrow_idx];
    if !between.is_empty() {
        if between.len() >= 2 && is_ident_token(&between[0], "as") {
            let ty = match parse_type_ref(&between[1..]) {
                Some(ty) => ty,
                None => return Ok(None),
            };
            return_type = Some(ty);
        } else {
            return Ok(None);
        }
    }

    if tokens.len() <= arrow_idx + 1 {
        return Ok(None);
    }

    let body_tokens = &tokens[arrow_idx + 2..];
    if body_tokens.is_empty() {
        return Ok(None);
    }
    let body = parse_expression(body_tokens)?;

    Ok(Some(MExpr::FunctionLiteral {
        params,
        return_type,
        body: Box::new(body),
    }))
}

fn parse_access_chain(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if tokens.len() < 3 {
        return Ok(None);
    }

    let mut end = tokens.len();
    let mut segments: Vec<(AccessKind, &[MToken])> = Vec::new();

    loop {
        if end < 2 {
            break;
        }

        let (kind, open_ch, close_ch) = match &tokens[end - 1] {
            MToken::Symbol(']') => (AccessKind::Field, '[', ']'),
            MToken::Symbol('}') => (AccessKind::Item, '{', '}'),
            _ => break,
        };

        let mut depth = 0i32;
        let mut found_open: Option<usize> = None;

        for i in (0..end - 1).rev() {
            match &tokens[i] {
                MToken::Symbol(c) if *c == close_ch => depth += 1,
                MToken::Symbol(c) if *c == open_ch => {
                    if depth == 0 {
                        found_open = Some(i);
                        break;
                    } else {
                        depth -= 1;
                    }
                }
                _ => {}
            }
        }

        let Some(open_idx) = found_open else {
            return Ok(None);
        };

        if open_idx + 1 > end - 1 {
            return Ok(None);
        }

        let inner = &tokens[open_idx + 1..end - 1];
        segments.push((kind, inner));
        end = open_idx;
    }

    if segments.is_empty() {
        return Ok(None);
    }

    let base_tokens = &tokens[..end];
    if base_tokens.is_empty() {
        return Ok(None);
    }

    let mut expr = parse_expression(base_tokens)?;

    for (kind, key_tokens) in segments.into_iter().rev() {
        if key_tokens.is_empty() {
            return Ok(None);
        }
        let key = parse_expression(key_tokens)?;
        expr = MExpr::Access {
            base: Box::new(expr),
            kind,
            key: Box::new(key),
        };
    }

    Ok(Some(expr))
}

fn parse_record_literal(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if !is_wrapped_by(tokens, '[', ']') {
        return Ok(None);
    }

    let inner = &tokens[1..tokens.len() - 1];
    if inner.is_empty() {
        return Ok(Some(MExpr::Record { fields: Vec::new() }));
    }

    let parts = split_top_level(inner, ',');
    let mut fields = Vec::with_capacity(parts.len());

    for part in parts {
        if part.len() < 3 {
            return Ok(None);
        }

        let name = match &part[0] {
            MToken::StringLiteral(v) => v.clone(),
            tok => match token_as_name(tok) {
                Some(name) => name,
                None => return Ok(None),
            },
        };

        if !matches!(part[1], MToken::Symbol('=')) {
            return Ok(None);
        }

        let value_tokens = &part[2..];
        if value_tokens.is_empty() {
            return Ok(None);
        }

        let value_expr = parse_expression(value_tokens)?;
        fields.push(RecordField {
            name,
            value: Box::new(value_expr),
        });
    }

    Ok(Some(MExpr::Record { fields }))
}

fn parse_list_literal(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if !is_wrapped_by(tokens, '{', '}') {
        return Ok(None);
    }

    let inner = &tokens[1..tokens.len() - 1];
    if inner.is_empty() {
        return Ok(Some(MExpr::List { items: Vec::new() }));
    }

    let parts = split_top_level(inner, ',');
    let mut items = Vec::with_capacity(parts.len());
    for part in parts {
        if part.is_empty() {
            return Ok(None);
        }
        items.push(parse_expression(part)?);
    }

    Ok(Some(MExpr::List { items }))
}

fn parse_function_call(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if tokens.len() < 3 {
        return Ok(None);
    }
    if matches!(tokens.first(), Some(MToken::Symbol('('))) {
        return Ok(None);
    }

    let mut open_idx = None;
    let mut depth = 0i32;
    let mut let_depth = 0i32;

    for (i, tok) in tokens.iter().enumerate() {
        match tok {
            MToken::Symbol('(') => {
                if depth == 0 && let_depth == 0 {
                    open_idx = Some(i);
                    break;
                }
                depth += 1;
            }
            MToken::Symbol('[') | MToken::Symbol('{') => depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if depth > 0 {
                    depth -= 1;
                }
            }
            MToken::KeywordLet => let_depth += 1,
            MToken::KeywordIn => {
                if let_depth > 0 {
                    let_depth -= 1;
                }
            }
            _ => {}
        }
    }

    let open_idx = match open_idx {
        Some(i) if i > 0 => i,
        _ => return Ok(None),
    };

    if !matches!(tokens.last(), Some(MToken::Symbol(')'))) {
        return Ok(None);
    }

    let mut suffix_depth = 0i32;
    for (i, tok) in tokens[open_idx..].iter().enumerate() {
        match tok {
            MToken::Symbol('(') | MToken::Symbol('[') | MToken::Symbol('{') => suffix_depth += 1,
            MToken::Symbol(')') | MToken::Symbol(']') | MToken::Symbol('}') => {
                if suffix_depth > 0 {
                    suffix_depth -= 1;
                }
                if suffix_depth == 0 && i != tokens.len() - open_idx - 1 {
                    return Ok(None);
                }
            }
            _ => {}
        }
    }

    if suffix_depth != 0 {
        return Ok(None);
    }

    let name = match parse_qualified_name(&tokens[..open_idx]) {
        Some(v) => v,
        None => return Ok(None),
    };

    let arg_tokens = &tokens[open_idx + 1..tokens.len() - 1];
    let args = if arg_tokens.is_empty() {
        Vec::new()
    } else {
        let parts = split_top_level(arg_tokens, ',');
        let mut args = Vec::with_capacity(parts.len());
        for part in parts {
            if part.is_empty() {
                return Ok(None);
            }
            args.push(parse_expression(part)?);
        }
        args
    };

    Ok(Some(MExpr::FunctionCall { name, args }))
}

fn parse_qualified_name(tokens: &[MToken]) -> Option<String> {
    if tokens.is_empty() {
        return None;
    }

    let mut parts = Vec::new();
    let mut i = 0usize;

    match &tokens[i] {
        MToken::Identifier(v) => parts.push(v.clone()),
        _ => return None,
    }
    i += 1;

    while i < tokens.len() {
        match (&tokens[i], tokens.get(i + 1)) {
            (MToken::Symbol('.'), Some(MToken::Identifier(v))) => {
                parts.push(v.clone());
                i += 2;
            }
            _ => return None,
        }
    }

    Some(parts.join("."))
}

fn parse_type_ref(tokens: &[MToken]) -> Option<MTypeRef> {
    let name = parse_qualified_name(tokens)?;
    Some(MTypeRef {
        name: name.to_ascii_lowercase(),
    })
}

fn parse_primitive(tokens: &[MToken]) -> Option<MExpr> {
    if tokens.len() == 1 {
        match &tokens[0] {
            MToken::StringLiteral(v) => {
                return Some(MExpr::Primitive(MPrimitive::String(v.clone())));
            }
            MToken::Number(v) => {
                return Some(MExpr::Primitive(MPrimitive::Number(v.clone())));
            }
            MToken::Identifier(v) => {
                if v.eq_ignore_ascii_case("true") {
                    return Some(MExpr::Primitive(MPrimitive::Boolean(true)));
                }
                if v.eq_ignore_ascii_case("false") {
                    return Some(MExpr::Primitive(MPrimitive::Boolean(false)));
                }
                if v.eq_ignore_ascii_case("null") {
                    return Some(MExpr::Primitive(MPrimitive::Null));
                }
            }
            _ => {}
        }
    }

    if tokens.len() == 2 {
        if matches!(tokens[0], MToken::Symbol('-')) {
            if let MToken::Number(v) = &tokens[1] {
                return Some(MExpr::Primitive(MPrimitive::Number(format!("-{}", v))));
            }
        }
    }

    None
}

fn parse_let(tokens: &[MToken]) -> Result<Option<MExpr>, MParseError> {
    if tokens.is_empty() {
        return Err(MParseError::Empty);
    }
    if !matches!(tokens.first(), Some(MToken::KeywordLet)) {
        return Ok(None);
    }

    let mut idx = 1usize;
    let mut bindings = Vec::new();
    let mut found_in = false;

    while idx < tokens.len() {
        if matches!(tokens.get(idx), Some(MToken::KeywordIn)) {
            found_in = true;
            idx += 1;
            break;
        }

        let name = match tokens.get(idx) {
            Some(MToken::Identifier(name)) => name.clone(),
            _ => return Err(MParseError::InvalidLetBinding),
        };
        idx += 1;

        if !matches!(tokens.get(idx), Some(MToken::Symbol('='))) {
            return Err(MParseError::InvalidLetBinding);
        }
        idx += 1;

        let value_start = idx;
        let mut value_end = None;
        let mut depth = 0i32;
        let mut let_depth_in_value = 0i32;

        while idx < tokens.len() {
            match tokens.get(idx) {
                Some(MToken::KeywordLet) => {
                    let_depth_in_value += 1;
                }
                Some(MToken::KeywordIn) => {
                    if let_depth_in_value > 0 {
                        let_depth_in_value -= 1;
                    } else if depth == 0 {
                        value_end = Some(idx);
                        break;
                    }
                }
                Some(MToken::Symbol(c)) if is_open_delimiter(*c) => depth += 1,
                Some(MToken::Symbol(c)) if is_close_delimiter(*c) => {
                    if depth > 0 {
                        depth -= 1;
                    }
                }
                Some(MToken::Symbol(',')) if depth == 0 && let_depth_in_value == 0 => {
                    value_end = Some(idx);
                    break;
                }
                _ => {}
            }
            idx += 1;
        }

        let value_end = value_end.unwrap_or(idx);
        if value_end <= value_start {
            return Err(MParseError::InvalidLetBinding);
        }

        let value_tokens = &tokens[value_start..value_end];
        let value_expr = parse_expression(value_tokens)?;

        bindings.push(LetBinding {
            name,
            value: Box::new(value_expr),
        });

        idx = value_end;
        if matches!(tokens.get(idx), Some(MToken::Symbol(','))) {
            idx += 1;
        }
    }

    if !found_in {
        return Err(MParseError::MissingInClause);
    }
    if idx >= tokens.len() {
        return Err(MParseError::InvalidLetBinding);
    }

    let body_expr = parse_expression(&tokens[idx..])?;
    Ok(Some(MExpr::Let {
        bindings,
        body: Box::new(body_expr),
    }))
}

fn tokenize(source: &str) -> Result<Vec<MToken>, MParseError> {
    let mut tokens = Vec::new();
    let mut chars = source.chars().peekable();
    let mut delimiters: Vec<char> = Vec::new();

    while let Some(ch) = chars.next() {
        if ch.is_whitespace() {
            continue;
        }

        if ch == '/' {
            if matches!(chars.peek(), Some('/')) {
                skip_line_comment(&mut chars);
                continue;
            }
            if matches!(chars.peek(), Some('*')) {
                chars.next();
                skip_block_comment(&mut chars)?;
                continue;
            }
        }

        if ch == '"' {
            let literal = parse_string(&mut chars)?;
            tokens.push(MToken::StringLiteral(literal));
            continue;
        }

        if ch == '#' {
            if matches!(chars.peek(), Some('"')) {
                chars.next();
                let ident = parse_string(&mut chars)?;
                tokens.push(MToken::Identifier(ident));
                continue;
            }
            if let Some(next) = chars.peek().copied()
                && is_identifier_start(next)
            {
                chars.next();
                let ident = parse_identifier(next, &mut chars);
                tokens.push(MToken::Identifier(format!("#{ident}")));
                continue;
            }
            tokens.push(MToken::Symbol('#'));
            continue;
        }

        if is_identifier_start(ch) {
            let ident = parse_identifier(ch, &mut chars);
            if ident.eq_ignore_ascii_case("let") {
                tokens.push(MToken::KeywordLet);
            } else if ident.eq_ignore_ascii_case("in") {
                tokens.push(MToken::KeywordIn);
            } else if ident.eq_ignore_ascii_case("if") {
                tokens.push(MToken::KeywordIf);
            } else if ident.eq_ignore_ascii_case("then") {
                tokens.push(MToken::KeywordThen);
            } else if ident.eq_ignore_ascii_case("else") {
                tokens.push(MToken::KeywordElse);
            } else if ident.eq_ignore_ascii_case("each") {
                tokens.push(MToken::KeywordEach);
            } else {
                tokens.push(MToken::Identifier(ident));
            }
            continue;
        }

        if ch.is_ascii_digit() {
            let number = parse_number(ch, &mut chars);
            tokens.push(MToken::Number(number));
            continue;
        }

        if is_open_delimiter(ch) {
            delimiters.push(ch);
        } else if is_close_delimiter(ch) {
            let Some(open) = delimiters.pop() else {
                return Err(MParseError::UnbalancedDelimiter);
            };
            if !delimiters_match(open, ch) {
                return Err(MParseError::UnbalancedDelimiter);
            }
        }

        tokens.push(MToken::Symbol(ch));
    }

    if !delimiters.is_empty() {
        return Err(MParseError::UnbalancedDelimiter);
    }

    Ok(tokens)
}

#[allow(clippy::while_let_on_iterator)]
fn skip_line_comment(chars: &mut Peekable<Chars<'_>>) {
    while let Some(ch) = chars.next() {
        if ch == '\n' {
            break;
        }
    }
}

#[allow(clippy::while_let_on_iterator)]
fn skip_block_comment(chars: &mut Peekable<Chars<'_>>) -> Result<(), MParseError> {
    while let Some(ch) = chars.next() {
        if ch == '*' && matches!(chars.peek(), Some('/')) {
            chars.next();
            return Ok(());
        }
    }

    Err(MParseError::UnterminatedBlockComment)
}

fn parse_string(chars: &mut Peekable<Chars<'_>>) -> Result<String, MParseError> {
    let mut buf = String::new();

    while let Some(ch) = chars.next() {
        if ch == '"' {
            if matches!(chars.peek(), Some('"')) {
                buf.push('"');
                chars.next();
                continue;
            }
            return Ok(buf);
        }

        buf.push(ch);
    }

    Err(MParseError::UnterminatedString)
}

fn parse_identifier(first: char, chars: &mut Peekable<Chars<'_>>) -> String {
    let mut ident = String::new();
    ident.push(first);

    while let Some(&next) = chars.peek() {
        if is_identifier_continue(next) {
            ident.push(next);
            chars.next();
        } else {
            break;
        }
    }

    ident
}

fn parse_number(first: char, chars: &mut Peekable<Chars<'_>>) -> String {
    let mut number = String::new();
    number.push(first);

    while let Some(&next) = chars.peek() {
        if next.is_ascii_digit() || next == '.' {
            number.push(next);
            chars.next();
        } else {
            break;
        }
    }

    number
}

fn is_identifier_start(ch: char) -> bool {
    ch.is_ascii_alphabetic() || ch == '_'
}

fn is_identifier_continue(ch: char) -> bool {
    ch.is_ascii_alphanumeric() || ch == '_'
}

fn is_open_delimiter(ch: char) -> bool {
    matches!(ch, '(' | '[' | '{')
}

fn is_close_delimiter(ch: char) -> bool {
    matches!(ch, ')' | ']' | '}')
}

fn delimiters_match(open: char, close: char) -> bool {
    matches!((open, close), ('(', ')') | ('[', ']') | ('{', '}'))
}

```

---

### File: `core\src\m_ast_diff\apted.rs`

```rust
use std::collections::HashMap;

use super::SimpleTree;

#[derive(Clone, Copy)]
pub(crate) struct EditCounts {
    pub(crate) cost: u32,
    pub(crate) inserted: u32,
    pub(crate) deleted: u32,
    pub(crate) updated: u32,
}

impl EditCounts {
    fn zero() -> Self {
        EditCounts {
            cost: 0,
            inserted: 0,
            deleted: 0,
            updated: 0,
        }
    }

    fn add(self, other: EditCounts) -> EditCounts {
        EditCounts {
            cost: self.cost + other.cost,
            inserted: self.inserted + other.inserted,
            deleted: self.deleted + other.deleted,
            updated: self.updated + other.updated,
        }
    }
}

fn better(a: EditCounts, b: EditCounts) -> EditCounts {
    if a.cost != b.cost {
        return if a.cost < b.cost { a } else { b };
    }
    if a.updated != b.updated {
        return if a.updated < b.updated { a } else { b };
    }
    let a_id = a.inserted + a.deleted;
    let b_id = b.inserted + b.deleted;
    if a_id != b_id {
        return if a_id < b_id { a } else { b };
    }
    a
}

pub(crate) fn tree_edit_counts(old: &SimpleTree, new: &SimpleTree) -> EditCounts {
    if old.labels.is_empty() {
        return EditCounts {
            cost: new.labels.len() as u32,
            inserted: new.labels.len() as u32,
            deleted: 0,
            updated: 0,
        };
    }
    if new.labels.is_empty() {
        return EditCounts {
            cost: old.labels.len() as u32,
            inserted: 0,
            deleted: old.labels.len() as u32,
            updated: 0,
        };
    }

    let mut memo: HashMap<(usize, usize), EditCounts> = HashMap::new();
    tree_edit_counts_at(old, new, 0, 0, &mut memo)
}

fn tree_edit_counts_at(
    old: &SimpleTree,
    new: &SimpleTree,
    oi: usize,
    ni: usize,
    memo: &mut HashMap<(usize, usize), EditCounts>,
) -> EditCounts {
    if let Some(v) = memo.get(&(oi, ni)) {
        return *v;
    }

    let mut base = if old.labels[oi] == new.labels[ni] {
        EditCounts::zero()
    } else {
        EditCounts {
            cost: 1,
            inserted: 0,
            deleted: 0,
            updated: 1,
        }
    };

    let children_cost = align_children(old, new, &old.children[oi], &new.children[ni], memo);
    base = base.add(children_cost);
    memo.insert((oi, ni), base);
    base
}

fn align_children(
    old: &SimpleTree,
    new: &SimpleTree,
    old_children: &[usize],
    new_children: &[usize],
    memo: &mut HashMap<(usize, usize), EditCounts>,
) -> EditCounts {
    let m = old_children.len();
    let n = new_children.len();
    let mut dp = vec![EditCounts::zero(); (m + 1) * (n + 1)];

    let idx = |i: usize, j: usize, n: usize| -> usize { i * (n + 1) + j };

    for i in 1..=m {
        let del = delete_cost(old, old_children[i - 1]);
        let prev = dp[idx(i - 1, 0, n)];
        dp[idx(i, 0, n)] = prev.add(del);
    }
    for j in 1..=n {
        let ins = insert_cost(new, new_children[j - 1]);
        let prev = dp[idx(0, j - 1, n)];
        dp[idx(0, j, n)] = prev.add(ins);
    }

    for i in 1..=m {
        for j in 1..=n {
            let del = dp[idx(i - 1, j, n)].add(delete_cost(old, old_children[i - 1]));
            let ins = dp[idx(i, j - 1, n)].add(insert_cost(new, new_children[j - 1]));
            let sub = dp[idx(i - 1, j - 1, n)].add(tree_edit_counts_at(
                old,
                new,
                old_children[i - 1],
                new_children[j - 1],
                memo,
            ));
            dp[idx(i, j, n)] = better(better(del, ins), sub);
        }
    }

    dp[idx(m, n, n)]
}

fn delete_cost(tree: &SimpleTree, idx: usize) -> EditCounts {
    let size = tree.subtree_size[idx];
    EditCounts {
        cost: size,
        inserted: 0,
        deleted: size,
        updated: 0,
    }
}

fn insert_cost(tree: &SimpleTree, idx: usize) -> EditCounts {
    let size = tree.subtree_size[idx];
    EditCounts {
        cost: size,
        inserted: size,
        deleted: 0,
        updated: 0,
    }
}

pub(crate) fn approximate_counts(old: &SimpleTree, new: &SimpleTree) -> EditCounts {
    let old_count = old.labels.len() as u32;
    let new_count = new.labels.len() as u32;

    let mut old_hist: HashMap<u64, u32> = HashMap::new();
    for label in &old.labels {
        *old_hist.entry(*label).or_default() += 1;
    }

    let mut common: u32 = 0;
    for label in &new.labels {
        let Some(v) = old_hist.get_mut(label) else {
            continue;
        };
        if *v > 0 {
            *v -= 1;
            common += 1;
        }
    }

    let min_nodes = old_count.min(new_count);
    let updated = min_nodes.saturating_sub(common);
    let deleted = old_count.saturating_sub(min_nodes);
    let inserted = new_count.saturating_sub(min_nodes);

    EditCounts {
        cost: inserted + deleted + updated,
        inserted,
        deleted,
        updated,
    }
}

```

---

### File: `core\src\m_ast_diff\gumtree.rs`

```rust
use std::collections::HashMap;

use crate::diff::AstMoveHint;

use super::{FlatTree, move_hints_for_matches};

pub(crate) struct GumTreeResult {
    pub(crate) covered_old: Vec<bool>,
    pub(crate) covered_new: Vec<bool>,
    pub(crate) move_hints: Vec<AstMoveHint>,
}

#[derive(Clone, Copy)]
struct SubtreeMatch {
    old_idx: usize,
    new_idx: usize,
    size: u32,
}

pub(crate) fn match_unique_subtrees(
    old_tree: &FlatTree,
    new_tree: &FlatTree,
    min_move_size: u32,
) -> GumTreeResult {
    let candidates = unique_subtree_candidates(old_tree, new_tree);
    let mut matches: Vec<SubtreeMatch> = Vec::new();

    let mut covered_old = vec![false; old_tree.nodes.len()];
    let mut covered_new = vec![false; new_tree.nodes.len()];

    for cand in candidates {
        if range_overlaps(&covered_old, cand.old_idx, cand.size)
            || range_overlaps(&covered_new, cand.new_idx, cand.size)
        {
            continue;
        }
        mark_range(&mut covered_old, cand.old_idx, cand.size);
        mark_range(&mut covered_new, cand.new_idx, cand.size);
        matches.push(cand);
    }

    let match_pairs: Vec<(usize, usize)> =
        matches.iter().map(|m| (m.old_idx, m.new_idx)).collect();
    let move_hints = move_hints_for_matches(old_tree, new_tree, &match_pairs, min_move_size);

    GumTreeResult {
        covered_old,
        covered_new,
        move_hints,
    }
}

fn unique_subtree_candidates(old_tree: &FlatTree, new_tree: &FlatTree) -> Vec<SubtreeMatch> {
    let mut old_map: HashMap<u64, Vec<usize>> = HashMap::new();
    let mut new_map: HashMap<u64, Vec<usize>> = HashMap::new();

    for (i, n) in old_tree.nodes.iter().enumerate() {
        old_map.entry(n.subtree_hash).or_default().push(i);
    }
    for (i, n) in new_tree.nodes.iter().enumerate() {
        new_map.entry(n.subtree_hash).or_default().push(i);
    }

    let mut out = Vec::new();
    for (h, ois) in old_map {
        let Some(nis) = new_map.get(&h) else {
            continue;
        };
        if ois.len() == 1 && nis.len() == 1 {
            let oi = ois[0];
            let ni = nis[0];
            let size = old_tree.nodes[oi].subtree_size;
            out.push(SubtreeMatch {
                old_idx: oi,
                new_idx: ni,
                size,
            });
        }
    }

    out.sort_by(|a, b| b.size.cmp(&a.size));
    out
}

fn range_overlaps(covered: &[bool], start: usize, size: u32) -> bool {
    let end = start.saturating_add(size as usize);
    covered[start..end].iter().any(|v| *v)
}

fn mark_range(covered: &mut [bool], start: usize, size: u32) {
    let end = start.saturating_add(size as usize);
    for v in covered.iter_mut().take(end).skip(start) {
        *v = true;
    }
}

```

---

### File: `core\src\m_ast_diff\mod.rs`

```rust
use std::hash::{Hash, Hasher};

use crate::diff::{AstDiffMode, AstDiffSummary, AstMoveHint};
use crate::m_ast::{MExpr, MModuleAst};

mod apted;
mod gumtree;

const SMALL_AST_NODE_LIMIT: usize = 240;
const REDUCED_TED_MAX_NODES: usize = 320;
const MOVE_SUBTREE_MIN_SIZE: u32 = 6;

pub(crate) fn diff_summary(old_ast: &MModuleAst, new_ast: &MModuleAst) -> AstDiffSummary {
    let mut old_tree = build_flat_tree(old_ast);
    let mut new_tree = build_flat_tree(new_ast);

    if !old_tree.nodes.is_empty() {
        compute_subtree_hashes(0, &mut old_tree.nodes);
    }
    if !new_tree.nodes.is_empty() {
        compute_subtree_hashes(0, &mut new_tree.nodes);
    }

    let old_count = old_tree.nodes.len();
    let new_count = new_tree.nodes.len();

    if old_count.max(new_count) <= SMALL_AST_NODE_LIMIT {
        let old_simple = simple_tree_from_flat(&old_tree);
        let new_simple = simple_tree_from_flat(&new_tree);
        let counts = apted::tree_edit_counts(&old_simple, &new_simple);

        return AstDiffSummary {
            mode: AstDiffMode::SmallExact,
            node_count_old: old_count as u32,
            node_count_new: new_count as u32,
            inserted: counts.inserted,
            deleted: counts.deleted,
            updated: counts.updated,
            moved: 0,
            move_hints: Vec::new(),
        };
    }

    let gum = gumtree::match_unique_subtrees(&old_tree, &new_tree, MOVE_SUBTREE_MIN_SIZE);
    let reduced_old = build_reduced_tree(&old_tree, &gum.covered_old);
    let reduced_new = build_reduced_tree(&new_tree, &gum.covered_new);

    let counts = if reduced_old.labels.len().max(reduced_new.labels.len()) <= REDUCED_TED_MAX_NODES {
        apted::tree_edit_counts(&reduced_old, &reduced_new)
    } else {
        apted::approximate_counts(&reduced_old, &reduced_new)
    };

    AstDiffSummary {
        mode: AstDiffMode::LargeHeuristic,
        node_count_old: old_count as u32,
        node_count_new: new_count as u32,
        inserted: counts.inserted,
        deleted: counts.deleted,
        updated: counts.updated,
        moved: gum.move_hints.len() as u32,
        move_hints: gum.move_hints,
    }
}

#[derive(Clone)]
pub(crate) struct FlatNode {
    pub(crate) label: u64,
    pub(crate) parent: Option<usize>,
    pub(crate) child_index: u32,
    pub(crate) children: Vec<usize>,
    pub(crate) subtree_hash: u64,
    pub(crate) subtree_size: u32,
}

#[derive(Clone)]
pub(crate) struct FlatTree {
    pub(crate) nodes: Vec<FlatNode>,
}

fn build_flat_tree(ast: &MModuleAst) -> FlatTree {
    let mut nodes = Vec::new();
    let root = ast.root_expr();
    build_flat_tree_inner(root, None, 0, &mut nodes);
    FlatTree { nodes }
}

fn build_flat_tree_inner(
    expr: &MExpr,
    parent: Option<usize>,
    child_index: u32,
    nodes: &mut Vec<FlatNode>,
) -> usize {
    let idx = nodes.len();
    nodes.push(FlatNode {
        label: expr.diff_label_hash(),
        parent,
        child_index,
        children: Vec::new(),
        subtree_hash: 0,
        subtree_size: 0,
    });

    let children = expr.diff_children();
    for (i, child) in children.iter().enumerate() {
        let child_idx = build_flat_tree_inner(child, Some(idx), i as u32, nodes);
        nodes[idx].children.push(child_idx);
    }

    idx
}

fn compute_subtree_hashes(root: usize, nodes: &mut [FlatNode]) -> (u64, u32) {
    use crate::hashing::XXH64_SEED;
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    nodes[root].label.hash(&mut h);

    let mut size: u32 = 1;
    let children = nodes[root].children.clone();
    for c in children {
        let (ch, cs) = compute_subtree_hashes(c, nodes);
        ch.hash(&mut h);
        size += cs;
    }
    let hash = h.finish();
    nodes[root].subtree_hash = hash;
    nodes[root].subtree_size = size;
    (hash, size)
}

#[derive(Clone)]
pub(crate) struct SimpleTree {
    pub(crate) labels: Vec<u64>,
    pub(crate) children: Vec<Vec<usize>>,
    pub(crate) subtree_size: Vec<u32>,
}

fn simple_tree_from_flat(tree: &FlatTree) -> SimpleTree {
    let mut labels = Vec::with_capacity(tree.nodes.len());
    let mut children = Vec::with_capacity(tree.nodes.len());
    for node in &tree.nodes {
        labels.push(node.label);
        children.push(node.children.clone());
    }
    let mut out = SimpleTree {
        labels,
        children,
        subtree_size: vec![0; tree.nodes.len()],
    };
    if !out.labels.is_empty() {
        compute_simple_subtree_sizes(&mut out, 0);
    }
    out
}

fn build_reduced_tree(tree: &FlatTree, covered: &[bool]) -> SimpleTree {
    let mut labels = Vec::new();
    let mut children: Vec<Vec<usize>> = Vec::new();

    fn visit(
        idx: usize,
        tree: &FlatTree,
        covered: &[bool],
        parent_covered: bool,
        labels: &mut Vec<u64>,
        children: &mut Vec<Vec<usize>>,
    ) -> Option<usize> {
        let is_covered = covered.get(idx).copied().unwrap_or(false);
        if is_covered && !parent_covered {
            let new_idx = labels.len();
            labels.push(atom_label(tree.nodes[idx].subtree_hash));
            children.push(Vec::new());
            return Some(new_idx);
        }
        if is_covered && parent_covered {
            return None;
        }

        let new_idx = labels.len();
        labels.push(tree.nodes[idx].label);
        children.push(Vec::new());
        for &child in &tree.nodes[idx].children {
            if let Some(child_idx) = visit(child, tree, covered, is_covered, labels, children) {
                children[new_idx].push(child_idx);
            }
        }
        Some(new_idx)
    }

    let _root = visit(0, tree, covered, false, &mut labels, &mut children);

    let labels_len = labels.len();
    let mut out = SimpleTree {
        labels,
        children,
        subtree_size: vec![0; labels_len],
    };
    if !out.labels.is_empty() {
        compute_simple_subtree_sizes(&mut out, 0);
    }
    out
}

fn atom_label(subtree_hash: u64) -> u64 {
    use crate::hashing::XXH64_SEED;
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    0xA7u8.hash(&mut h);
    subtree_hash.hash(&mut h);
    h.finish()
}

fn compute_simple_subtree_sizes(tree: &mut SimpleTree, idx: usize) -> u32 {
    let mut size: u32 = 1;
    let children = tree.children[idx].clone();
    for child in children {
        size += compute_simple_subtree_sizes(tree, child);
    }
    tree.subtree_size[idx] = size;
    size
}

pub(crate) fn move_hints_for_matches(
    old_tree: &FlatTree,
    new_tree: &FlatTree,
    matches: &[(usize, usize)],
    min_move_size: u32,
) -> Vec<AstMoveHint> {
    let mut move_hints = Vec::new();
    for &(old_idx, new_idx) in matches {
        let old_node = &old_tree.nodes[old_idx];
        let new_node = &new_tree.nodes[new_idx];
        if old_node.subtree_size < min_move_size {
            continue;
        }
        let (Some(op), Some(np)) = (old_node.parent, new_node.parent) else {
            continue;
        };
        let old_parent_hash = old_tree.nodes[op].subtree_hash;
        let new_parent_hash = new_tree.nodes[np].subtree_hash;
        if old_parent_hash != new_parent_hash || old_node.child_index != new_node.child_index {
            move_hints.push(AstMoveHint {
                subtree_hash: old_node.subtree_hash,
                from_preorder: old_idx as u32,
                to_preorder: new_idx as u32,
                subtree_size: old_node.subtree_size,
            });
        }
    }
    move_hints
}

```

---

### File: `core\src\m_diff.rs`

```rust
use std::collections::{BTreeMap, BTreeSet, HashMap};
use std::hash::{Hash, Hasher};

use crate::config::{DiffConfig, SemanticNoisePolicy};
use crate::datamashup::{DataMashup, Query, build_embedded_queries, build_queries};
use crate::diff::{DiffOp, QueryChangeKind as DiffQueryChangeKind, QueryMetadataField};
use crate::diffable::{DiffContext, Diffable};
use crate::hashing::XXH64_SEED;
use crate::m_ast::{StepKind, canonicalize_m_ast, extract_steps, parse_m_expression};
use crate::matching::hungarian;
use crate::string_pool::{StringId, StringPool};
use crate::m_section::SectionParseError;

#[deprecated(note = "use WorkbookPackage::diff instead")]
#[cfg(any(test, feature = "dev-apis"))]
#[allow(dead_code)]
pub fn diff_m_queries(
    old_queries: &[Query],
    new_queries: &[Query],
    config: &DiffConfig,
) -> Vec<DiffOp> {
    crate::with_default_session(|session| {
        diff_queries_to_ops(old_queries, new_queries, &mut session.strings, config)
    })
}

fn hash64<T: Hash>(value: &T) -> u64 {
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    value.hash(&mut h);
    h.finish()
}

fn intern_bool(pool: &mut StringPool, v: bool) -> StringId {
    if v {
        pool.intern("true")
    } else {
        pool.intern("false")
    }
}

fn canonical_ast_and_hash(expr: &str) -> Option<u64> {
    let mut ast = parse_m_expression(expr).ok()?;
    canonicalize_m_ast(&mut ast);
    Some(hash64(&ast))
}

#[derive(Clone)]
struct QuerySignature {
    ast_hash: Option<u64>,
    expr_hash: u64,
    step_counts: Option<HashMap<u8, u32>>,
}

fn build_query_signature(query: &Query) -> QuerySignature {
    let ast_hash = canonical_ast_and_hash(&query.expression_m);
    let expr_hash = hash64(&query.expression_m);
    let step_counts = extract_steps(&query.expression_m).map(|pipeline| {
        let mut counts = HashMap::new();
        for step in &pipeline.steps {
            let tag = step_kind_tag(&step.kind);
            *counts.entry(tag).or_insert(0) += 1;
        }
        counts
    });

    QuerySignature {
        ast_hash,
        expr_hash,
        step_counts,
    }
}

fn step_kind_tag(kind: &StepKind) -> u8 {
    match kind {
        StepKind::TableSelectRows { .. } => 1,
        StepKind::TableRemoveColumns { .. } => 2,
        StepKind::TableRenameColumns { .. } => 3,
        StepKind::TableTransformColumnTypes { .. } => 4,
        StepKind::TableNestedJoin { .. } => 5,
        StepKind::TableJoin { .. } => 6,
        StepKind::Other { .. } => 7,
    }
}

fn multiset_similarity(a: &HashMap<u8, u32>, b: &HashMap<u8, u32>) -> f64 {
    let mut union = 0u32;
    let mut intersection = 0u32;

    for (k, av) in a {
        let av = *av;
        let bv = b.get(k).copied().unwrap_or(0);
        union += av.max(bv);
        intersection += av.min(bv);
    }
    for (k, bv) in b {
        if !a.contains_key(k) {
            union += *bv;
        }
    }

    if union == 0 {
        0.0
    } else {
        intersection as f64 / union as f64
    }
}

fn query_similarity(a: &QuerySignature, b: &QuerySignature) -> f64 {
    let ast_sim = match (a.ast_hash, b.ast_hash) {
        (Some(ha), Some(hb)) => {
            if ha == hb {
                1.0
            } else {
                0.0
            }
        }
        _ => {
            if a.expr_hash == b.expr_hash {
                1.0
            } else {
                0.0
            }
        }
    };

    let mut parts = vec![ast_sim];
    if let (Some(sa), Some(sb)) = (&a.step_counts, &b.step_counts) {
        parts.push(multiset_similarity(sa, sb));
    }

    if parts.is_empty() {
        0.0
    } else {
        parts.iter().sum::<f64>() / parts.len() as f64
    }
}

fn definition_change(
    old_expr: &str,
    new_expr: &str,
    enable_semantic: bool,
    noise_policy: SemanticNoisePolicy,
) -> Option<(DiffQueryChangeKind, u64, u64)> {
    if old_expr == new_expr {
        return None;
    }

    if enable_semantic {
        if let (Some(old_h), Some(new_h)) =
            (canonical_ast_and_hash(old_expr), canonical_ast_and_hash(new_expr))
        {
            let kind = if old_h == new_h {
                DiffQueryChangeKind::FormattingOnly
            } else {
                DiffQueryChangeKind::Semantic
            };
            if kind == DiffQueryChangeKind::FormattingOnly
                && matches!(noise_policy, SemanticNoisePolicy::SuppressFormattingOnly)
            {
                return None;
            }
            return Some((kind, old_h, new_h));
        }
    }

    let old_h = hash64(&old_expr);
    let new_h = hash64(&new_expr);
    Some((DiffQueryChangeKind::Semantic, old_h, new_h))
}

fn emit_metadata_diffs(
    pool: &mut StringPool,
    out: &mut Vec<DiffOp>,
    name: StringId,
    old_q: &Query,
    new_q: &Query,
) {
    if old_q.metadata.load_to_sheet != new_q.metadata.load_to_sheet {
        out.push(DiffOp::QueryMetadataChanged {
            name,
            field: QueryMetadataField::LoadToSheet,
            old: Some(intern_bool(pool, old_q.metadata.load_to_sheet)),
            new: Some(intern_bool(pool, new_q.metadata.load_to_sheet)),
        });
    }

    if old_q.metadata.load_to_model != new_q.metadata.load_to_model {
        out.push(DiffOp::QueryMetadataChanged {
            name,
            field: QueryMetadataField::LoadToModel,
            old: Some(intern_bool(pool, old_q.metadata.load_to_model)),
            new: Some(intern_bool(pool, new_q.metadata.load_to_model)),
        });
    }

    if old_q.metadata.is_connection_only != new_q.metadata.is_connection_only {
        out.push(DiffOp::QueryMetadataChanged {
            name,
            field: QueryMetadataField::ConnectionOnly,
            old: Some(intern_bool(pool, old_q.metadata.is_connection_only)),
            new: Some(intern_bool(pool, new_q.metadata.is_connection_only)),
        });
    }

    if old_q.metadata.group_path != new_q.metadata.group_path {
        let old = old_q.metadata.group_path.as_deref().map(|s| pool.intern(s));
        let new = new_q.metadata.group_path.as_deref().map(|s| pool.intern(s));
        out.push(DiffOp::QueryMetadataChanged {
            name,
            field: QueryMetadataField::GroupPath,
            old,
            new,
        });
    }
}

fn match_query_renames(
    old_only: &[&Query],
    new_only: &[&Query],
    similarity_threshold: f64,
) -> Vec<(usize, usize)> {
    if old_only.is_empty() || new_only.is_empty() {
        return Vec::new();
    }

    let old_sigs: Vec<QuerySignature> = old_only.iter().map(|q| build_query_signature(q)).collect();
    let new_sigs: Vec<QuerySignature> = new_only.iter().map(|q| build_query_signature(q)).collect();

    let rows = old_only.len();
    let cols = new_only.len();
    let size = rows.max(cols);
    let bias_scale = (size * size + 1) as i64;
    let cost_unit = 1000i64;
    let pad_cost = cost_unit.saturating_mul(bias_scale).saturating_mul(10);
    let reject_cost = pad_cost.saturating_add(bias_scale);

    let mut costs = vec![vec![pad_cost; cols]; rows];
    let mut sims = vec![vec![0.0f64; cols]; rows];

    for i in 0..rows {
        for j in 0..cols {
            let sim = query_similarity(&old_sigs[i], &new_sigs[j]);
            sims[i][j] = sim;
            if sim >= similarity_threshold {
                let base_cost = ((1.0 - sim) * cost_unit as f64).round() as i64;
                let bias = (i as i64).saturating_mul(size as i64).saturating_add(j as i64);
                costs[i][j] = base_cost.saturating_mul(bias_scale).saturating_add(bias);
            } else {
                costs[i][j] = reject_cost;
            }
        }
    }

    let assignment = hungarian::solve_rect(&costs, pad_cost);
    let mut matches = Vec::new();

    for i in 0..rows {
        let j = assignment.get(i).copied().unwrap_or(cols);
        if j < cols && sims[i][j] >= similarity_threshold {
            matches.push((i, j));
        }
    }

    matches
}

fn diff_queries_to_ops(
    old_queries: &[Query],
    new_queries: &[Query],
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Vec<DiffOp> {
    let mut old_by_name: BTreeMap<&str, &Query> = BTreeMap::new();
    let mut new_by_name: BTreeMap<&str, &Query> = BTreeMap::new();

    for q in old_queries {
        old_by_name.insert(q.name.as_str(), q);
    }
    for q in new_queries {
        new_by_name.insert(q.name.as_str(), q);
    }

    let old_only: Vec<&Query> = old_by_name
        .iter()
        .filter_map(|(name, q)| {
            if new_by_name.contains_key(*name) {
                None
            } else {
                Some(*q)
            }
        })
        .collect();

    let new_only: Vec<&Query> = new_by_name
        .iter()
        .filter_map(|(name, q)| {
            if old_by_name.contains_key(*name) {
                None
            } else {
                Some(*q)
            }
        })
        .collect();

    let mut renamed_old: BTreeSet<&str> = BTreeSet::new();
    let mut renamed_new: BTreeSet<&str> = BTreeSet::new();
    let mut rename_ops: Vec<(StringId, StringId, &Query, &Query)> = Vec::new();
    let rename_pairs = match_query_renames(
        &old_only,
        &new_only,
        config.moves.fuzzy_similarity_threshold,
    );
    for (old_idx, new_idx) in rename_pairs {
        let old_q = old_only[old_idx];
        let new_q = new_only[new_idx];
        let from = pool.intern(old_q.name.as_str());
        let to = pool.intern(new_q.name.as_str());
        renamed_old.insert(old_q.name.as_str());
        renamed_new.insert(new_q.name.as_str());
        rename_ops.push((from, to, old_q, new_q));
    }

    rename_ops.sort_by(|a, b| {
        let from_a = pool.resolve(a.0);
        let from_b = pool.resolve(b.0);
        from_a.cmp(from_b)
    });

    let mut ops: Vec<DiffOp> = Vec::new();

    for (from, to, old_q, new_q) in rename_ops {
        ops.push(DiffOp::QueryRenamed { from, to });
        if let Some((kind, old_h, new_h)) = definition_change(
            &old_q.expression_m,
            &new_q.expression_m,
            config.semantic.enable_m_semantic_diff,
            config.semantic.semantic_noise_policy,
        ) {
            let semantic_detail =
                if config.semantic.enable_m_semantic_diff && kind == DiffQueryChangeKind::Semantic {
                    crate::m_semantic_detail::build_query_semantic_detail(
                        &old_q.expression_m,
                        &new_q.expression_m,
                        pool,
                    )
                } else {
                    None
                };
            ops.push(DiffOp::QueryDefinitionChanged {
                name: to,
                change_kind: kind,
                old_hash: old_h,
                new_hash: new_h,
                semantic_detail,
            });
        }
        emit_metadata_diffs(pool, &mut ops, to, old_q, new_q);
    }

    let mut all_names: Vec<&str> = old_by_name
        .keys()
        .copied()
        .chain(new_by_name.keys().copied())
        .collect();
    all_names.sort();
    all_names.dedup();

    for name in all_names {
        if renamed_old.contains(name) || renamed_new.contains(name) {
            continue;
        }

        match (old_by_name.get(name), new_by_name.get(name)) {
            (None, Some(_new_q)) => {
                ops.push(DiffOp::QueryAdded {
                    name: pool.intern(name),
                });
            }
            (Some(_old_q), None) => {
                ops.push(DiffOp::QueryRemoved {
                    name: pool.intern(name),
                });
            }
            (Some(old_q), Some(new_q)) => {
                let name_id = pool.intern(name);

                if let Some((kind, old_h, new_h)) = definition_change(
                    &old_q.expression_m,
                    &new_q.expression_m,
                    config.semantic.enable_m_semantic_diff,
                    config.semantic.semantic_noise_policy,
                ) {
                    let semantic_detail =
                        if config.semantic.enable_m_semantic_diff
                            && kind == DiffQueryChangeKind::Semantic
                        {
                            crate::m_semantic_detail::build_query_semantic_detail(
                                &old_q.expression_m,
                                &new_q.expression_m,
                                pool,
                            )
                        } else {
                            None
                        };

                    ops.push(DiffOp::QueryDefinitionChanged {
                        name: name_id,
                        change_kind: kind,
                        old_hash: old_h,
                        new_hash: new_h,
                        semantic_detail,
                    });
                }

                emit_metadata_diffs(pool, &mut ops, name_id, old_q, new_q);
            }
            (None, None) => {}
        }
    }

    ops
}

fn build_all_queries(dm: &DataMashup) -> Result<Vec<Query>, SectionParseError> {
    let mut q = build_queries(dm)?;
    q.extend(build_embedded_queries(dm));
    Ok(q)
}

pub(crate) fn diff_m_ops_for_packages(
    old_dm: &Option<DataMashup>,
    new_dm: &Option<DataMashup>,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Vec<DiffOp> {
    match (old_dm.as_ref(), new_dm.as_ref()) {
        (None, None) => Vec::new(),
        (Some(old_dm), None) => {
            let old_q = match build_all_queries(old_dm) {
                Ok(v) => v,
                Err(_) => return Vec::new(),
            };
            let mut ops = Vec::new();
            for q in old_q {
                ops.push(DiffOp::QueryRemoved {
                    name: pool.intern(&q.name),
                });
            }
            ops
        }
        (None, Some(new_dm)) => {
            let new_q = match build_all_queries(new_dm) {
                Ok(v) => v,
                Err(_) => return Vec::new(),
            };
            let mut ops = Vec::new();
            for q in new_q {
                ops.push(DiffOp::QueryAdded {
                    name: pool.intern(&q.name),
                });
            }
            ops
        }
        (Some(old_dm), Some(new_dm)) => {
            let old_q = match build_all_queries(old_dm) {
                Ok(v) => v,
                Err(_) => return Vec::new(),
            };
            let new_q = match build_all_queries(new_dm) {
                Ok(v) => v,
                Err(_) => return Vec::new(),
            };
            diff_queries_to_ops(&old_q, &new_q, pool, config)
        }
    }
}

impl Diffable for Vec<Query> {
    type Output = Vec<DiffOp>;

    fn diff(&self, other: &Self, ctx: &mut DiffContext<'_>) -> Vec<DiffOp> {
        diff_queries_to_ops(self, other, ctx.pool, ctx.config)
    }
}

```

---

### File: `core\src\m_section.rs`

```rust
use std::str::Lines;

#[derive(Debug, thiserror::Error, PartialEq, Eq)]
pub enum SectionParseError {
    #[error("missing section header")]
    MissingSectionHeader,
    #[error("invalid section header")]
    InvalidHeader,
    #[error("invalid member syntax")]
    InvalidMemberSyntax,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct SectionMember {
    pub section_name: String,
    pub member_name: String,
    pub expression_m: String,
    pub is_shared: bool,
}

pub fn parse_section_members(source: &str) -> Result<Vec<SectionMember>, SectionParseError> {
    let source = strip_leading_bom(source);
    let mut lines = source.lines();
    let section_name = find_section_name(&mut lines)?;

    let mut members = Vec::new();
    while let Some(line) = lines.next() {
        let trimmed = line.trim();
        if trimmed.is_empty() || trimmed.starts_with("//") {
            continue;
        }

        if !trimmed.starts_with("shared") {
            continue;
        }

        let member = parse_shared_member(trimmed, &mut lines, &section_name)
            .ok_or(SectionParseError::InvalidMemberSyntax)?;
        members.push(member);
    }

    Ok(members)
}

fn find_section_name(lines: &mut Lines<'_>) -> Result<String, SectionParseError> {
    for line in lines.by_ref() {
        let trimmed = line.trim();
        if trimmed.is_empty() || trimmed.starts_with("//") {
            continue;
        }

        match try_parse_section_header(trimmed) {
            Ok(Some(name)) => return Ok(name),
            Ok(None) => continue,
            Err(err) => return Err(err),
        }
    }

    Err(SectionParseError::MissingSectionHeader)
}

fn try_parse_section_header(line: &str) -> Result<Option<String>, SectionParseError> {
    let Some(rest) = line.strip_prefix("section") else {
        return Ok(None);
    };

    if !rest.starts_with(char::is_whitespace) && !rest.is_empty() {
        return Err(SectionParseError::InvalidHeader);
    }

    let header_body = rest.trim_start();
    if !header_body.ends_with(';') {
        return Err(SectionParseError::InvalidHeader);
    }

    let without_semicolon = &header_body[..header_body.len() - 1];
    let name_candidate = without_semicolon.trim();
    if name_candidate.is_empty() {
        return Err(SectionParseError::InvalidHeader);
    }

    let mut parts = name_candidate.split_whitespace();
    let name = parts.next().ok_or(SectionParseError::InvalidHeader)?;
    if parts.next().is_some() {
        return Err(SectionParseError::InvalidHeader);
    }

    if !is_valid_identifier(name) {
        return Err(SectionParseError::InvalidHeader);
    }

    Ok(Some(name.to_string()))
}

fn parse_shared_member(
    line: &str,
    remaining_lines: &mut Lines<'_>,
    section_name: &str,
) -> Option<SectionMember> {
    let rest = line.strip_prefix("shared")?;
    if !rest.starts_with(char::is_whitespace) && !rest.is_empty() {
        return None;
    }

    let body = rest.trim_start();
    if body.is_empty() {
        return None;
    }

    let (member_name, after_name) = parse_identifier(body)?;

    let mut expression_source = after_name;
    let eq_index = expression_source.find('=')?;
    if !expression_source[..eq_index].trim().is_empty() {
        return None;
    }
    expression_source = &expression_source[eq_index + 1..];

    let mut expression = expression_source.to_string();
    if let Some(idx) = expression_source.find(';') {
        expression.truncate(idx);
    } else {
        let mut terminator_index = None;
        while terminator_index.is_none() {
            let Some(next_line) = remaining_lines.next() else {
                break;
            };

            expression.push('\n');
            let offset = expression.len();
            expression.push_str(next_line);
            if let Some(idx) = next_line.find(';') {
                terminator_index = Some(offset + idx);
            }
        }

        if let Some(idx) = terminator_index {
            expression.truncate(idx);
        } else {
            return None;
        }
    }

    let expression_m = expression.trim().to_string();

    Some(SectionMember {
        section_name: section_name.to_string(),
        member_name: member_name.to_string(),
        expression_m,
        is_shared: true,
    })
}

fn parse_identifier(text: &str) -> Option<(String, &str)> {
    let trimmed = text.trim_start();
    if trimmed.is_empty() {
        return None;
    }

    if trimmed.starts_with("#\"") {
        return parse_quoted_identifier(trimmed);
    }

    parse_unquoted_identifier(trimmed)
}

fn parse_unquoted_identifier(text: &str) -> Option<(String, &str)> {
    if text.is_empty() {
        return None;
    }

    let mut end = 0;
    for ch in text.chars() {
        if ch.is_whitespace() || ch == '=' {
            break;
        }
        end += ch.len_utf8();
    }

    if end == 0 {
        return None;
    }

    let (name, rest) = text.split_at(end);
    if !is_valid_identifier(name) {
        return None;
    }

    Some((name.to_string(), rest))
}

fn parse_quoted_identifier(text: &str) -> Option<(String, &str)> {
    let mut chars = text.char_indices();
    let (_, hash) = chars.next()?;
    if hash != '#' {
        return None;
    }
    if !matches!(chars.next(), Some((_, '"'))) {
        return None;
    }

    let mut name = String::new();
    while let Some((idx, ch)) = chars.next() {
        if ch == '"' {
            if let Some((_, next_ch)) = chars.clone().next()
                && next_ch == '"'
            {
                name.push('"');
                chars.next();
                continue;
            }
            let rest_start = idx + 1;
            let rest = &text[rest_start..];
            return Some((name, rest));
        }

        name.push(ch);
    }

    None
}

fn is_valid_identifier(name: &str) -> bool {
    !name.is_empty() && name.chars().all(|c| c.is_ascii_alphanumeric() || c == '_')
}

fn strip_leading_bom(text: &str) -> &str {
    text.strip_prefix('\u{FEFF}').unwrap_or(text)
}

```

---

### File: `core\src\m_semantic_detail.rs`

```rust
use std::collections::{HashMap, HashSet};
use std::hash::{Hash, Hasher};

use crate::diff::{
    ColumnTypeChange, ExtractedColumnTypeChanges, ExtractedRenamePairs, ExtractedString,
    ExtractedStringList, QuerySemanticDetail, RenamePair, StepChange, StepDiff, StepParams,
    StepSnapshot, StepType,
};
#[cfg(test)]
use crate::diff::AstDiffMode;
use crate::m_ast::{
    canonicalize_m_ast, extract_steps, parse_m_expression, MStep, StepColumnTypeChange,
    StepExtracted, StepKind, StepRenamePair,
};
use crate::m_ast_diff;
use crate::string_pool::{StringId, StringPool};

pub(crate) fn build_query_semantic_detail(
    old_expr: &str,
    new_expr: &str,
    pool: &mut StringPool,
) -> Option<QuerySemanticDetail> {
    let mut detail = QuerySemanticDetail {
        step_diffs: Vec::new(),
        ast_summary: None,
    };

    let old_steps = extract_steps(old_expr);
    let new_steps = extract_steps(new_expr);

    if let (Some(oldp), Some(newp)) = (old_steps, new_steps) {
        detail.step_diffs = diff_step_pipelines(&oldp.steps, &newp.steps, pool);
        if !detail.step_diffs.is_empty() {
            return Some(detail);
        }
    }

    let mut old_ast = parse_m_expression(old_expr).ok()?;
    let mut new_ast = parse_m_expression(new_expr).ok()?;
    canonicalize_m_ast(&mut old_ast);
    canonicalize_m_ast(&mut new_ast);

    detail.ast_summary = Some(m_ast_diff::diff_summary(&old_ast, &new_ast));
    Some(detail)
}

fn diff_step_pipelines(
    old_steps: &[MStep],
    new_steps: &[MStep],
    pool: &mut StringPool,
) -> Vec<StepDiff> {
    let matches = align_steps(old_steps, new_steps);

    let mut out = Vec::new();

    let mut matched_old: HashSet<usize> = HashSet::new();
    let mut matched_new: HashSet<usize> = HashSet::new();
    for (oi, ni) in &matches {
        matched_old.insert(*oi);
        matched_new.insert(*ni);
    }

    for (oi, s) in old_steps.iter().enumerate() {
        if matched_old.contains(&oi) {
            continue;
        }
        out.push(StepDiff::StepRemoved {
            step: snapshot_step(s, oi as u32, pool),
        });
    }

    for (ni, s) in new_steps.iter().enumerate() {
        if matched_new.contains(&ni) {
            continue;
        }
        out.push(StepDiff::StepAdded {
            step: snapshot_step(s, ni as u32, pool),
        });
    }

    for (oi, ni) in matches {
        let a = &old_steps[oi];
        let b = &new_steps[ni];

        let renamed = a.name != b.name;
        let reordered = oi != ni;

        let params_a = step_params(&a.kind, pool);
        let params_b = step_params(&b.kind, pool);

        let mut changes = Vec::new();
        if renamed {
            changes.push(StepChange::Renamed {
                from: pool.intern(&a.name),
                to: pool.intern(&b.name),
            });
        }

        let (src_removed, src_added) = diff_string_sets(&a.source_refs, &b.source_refs, pool);
        let has_source_change = !src_removed.is_empty() || !src_added.is_empty();
        if has_source_change {
            changes.push(StepChange::SourceRefsChanged {
                removed: src_removed,
                added: src_added,
            });
        }

        let same_sig = a.signature == b.signature;
        if !same_sig || params_a != params_b {
            changes.push(StepChange::ParamsChanged);
        }

        if renamed || !same_sig || has_source_change || params_a != params_b {
            out.push(StepDiff::StepModified {
                before: snapshot_step_with_params(a, oi as u32, params_a, pool),
                after: snapshot_step_with_params(b, ni as u32, params_b, pool),
                changes,
            });
        } else if reordered {
            out.push(StepDiff::StepReordered {
                name: pool.intern(&a.name),
                from_index: oi as u32,
                to_index: ni as u32,
            });
        }
    }

    out.sort_by_key(step_diff_sort_key);
    out
}

fn step_diff_sort_key(d: &StepDiff) -> u32 {
    match d {
        StepDiff::StepAdded { step } => step.index,
        StepDiff::StepRemoved { step } => step.index,
        StepDiff::StepReordered { to_index, .. } => *to_index,
        StepDiff::StepModified { after, .. } => after.index,
    }
}

fn align_steps(old_steps: &[MStep], new_steps: &[MStep]) -> Vec<(usize, usize)> {
    let mut matches = align_steps_dp(old_steps, new_steps);

    let mut matched_old = vec![false; old_steps.len()];
    let mut matched_new = vec![false; new_steps.len()];
    for (oi, ni) in &matches {
        matched_old[*oi] = true;
        matched_new[*ni] = true;
    }

    let mut extra = match_unordered_steps(old_steps, new_steps, &matched_old, &matched_new);
    for (oi, ni) in &extra {
        matched_old[*oi] = true;
        matched_new[*ni] = true;
    }

    matches.append(&mut extra);
    matches.sort_by_key(|(oi, _)| *oi);
    matches
}

#[derive(Clone, Copy)]
enum AlignOp {
    Match,
    Insert,
    Delete,
    None,
}

#[derive(Clone, Copy)]
struct AlignCell {
    cost: u32,
    matches: u32,
    op: AlignOp,
}

fn align_steps_dp(old_steps: &[MStep], new_steps: &[MStep]) -> Vec<(usize, usize)> {
    let m = old_steps.len();
    let n = new_steps.len();
    if m == 0 || n == 0 {
        return Vec::new();
    }

    const COST_INSERT: u32 = 1;
    const COST_DELETE: u32 = 1;

    let idx = |i: usize, j: usize, n: usize| -> usize { i * (n + 1) + j };
    let mut dp = vec![
        AlignCell {
            cost: 0,
            matches: 0,
            op: AlignOp::None,
        };
        (m + 1) * (n + 1)
    ];

    for i in 1..=m {
        let prev = dp[idx(i - 1, 0, n)];
        dp[idx(i, 0, n)] = AlignCell {
            cost: prev.cost + COST_DELETE,
            matches: prev.matches,
            op: AlignOp::Delete,
        };
    }
    for j in 1..=n {
        let prev = dp[idx(0, j - 1, n)];
        dp[idx(0, j, n)] = AlignCell {
            cost: prev.cost + COST_INSERT,
            matches: prev.matches,
            op: AlignOp::Insert,
        };
    }

    for i in 1..=m {
        for j in 1..=n {
            let mut best = dp[idx(i - 1, j, n)];
            best.cost = best.cost.saturating_add(COST_DELETE);
            best.op = AlignOp::Delete;

            let ins = {
                let prev = dp[idx(i, j - 1, n)];
                AlignCell {
                    cost: prev.cost.saturating_add(COST_INSERT),
                    matches: prev.matches,
                    op: AlignOp::Insert,
                }
            };
            if is_better(ins, best) {
                best = ins;
            }

            if let Some(match_cost) = step_match_cost(&old_steps[i - 1], &new_steps[j - 1]) {
                let prev = dp[idx(i - 1, j - 1, n)];
                let mat = AlignCell {
                    cost: prev.cost.saturating_add(match_cost),
                    matches: prev.matches.saturating_add(1),
                    op: AlignOp::Match,
                };
                if is_better(mat, best) {
                    best = mat;
                }
            }

            dp[idx(i, j, n)] = best;
        }
    }

    let mut out = Vec::new();
    let mut i = m;
    let mut j = n;
    while i > 0 || j > 0 {
        match dp[idx(i, j, n)].op {
            AlignOp::Match => {
                out.push((i - 1, j - 1));
                i -= 1;
                j -= 1;
            }
            AlignOp::Delete => {
                i = i.saturating_sub(1);
            }
            AlignOp::Insert => {
                j = j.saturating_sub(1);
            }
            AlignOp::None => break,
        }
    }

    out.reverse();
    out
}

fn is_better(a: AlignCell, b: AlignCell) -> bool {
    if a.cost != b.cost {
        return a.cost < b.cost;
    }
    if a.matches != b.matches {
        return a.matches > b.matches;
    }
    align_op_priority(a.op) > align_op_priority(b.op)
}

fn align_op_priority(op: AlignOp) -> u8 {
    match op {
        AlignOp::Match => 2,
        AlignOp::Delete => 1,
        AlignOp::Insert => 0,
        AlignOp::None => 0,
    }
}

fn match_unordered_steps(
    old_steps: &[MStep],
    new_steps: &[MStep],
    matched_old: &[bool],
    matched_new: &[bool],
) -> Vec<(usize, usize)> {
    let mut extra = Vec::new();

    let mut sig_old: HashMap<u64, Vec<usize>> = HashMap::new();
    let mut sig_new: HashMap<u64, Vec<usize>> = HashMap::new();

    for (idx, step) in old_steps.iter().enumerate() {
        if matched_old.get(idx).copied().unwrap_or(false) {
            continue;
        }
        sig_old.entry(step.signature).or_default().push(idx);
    }
    for (idx, step) in new_steps.iter().enumerate() {
        if matched_new.get(idx).copied().unwrap_or(false) {
            continue;
        }
        sig_new.entry(step.signature).or_default().push(idx);
    }

    let mut used_old = HashSet::new();
    let mut used_new = HashSet::new();

    for (sig, old_idxs) in &sig_old {
        let Some(new_idxs) = sig_new.get(sig) else {
            continue;
        };
        if old_idxs.len() == 1 && new_idxs.len() == 1 {
            let oi = old_idxs[0];
            let ni = new_idxs[0];
            extra.push((oi, ni));
            used_old.insert(oi);
            used_new.insert(ni);
        }
    }

    let mut name_old: HashMap<&str, Vec<usize>> = HashMap::new();
    let mut name_new: HashMap<&str, Vec<usize>> = HashMap::new();

    for (idx, step) in old_steps.iter().enumerate() {
        if matched_old.get(idx).copied().unwrap_or(false) || used_old.contains(&idx) {
            continue;
        }
        name_old.entry(step.name.as_str()).or_default().push(idx);
    }
    for (idx, step) in new_steps.iter().enumerate() {
        if matched_new.get(idx).copied().unwrap_or(false) || used_new.contains(&idx) {
            continue;
        }
        name_new.entry(step.name.as_str()).or_default().push(idx);
    }

    for (name, old_idxs) in name_old {
        let Some(new_idxs) = name_new.get(name) else {
            continue;
        };
        if old_idxs.len() == 1 && new_idxs.len() == 1 {
            extra.push((old_idxs[0], new_idxs[0]));
        }
    }

    extra
}

fn step_match_cost(a: &MStep, b: &MStep) -> Option<u32> {
    let kind_a = step_kind_id(&a.kind);
    let kind_b = step_kind_id(&b.kind);

    if kind_a == kind_b {
        let key_a = step_key_hash(&a.kind);
        let key_b = step_key_hash(&b.kind);
        if key_a == key_b {
            return Some(0);
        }
        return Some(1);
    }

    if a.name == b.name {
        return Some(2);
    }

    None
}

fn step_kind_id(kind: &StepKind) -> u8 {
    match kind {
        StepKind::TableSelectRows { .. } => 1,
        StepKind::TableRemoveColumns { .. } => 2,
        StepKind::TableRenameColumns { .. } => 3,
        StepKind::TableTransformColumnTypes { .. } => 4,
        StepKind::TableNestedJoin { .. } => 5,
        StepKind::TableJoin { .. } => 6,
        StepKind::Other { .. } => 7,
    }
}

fn step_key_hash(kind: &StepKind) -> u64 {
    let mut h = xxhash_rust::xxh64::Xxh64::new(crate::hashing::XXH64_SEED);
    step_kind_id(kind).hash(&mut h);
    match kind {
        StepKind::TableSelectRows { predicate_hash, .. } => {
            predicate_hash.hash(&mut h);
        }
        StepKind::TableRemoveColumns { columns, .. } => {
            columns.hash(&mut h);
        }
        StepKind::TableRenameColumns { renames, .. } => {
            renames.hash(&mut h);
        }
        StepKind::TableTransformColumnTypes { transforms, .. } => {
            transforms.hash(&mut h);
        }
        StepKind::TableNestedJoin {
            left_keys,
            right_keys,
            new_column,
            join_kind_hash,
            ..
        } => {
            left_keys.hash(&mut h);
            right_keys.hash(&mut h);
            new_column.hash(&mut h);
            join_kind_hash.hash(&mut h);
        }
        StepKind::TableJoin {
            left_keys,
            right_keys,
            join_kind_hash,
            ..
        } => {
            left_keys.hash(&mut h);
            right_keys.hash(&mut h);
            join_kind_hash.hash(&mut h);
        }
        StepKind::Other {
            function_name_hash,
            arity,
            ..
        } => {
            function_name_hash.hash(&mut h);
            arity.hash(&mut h);
        }
    }
    h.finish()
}

fn snapshot_step(s: &MStep, index: u32, pool: &mut StringPool) -> StepSnapshot {
    snapshot_step_with_params(s, index, step_params(&s.kind, pool), pool)
}

fn snapshot_step_with_params(
    s: &MStep,
    index: u32,
    params: Option<StepParams>,
    pool: &mut StringPool,
) -> StepSnapshot {
    let name = pool.intern(&s.name);
    let step_type = step_type(&s.kind);
    let mut source_refs = Vec::with_capacity(s.source_refs.len());
    for r in &s.source_refs {
        source_refs.push(pool.intern(r));
    }

    StepSnapshot {
        name,
        index,
        step_type,
        source_refs,
        params,
        signature: Some(s.signature),
    }
}

fn step_type(k: &StepKind) -> StepType {
    match k {
        StepKind::TableSelectRows { .. } => StepType::TableSelectRows,
        StepKind::TableRemoveColumns { .. } => StepType::TableRemoveColumns,
        StepKind::TableRenameColumns { .. } => StepType::TableRenameColumns,
        StepKind::TableTransformColumnTypes { .. } => StepType::TableTransformColumnTypes,
        StepKind::TableNestedJoin { .. } => StepType::TableNestedJoin,
        StepKind::TableJoin { .. } => StepType::TableJoin,
        StepKind::Other { .. } => StepType::Other,
    }
}

fn step_params(k: &StepKind, pool: &mut StringPool) -> Option<StepParams> {
    match k {
        StepKind::TableSelectRows { predicate_hash, .. } => Some(StepParams::TableSelectRows {
            predicate_hash: *predicate_hash,
        }),
        StepKind::TableRemoveColumns { columns, .. } => Some(StepParams::TableRemoveColumns {
            columns: map_string_list(columns, pool),
        }),
        StepKind::TableRenameColumns { renames, .. } => Some(StepParams::TableRenameColumns {
            renames: map_rename_pairs(renames, pool),
        }),
        StepKind::TableTransformColumnTypes { transforms, .. } => {
            Some(StepParams::TableTransformColumnTypes {
                transforms: map_column_type_changes(transforms, pool),
            })
        }
        StepKind::TableNestedJoin {
            left_keys,
            right_keys,
            new_column,
            join_kind_hash,
            ..
        } => Some(StepParams::TableNestedJoin {
            left_keys: map_string_list(left_keys, pool),
            right_keys: map_string_list(right_keys, pool),
            new_column: map_string(new_column, pool),
            join_kind_hash: *join_kind_hash,
        }),
        StepKind::TableJoin {
            left_keys,
            right_keys,
            join_kind_hash,
            ..
        } => Some(StepParams::TableJoin {
            left_keys: map_string_list(left_keys, pool),
            right_keys: map_string_list(right_keys, pool),
            join_kind_hash: *join_kind_hash,
        }),
        StepKind::Other {
            function_name_hash,
            arity,
            expr_hash,
        } => Some(StepParams::Other {
            function_name_hash: *function_name_hash,
            arity: arity.map(|v| v as u32),
            expr_hash: *expr_hash,
        }),
    }
}

fn map_string(v: &StepExtracted<String>, pool: &mut StringPool) -> ExtractedString {
    match v {
        StepExtracted::Known(value) => ExtractedString::Known {
            value: pool.intern(value),
        },
        StepExtracted::Unknown { hash } => ExtractedString::Unknown { hash: *hash },
    }
}

fn map_string_list(v: &StepExtracted<Vec<String>>, pool: &mut StringPool) -> ExtractedStringList {
    match v {
        StepExtracted::Known(values) => ExtractedStringList::Known {
            values: values.iter().map(|value| pool.intern(value)).collect(),
        },
        StepExtracted::Unknown { hash } => ExtractedStringList::Unknown { hash: *hash },
    }
}

fn map_rename_pairs(
    v: &StepExtracted<Vec<StepRenamePair>>,
    pool: &mut StringPool,
) -> ExtractedRenamePairs {
    match v {
        StepExtracted::Known(pairs) => ExtractedRenamePairs::Known {
            pairs: pairs
                .iter()
                .map(|p| RenamePair {
                    from: pool.intern(&p.from),
                    to: pool.intern(&p.to),
                })
                .collect(),
        },
        StepExtracted::Unknown { hash } => ExtractedRenamePairs::Unknown { hash: *hash },
    }
}

fn map_column_type_changes(
    v: &StepExtracted<Vec<StepColumnTypeChange>>,
    pool: &mut StringPool,
) -> ExtractedColumnTypeChanges {
    match v {
        StepExtracted::Known(changes) => ExtractedColumnTypeChanges::Known {
            changes: changes
                .iter()
                .map(|c| ColumnTypeChange {
                    column: pool.intern(&c.column),
                    ty_hash: c.ty_hash,
                })
                .collect(),
        },
        StepExtracted::Unknown { hash } => ExtractedColumnTypeChanges::Unknown { hash: *hash },
    }
}

fn diff_string_sets(
    old: &[String],
    new: &[String],
    pool: &mut StringPool,
) -> (Vec<StringId>, Vec<StringId>) {
    let old_set: HashSet<&str> = old.iter().map(|v| v.as_str()).collect();
    let new_set: HashSet<&str> = new.iter().map(|v| v.as_str()).collect();

    let removed = old
        .iter()
        .filter(|v| !new_set.contains(v.as_str()))
        .map(|v| pool.intern(v))
        .collect();
    let added = new
        .iter()
        .filter(|v| !old_set.contains(v.as_str()))
        .map(|v| pool.intern(v))
        .collect();

    (removed, added)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::string_pool::StringPool;

    fn detail_for(old_expr: &str, new_expr: &str) -> QuerySemanticDetail {
        let mut pool = StringPool::default();
        build_query_semantic_detail(old_expr, new_expr, &mut pool).expect("detail")
    }

    #[test]
    fn step_param_change_produces_modified_step() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"A", "B"})
            in
                #"Removed Columns"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"A", "C"})
            in
                #"Removed Columns"
        "#;

        let detail = detail_for(old_expr, new_expr);
        assert_eq!(detail.step_diffs.len(), 1);
        match &detail.step_diffs[0] {
            StepDiff::StepModified { after, .. } => {
                assert_eq!(after.step_type, StepType::TableRemoveColumns);
            }
            other => panic!("expected StepModified, got {:?}", other),
        }
    }

    #[test]
    fn step_rename_produces_modified_with_rename_change() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"A", "B"}),
                #"Changed Type" = Table.TransformColumnTypes(#"Removed Columns", {{"C", type text}})
            in
                #"Changed Type"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Dropped Columns" = Table.RemoveColumns(Source, {"A", "B"}),
                #"Changed Type" = Table.TransformColumnTypes(#"Dropped Columns", {{"C", type text}})
            in
                #"Changed Type"
        "#;

        let detail = detail_for(old_expr, new_expr);
        let mut has_rename = false;
        for diff in &detail.step_diffs {
            if let StepDiff::StepModified { changes, .. } = diff {
                if changes.iter().any(|c| matches!(c, StepChange::Renamed { .. })) {
                    has_rename = true;
                }
            }
        }
        assert!(has_rename, "expected StepChange::Renamed");
    }

    #[test]
    fn dependency_change_sets_source_refs_changed() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [A] > 0),
                #"Removed Columns" = Table.RemoveColumns(#"Filtered Rows", {"A"})
            in
                #"Removed Columns"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [A] > 0),
                #"Removed Columns" = Table.RemoveColumns(Source, {"A"})
            in
                #"Removed Columns"
        "#;

        let detail = detail_for(old_expr, new_expr);
        let mut has_source_change = false;
        for diff in &detail.step_diffs {
            if let StepDiff::StepModified { changes, .. } = diff {
                if changes
                    .iter()
                    .any(|c| matches!(c, StepChange::SourceRefsChanged { .. }))
                {
                    has_source_change = true;
                }
            }
        }
        assert!(has_source_change, "expected SourceRefsChanged");
    }

    #[test]
    fn reorder_produces_step_reordered() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [A] > 0),
                #"Removed Columns" = Table.RemoveColumns(Source, {"B"})
            in
                #"Removed Columns"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Removed Columns" = Table.RemoveColumns(Source, {"B"}),
                #"Filtered Rows" = Table.SelectRows(Source, each [A] > 0)
            in
                #"Removed Columns"
        "#;

        let detail = detail_for(old_expr, new_expr);
        let reordered: Vec<_> = detail
            .step_diffs
            .iter()
            .filter(|d| matches!(d, StepDiff::StepReordered { .. }))
            .collect();
        assert_eq!(reordered.len(), 2);
    }

    #[test]
    fn filter_change_reports_params_changed() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [Amount] > 0)
            in
                #"Filtered Rows"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Filtered Rows" = Table.SelectRows(Source, each [Amount] > 10)
            in
                #"Filtered Rows"
        "#;

        let detail = detail_for(old_expr, new_expr);
        assert_eq!(detail.step_diffs.len(), 1);
        match &detail.step_diffs[0] {
            StepDiff::StepModified { after, changes, .. } => {
                assert_eq!(after.step_type, StepType::TableSelectRows);
                assert!(changes.iter().any(|c| matches!(c, StepChange::ParamsChanged)));
            }
            other => panic!("expected StepModified, got {:?}", other),
        }
    }

    #[test]
    fn join_change_reports_params_changed() {
        let old_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Joined" = Table.Join(Source, {"A"}, Source, {"A"}, JoinKind.Inner)
            in
                #"Joined"
        "#;
        let new_expr = r#"
            let
                Source = Excel.CurrentWorkbook(){[Name="Table1"]}[Content],
                #"Joined" = Table.Join(Source, {"B"}, Source, {"B"}, JoinKind.Inner)
            in
                #"Joined"
        "#;

        let detail = detail_for(old_expr, new_expr);
        assert_eq!(detail.step_diffs.len(), 1);
        match &detail.step_diffs[0] {
            StepDiff::StepModified { after, changes, .. } => {
                assert_eq!(after.step_type, StepType::TableJoin);
                assert!(changes.iter().any(|c| matches!(c, StepChange::ParamsChanged)));
            }
            other => panic!("expected StepModified, got {:?}", other),
        }
    }

    #[test]
    fn small_exact_ast_summary_handles_deep_if_change() {
        fn nested_if(depth: usize, leaf: &str) -> String {
            let mut expr = leaf.to_string();
            for i in (1..=depth).rev() {
                expr = format!("if c{} then ({}) else 0", i, expr);
            }
            expr
        }

        let old_expr = nested_if(8, "1");
        let new_expr = nested_if(8, "2");

        let detail = detail_for(&old_expr, &new_expr);
        let summary = detail.ast_summary.expect("ast summary");
        assert_eq!(summary.mode, AstDiffMode::SmallExact);
        assert!(summary.updated > 0);
        assert_eq!(summary.moved, 0);
    }

    #[test]
    fn large_ast_summary_reports_moved_subtree() {
        fn record(prefix: &str, count: usize) -> String {
            let mut fields = Vec::with_capacity(count);
            for i in 0..count {
                fields.push(format!(r#"{prefix}{i} = "{prefix}{i}""#));
            }
            format!("[{}]", fields.join(", "))
        }

        let big = record("Big", 260);
        let small = record("Small", 2);

        let old_expr = format!("if x then {} else {}", big, small);
        let new_expr = format!("if x then {} else {}", small, big);

        let detail = detail_for(&old_expr, &new_expr);
        let summary = detail.ast_summary.expect("ast summary");
        assert_eq!(summary.mode, AstDiffMode::LargeHeuristic);
        assert!(summary.moved >= 1);
        assert!(
            summary.move_hints.iter().any(|mh| mh.subtree_size > 50),
            "expected a large moved subtree hint"
        );
    }

    #[test]
    fn wrap_unwrap_reports_structural_insert() {
        let old_expr = "1";
        let new_expr = "if cond then 1 else 0";

        let detail = detail_for(old_expr, new_expr);
        let summary = detail.ast_summary.expect("ast summary");
        assert!(summary.inserted > 0, "expected inserted nodes for wrap");
    }
}

```

---

### File: `core\src\matching\hungarian.rs`

```rust
//! Hungarian assignment wrapper for rectangular cost matrices.
//!
//! Uses the dense Hungarian implementation in `alignment::lap` and pads to a square matrix.

pub(crate) fn solve_rect(costs: &[Vec<i64>], pad_cost: i64) -> Vec<usize> {
    let rows = costs.len();
    let cols = costs.iter().map(|row| row.len()).max().unwrap_or(0);
    let size = rows.max(cols);

    if size == 0 {
        return Vec::new();
    }

    let mut square = vec![vec![pad_cost; size]; size];
    for (i, row) in costs.iter().enumerate() {
        for (j, &cost) in row.iter().enumerate() {
            square[i][j] = cost;
        }
    }

    crate::alignment::lap::solve(&square)
}

```

---

### File: `core\src\matching\mod.rs`

```rust
pub(crate) mod hungarian;

```

---

### File: `core\src\memory_estimate.rs`

```rust
use crate::grid_view::{ColMeta, RowMeta, RowView};
use crate::workbook::{Cell, Grid};
use std::mem::size_of;

pub(crate) fn estimate_gridview_bytes(grid: &Grid) -> u64 {
    let nrows = grid.nrows as u64;
    let ncols = grid.ncols as u64;
    let cell_count = grid.cell_count() as u64;

    let row_view_bytes = nrows.saturating_mul(size_of::<RowView<'static>>() as u64);
    let row_meta_bytes = nrows.saturating_mul(size_of::<RowMeta>() as u64);
    let col_meta_bytes = ncols.saturating_mul(size_of::<ColMeta>() as u64);

    let cell_entry_bytes = cell_count.saturating_mul(size_of::<(u32, &'static Cell)>() as u64);

    let build_row_counts_bytes = nrows
        .saturating_mul(size_of::<u32>() as u64)
        .saturating_add(nrows.saturating_mul(size_of::<Option<u32>>() as u64));
    let build_col_counts_bytes = ncols
        .saturating_mul(size_of::<u32>() as u64)
        .saturating_add(ncols.saturating_mul(size_of::<Option<u32>>() as u64));
    let build_hashers_bytes =
        ncols.saturating_mul(size_of::<xxhash_rust::xxh3::Xxh3>() as u64);

    row_view_bytes
        .saturating_add(row_meta_bytes)
        .saturating_add(col_meta_bytes)
        .saturating_add(cell_entry_bytes)
        .saturating_add(build_row_counts_bytes)
        .saturating_add(build_col_counts_bytes)
        .saturating_add(build_hashers_bytes)
}

pub(crate) fn estimate_advanced_sheet_diff_peak(old: &Grid, new: &Grid) -> u64 {
    let base = estimate_gridview_bytes(old).saturating_add(estimate_gridview_bytes(new));
    let max_rows = old.nrows.max(new.nrows) as u64;
    let max_cols = old.ncols.max(new.ncols) as u64;

    let alignment_overhead = max_rows
        .saturating_add(max_cols)
        .saturating_mul(size_of::<u32>() as u64)
        .saturating_mul(8);

    base.saturating_add(alignment_overhead)
}

```

---

### File: `core\src\memory_metrics.rs`

```rust
use std::alloc::{GlobalAlloc, Layout};
use std::sync::atomic::{AtomicU64, Ordering};

static CURRENT: AtomicU64 = AtomicU64::new(0);
static PEAK: AtomicU64 = AtomicU64::new(0);

pub struct CountingAllocator<A> {
    inner: A,
}

impl<A> CountingAllocator<A> {
    pub const fn new(inner: A) -> Self {
        Self { inner }
    }
}

fn update_peak(new_current: u64) {
    let mut peak = PEAK.load(Ordering::Relaxed);
    while new_current > peak {
        match PEAK.compare_exchange_weak(
            peak,
            new_current,
            Ordering::Relaxed,
            Ordering::Relaxed,
        ) {
            Ok(_) => return,
            Err(p) => peak = p,
        }
    }
}

unsafe impl<A: GlobalAlloc> GlobalAlloc for CountingAllocator<A> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let ptr = unsafe { self.inner.alloc(layout) };
        if !ptr.is_null() {
            let size = layout.size() as u64;
            let new_current = CURRENT.fetch_add(size, Ordering::Relaxed).saturating_add(size);
            update_peak(new_current);
        }
        ptr
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        unsafe { self.inner.dealloc(ptr, layout) };
        let size = layout.size() as u64;
        let mut cur = CURRENT.load(Ordering::Relaxed);
        loop {
            let next = cur.saturating_sub(size);
            match CURRENT.compare_exchange_weak(cur, next, Ordering::Relaxed, Ordering::Relaxed) {
                Ok(_) => break,
                Err(actual) => cur = actual,
            }
        }
    }

    unsafe fn realloc(&self, ptr: *mut u8, layout: Layout, new_size: usize) -> *mut u8 {
        let old_size = layout.size() as u64;
        let ptr2 = unsafe { self.inner.realloc(ptr, layout, new_size) };
        if !ptr2.is_null() {
            let new_size_u = new_size as u64;
            if new_size_u >= old_size {
                let delta = new_size_u - old_size;
                let new_current =
                    CURRENT.fetch_add(delta, Ordering::Relaxed).saturating_add(delta);
                update_peak(new_current);
            } else {
                let delta = old_size - new_size_u;
                let mut cur = CURRENT.load(Ordering::Relaxed);
                loop {
                    let next = cur.saturating_sub(delta);
                    match CURRENT.compare_exchange_weak(
                        cur,
                        next,
                        Ordering::Relaxed,
                        Ordering::Relaxed,
                    ) {
                        Ok(_) => break,
                        Err(actual) => cur = actual,
                    }
                }
            }
        }
        ptr2
    }
}

pub fn reset_peak_to_current() {
    let cur = CURRENT.load(Ordering::Relaxed);
    PEAK.store(cur, Ordering::Relaxed);
}

pub fn peak_bytes() -> u64 {
    PEAK.load(Ordering::Relaxed)
}

```

---

### File: `core\src\model.rs`

```rust
use crate::string_pool::StringId;

/// Minimal tabular model IR for future DAX/model diffing.
#[derive(Debug, Clone, PartialEq, Eq, Default)]
pub struct Model {
    pub measures: Vec<Measure>,
    pub tables: Vec<ModelTable>,
    pub relationships: Vec<ModelRelationship>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Measure {
    pub name: StringId,
    pub expression: StringId,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ModelTable {
    pub name: StringId,
    pub columns: Vec<ModelColumn>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ModelColumn {
    pub name: StringId,
    pub data_type: Option<StringId>,
    pub is_hidden: Option<bool>,
    pub format_string: Option<StringId>,
    pub sort_by: Option<StringId>,
    pub summarize_by: Option<StringId>,
    pub expression: Option<StringId>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ModelRelationship {
    pub from_table: StringId,
    pub from_column: StringId,
    pub to_table: StringId,
    pub to_column: StringId,
    pub cross_filtering_behavior: Option<StringId>,
    pub cardinality: Option<StringId>,
    pub is_active: Option<bool>,
    pub name: Option<StringId>,
}

```

---

### File: `core\src\model_diff.rs`

```rust
use std::collections::{BTreeMap, BTreeSet};
use std::hash::{Hash, Hasher};

use crate::config::{DiffConfig, SemanticNoisePolicy};
use crate::dax;
use crate::diff::{DiffOp, ExpressionChangeKind, ModelColumnProperty, RelationshipProperty};
use crate::hashing::XXH64_SEED;
use crate::model::{Measure, Model, ModelColumn, ModelRelationship, ModelTable};
use crate::string_pool::{StringId, StringPool};

fn hash64<T: Hash>(value: &T) -> u64 {
    let mut h = xxhash_rust::xxh64::Xxh64::new(XXH64_SEED);
    value.hash(&mut h);
    h.finish()
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ModelDiffResult {
    pub ops: Vec<DiffOp>,
    pub complete: bool,
    pub warnings: Vec<String>,
}

impl ModelDiffResult {
    fn new(ops: Vec<DiffOp>) -> Self {
        Self {
            ops,
            complete: true,
            warnings: Vec::new(),
        }
    }
}

struct OpEmitter {
    ops: Vec<DiffOp>,
    max_ops: Option<usize>,
    truncated: bool,
}

impl OpEmitter {
    fn new(max_ops: Option<usize>) -> Self {
        Self {
            ops: Vec::new(),
            max_ops,
            truncated: false,
        }
    }

    fn push(&mut self, op: DiffOp) {
        if self.truncated {
            return;
        }
        if let Some(max) = self.max_ops {
            if self.ops.len() >= max {
                self.truncated = true;
                return;
            }
        }
        self.ops.push(op);
    }
}

/// Diff two tabular models (tables/columns/relationships/measures).
pub fn diff_models(
    old: &Model,
    new: &Model,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> ModelDiffResult {
    let mut emitter = OpEmitter::new(config.hardening.max_ops);

    diff_tables(old, new, pool, config, &mut emitter);
    if !emitter.truncated {
        diff_relationships(old, new, pool, &mut emitter);
    }
    if !emitter.truncated {
        diff_measures(old, new, pool, config, &mut emitter);
    }

    let mut result = ModelDiffResult::new(emitter.ops);
    if emitter.truncated {
        result.complete = false;
        let limit = config.hardening.max_ops.unwrap_or(0);
        result.warnings.push(format!(
            "model-diff: max ops limit ({}) reached; model ops truncated",
            limit
        ));
    }
    result
}

fn diff_tables(
    old: &Model,
    new: &Model,
    pool: &mut StringPool,
    config: &DiffConfig,
    emitter: &mut OpEmitter,
) {
    let old_tables = map_tables(&old.tables, pool);
    let new_tables = map_tables(&new.tables, pool);

    let mut keys: BTreeSet<String> = BTreeSet::new();
    keys.extend(old_tables.keys().cloned());
    keys.extend(new_tables.keys().cloned());

    for key in keys {
        if emitter.truncated {
            return;
        }
        match (old_tables.get(&key), new_tables.get(&key)) {
            (None, Some(new_table)) => {
                emitter.push(DiffOp::TableAdded {
                    name: new_table.name,
                });
            }
            (Some(old_table), None) => {
                emitter.push(DiffOp::TableRemoved {
                    name: old_table.name,
                });
            }
            (Some(old_table), Some(new_table)) => {
                diff_columns(old_table, new_table, pool, config, emitter);
            }
            (None, None) => {}
        }
    }
}

fn diff_columns(
    old_table: &ModelTable,
    new_table: &ModelTable,
    pool: &mut StringPool,
    config: &DiffConfig,
    emitter: &mut OpEmitter,
) {
    let old_cols = map_columns(&old_table.columns, pool);
    let new_cols = map_columns(&new_table.columns, pool);
    let mut keys: BTreeSet<String> = BTreeSet::new();
    keys.extend(old_cols.keys().cloned());
    keys.extend(new_cols.keys().cloned());

    let table_id = new_table.name;

    for key in keys {
        if emitter.truncated {
            return;
        }
        match (old_cols.get(&key), new_cols.get(&key)) {
            (None, Some(new_col)) => {
                emitter.push(DiffOp::ModelColumnAdded {
                    table: table_id,
                    name: new_col.name,
                    data_type: new_col.data_type,
                });
            }
            (Some(old_col), None) => {
                emitter.push(DiffOp::ModelColumnRemoved {
                    table: table_id,
                    name: old_col.name,
                });
            }
            (Some(old_col), Some(new_col)) => {
                diff_column_properties(table_id, old_col, new_col, pool, config, emitter);
            }
            (None, None) => {}
        }
    }
}

fn diff_column_properties(
    table_id: StringId,
    old_col: &ModelColumn,
    new_col: &ModelColumn,
    pool: &mut StringPool,
    config: &DiffConfig,
    emitter: &mut OpEmitter,
) {
    if old_col.data_type != new_col.data_type {
        emitter.push(DiffOp::ModelColumnTypeChanged {
            table: table_id,
            name: new_col.name,
            old_type: old_col.data_type,
            new_type: new_col.data_type,
        });
    }

    if old_col.is_hidden != new_col.is_hidden {
        emitter.push(DiffOp::ModelColumnPropertyChanged {
            table: table_id,
            name: new_col.name,
            field: ModelColumnProperty::Hidden,
            old: old_col.is_hidden.map(|v| intern_bool(pool, v)),
            new: new_col.is_hidden.map(|v| intern_bool(pool, v)),
        });
    }

    if old_col.format_string != new_col.format_string {
        emitter.push(DiffOp::ModelColumnPropertyChanged {
            table: table_id,
            name: new_col.name,
            field: ModelColumnProperty::FormatString,
            old: old_col.format_string,
            new: new_col.format_string,
        });
    }

    if old_col.sort_by != new_col.sort_by {
        emitter.push(DiffOp::ModelColumnPropertyChanged {
            table: table_id,
            name: new_col.name,
            field: ModelColumnProperty::SortBy,
            old: old_col.sort_by,
            new: new_col.sort_by,
        });
    }

    if old_col.summarize_by != new_col.summarize_by {
        emitter.push(DiffOp::ModelColumnPropertyChanged {
            table: table_id,
            name: new_col.name,
            field: ModelColumnProperty::SummarizeBy,
            old: old_col.summarize_by,
            new: new_col.summarize_by,
        });
    }

    if emitter.truncated {
        return;
    }

    if let Some((kind, old_hash, new_hash)) =
        column_expression_change(old_col.expression, new_col.expression, pool, config)
    {
        emitter.push(DiffOp::CalculatedColumnDefinitionChanged {
            table: table_id,
            name: new_col.name,
            change_kind: kind,
            old_hash,
            new_hash,
        });
    }
}

fn diff_relationships(
    old: &Model,
    new: &Model,
    pool: &mut StringPool,
    emitter: &mut OpEmitter,
) {
    let old_rels = map_relationships(&old.relationships, pool);
    let new_rels = map_relationships(&new.relationships, pool);
    let mut keys: BTreeSet<RelationshipKey> = BTreeSet::new();
    keys.extend(old_rels.keys().cloned());
    keys.extend(new_rels.keys().cloned());

    for key in keys {
        if emitter.truncated {
            return;
        }
        match (old_rels.get(&key), new_rels.get(&key)) {
            (None, Some(new_rel)) => {
                emitter.push(DiffOp::RelationshipAdded {
                    from_table: new_rel.from_table,
                    from_column: new_rel.from_column,
                    to_table: new_rel.to_table,
                    to_column: new_rel.to_column,
                });
            }
            (Some(old_rel), None) => {
                emitter.push(DiffOp::RelationshipRemoved {
                    from_table: old_rel.from_table,
                    from_column: old_rel.from_column,
                    to_table: old_rel.to_table,
                    to_column: old_rel.to_column,
                });
            }
            (Some(old_rel), Some(new_rel)) => {
                if old_rel.cross_filtering_behavior != new_rel.cross_filtering_behavior {
                    emitter.push(DiffOp::RelationshipPropertyChanged {
                        from_table: new_rel.from_table,
                        from_column: new_rel.from_column,
                        to_table: new_rel.to_table,
                        to_column: new_rel.to_column,
                        field: RelationshipProperty::CrossFilteringBehavior,
                        old: old_rel.cross_filtering_behavior,
                        new: new_rel.cross_filtering_behavior,
                    });
                }

                if old_rel.cardinality != new_rel.cardinality {
                    emitter.push(DiffOp::RelationshipPropertyChanged {
                        from_table: new_rel.from_table,
                        from_column: new_rel.from_column,
                        to_table: new_rel.to_table,
                        to_column: new_rel.to_column,
                        field: RelationshipProperty::Cardinality,
                        old: old_rel.cardinality,
                        new: new_rel.cardinality,
                    });
                }

                if old_rel.is_active != new_rel.is_active {
                    emitter.push(DiffOp::RelationshipPropertyChanged {
                        from_table: new_rel.from_table,
                        from_column: new_rel.from_column,
                        to_table: new_rel.to_table,
                        to_column: new_rel.to_column,
                        field: RelationshipProperty::IsActive,
                        old: old_rel.is_active.map(|v| intern_bool(pool, v)),
                        new: new_rel.is_active.map(|v| intern_bool(pool, v)),
                    });
                }
            }
            (None, None) => {}
        }
    }
}

fn diff_measures(
    old: &Model,
    new: &Model,
    pool: &mut StringPool,
    config: &DiffConfig,
    emitter: &mut OpEmitter,
) {
    let old_measures = map_measures(&old.measures, pool);
    let new_measures = map_measures(&new.measures, pool);
    let mut keys: BTreeSet<String> = BTreeSet::new();
    keys.extend(old_measures.keys().cloned());
    keys.extend(new_measures.keys().cloned());

    for key in keys {
        if emitter.truncated {
            return;
        }
        match (old_measures.get(&key), new_measures.get(&key)) {
            (Some(old_measure), None) => {
                emitter.push(DiffOp::MeasureRemoved {
                    name: old_measure.name,
                });
            }
            (None, Some(new_measure)) => {
                emitter.push(DiffOp::MeasureAdded {
                    name: new_measure.name,
                });
            }
            (Some(old_measure), Some(new_measure)) => {
                let old_expr = pool.resolve(old_measure.expression);
                let new_expr = pool.resolve(new_measure.expression);
                if let Some((kind, old_hash, new_hash)) =
                    expression_change(old_expr, new_expr, config)
                {
                    emitter.push(DiffOp::MeasureDefinitionChanged {
                        name: new_measure.name,
                        change_kind: kind,
                        old_hash,
                        new_hash,
                    });
                }
            }
            (None, None) => {}
        }
    }
}

fn column_expression_change(
    old_expr: Option<StringId>,
    new_expr: Option<StringId>,
    pool: &mut StringPool,
    config: &DiffConfig,
) -> Option<(ExpressionChangeKind, u64, u64)> {
    match (old_expr, new_expr) {
        (None, None) => None,
        (Some(old_id), Some(new_id)) => {
            let old_expr = pool.resolve(old_id);
            let new_expr = pool.resolve(new_id);
            expression_change(old_expr, new_expr, config)
        }
        (Some(old_id), None) => {
            let old_expr = pool.resolve(old_id);
            let old_hash = hash64(&old_expr);
            let new_hash = hash64(&"");
            Some((ExpressionChangeKind::Semantic, old_hash, new_hash))
        }
        (None, Some(new_id)) => {
            let new_expr = pool.resolve(new_id);
            let old_hash = hash64(&"");
            let new_hash = hash64(&new_expr);
            Some((ExpressionChangeKind::Semantic, old_hash, new_hash))
        }
    }
}

fn expression_change(
    old_expr: &str,
    new_expr: &str,
    config: &DiffConfig,
) -> Option<(ExpressionChangeKind, u64, u64)> {
    if old_expr == new_expr {
        return None;
    }

    if config.semantic.enable_dax_semantic_diff {
        if let (Ok(old_h), Ok(new_h)) = (dax::semantic_hash(old_expr), dax::semantic_hash(new_expr)) {
            let kind = if old_h == new_h {
                ExpressionChangeKind::FormattingOnly
            } else {
                ExpressionChangeKind::Semantic
            };
            if kind == ExpressionChangeKind::FormattingOnly
                && matches!(config.semantic.semantic_noise_policy, SemanticNoisePolicy::SuppressFormattingOnly)
            {
                return None;
            }
            return Some((kind, old_h, new_h));
        }
    }

    let old_h = hash64(&old_expr);
    let new_h = hash64(&new_expr);
    Some((ExpressionChangeKind::Unknown, old_h, new_h))
}

fn map_tables<'a>(tables: &'a [ModelTable], pool: &StringPool) -> BTreeMap<String, &'a ModelTable> {
    let mut out = BTreeMap::new();
    for table in tables {
        let key = pool.resolve(table.name).to_lowercase();
        out.entry(key).or_insert(table);
    }
    out
}

fn map_columns<'a>(
    cols: &'a [ModelColumn],
    pool: &StringPool,
) -> BTreeMap<String, &'a ModelColumn> {
    let mut out = BTreeMap::new();
    for col in cols {
        let key = pool.resolve(col.name).to_lowercase();
        out.entry(key).or_insert(col);
    }
    out
}

fn map_measures<'a>(
    measures: &'a [Measure],
    pool: &StringPool,
) -> BTreeMap<String, &'a Measure> {
    let mut out = BTreeMap::new();
    for measure in measures {
        let key = pool.resolve(measure.name).to_lowercase();
        out.entry(key).or_insert(measure);
    }
    out
}

#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
struct RelationshipKey {
    from_table: String,
    from_column: String,
    to_table: String,
    to_column: String,
}

fn map_relationships<'a>(
    relationships: &'a [ModelRelationship],
    pool: &StringPool,
) -> BTreeMap<RelationshipKey, &'a ModelRelationship> {
    let mut out = BTreeMap::new();
    for rel in relationships {
        let key = RelationshipKey {
            from_table: pool.resolve(rel.from_table).to_lowercase(),
            from_column: pool.resolve(rel.from_column).to_lowercase(),
            to_table: pool.resolve(rel.to_table).to_lowercase(),
            to_column: pool.resolve(rel.to_column).to_lowercase(),
        };
        out.entry(key).or_insert(rel);
    }
    out
}

fn intern_bool(pool: &mut StringPool, v: bool) -> StringId {
    if v {
        pool.intern("true")
    } else {
        pool.intern("false")
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_measure_model(pool: &mut StringPool, expr: &str) -> Model {
        let name = pool.intern("Sales/Total");
        let expr_id = pool.intern(expr);
        Model {
            measures: vec![Measure {
                name,
                expression: expr_id,
            }],
            ..Default::default()
        }
    }

    fn make_calc_column_model(pool: &mut StringPool, expr: &str) -> Model {
        let table_name = pool.intern("Sales");
        let column_name = pool.intern("Calc");
        let expr_id = pool.intern(expr);
        let column = ModelColumn {
            name: column_name,
            data_type: None,
            is_hidden: None,
            format_string: None,
            sort_by: None,
            summarize_by: None,
            expression: Some(expr_id),
        };
        let table = ModelTable {
            name: table_name,
            columns: vec![column],
        };
        Model {
            tables: vec![table],
            ..Default::default()
        }
    }

    #[test]
    fn dax_formatting_only_suppressed_when_configured() {
        let mut pool = StringPool::new();
        let old_model = make_measure_model(&mut pool, "SUM( Sales[Amount] )");
        let new_model = make_measure_model(&mut pool, "sum(sales[amount]) // comment");

        let mut config = DiffConfig::default();
        config.semantic.enable_dax_semantic_diff = true;
        config.semantic.semantic_noise_policy = SemanticNoisePolicy::SuppressFormattingOnly;

        let result = diff_models(&old_model, &new_model, &mut pool, &config);
        assert!(result.ops.is_empty(), "formatting-only change should be suppressed");
        assert!(result.complete);
    }

    #[test]
    fn dax_semantic_change_reports_change_kind() {
        let mut pool = StringPool::new();
        let old_model = make_measure_model(&mut pool, "SUM(Sales[Amount])");
        let new_model = make_measure_model(&mut pool, "SUM(Sales[Net])");

        let mut config = DiffConfig::default();
        config.semantic.enable_dax_semantic_diff = true;

        let result = diff_models(&old_model, &new_model, &mut pool, &config);
        assert_eq!(result.ops.len(), 1);
        match &result.ops[0] {
            DiffOp::MeasureDefinitionChanged { change_kind, .. } => {
                assert_eq!(*change_kind, ExpressionChangeKind::Semantic);
            }
            other => panic!("unexpected op: {:?}", other),
        }
    }

    #[test]
    fn dax_formatting_only_column_change_is_classified() {
        let mut pool = StringPool::new();
        let old_model = make_calc_column_model(&mut pool, "1+2");
        let new_model = make_calc_column_model(&mut pool, "1 + 2");

        let mut config = DiffConfig::default();
        config.semantic.enable_dax_semantic_diff = true;

        let result = diff_models(&old_model, &new_model, &mut pool, &config);
        assert_eq!(result.ops.len(), 1);
        match &result.ops[0] {
            DiffOp::CalculatedColumnDefinitionChanged { change_kind, .. } => {
                assert_eq!(*change_kind, ExpressionChangeKind::FormattingOnly);
            }
            other => panic!("unexpected op: {:?}", other),
        }
    }

    #[test]
    fn dax_parse_failure_sets_unknown_change_kind() {
        let mut pool = StringPool::new();
        let old_model = make_measure_model(&mut pool, "SUM(");
        let new_model = make_measure_model(&mut pool, "SUMX(");

        let mut config = DiffConfig::default();
        config.semantic.enable_dax_semantic_diff = true;

        let result = diff_models(&old_model, &new_model, &mut pool, &config);
        assert_eq!(result.ops.len(), 1);
        match &result.ops[0] {
            DiffOp::MeasureDefinitionChanged { change_kind, .. } => {
                assert_eq!(*change_kind, ExpressionChangeKind::Unknown);
            }
            other => panic!("unexpected op: {:?}", other),
        }
    }
}

```

---

### File: `core\src\object_diff.rs`

```rust
use crate::diff::DiffOp;
use crate::string_pool::StringPool;
use crate::vba::VbaModule;
use crate::workbook::{ChartObject, NamedRange, Workbook};
use std::collections::{BTreeMap, BTreeSet, HashMap, HashSet};

pub(crate) fn diff_named_ranges(old: &Workbook, new: &Workbook, pool: &StringPool) -> Vec<DiffOp> {
    fn key(range: &NamedRange, pool: &StringPool) -> String {
        pool.resolve(range.name).to_lowercase()
    }

    let mut old_map: BTreeMap<String, &NamedRange> = BTreeMap::new();
    for range in &old.named_ranges {
        old_map.insert(key(range, pool), range);
    }

    let mut new_map: BTreeMap<String, &NamedRange> = BTreeMap::new();
    for range in &new.named_ranges {
        new_map.insert(key(range, pool), range);
    }

    let mut keys: BTreeSet<String> = BTreeSet::new();
    keys.extend(old_map.keys().cloned());
    keys.extend(new_map.keys().cloned());

    let mut ops = Vec::new();
    for k in keys {
        match (old_map.get(&k), new_map.get(&k)) {
            (None, Some(new_range)) => ops.push(DiffOp::NamedRangeAdded {
                name: new_range.name,
            }),
            (Some(old_range), None) => ops.push(DiffOp::NamedRangeRemoved {
                name: old_range.name,
            }),
            (Some(old_range), Some(new_range)) => {
                if old_range.refers_to != new_range.refers_to {
                    ops.push(DiffOp::NamedRangeChanged {
                        name: new_range.name,
                        old_ref: old_range.refers_to,
                        new_ref: new_range.refers_to,
                    });
                }
            }
            (None, None) => {}
        }
    }

    ops
}

pub(crate) fn diff_charts(old: &Workbook, new: &Workbook, pool: &StringPool) -> Vec<DiffOp> {
    #[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
    struct ChartIdKey {
        sheet_id: u32,
        chart_name_lower: String,
    }

    #[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
    struct ChartNameKey {
        sheet_lower: String,
        chart_name_lower: String,
    }

    fn chart_ptr(chart: &ChartObject) -> *const ChartObject {
        chart as *const ChartObject
    }

    fn chart_name_lower(chart: &ChartObject, pool: &StringPool) -> String {
        pool.resolve(chart.info.name).to_lowercase()
    }

    fn sheet_name_lower(chart: &ChartObject, pool: &StringPool) -> String {
        pool.resolve(chart.sheet).to_lowercase()
    }

    fn ambiguous_sheet_ids(charts: &[ChartObject], pool: &StringPool) -> HashSet<u32> {
        let mut id_to_sheet: HashMap<u32, String> = HashMap::new();
        let mut ambiguous: HashSet<u32> = HashSet::new();
        for chart in charts {
            let Some(sheet_id) = chart.workbook_sheet_id else {
                continue;
            };
            let sheet_lower = sheet_name_lower(chart, pool);
            if let Some(existing) = id_to_sheet.get(&sheet_id) {
                if existing != &sheet_lower {
                    ambiguous.insert(sheet_id);
                }
            } else {
                id_to_sheet.insert(sheet_id, sheet_lower);
            }
        }
        ambiguous
    }

    let ambiguous_old = ambiguous_sheet_ids(&old.charts, pool);
    let ambiguous_new = ambiguous_sheet_ids(&new.charts, pool);

    let mut old_by_id: BTreeMap<ChartIdKey, &ChartObject> = BTreeMap::new();
    for chart in &old.charts {
        let Some(sheet_id) = chart.workbook_sheet_id else {
            continue;
        };
        if ambiguous_old.contains(&sheet_id) {
            continue;
        }
        let key = ChartIdKey {
            sheet_id,
            chart_name_lower: chart_name_lower(chart, pool),
        };
        old_by_id.insert(key, chart);
    }

    let mut new_by_id: BTreeMap<ChartIdKey, &ChartObject> = BTreeMap::new();
    for chart in &new.charts {
        let Some(sheet_id) = chart.workbook_sheet_id else {
            continue;
        };
        if ambiguous_new.contains(&sheet_id) {
            continue;
        }
        let key = ChartIdKey {
            sheet_id,
            chart_name_lower: chart_name_lower(chart, pool),
        };
        new_by_id.insert(key, chart);
    }

    let mut old_by_name: BTreeMap<ChartNameKey, &ChartObject> = BTreeMap::new();
    for chart in &old.charts {
        let key = ChartNameKey {
            sheet_lower: sheet_name_lower(chart, pool),
            chart_name_lower: chart_name_lower(chart, pool),
        };
        old_by_name.insert(key, chart);
    }

    let mut new_by_name: BTreeMap<ChartNameKey, &ChartObject> = BTreeMap::new();
    for chart in &new.charts {
        let key = ChartNameKey {
            sheet_lower: sheet_name_lower(chart, pool),
            chart_name_lower: chart_name_lower(chart, pool),
        };
        new_by_name.insert(key, chart);
    }

    let mut consumed_old: HashSet<*const ChartObject> = HashSet::new();
    let mut consumed_new: HashSet<*const ChartObject> = HashSet::new();

    let mut ops = Vec::new();

    let mut id_keys: BTreeSet<ChartIdKey> = BTreeSet::new();
    id_keys.extend(old_by_id.keys().cloned());
    id_keys.extend(new_by_id.keys().cloned());
    for key in id_keys {
        let old_chart = old_by_id.get(&key).copied();
        let new_chart = new_by_id.get(&key).copied();
        if let Some(old_chart) = old_chart {
            consumed_old.insert(chart_ptr(old_chart));
        }
        if let Some(new_chart) = new_chart {
            consumed_new.insert(chart_ptr(new_chart));
        }

        match (old_chart, new_chart) {
            (None, Some(new_chart)) => ops.push(DiffOp::ChartAdded {
                sheet: new_chart.sheet,
                name: new_chart.info.name,
            }),
            (Some(old_chart), None) => ops.push(DiffOp::ChartRemoved {
                sheet: old_chart.sheet,
                name: old_chart.info.name,
            }),
            (Some(old_chart), Some(new_chart)) => {
                if old_chart.xml_hash != new_chart.xml_hash {
                    ops.push(DiffOp::ChartChanged {
                        sheet: new_chart.sheet,
                        name: new_chart.info.name,
                    });
                }
            }
            (None, None) => {}
        }
    }

    let mut name_keys: BTreeSet<ChartNameKey> = BTreeSet::new();
    name_keys.extend(old_by_name.keys().cloned());
    name_keys.extend(new_by_name.keys().cloned());
    for key in name_keys {
        let old_chart = old_by_name
            .get(&key)
            .copied()
            .filter(|chart| !consumed_old.contains(&chart_ptr(chart)));
        let new_chart = new_by_name
            .get(&key)
            .copied()
            .filter(|chart| !consumed_new.contains(&chart_ptr(chart)));

        match (old_chart, new_chart) {
            (None, Some(new_chart)) => ops.push(DiffOp::ChartAdded {
                sheet: new_chart.sheet,
                name: new_chart.info.name,
            }),
            (Some(old_chart), None) => ops.push(DiffOp::ChartRemoved {
                sheet: old_chart.sheet,
                name: old_chart.info.name,
            }),
            (Some(old_chart), Some(new_chart)) => {
                if old_chart.xml_hash != new_chart.xml_hash {
                    ops.push(DiffOp::ChartChanged {
                        sheet: new_chart.sheet,
                        name: new_chart.info.name,
                    });
                }
            }
            (None, None) => {}
        }
    }

    ops
}

pub(crate) fn diff_vba_modules(
    old: Option<&[VbaModule]>,
    new: Option<&[VbaModule]>,
    pool: &StringPool,
) -> Vec<DiffOp> {
    fn key(module: &VbaModule, pool: &StringPool) -> String {
        pool.resolve(module.name).to_lowercase()
    }

    fn normalize(code: &str) -> String {
        code.replace("\r\n", "\n").replace('\r', "\n")
    }

    let old = old.unwrap_or_default();
    let new = new.unwrap_or_default();

    let mut old_map: BTreeMap<String, &VbaModule> = BTreeMap::new();
    for module in old {
        old_map.insert(key(module, pool), module);
    }

    let mut new_map: BTreeMap<String, &VbaModule> = BTreeMap::new();
    for module in new {
        new_map.insert(key(module, pool), module);
    }

    let mut keys: BTreeSet<String> = BTreeSet::new();
    keys.extend(old_map.keys().cloned());
    keys.extend(new_map.keys().cloned());

    let mut ops = Vec::new();
    for k in keys {
        match (old_map.get(&k), new_map.get(&k)) {
            (None, Some(new_module)) => ops.push(DiffOp::VbaModuleAdded {
                name: new_module.name,
            }),
            (Some(old_module), None) => ops.push(DiffOp::VbaModuleRemoved {
                name: old_module.name,
            }),
            (Some(old_module), Some(new_module)) => {
                if normalize(&old_module.code) != normalize(&new_module.code) {
                    ops.push(DiffOp::VbaModuleChanged {
                        name: new_module.name,
                    });
                }
            }
            (None, None) => {}
        }
    }

    ops
}


```

---

### File: `core\src\output\json.rs`

```rust
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use crate::config::DiffConfig;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use crate::datamashup::build_data_mashup;
use crate::diff::DiffReport;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use crate::diff::DiffSummary;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use crate::excel_open_xml::{PackageError, open_data_mashup, open_vba_modules, open_workbook};
#[allow(unused_imports)]
use crate::session::DiffSession;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use crate::sink::VecSink;
use serde::Serialize;
use serde::ser::Error as SerdeError;
#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
use std::path::Path;

#[derive(Debug, Clone, PartialEq, Serialize)]
pub struct CellDiff {
    #[serde(rename = "coords")]
    pub coords: String,
    #[serde(rename = "value_file1")]
    pub value_file1: Option<String>,
    #[serde(rename = "value_file2")]
    pub value_file2: Option<String>,
}

pub fn serialize_cell_diffs(diffs: &[CellDiff]) -> serde_json::Result<String> {
    serde_json::to_string(diffs)
}

pub fn serialize_diff_report(report: &DiffReport) -> serde_json::Result<String> {
    if contains_non_finite_numbers(report) {
        return Err(SerdeError::custom(
            "non-finite numbers (NaN or infinity) are not supported in DiffReport JSON output",
        ));
    }
    serde_json::to_string(report)
}

#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
pub fn diff_workbooks(
    path_a: impl AsRef<Path>,
    path_b: impl AsRef<Path>,
    config: &DiffConfig,
) -> Result<DiffReport, PackageError> {
    let path_a = path_a.as_ref();
    let path_b = path_b.as_ref();

    let mut session = DiffSession::new();

    let wb_a = open_workbook(path_a, session.strings_mut())?;
    let wb_b = open_workbook(path_b, session.strings_mut())?;

    let dm_a = open_data_mashup(path_a)?
        .map(|raw| build_data_mashup(&raw))
        .transpose()?;
    let dm_b = open_data_mashup(path_b)?
        .map(|raw| build_data_mashup(&raw))
        .transpose()?;

    let vba_a = open_vba_modules(path_a, session.strings_mut())?;
    let vba_b = open_vba_modules(path_b, session.strings_mut())?;

    let mut sink = VecSink::new();
    let grid_result = crate::engine::try_diff_workbooks_streaming(
        &wb_a,
        &wb_b,
        session.strings_mut(),
        config,
        &mut sink,
    );

    let (mut ops, summary) = match grid_result {
        Ok(summary) => (sink.into_ops(), summary),
        Err(err) => (
            Vec::new(),
            DiffSummary {
                complete: false,
                warnings: vec![err.to_string()],
                op_count: 0,
                #[cfg(feature = "perf-metrics")]
                metrics: None,
            },
        ),
    };

    let mut object_ops = crate::object_diff::diff_named_ranges(&wb_a, &wb_b, session.strings());
    object_ops.extend(crate::object_diff::diff_charts(
        &wb_a,
        &wb_b,
        session.strings(),
    ));
    object_ops.extend(crate::object_diff::diff_vba_modules(
        vba_a.as_deref(),
        vba_b.as_deref(),
        session.strings(),
    ));
    ops.extend(object_ops);

    let m_ops = crate::m_diff::diff_m_ops_for_packages(&dm_a, &dm_b, session.strings_mut(), config);

    ops.extend(m_ops);

    let strings = session.strings.into_strings();
    Ok(DiffReport::from_ops_and_summary(ops, summary, strings))
}

#[cfg(all(feature = "excel-open-xml", feature = "std-fs"))]
pub fn diff_workbooks_to_json(
    path_a: impl AsRef<Path>,
    path_b: impl AsRef<Path>,
    config: &DiffConfig,
) -> Result<String, PackageError> {
    let report = diff_workbooks(path_a, path_b, config)?;
    serialize_diff_report(&report).map_err(|e| PackageError::SerializationError(e.to_string()))
}

pub fn diff_report_to_cell_diffs(report: &DiffReport) -> Vec<CellDiff> {
    use crate::diff::DiffOp;
    use crate::workbook::CellValue;

    fn render_value(report: &DiffReport, value: &Option<CellValue>) -> Option<String> {
        match value {
            Some(CellValue::Number(n)) => Some(n.to_string()),
            Some(CellValue::Text(id)) => report.resolve(*id).map(|s| s.to_string()),
            Some(CellValue::Bool(b)) => Some(b.to_string()),
            Some(CellValue::Error(id)) => report.resolve(*id).map(|s| s.to_string()),
            Some(CellValue::Blank) => Some(String::new()),
            None => None,
        }
    }

    report
        .ops
        .iter()
        .filter_map(|op| {
            if let DiffOp::CellEdited { addr, from, to, .. } = op {
                if from == to {
                    return None;
                }
                Some(CellDiff {
                    coords: addr.to_a1(),
                    value_file1: render_value(report, &from.value),
                    value_file2: render_value(report, &to.value),
                })
            } else {
                None
            }
        })
        .collect()
}

fn contains_non_finite_numbers(report: &DiffReport) -> bool {
    use crate::diff::DiffOp;
    use crate::workbook::CellValue;

    report.ops.iter().any(|op| match op {
        DiffOp::CellEdited { from, to, .. } => {
            matches!(from.value, Some(CellValue::Number(n)) if !n.is_finite())
                || matches!(to.value, Some(CellValue::Number(n)) if !n.is_finite())
        }
        DiffOp::DuplicateKeyCluster { key, .. } => key
            .iter()
            .any(|value| matches!(value, Some(CellValue::Number(n)) if !n.is_finite())),
        _ => false,
    })
}

```

---

### File: `core\src\output\json_lines.rs`

```rust
use crate::diff::{DiffError, DiffOp};
use crate::sink::DiffSink;
use crate::string_pool::StringPool;
use serde::Serialize;
use std::io::Write;

#[derive(Serialize)]
struct JsonLinesHeader<'a> {
    kind: &'static str,
    version: &'a str,
    strings: &'a [String],
}

/// A [`DiffSink`] that writes a JSON Lines stream.
///
/// The first line is a header containing the schema version and the string table. Each
/// subsequent line is a JSON-serialized [`DiffOp`].
///
/// All strings referenced by emitted ops must be interned before `begin`, because the
/// header captures the string table once. See `docs/streaming_contract.md`.
pub struct JsonLinesSink<W: Write> {
    w: W,
    wrote_header: bool,
    version: &'static str,
}

impl<W: Write> JsonLinesSink<W> {
    /// Create a JSON Lines sink that writes to `w`.
    ///
    /// The output format is:
    ///
    /// 1. A header line: `{ "kind": "Header", "version": "...", "strings": [...] }`
    /// 2. One JSON-serialized [`DiffOp`] per line
    ///
    /// Ops contain interned [`crate::StringId`] values that index into the header's `strings` table.
    pub fn new(w: W) -> Self {
        Self {
            w,
            wrote_header: false,
            version: crate::diff::DiffReport::SCHEMA_VERSION,
        }
    }

    /// Write the header line (idempotent).
    pub fn begin(&mut self, pool: &StringPool) -> Result<(), DiffError> {
        if self.wrote_header {
            return Ok(());
        }

        let header = JsonLinesHeader {
            kind: "Header",
            version: self.version,
            strings: pool.strings(),
        };

        serde_json::to_writer(&mut self.w, &header).map_err(|e| DiffError::SinkError {
            message: e.to_string(),
        })?;
        self.w.write_all(b"\n").map_err(|e| DiffError::SinkError {
            message: e.to_string(),
        })?;

        self.wrote_header = true;
        Ok(())
    }
}

impl<W: Write> DiffSink for JsonLinesSink<W> {
    fn begin(&mut self, pool: &StringPool) -> Result<(), DiffError> {
        JsonLinesSink::begin(self, pool)
    }

    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        serde_json::to_writer(&mut self.w, &op).map_err(|e| DiffError::SinkError {
            message: e.to_string(),
        })?;
        self.w.write_all(b"\n").map_err(|e| DiffError::SinkError {
            message: e.to_string(),
        })?;
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        self.w.flush().map_err(|e| DiffError::SinkError {
            message: e.to_string(),
        })
    }
}

```

---

### File: `core\src\output\mod.rs`

```rust
pub mod json;
pub mod json_lines;

```

---

### File: `core\src\package.rs`

```rust
use crate::config::DiffConfig;
use crate::container::ZipContainer;
use crate::datamashup::DataMashup;
use crate::diff::{DiffError, DiffReport, DiffSummary, SheetId};
use crate::diffable::{DiffContext, Diffable};
use crate::permission_bindings::{PermissionBindingsStatus, permission_bindings_warning};
use crate::progress::ProgressCallback;
use crate::sink::{DiffSink, NoFinishSink, SinkFinishGuard, VecSink};
use crate::string_pool::StringPool;
use crate::vba::VbaModule;
use crate::workbook::{Sheet, Workbook};
#[cfg(feature = "perf-metrics")]
use crate::perf::DiffMetrics;

/// A parsed workbook plus optional associated content (Power Query and VBA).
///
/// This is the recommended high-level entry point for most callers. It wraps the workbook IR
/// (`Workbook`, `Sheet`, `Grid`) together with:
/// - optional DataMashup content (Power Query / M)
/// - optional extracted VBA modules (for `.xlsm`)
///
/// Diffs produced via [`WorkbookPackage::diff`] and related APIs include grid ops, object ops
/// (named ranges, charts, VBA), and M ops when present.
#[derive(Debug, Clone)]
pub struct WorkbookPackage {
    /// Parsed workbook IR (sheets, grids, named ranges, charts).
    pub workbook: Workbook,
    /// Parsed DataMashup content (Power Query), if present.
    pub data_mashup: Option<DataMashup>,
    /// Extracted VBA modules, if present and the `vba` feature is enabled.
    pub vba_modules: Option<Vec<VbaModule>>,
    #[cfg(feature = "perf-metrics")]
    /// Parse time for this package (ms), captured when opening from bytes.
    pub parse_time_ms: u64,
}

impl From<Workbook> for WorkbookPackage {
    fn from(workbook: Workbook) -> Self {
        Self {
            workbook,
            data_mashup: None,
            vba_modules: None,
            #[cfg(feature = "perf-metrics")]
            parse_time_ms: 0,
        }
    }
}

impl WorkbookPackage {
    #[cfg(feature = "excel-open-xml")]
    /// Parse a workbook from any `Read + Seek` source.
    ///
    /// This is available when the `excel-open-xml` feature is enabled (enabled by default).
    ///
    /// # Examples
    ///
    /// ```no_run
    /// use excel_diff::WorkbookPackage;
    /// use std::fs::File;
    ///
    /// # fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// let _pkg = WorkbookPackage::open(File::open("workbook.xlsx")?)?;
    /// # Ok(())
    /// # }
    /// ```
    pub fn open<R: std::io::Read + std::io::Seek + 'static>(
        reader: R,
    ) -> Result<Self, crate::excel_open_xml::PackageError> {
        crate::with_default_session(|session| {
            #[cfg(feature = "perf-metrics")]
            let start = std::time::Instant::now();
            let mut container = crate::container::OpcContainer::open_from_reader(reader)?;
            let workbook = crate::excel_open_xml::open_workbook_from_container(
                &mut container,
                &mut session.strings,
            )?;
            let raw = crate::excel_open_xml::open_data_mashup_from_container(&mut container)?;
            let data_mashup = match raw {
                Some(raw) => Some(crate::datamashup::build_data_mashup(&raw)?),
                None => None,
            };
            let vba_modules =
                crate::excel_open_xml::open_vba_modules_from_container(&mut container, &mut session.strings)?;
            Ok(Self {
                workbook,
                data_mashup,
                vba_modules,
                #[cfg(feature = "perf-metrics")]
                parse_time_ms: start.elapsed().as_millis() as u64,
            })
        })
    }

    #[cfg(feature = "excel-open-xml")]
    /// Parse a workbook with custom container limits (trusted overrides).
    pub fn open_with_limits<R: std::io::Read + std::io::Seek + 'static>(
        reader: R,
        limits: crate::ContainerLimits,
    ) -> Result<Self, crate::excel_open_xml::PackageError> {
        crate::with_default_session(|session| {
            #[cfg(feature = "perf-metrics")]
            let start = std::time::Instant::now();
            let mut container =
                crate::container::OpcContainer::open_from_reader_with_limits(reader, limits)?;
            let workbook = crate::excel_open_xml::open_workbook_from_container(
                &mut container,
                &mut session.strings,
            )?;
            let raw = crate::excel_open_xml::open_data_mashup_from_container(&mut container)?;
            let data_mashup = match raw {
                Some(raw) => Some(crate::datamashup::build_data_mashup(&raw)?),
                None => None,
            };
            let vba_modules =
                crate::excel_open_xml::open_vba_modules_from_container(&mut container, &mut session.strings)?;
            Ok(Self {
                workbook,
                data_mashup,
                vba_modules,
                #[cfg(feature = "perf-metrics")]
                parse_time_ms: start.elapsed().as_millis() as u64,
            })
        })
    }

    /// Diff this package against `other`, returning an in-memory [`DiffReport`].
    ///
    /// This collects all ops into memory and returns a report containing both the ops and the
    /// string table required to resolve [`StringId`] values referenced by ops.
    ///
    /// For very large workbooks, consider [`WorkbookPackage::diff_streaming`] instead.
    pub fn diff(&self, other: &Self, config: &DiffConfig) -> DiffReport {
        crate::with_default_session(|session| {
            self.diff_with_pool(other, &mut session.strings, config)
        })
    }

    /// Like [`WorkbookPackage::diff`], but reports best-effort progress via `progress`.
    pub fn diff_with_progress(
        &self,
        other: &Self,
        config: &DiffConfig,
        progress: &dyn ProgressCallback,
    ) -> DiffReport {
        crate::with_default_session(|session| {
            self.diff_with_progress_with_pool(other, &mut session.strings, config, progress)
        })
    }

    pub fn diff_with_pool(
        &self,
        other: &Self,
        pool: &mut crate::string_pool::StringPool,
        config: &DiffConfig,
    ) -> DiffReport {
        let mut report = {
            let mut ctx = DiffContext::new(pool, config);
            self.workbook.diff(&other.workbook, &mut ctx)
        };

        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));
        report.ops.extend(object_ops);

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        report.ops.extend(m_ops);
        report.strings = pool.strings().to_vec();
        #[cfg(feature = "perf-metrics")]
        apply_parse_metrics(self, other, &mut report.metrics);
        append_permission_bindings_warnings(&mut report, &self.data_mashup, &other.data_mashup);
        report
    }

    pub fn diff_with_progress_with_pool(
        &self,
        other: &Self,
        pool: &mut crate::string_pool::StringPool,
        config: &DiffConfig,
        progress: &dyn ProgressCallback,
    ) -> DiffReport {
        let mut report = crate::engine::diff_workbooks_with_progress(
            &self.workbook,
            &other.workbook,
            pool,
            config,
            progress,
        );

        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));
        report.ops.extend(object_ops);

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        report.ops.extend(m_ops);
        report.strings = pool.strings().to_vec();
        #[cfg(feature = "perf-metrics")]
        apply_parse_metrics(self, other, &mut report.metrics);
        append_permission_bindings_warnings(&mut report, &self.data_mashup, &other.data_mashup);
        report
    }

    /// Diff this package against `other`, streaming ops into `sink`.
    ///
    /// This is the preferred API for very large workbooks because it does not require holding
    /// the entire op list in memory. Instead, ops are emitted incrementally and a [`DiffSummary`]
    /// is returned at the end.
    ///
    /// Streaming output follows the contract in `docs/streaming_contract.md` (determinism,
    /// sink lifecycle, and JSONL string table invariants).
    ///
    /// # Examples
    ///
    /// ```no_run
    /// use excel_diff::{DiffConfig, JsonLinesSink, WorkbookPackage};
    /// use std::fs::File;
    /// use std::io::{self, BufWriter};
    ///
    /// # fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// let old_pkg = WorkbookPackage::open(File::open("old.xlsx")?)?;
    /// let new_pkg = WorkbookPackage::open(File::open("new.xlsx")?)?;
    ///
    /// let stdout = io::stdout();
    /// let mut sink = JsonLinesSink::new(BufWriter::new(stdout.lock()));
    /// let summary = old_pkg.diff_streaming(&new_pkg, &DiffConfig::default(), &mut sink)?;
    /// eprintln!("complete={} ops={}", summary.complete, summary.op_count);
    /// # Ok(())
    /// # }
    /// ```
    pub fn diff_streaming<S: DiffSink>(
        &self,
        other: &Self,
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        crate::with_default_session(|session| {
            self.diff_streaming_with_pool(other, &mut session.strings, config, sink)
        })
    }

    /// Like [`WorkbookPackage::diff_streaming`], but reports best-effort progress via `progress`.
    pub fn diff_streaming_with_progress<S: DiffSink>(
        &self,
        other: &Self,
        config: &DiffConfig,
        sink: &mut S,
        progress: &dyn ProgressCallback,
    ) -> Result<DiffSummary, DiffError> {
        crate::with_default_session(|session| {
            self.diff_streaming_with_progress_with_pool(
                other,
                &mut session.strings,
                config,
                sink,
                progress,
            )
        })
    }

    /// Streaming variant of [`WorkbookPackage::diff`], using a caller-provided string pool.
    ///
    /// Most callers should prefer [`WorkbookPackage::diff_streaming`], which uses the default
    /// session string pool internally.
    pub fn diff_streaming_with_pool<S: DiffSink>(
        &self,
        other: &Self,
        pool: &mut crate::string_pool::StringPool,
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        let grid_result = {
            let mut no_finish = NoFinishSink::new(sink);
            crate::engine::try_diff_workbooks_streaming(
                &self.workbook,
                &other.workbook,
                pool,
                config,
                &mut no_finish,
            )
        };

        let mut summary = match grid_result {
            Ok(summary) => summary,
            Err(e) => {
                let _ = sink.finish();
                return Err(e);
            }
        };

        for op in object_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        for op in m_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        sink.finish()?;

        #[cfg(feature = "perf-metrics")]
        apply_parse_metrics(self, other, &mut summary.metrics);

        append_permission_bindings_warnings_summary(&mut summary, &self.data_mashup, &other.data_mashup);
        Ok(summary)
    }

    pub fn diff_streaming_with_progress_with_pool<S: DiffSink>(
        &self,
        other: &Self,
        pool: &mut crate::string_pool::StringPool,
        config: &DiffConfig,
        sink: &mut S,
        progress: &dyn ProgressCallback,
    ) -> Result<DiffSummary, DiffError> {
        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        let grid_result = {
            let mut no_finish = NoFinishSink::new(sink);
            crate::engine::try_diff_workbooks_streaming_with_progress(
                &self.workbook,
                &other.workbook,
                pool,
                config,
                &mut no_finish,
                progress,
            )
        };

        let mut summary = match grid_result {
            Ok(summary) => summary,
            Err(e) => {
                let _ = sink.finish();
                return Err(e);
            }
        };

        for op in object_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        for op in m_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        sink.finish()?;

        #[cfg(feature = "perf-metrics")]
        apply_parse_metrics(self, other, &mut summary.metrics);

        append_permission_bindings_warnings_summary(&mut summary, &self.data_mashup, &other.data_mashup);
        Ok(summary)
    }

    /// Diff a single sheet using key-based row alignment ("database mode").
    ///
    /// `sheet_name` must exist in both workbooks (matching is case-insensitive). `key_columns`
    /// are 0-based column indices (A=0, B=1, ...).
    ///
    /// # Examples
    ///
    /// ```no_run
    /// use excel_diff::{DiffConfig, WorkbookPackage};
    /// use std::fs::File;
    ///
    /// # fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// let old_pkg = WorkbookPackage::open(File::open("old.xlsx")?)?;
    /// let new_pkg = WorkbookPackage::open(File::open("new.xlsx")?)?;
    ///
    /// let keys = vec![0u32, 2u32]; // A,C
    /// let report = old_pkg.diff_database_mode(&new_pkg, "Data", &keys, &DiffConfig::default())?;
    /// println!("complete={} ops={}", report.complete, report.ops.len());
    /// # Ok(())
    /// # }
    /// ```
    pub fn diff_database_mode(
        &self,
        other: &Self,
        sheet_name: &str,
        key_columns: &[u32],
        config: &DiffConfig,
    ) -> Result<DiffReport, DiffError> {
        crate::with_default_session(|session| {
            self.diff_database_mode_with_pool(
                other,
                sheet_name,
                key_columns,
                &mut session.strings,
                config,
            )
        })
    }

    /// Like [`WorkbookPackage::diff_database_mode`], but uses a caller-provided string pool.
    pub fn diff_database_mode_with_pool(
        &self,
        other: &Self,
        sheet_name: &str,
        key_columns: &[u32],
        pool: &mut StringPool,
        config: &DiffConfig,
    ) -> Result<DiffReport, DiffError> {
        let (old_sheet, new_sheet, sheet_id) =
            find_sheets_case_insensitive(&self.workbook, &other.workbook, sheet_name, pool)?;

        let mut sink = VecSink::new();
        let mut op_count = 0usize;

        let summary = crate::engine::try_diff_grids_database_mode_streaming(
            sheet_id,
            &old_sheet.grid,
            &new_sheet.grid,
            key_columns,
            pool,
            config,
            &mut sink,
            &mut op_count,
        )?;

        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        let mut ops = sink.into_ops();
        ops.extend(object_ops);
        ops.extend(m_ops);

        let strings = pool.strings().to_vec();
        let mut report = DiffReport::from_ops_and_summary(ops, summary, strings);
        append_permission_bindings_warnings(&mut report, &self.data_mashup, &other.data_mashup);
        Ok(report)
    }

    /// Streaming database mode diff. Emits ops into `sink` and returns a [`DiffSummary`].
    ///
    /// Streaming output follows the contract in `docs/streaming_contract.md`.
    pub fn diff_database_mode_streaming<S: DiffSink>(
        &self,
        other: &Self,
        sheet_name: &str,
        key_columns: &[u32],
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        crate::with_default_session(|session| {
            self.diff_database_mode_streaming_with_pool(
                other,
                sheet_name,
                key_columns,
                &mut session.strings,
                config,
                sink,
            )
        })
    }

    /// Like [`WorkbookPackage::diff_database_mode_streaming`], but uses a caller-provided string pool.
    pub fn diff_database_mode_streaming_with_pool<S: DiffSink>(
        &self,
        other: &Self,
        sheet_name: &str,
        key_columns: &[u32],
        pool: &mut StringPool,
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        let mut object_ops =
            crate::object_diff::diff_named_ranges(&self.workbook, &other.workbook, pool);
        object_ops.extend(crate::object_diff::diff_charts(
            &self.workbook,
            &other.workbook,
            pool,
        ));
        object_ops.extend(crate::object_diff::diff_vba_modules(
            self.vba_modules.as_deref(),
            other.vba_modules.as_deref(),
            pool,
        ));

        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        let (old_sheet, new_sheet, sheet_id) =
            find_sheets_case_insensitive(&self.workbook, &other.workbook, sheet_name, pool)?;

        let grid_result = {
            let mut no_finish = NoFinishSink::new(sink);
            crate::engine::try_diff_grids_database_mode_streaming(
                sheet_id,
                &old_sheet.grid,
                &new_sheet.grid,
                key_columns,
                pool,
                config,
                &mut no_finish,
                &mut 0usize,
            )
        };

        let mut summary = match grid_result {
            Ok(summary) => summary,
            Err(e) => {
                let _ = sink.finish();
                return Err(e);
            }
        };

        for op in object_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        for op in m_ops {
            if let Err(e) = sink.emit(op) {
                let _ = sink.finish();
                return Err(e);
            }
            summary.op_count = summary.op_count.saturating_add(1);
        }

        sink.finish()?;

        append_permission_bindings_warnings_summary(&mut summary, &self.data_mashup, &other.data_mashup);
        Ok(summary)
    }
}

/// A parsed PBIX/PBIT package (Power BI) containing Power Query data.
#[derive(Debug, Clone)]
pub struct PbixPackage {
    pub(crate) data_mashup: Option<DataMashup>,
    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    pub(crate) model_schema: Option<crate::tabular_schema::RawTabularModel>,
}

impl PbixPackage {
    #[cfg(feature = "excel-open-xml")]
    pub fn open<R: std::io::Read + std::io::Seek + 'static>(
        reader: R,
    ) -> Result<Self, crate::excel_open_xml::PackageError> {
        let mut container = ZipContainer::open_from_reader(reader)?;

        let data_mashup_opt = container.read_file_optional_checked("DataMashup")?;

        let data_mashup = if let Some(bytes) = data_mashup_opt {
            let raw = crate::datamashup_framing::parse_data_mashup(&bytes)?;
            Some(crate::datamashup::build_data_mashup(&raw)?)
        } else {
            None
        };

        #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
        let mut model_schema = None;

        if data_mashup.is_none() {
            if looks_like_pbix(&container) {
                #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
                {
                    if let Some(bytes) = container.read_file_optional_checked("DataModelSchema")? {
                        model_schema =
                            Some(crate::tabular_schema::parse_data_model_schema(&bytes)?);
                        return Ok(Self {
                            data_mashup,
                            model_schema,
                        });
                    }
                }

                return Err(crate::excel_open_xml::PackageError::NoDataMashupUseTabularModel);
            }

            return Err(crate::excel_open_xml::PackageError::UnsupportedFormat {
                message: "missing DataMashup at ZIP root".to_string(),
            });
        }

        Ok(Self {
            data_mashup,
            #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
            model_schema,
        })
    }

    #[cfg(feature = "excel-open-xml")]
    pub fn open_with_limits<R: std::io::Read + std::io::Seek + 'static>(
        reader: R,
        limits: crate::ContainerLimits,
    ) -> Result<Self, crate::excel_open_xml::PackageError> {
        let mut container = ZipContainer::open_from_reader_with_limits(reader, limits)?;

        let data_mashup_opt = container.read_file_optional_checked("DataMashup")?;

        let data_mashup = if let Some(bytes) = data_mashup_opt {
            let raw = crate::datamashup_framing::parse_data_mashup(&bytes)?;
            Some(crate::datamashup::build_data_mashup(&raw)?)
        } else {
            None
        };

        #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
        let mut model_schema = None;

        if data_mashup.is_none() {
            if looks_like_pbix(&container) {
                #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
                {
                    if let Some(bytes) = container.read_file_optional_checked("DataModelSchema")? {
                        model_schema =
                            Some(crate::tabular_schema::parse_data_model_schema(&bytes)?);
                        return Ok(Self {
                            data_mashup,
                            model_schema,
                        });
                    }
                }

                return Err(crate::excel_open_xml::PackageError::NoDataMashupUseTabularModel);
            }

            return Err(crate::excel_open_xml::PackageError::UnsupportedFormat {
                message: "missing DataMashup at ZIP root".to_string(),
            });
        }

        Ok(Self {
            data_mashup,
            #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
            model_schema,
        })
    }

    pub fn data_mashup(&self) -> Option<&DataMashup> {
        self.data_mashup.as_ref()
    }

    pub fn diff(&self, other: &Self, config: &DiffConfig) -> DiffReport {
        crate::with_default_session(|session| {
            let mut report = DiffReport::new(Vec::new());
            let mut ops = crate::m_diff::diff_m_ops_for_packages(
                &self.data_mashup,
                &other.data_mashup,
                &mut session.strings,
                config,
            );

            report.ops.append(&mut ops);

            #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
            {
                let old_raw = self.model_schema.as_ref();
                let new_raw = other.model_schema.as_ref();

                if old_raw.is_some() || new_raw.is_some() {
                    let old_model = old_raw
                        .map(|r| crate::tabular_schema::build_model(r, &mut session.strings))
                        .unwrap_or_default();

                    let new_model = new_raw
                        .map(|r| crate::tabular_schema::build_model(r, &mut session.strings))
                        .unwrap_or_default();

                    let model_result = crate::model_diff::diff_models(
                        &old_model,
                        &new_model,
                        &mut session.strings,
                        config,
                    );
                    report.ops.extend(model_result.ops);
                    if !model_result.complete {
                        report.complete = false;
                        report.warnings.extend(model_result.warnings);
                    }
                }
            }

            report.strings = session.strings.strings().to_vec();
            append_permission_bindings_warnings(&mut report, &self.data_mashup, &other.data_mashup);
            report
        })
    }

    /// Stream a PBIX/PBIT diff into `sink`.
    ///
    /// Streaming output follows the contract in `docs/streaming_contract.md`.
    pub fn diff_streaming<S: DiffSink>(
        &self,
        other: &Self,
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        crate::with_default_session(|session| {
            self.diff_streaming_with_pool(other, &mut session.strings, config, sink)
        })
    }

    pub fn diff_streaming_with_pool<S: DiffSink>(
        &self,
        other: &Self,
        pool: &mut StringPool,
        config: &DiffConfig,
        sink: &mut S,
    ) -> Result<DiffSummary, DiffError> {
        let m_ops = crate::m_diff::diff_m_ops_for_packages(
            &self.data_mashup,
            &other.data_mashup,
            pool,
            config,
        );

        #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
        let model_result = {
            let old_raw = self.model_schema.as_ref();
            let new_raw = other.model_schema.as_ref();

            if old_raw.is_some() || new_raw.is_some() {
                let old_model = old_raw
                    .map(|r| crate::tabular_schema::build_model(r, pool))
                    .unwrap_or_default();

                let new_model = new_raw
                    .map(|r| crate::tabular_schema::build_model(r, pool))
                    .unwrap_or_default();

                Some(crate::model_diff::diff_models(&old_model, &new_model, pool, config))
            } else {
                None
            }
        };

        sink.begin(pool)?;
        let mut finish_guard = SinkFinishGuard::new(sink);

        let mut op_count = 0usize;
        #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
        let (mut complete, mut warnings) = (true, Vec::new());
        #[cfg(not(all(feature = "model-diff", feature = "excel-open-xml")))]
        let (complete, warnings) = (true, Vec::new());
        for op in m_ops {
            sink.emit(op)?;
            op_count = op_count.saturating_add(1);
        }

        #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
        if let Some(model_result) = model_result {
            for op in model_result.ops {
                sink.emit(op)?;
                op_count = op_count.saturating_add(1);
            }

            if !model_result.complete {
                complete = false;
                warnings.extend(model_result.warnings);
            }
        }

        finish_guard.finish_and_disarm()?;

        let mut summary = DiffSummary {
            complete,
            warnings,
            op_count,
            #[cfg(feature = "perf-metrics")]
            metrics: None,
        };
        append_permission_bindings_warnings_summary(&mut summary, &self.data_mashup, &other.data_mashup);
        Ok(summary)
    }

    pub fn diff_streaming_with_progress<S: DiffSink>(
        &self,
        other: &Self,
        config: &DiffConfig,
        sink: &mut S,
        progress: &dyn ProgressCallback,
    ) -> Result<DiffSummary, DiffError> {
        progress.on_progress("m_diff", 0.0);
        let out = self.diff_streaming(other, config, sink);
        progress.on_progress("m_diff", 1.0);
        out
    }
}

#[cfg(feature = "perf-metrics")]
fn apply_parse_metrics(
    old_pkg: &WorkbookPackage,
    new_pkg: &WorkbookPackage,
    metrics: &mut Option<DiffMetrics>,
) {
    let Some(m) = metrics.as_mut() else {
        return;
    };

    let added = old_pkg
        .parse_time_ms
        .saturating_add(new_pkg.parse_time_ms);
    m.parse_time_ms = m.parse_time_ms.saturating_add(added);
    m.total_time_ms = m.total_time_ms.saturating_add(added);
    m.diff_time_ms = m.total_time_ms.saturating_sub(m.parse_time_ms);
}

#[cfg(feature = "excel-open-xml")]
fn looks_like_pbix(container: &ZipContainer) -> bool {
    container.file_names().any(|name| {
        name == "Report/Layout"
            || name == "Report/Version"
            || name == "DataModelSchema"
            || name == "DataModel"
            || name == "Connections"
            || name == "DiagramLayout"
    })
}

fn find_sheets_case_insensitive<'a>(
    old_wb: &'a Workbook,
    new_wb: &'a Workbook,
    sheet_name: &str,
    pool: &StringPool,
) -> Result<(&'a Sheet, &'a Sheet, SheetId), DiffError> {
    let sheet_name_lower = sheet_name.to_lowercase();

    let old_sheet = old_wb.sheets.iter().find(|s| {
        let name = pool.resolve(s.name);
        name.to_lowercase() == sheet_name_lower
    });

    let new_sheet = new_wb.sheets.iter().find(|s| {
        let name = pool.resolve(s.name);
        name.to_lowercase() == sheet_name_lower
    });

    match (old_sheet, new_sheet) {
        (Some(old), Some(new)) => {
            let sheet_id = old.name;
            Ok((old, new, sheet_id))
        }
        _ => {
            let mut available: Vec<String> = old_wb
                .sheets
                .iter()
                .map(|s| pool.resolve(s.name).to_string())
                .collect();
            for s in &new_wb.sheets {
                let name = pool.resolve(s.name).to_string();
                if !available.iter().any(|n| n.to_lowercase() == name.to_lowercase()) {
                    available.push(name);
                }
            }
            available.sort();
            Err(DiffError::SheetNotFound {
                requested: sheet_name.to_string(),
                available,
            })
        }
    }
}

fn append_permission_bindings_warnings(
    report: &mut DiffReport,
    old_dm: &Option<DataMashup>,
    new_dm: &Option<DataMashup>,
) {
    for warning in collect_permission_bindings_warnings(old_dm, new_dm) {
        report.add_warning(warning);
    }
}

fn append_permission_bindings_warnings_summary(
    summary: &mut DiffSummary,
    old_dm: &Option<DataMashup>,
    new_dm: &Option<DataMashup>,
) {
    let warnings = collect_permission_bindings_warnings(old_dm, new_dm);
    if warnings.is_empty() {
        return;
    }
    summary.complete = false;
    summary.warnings.extend(warnings);
}

fn collect_permission_bindings_warnings(
    old_dm: &Option<DataMashup>,
    new_dm: &Option<DataMashup>,
) -> Vec<String> {
    let mut warn_unverifiable = false;
    let mut warn_invalid = false;

    for dm in [old_dm, new_dm].iter().filter_map(|dm| dm.as_ref()) {
        match dm.permission_bindings_status {
            PermissionBindingsStatus::Unverifiable => warn_unverifiable = true,
            PermissionBindingsStatus::InvalidOrTampered => warn_invalid = true,
            _ => {}
        }
    }

    let mut warnings = Vec::new();
    if warn_invalid {
        if let Some(warning) =
            permission_bindings_warning(PermissionBindingsStatus::InvalidOrTampered)
        {
            warnings.push(warning);
        }
    }
    if warn_unverifiable {
        if let Some(warning) =
            permission_bindings_warning(PermissionBindingsStatus::Unverifiable)
        {
            warnings.push(warning);
        }
    }
    warnings
}

#[cfg(test)]
mod tests {
    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    use super::*;
    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    use crate::{
        DiffOp, Metadata, PackageParts, PackageXml, PermissionBindingsStatus, Permissions,
        SectionDocument,
    };

    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    use crate::tabular_schema::{RawMeasure, RawTabularModel};

    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    fn make_dm(section_source: &str) -> DataMashup {
        DataMashup {
            version: 0,
            package_parts: PackageParts {
                package_xml: PackageXml {
                    raw_xml: "<Package/>".to_string(),
                },
                main_section: SectionDocument {
                    source: section_source.to_string(),
                },
                embedded_contents: Vec::new(),
            },
            permissions: Permissions::default(),
            metadata: Metadata { formulas: Vec::new() },
            permission_bindings_raw: Vec::new(),
            permission_bindings_status: PermissionBindingsStatus::Missing,
        }
    }

    #[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
    #[test]
    fn pbix_streaming_orders_query_ops_before_model_ops() {
        let dm_a = make_dm("section Section1;\nshared Foo = 1;");
        let dm_b = make_dm("section Section1;\nshared Bar = 1;");

        let raw_a = RawTabularModel {
            tables: Vec::new(),
            relationships: Vec::new(),
            measures: vec![RawMeasure {
                full_name: "Table/Measure1".to_string(),
                expression: "1".to_string(),
            }],
        };
        let raw_b = RawTabularModel {
            tables: Vec::new(),
            relationships: Vec::new(),
            measures: vec![RawMeasure {
                full_name: "Table/Measure1".to_string(),
                expression: "2".to_string(),
            }],
        };

        let pkg_a = PbixPackage {
            data_mashup: Some(dm_a),
            model_schema: Some(raw_a),
        };
        let pkg_b = PbixPackage {
            data_mashup: Some(dm_b),
            model_schema: Some(raw_b),
        };

        let mut pool = StringPool::new();
        let mut sink = VecSink::new();
        pkg_a
            .diff_streaming_with_pool(&pkg_b, &mut pool, &DiffConfig::default(), &mut sink)
            .expect("pbix streaming should succeed");
        let ops = sink.into_ops();

        assert!(ops.iter().any(DiffOp::is_m_op), "expected query ops");
        assert!(
            ops.iter().any(DiffOp::is_model_op),
            "expected model ops"
        );

        let mut seen_model = false;
        for op in ops {
            if op.is_model_op() {
                seen_model = true;
                continue;
            }
            if op.is_m_op() && seen_model {
                panic!("query op appeared after model ops");
            }
        }
    }
}

```

---

### File: `core\src\perf.rs`

```rust
use std::collections::HashMap;
use std::time::Instant;

#[cfg(not(target_arch = "wasm32"))]
use crate::memory_metrics;

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Phase {
    Total,
    Parse,
    SignatureBuild,
    MoveDetection,
    Alignment,
    CellDiff,
    OpEmit,
    ReportSerialize,
}

#[derive(Debug, Clone, Default, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
#[serde(default)]
pub struct DiffMetrics {
    pub parse_time_ms: u64,
    pub signature_build_time_ms: u64,
    pub move_detection_time_ms: u64,
    pub alignment_time_ms: u64,
    pub cell_diff_time_ms: u64,
    pub op_emit_time_ms: u64,
    pub report_serialize_time_ms: u64,
    pub total_time_ms: u64,
    pub diff_time_ms: u64,
    pub peak_memory_bytes: u64,
    pub grid_storage_bytes: u64,
    pub string_pool_bytes: u64,
    pub op_buffer_bytes: u64,
    pub alignment_buffer_bytes: u64,
    pub rows_processed: u64,
    pub cells_compared: u64,
    pub anchors_found: u32,
    pub moves_detected: u32,
    pub hash_lookups_est: u64,
    pub allocations_est: u64,
    #[serde(skip)]
    phase_start: HashMap<Phase, Instant>,
}

impl DiffMetrics {
    pub fn start_phase(&mut self, phase: Phase) {
        if matches!(phase, Phase::Total) {
            #[cfg(not(target_arch = "wasm32"))]
            memory_metrics::reset_peak_to_current();
        }
        self.phase_start.insert(phase, Instant::now());
    }

    pub fn end_phase(&mut self, phase: Phase) {
        if let Some(start) = self.phase_start.remove(&phase) {
            let elapsed = start.elapsed().as_millis() as u64;
            match phase {
                Phase::Parse => self.parse_time_ms += elapsed,
                Phase::SignatureBuild => self.signature_build_time_ms += elapsed,
                Phase::MoveDetection => self.move_detection_time_ms += elapsed,
                Phase::Alignment => self.alignment_time_ms += elapsed,
                Phase::CellDiff => self.cell_diff_time_ms += elapsed,
                Phase::OpEmit => self.op_emit_time_ms += elapsed,
                Phase::ReportSerialize => self.report_serialize_time_ms += elapsed,
                Phase::Total => {
                    self.total_time_ms += elapsed;
                    self.diff_time_ms = self.total_time_ms.saturating_sub(self.parse_time_ms);
                    #[cfg(not(target_arch = "wasm32"))]
                    {
                        self.peak_memory_bytes = memory_metrics::peak_bytes();
                    }
                }
            }
        }
    }

    pub fn add_cells_compared(&mut self, count: u64) {
        self.cells_compared = self.cells_compared.saturating_add(count);
    }

    pub fn add_hash_lookups_est(&mut self, count: u64) {
        self.hash_lookups_est = self.hash_lookups_est.saturating_add(count);
    }

    pub fn add_allocations_est(&mut self, count: u64) {
        self.allocations_est = self.allocations_est.saturating_add(count);
    }

    pub fn phase_guard(&mut self, phase: Phase) -> PhaseGuard<'_> {
        PhaseGuard::new(self, phase)
    }
}

pub struct PhaseGuard<'a> {
    metrics: &'a mut DiffMetrics,
    phase: Phase,
}

impl<'a> PhaseGuard<'a> {
    pub fn new(metrics: &'a mut DiffMetrics, phase: Phase) -> Self {
        metrics.start_phase(phase);
        Self { metrics, phase }
    }
}

impl Drop for PhaseGuard<'_> {
    fn drop(&mut self) {
        self.metrics.end_phase(self.phase);
    }
}

```

---

### File: `core\src\permission_bindings.rs`

```rust
use crate::datamashup::Permissions;
use crate::datamashup_framing::RawDataMashup;
use crate::error_codes;
use sha2::{Digest, Sha256};

const DPAPI_ENTROPY: &[u8] = b"DataExplorer Package Components";
const SHA256_LEN: usize = 32;

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PermissionBindingsKind {
    Missing,
    NullByteSentinel,
    DpapiEncryptedBlob,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PermissionBindingsStatus {
    Missing,
    Disabled,
    Verified,
    InvalidOrTampered,
    Unverifiable,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DpapiDecryptError {
    Unavailable,
    Failed,
}

pub trait DpapiDecryptor {
    fn decrypt(&self, blob: &[u8], entropy: &[u8]) -> Result<Vec<u8>, DpapiDecryptError>;
}

#[allow(dead_code)]
pub struct NoDpapiDecryptor;

impl DpapiDecryptor for NoDpapiDecryptor {
    fn decrypt(&self, _blob: &[u8], _entropy: &[u8]) -> Result<Vec<u8>, DpapiDecryptError> {
        Err(DpapiDecryptError::Unavailable)
    }
}

#[cfg(all(windows, feature = "dpapi"))]
pub struct WindowsDpapiDecryptor;

#[cfg(all(windows, feature = "dpapi"))]
impl DpapiDecryptor for WindowsDpapiDecryptor {
    fn decrypt(&self, blob: &[u8], entropy: &[u8]) -> Result<Vec<u8>, DpapiDecryptError> {
        use windows_sys::Win32::Foundation::LocalFree;
        use windows_sys::Win32::Security::Cryptography::CRYPTPROTECT_UI_FORBIDDEN;
        use windows_sys::Win32::Security::Cryptography::CRYPT_INTEGER_BLOB;
        use windows_sys::Win32::Security::Cryptography::CryptUnprotectData;

        if blob.is_empty() {
            return Err(DpapiDecryptError::Failed);
        }

        let in_blob = CRYPT_INTEGER_BLOB {
            cbData: blob.len() as u32,
            pbData: blob.as_ptr() as *mut u8,
        };
        let mut out_blob = CRYPT_INTEGER_BLOB {
            cbData: 0,
            pbData: std::ptr::null_mut(),
        };

        let entropy_blob = CRYPT_INTEGER_BLOB {
            cbData: entropy.len() as u32,
            pbData: entropy.as_ptr() as *mut u8,
        };
        let entropy_ptr = if entropy.is_empty() {
            std::ptr::null()
        } else {
            &entropy_blob as *const CRYPT_INTEGER_BLOB
        };

        let ok = unsafe {
            CryptUnprotectData(
                &in_blob as *const CRYPT_INTEGER_BLOB,
                std::ptr::null_mut(),
                entropy_ptr,
                std::ptr::null(),
                std::ptr::null_mut(),
                CRYPTPROTECT_UI_FORBIDDEN,
                &mut out_blob as *mut CRYPT_INTEGER_BLOB,
            )
        };

        if ok == 0 || out_blob.pbData.is_null() {
            return Err(DpapiDecryptError::Failed);
        }

        let data = unsafe {
            let slice = std::slice::from_raw_parts(out_blob.pbData, out_blob.cbData as usize);
            let out = slice.to_vec();
            LocalFree(out_blob.pbData as *mut core::ffi::c_void);
            out
        };

        Ok(data)
    }
}

pub(crate) fn default_dpapi_decryptor() -> &'static dyn DpapiDecryptor {
    #[cfg(all(windows, feature = "dpapi"))]
    {
        static DECRYPTOR: WindowsDpapiDecryptor = WindowsDpapiDecryptor;
        &DECRYPTOR
    }
    #[cfg(not(all(windows, feature = "dpapi")))]
    {
        static DECRYPTOR: NoDpapiDecryptor = NoDpapiDecryptor;
        &DECRYPTOR
    }
}

pub fn classify_permission_bindings(raw: &[u8]) -> PermissionBindingsKind {
    if raw.is_empty() {
        return PermissionBindingsKind::Missing;
    }
    if raw.len() == 1 && raw[0] == 0x00 {
        return PermissionBindingsKind::NullByteSentinel;
    }
    PermissionBindingsKind::DpapiEncryptedBlob
}

pub fn validate_permission_bindings(
    raw: &RawDataMashup,
    decryptor: &dyn DpapiDecryptor,
) -> PermissionBindingsStatus {
    match classify_permission_bindings(&raw.permission_bindings) {
        PermissionBindingsKind::Missing => PermissionBindingsStatus::Missing,
        PermissionBindingsKind::NullByteSentinel => PermissionBindingsStatus::Disabled,
        PermissionBindingsKind::DpapiEncryptedBlob => {
            let plaintext = match decryptor.decrypt(&raw.permission_bindings, DPAPI_ENTROPY) {
                Ok(bytes) => bytes,
                Err(_) => return PermissionBindingsStatus::Unverifiable,
            };

            let Some((package_hash, permissions_hash)) = parse_plaintext_hashes(&plaintext) else {
                return PermissionBindingsStatus::InvalidOrTampered;
            };

            let expected_package_hash = sha256(&raw.package_parts);
            let expected_permissions_hash = sha256(&raw.permissions);

            if package_hash == expected_package_hash && permissions_hash == expected_permissions_hash {
                PermissionBindingsStatus::Verified
            } else {
                PermissionBindingsStatus::InvalidOrTampered
            }
        }
    }
}

pub fn effective_permissions(
    parsed_permissions: Permissions,
    status: PermissionBindingsStatus,
) -> Permissions {
    match status {
        PermissionBindingsStatus::Missing
        | PermissionBindingsStatus::Disabled
        | PermissionBindingsStatus::Verified => parsed_permissions,
        PermissionBindingsStatus::InvalidOrTampered | PermissionBindingsStatus::Unverifiable => {
            Permissions::default()
        }
    }
}

pub fn permission_bindings_warning(status: PermissionBindingsStatus) -> Option<String> {
    let default_desc = "FirewallEnabled=true and WorkbookGroupType=null";
    match status {
        PermissionBindingsStatus::Unverifiable => Some(format!(
            "[{}] Permission bindings are DPAPI-encrypted and could not be validated on this platform; permissions have been defaulted to {} to match Excel fallback behavior.",
            error_codes::DM_PERMISSION_BINDINGS_UNVERIFIED,
            default_desc
        )),
        PermissionBindingsStatus::InvalidOrTampered => Some(format!(
            "[{}] Permission bindings failed validation (hash mismatch or malformed plaintext); permissions have been defaulted to {} to match Excel fallback behavior.",
            error_codes::DM_PERMISSION_BINDINGS_UNVERIFIED,
            default_desc
        )),
        _ => None,
    }
}

fn parse_plaintext_hashes(plaintext: &[u8]) -> Option<([u8; SHA256_LEN], [u8; SHA256_LEN])> {
    let (len_a, rest) = read_length_prefix(plaintext)?;
    if len_a != SHA256_LEN {
        return None;
    }
    let (hash_a, rest) = rest.split_at(len_a);
    let (len_b, rest) = read_length_prefix(rest)?;
    if len_b != SHA256_LEN || rest.len() != len_b {
        return None;
    }
    let hash_b = rest;

    let hash_a: [u8; SHA256_LEN] = hash_a.try_into().ok()?;
    let hash_b: [u8; SHA256_LEN] = hash_b.try_into().ok()?;
    Some((hash_a, hash_b))
}

fn read_length_prefix(bytes: &[u8]) -> Option<(usize, &[u8])> {
    if bytes.len() < 4 {
        return None;
    }
    let len = u32::from_le_bytes(bytes[0..4].try_into().ok()?) as usize;
    let rest = bytes.get(4..)?;
    if rest.len() < len {
        return None;
    }
    Some((len, rest))
}

fn sha256(bytes: &[u8]) -> [u8; SHA256_LEN] {
    let mut hasher = Sha256::new();
    hasher.update(bytes);
    hasher.finalize().into()
}

```

---

### File: `core\src\policy.rs`

```rust
use crate::config::DiffConfig;

pub const AUTO_STREAM_CELL_THRESHOLD: u64 = 1_000_000;

pub fn should_use_large_mode(estimated_cell_volume: u64, _config: &DiffConfig) -> bool {
    estimated_cell_volume >= AUTO_STREAM_CELL_THRESHOLD
}

```

---

### File: `core\src\progress.rs`

```rust
/// Progress reporting for long-running diffs.
///
/// The diff engine may call the callback at throttled intervals with a best-effort percentage in
/// the range `[0.0, 1.0]`. Callers should treat progress as advisory and not assume monotonicity
/// across phases.

pub trait ProgressCallback: Send {
    fn on_progress(&self, phase: &str, percent: f32);
}

#[derive(Debug, Default, Clone, Copy)]
pub struct NoProgress;

impl ProgressCallback for NoProgress {
    fn on_progress(&self, _phase: &str, _percent: f32) {}
}


```

---

### File: `core\src\rect_block_move.rs`

```rust
//! Rectangular block move detection.
//!
//! This module implements detection of rectangular regions that have moved
//! between two grids. A rect block move is when a 2D region (rows  cols)
//! moves from one position to another without internal edits.
//!
//! This is used by the engine's masked move detection loop to identify
//! structural changes that preserve content but change position.

use crate::config::DiffConfig;
use crate::grid_view::{ColHash, GridView, HashStats, RowHash};
use crate::workbook::{Cell, Grid};

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub(crate) struct RectBlockMove {
    pub src_start_row: u32,
    pub src_row_count: u32,
    pub src_start_col: u32,
    pub src_col_count: u32,
    pub dst_start_row: u32,
    pub dst_start_col: u32,
    pub block_hash: Option<u64>,
}

pub(crate) fn detect_exact_rect_block_move(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RectBlockMove> {
    if old.nrows != new.nrows || old.ncols != new.ncols {
        return None;
    }

    if old.nrows == 0 || old.ncols == 0 {
        return None;
    }

    if !is_within_size_bounds(old, new, config) {
        return None;
    }

    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);

    if view_a.is_low_info_dominated() || view_b.is_low_info_dominated() {
        return None;
    }

    if view_a.is_blank_dominated() || view_b.is_blank_dominated() {
        return None;
    }

    let row_stats = HashStats::from_row_meta(&view_a.row_meta, &view_b.row_meta);
    let col_stats = HashStats::from_col_meta(&view_a.col_meta, &view_b.col_meta);

    if row_stats.has_heavy_repetition(config.alignment.max_hash_repeat)
        || col_stats.has_heavy_repetition(config.alignment.max_hash_repeat)
    {
        return None;
    }

    let shared_rows = row_stats
        .freq_a
        .keys()
        .filter(|h| row_stats.freq_b.contains_key(*h))
        .count();
    let shared_cols = col_stats
        .freq_a
        .keys()
        .filter(|h| col_stats.freq_b.contains_key(*h))
        .count();
    if shared_rows == 0 && shared_cols == 0 {
        return None;
    }

    let diff_positions = collect_differences(old, new);
    if diff_positions.is_empty() {
        return None;
    }

    let row_ranges = find_two_equal_ranges(diff_positions.iter().map(|(r, _)| *r))?;
    let col_ranges = find_two_equal_ranges(diff_positions.iter().map(|(_, c)| *c))?;

    let row_count = range_len(row_ranges.0);
    let col_count = range_len(col_ranges.0);

    let expected_mismatches = row_count.checked_mul(col_count)?.checked_mul(2)?;
    if diff_positions.len() as u32 != expected_mismatches {
        return None;
    }

    let mismatches = count_rect_mismatches(old, new, row_ranges.0, col_ranges.0)
        + count_rect_mismatches(old, new, row_ranges.1, col_ranges.1);
    if mismatches != diff_positions.len() as u32 {
        return None;
    }

    if !has_unique_meta(
        &view_a, &view_b, &row_stats, &col_stats, row_ranges, col_ranges,
    ) {
        return None;
    }

    let primary = validate_orientation(old, new, row_ranges, col_ranges);
    let swapped_ranges = ((row_ranges.1, row_ranges.0), (col_ranges.1, col_ranges.0));
    let alternate = validate_orientation(old, new, swapped_ranges.0, swapped_ranges.1);

    match (primary, alternate) {
        (Some(mv), None) => Some(mv),
        (None, Some(mv)) => Some(mv),
        _ => None,
    }
}

fn validate_orientation(
    old: &Grid,
    new: &Grid,
    row_ranges: ((u32, u32), (u32, u32)),
    col_ranges: ((u32, u32), (u32, u32)),
) -> Option<RectBlockMove> {
    if ranges_overlap(row_ranges.0, row_ranges.1) && ranges_overlap(col_ranges.0, col_ranges.1) {
        return None;
    }

    let row_count = range_len(row_ranges.0);
    let col_count = range_len(col_ranges.0);

    if rectangles_correspond(
        old,
        new,
        row_ranges.0,
        col_ranges.0,
        row_ranges.1,
        col_ranges.1,
    ) {
        return Some(RectBlockMove {
            src_start_row: row_ranges.0.0,
            src_row_count: row_count,
            src_start_col: col_ranges.0.0,
            src_col_count: col_count,
            dst_start_row: row_ranges.1.0,
            dst_start_col: col_ranges.1.0,
            block_hash: None,
        });
    }

    None
}

fn rectangles_correspond(
    old: &Grid,
    new: &Grid,
    src_rows: (u32, u32),
    src_cols: (u32, u32),
    dst_rows: (u32, u32),
    dst_cols: (u32, u32),
) -> bool {
    let row_count = range_len(src_rows);
    let col_count = range_len(src_cols);

    if row_count != range_len(dst_rows) || col_count != range_len(dst_cols) {
        return false;
    }

    for dr in 0..row_count {
        for dc in 0..col_count {
            let src_r = src_rows.0 + dr;
            let src_c = src_cols.0 + dc;
            let dst_r = dst_rows.0 + dr;
            let dst_c = dst_cols.0 + dc;

            if !cell_content_equal(old.get(src_r, src_c), new.get(dst_r, dst_c)) {
                return false;
            }
        }
    }

    true
}

fn collect_differences(old: &Grid, new: &Grid) -> Vec<(u32, u32)> {
    let mut diffs = Vec::new();

    for row in 0..old.nrows {
        for col in 0..old.ncols {
            if !cell_content_equal(old.get(row, col), new.get(row, col)) {
                diffs.push((row, col));
            }
        }
    }

    diffs
}

fn cell_content_equal(a: Option<&Cell>, b: Option<&Cell>) -> bool {
    match (a, b) {
        (None, None) => true,
        (Some(cell_a), Some(cell_b)) => {
            cell_a.value == cell_b.value && cell_a.formula == cell_b.formula
        }
        (Some(cell_a), None) => cell_a.value.is_none() && cell_a.formula.is_none(),
        (None, Some(cell_b)) => cell_b.value.is_none() && cell_b.formula.is_none(),
    }
}

fn count_rect_mismatches(old: &Grid, new: &Grid, rows: (u32, u32), cols: (u32, u32)) -> u32 {
    let mut mismatches = 0u32;
    for row in rows.0..=rows.1 {
        for col in cols.0..=cols.1 {
            if !cell_content_equal(old.get(row, col), new.get(row, col)) {
                mismatches = mismatches.saturating_add(1);
            }
        }
    }
    mismatches
}

fn has_unique_meta(
    view_a: &GridView<'_>,
    view_b: &GridView<'_>,
    row_stats: &HashStats<RowHash>,
    col_stats: &HashStats<ColHash>,
    row_ranges: ((u32, u32), (u32, u32)),
    col_ranges: ((u32, u32), (u32, u32)),
) -> bool {
    for range in [row_ranges.0, row_ranges.1] {
        for idx in range.0..=range.1 {
            if !is_unique_row_in_a(idx, view_a, row_stats)
                || !is_unique_row_in_b(idx, view_b, row_stats)
            {
                return false;
            }
        }
    }

    for range in [col_ranges.0, col_ranges.1] {
        for idx in range.0..=range.1 {
            if !is_unique_col_in_a(idx, view_a, col_stats)
                || !is_unique_col_in_b(idx, view_b, col_stats)
            {
                return false;
            }
        }
    }

    true
}

fn is_unique_row_in_a(idx: u32, view: &GridView<'_>, stats: &HashStats<RowHash>) -> bool {
    view.row_meta
        .get(idx as usize)
        .map(|meta| unique_in_a(meta.signature, stats))
        .unwrap_or(false)
}

fn is_unique_row_in_b(idx: u32, view: &GridView<'_>, stats: &HashStats<RowHash>) -> bool {
    view.row_meta
        .get(idx as usize)
        .map(|meta| unique_in_b(meta.signature, stats))
        .unwrap_or(false)
}

fn is_unique_col_in_a(idx: u32, view: &GridView<'_>, stats: &HashStats<ColHash>) -> bool {
    view.col_meta
        .get(idx as usize)
        .map(|meta| unique_in_a(meta.hash, stats))
        .unwrap_or(false)
}

fn is_unique_col_in_b(idx: u32, view: &GridView<'_>, stats: &HashStats<ColHash>) -> bool {
    view.col_meta
        .get(idx as usize)
        .map(|meta| unique_in_b(meta.hash, stats))
        .unwrap_or(false)
}

fn find_two_equal_ranges<I>(indices: I) -> Option<((u32, u32), (u32, u32))>
where
    I: IntoIterator<Item = u32>,
{
    let mut values: Vec<u32> = indices.into_iter().collect();
    if values.is_empty() {
        return None;
    }

    values.sort_unstable();
    values.dedup();

    let mut ranges: Vec<(u32, u32)> = Vec::new();
    let mut start = values[0];
    let mut prev = values[0];

    for &val in values.iter().skip(1) {
        if val == prev + 1 {
            prev = val;
            continue;
        }

        ranges.push((start, prev));
        start = val;
        prev = val;
    }
    ranges.push((start, prev));

    match ranges.len() {
        1 => Some((ranges[0], ranges[0])),
        2 => {
            let len0 = range_len(ranges[0]);
            let len1 = range_len(ranges[1]);
            if len0 != len1 {
                return None;
            }
            Some((ranges[0], ranges[1]))
        }
        _ => None,
    }
}

fn range_len(range: (u32, u32)) -> u32 {
    range.1.saturating_sub(range.0).saturating_add(1)
}

fn ranges_overlap(a: (u32, u32), b: (u32, u32)) -> bool {
    !(a.1 < b.0 || b.1 < a.0)
}

fn is_within_size_bounds(old: &Grid, new: &Grid, config: &DiffConfig) -> bool {
    let rows = old.nrows.max(new.nrows);
    let cols = old.ncols.max(new.ncols);
    rows <= config.alignment.max_align_rows && cols <= config.alignment.max_align_cols
}

fn unique_in_a<H>(hash: H, stats: &HashStats<H>) -> bool
where
    H: Eq + std::hash::Hash + Copy,
{
    stats.freq_a.get(&hash).copied().unwrap_or(0) == 1
        && stats.freq_b.get(&hash).copied().unwrap_or(0) <= 1
}

fn unique_in_b<H>(hash: H, stats: &HashStats<H>) -> bool
where
    H: Eq + std::hash::Hash + Copy,
{
    stats.freq_b.get(&hash).copied().unwrap_or(0) == 1
        && stats.freq_a.get(&hash).copied().unwrap_or(0) <= 1
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::workbook::CellValue;

    fn grid_from_numbers(values: &[&[i32]]) -> Grid {
        let nrows = values.len() as u32;
        let ncols = if nrows == 0 {
            0
        } else {
            values[0].len() as u32
        };

        let mut grid = Grid::new(nrows, ncols);
        for (r, row_vals) in values.iter().enumerate() {
            for (c, v) in row_vals.iter().enumerate() {
                grid.insert_cell(r as u32, c as u32, Some(CellValue::Number(*v as f64)), None);
            }
        }

        grid
    }

    fn base_background(rows: usize, cols: usize) -> Vec<Vec<i32>> {
        (0..rows)
            .map(|r| (0..cols).map(|c| (r as i32) * 1_000 + c as i32).collect())
            .collect()
    }

    fn place_block(target: &mut [Vec<i32>], top: usize, left: usize, block: &[Vec<i32>]) {
        for (r_offset, row_vals) in block.iter().enumerate() {
            for (c_offset, value) in row_vals.iter().enumerate() {
                let row = top + r_offset;
                let col = left + c_offset;
                if let Some(row_slice) = target.get_mut(row)
                    && let Some(cell) = row_slice.get_mut(col)
                {
                    *cell = *value;
                }
            }
        }
    }

    fn grid_from_matrix(matrix: Vec<Vec<i32>>) -> Grid {
        let refs: Vec<&[i32]> = matrix.iter().map(|row| row.as_slice()).collect();
        grid_from_numbers(&refs)
    }

    #[test]
    fn detect_simple_rect_block_move_success() {
        let mut grid_a = base_background(12, 12);
        let mut grid_b = base_background(12, 12);

        let block = vec![vec![11, 12, 13], vec![21, 22, 23], vec![31, 32, 33]];

        place_block(&mut grid_a, 1, 1, &block);
        place_block(&mut grid_b, 7, 6, &block);

        let old = grid_from_matrix(grid_a);
        let new = grid_from_matrix(grid_b);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_some(),
            "should detect exact rectangular block move"
        );

        let mv = result.unwrap();
        assert_eq!(mv.src_start_row, 1);
        assert_eq!(mv.src_row_count, 3);
        assert_eq!(mv.src_start_col, 1);
        assert_eq!(mv.src_col_count, 3);
        assert_eq!(mv.dst_start_row, 7);
        assert_eq!(mv.dst_start_col, 6);
    }

    #[test]
    fn detect_rect_block_move_with_shared_columns() {
        let mut grid_a = base_background(10, 10);
        let mut grid_b = base_background(10, 10);

        let block = vec![vec![11, 12], vec![21, 22]];

        place_block(&mut grid_a, 1, 2, &block);
        place_block(&mut grid_b, 6, 2, &block);

        let old = grid_from_matrix(grid_a);
        let new = grid_from_matrix(grid_b);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_some(),
            "should detect a vertical rect move when columns overlap"
        );

        let mv = result.unwrap();
        assert_eq!(mv.src_start_row, 1);
        assert_eq!(mv.dst_start_row, 6);
        assert_eq!(mv.src_start_col, 2);
        assert_eq!(mv.dst_start_col, 2);
        assert_eq!(mv.src_row_count, 2);
        assert_eq!(mv.src_col_count, 2);
    }

    #[test]
    fn detect_bails_on_different_grid_dimensions() {
        let old = grid_from_numbers(&[&[1, 2], &[3, 4]]);
        let new = grid_from_numbers(&[&[1, 2, 5], &[3, 4, 6]]);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(result.is_none(), "different dimensions should bail");
    }

    #[test]
    fn detect_bails_on_empty_grid() {
        let old = Grid::new(0, 0);
        let new = Grid::new(0, 0);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(result.is_none(), "empty grid should bail");
    }

    #[test]
    fn detect_bails_on_identical_grids() {
        let old = grid_from_numbers(&[&[1, 2], &[3, 4]]);
        let new = grid_from_numbers(&[&[1, 2], &[3, 4]]);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "identical grids should bail (no differences)"
        );
    }

    #[test]
    fn detect_bails_on_internal_cell_edit() {
        let mut grid_a = base_background(10, 10);
        let mut grid_b = base_background(10, 10);

        let block = vec![vec![11, 12, 13], vec![21, 22, 23], vec![31, 32, 33]];

        place_block(&mut grid_a, 1, 1, &block);
        place_block(&mut grid_b, 6, 4, &block);
        grid_b[7][5] = 9_999;

        let old = grid_from_matrix(grid_a);
        let new = grid_from_matrix(grid_b);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "move with internal edit should not be detected as exact rectangular move"
        );
    }

    #[test]
    fn detect_bails_on_ambiguous_block_swap() {
        let base: Vec<Vec<i32>> = (0..6)
            .map(|r| (0..6).map(|c| 100 * r + c).collect())
            .collect();
        let mut grid_a = base.clone();
        let mut grid_b = base.clone();

        let block_one = vec![vec![900, 901], vec![902, 903]];
        let block_two = vec![vec![700, 701], vec![702, 703]];

        place_block(&mut grid_a, 0, 0, &block_one);
        place_block(&mut grid_a, 3, 3, &block_two);

        place_block(&mut grid_b, 0, 0, &block_two);
        place_block(&mut grid_b, 3, 3, &block_one);

        let old = grid_from_matrix(grid_a);
        let new = grid_from_matrix(grid_b);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "ambiguous block swap should not emit a rectangular move"
        );
    }

    #[allow(clippy::field_reassign_with_default)]
    #[test]
    fn detect_bails_on_oversized_row_count() {
        let mut config = DiffConfig::default();
        config.alignment.max_align_rows = 10;
        let old = Grid::new(config.alignment.max_align_rows + 1, 10);
        let new = Grid::new(config.alignment.max_align_rows + 1, 10);

        let result = detect_exact_rect_block_move(&old, &new, &config);
        assert!(
            result.is_none(),
            "grids exceeding configured max_align_rows should bail"
        );
    }

    #[allow(clippy::field_reassign_with_default)]
    #[test]
    fn detect_bails_on_oversized_col_count() {
        let mut config = DiffConfig::default();
        config.alignment.max_align_cols = 8;
        let old = Grid::new(10, config.alignment.max_align_cols + 1);
        let new = Grid::new(10, config.alignment.max_align_cols + 1);

        let result = detect_exact_rect_block_move(&old, &new, &config);
        assert!(
            result.is_none(),
            "grids exceeding configured max_align_cols should bail"
        );
    }

    #[test]
    fn detect_bails_on_single_cell_edit() {
        let old = grid_from_numbers(&[&[1, 2, 3], &[4, 5, 6], &[7, 8, 9]]);
        let new = grid_from_numbers(&[&[1, 2, 3], &[4, 99, 6], &[7, 8, 9]]);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "single cell edit is not a rectangular block move"
        );
    }

    #[test]
    fn detect_bails_on_pure_row_move_pattern() {
        let old = grid_from_numbers(&[&[1, 2, 3], &[4, 5, 6], &[7, 8, 9], &[10, 11, 12]]);
        let new = grid_from_numbers(&[&[7, 8, 9], &[4, 5, 6], &[1, 2, 3], &[10, 11, 12]]);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "pure row swap without column displacement is not a rectangular block move"
        );
    }

    #[test]
    fn detect_bails_on_non_contiguous_differences() {
        let mut grid_a = base_background(8, 8);
        let mut grid_b = base_background(8, 8);

        grid_a[1][1] = 111;
        grid_a[5][5] = 555;
        grid_a[1][5] = 115;
        grid_b[1][1] = 555;
        grid_b[5][5] = 111;
        grid_b[1][5] = 999;

        let old = grid_from_matrix(grid_a);
        let new = grid_from_matrix(grid_b);

        let result = detect_exact_rect_block_move(&old, &new, &DiffConfig::default());
        assert!(
            result.is_none(),
            "non-contiguous differences should not form a rectangular block move"
        );
    }
}

```

---

### File: `core\src\region_mask.rs`

```rust
//! Region mask for tracking which cells have been accounted for during diff.
//!
//! The `RegionMask` tracks which rows and columns are "active" (still to be processed)
//! versus "excluded" (already accounted for by a move or other operation).

use std::collections::HashSet;

#[derive(Debug, Clone, Copy)]
struct RectMask {
    row_start: u32,
    row_count: u32,
    col_start: u32,
    col_count: u32,
}

#[derive(Debug, Clone)]
pub struct RegionMask {
    excluded_rows: HashSet<u32>,
    excluded_cols: HashSet<u32>,
    excluded_rects: Vec<RectMask>,
    nrows: u32,
    ncols: u32,
    row_shift_min: Option<u32>,
    row_shift_max: Option<u32>,
    col_shift_min: Option<u32>,
    col_shift_max: Option<u32>,
}

impl RegionMask {
    pub fn all_active(nrows: u32, ncols: u32) -> Self {
        Self {
            excluded_rows: HashSet::new(),
            excluded_cols: HashSet::new(),
            excluded_rects: Vec::new(),
            nrows,
            ncols,
            row_shift_min: None,
            row_shift_max: None,
            col_shift_min: None,
            col_shift_max: None,
        }
    }

    pub fn exclude_row(&mut self, row: u32) {
        self.excluded_rows.insert(row);
    }

    pub fn exclude_rows(&mut self, start: u32, count: u32) {
        let end = start.saturating_add(count).saturating_sub(1);
        for row in start..=end {
            self.excluded_rows.insert(row);
        }
        self.row_shift_min = Some(self.row_shift_min.map_or(start, |m| m.min(start)));
        self.row_shift_max = Some(self.row_shift_max.map_or(end, |m| m.max(end)));
    }

    pub fn exclude_col(&mut self, col: u32) {
        self.excluded_cols.insert(col);
    }

    pub fn exclude_cols(&mut self, start: u32, count: u32) {
        let end = start.saturating_add(count).saturating_sub(1);
        for col in start..=end {
            self.excluded_cols.insert(col);
        }
        self.col_shift_min = Some(self.col_shift_min.map_or(start, |m| m.min(start)));
        self.col_shift_max = Some(self.col_shift_max.map_or(end, |m| m.max(end)));
    }

    #[cfg(any(test, feature = "dev-apis"))]
    pub fn exclude_rect(&mut self, row_start: u32, row_count: u32, col_start: u32, col_count: u32) {
        self.exclude_rows(row_start, row_count);
        self.exclude_cols(col_start, col_count);
    }

    pub fn exclude_rect_cells(
        &mut self,
        row_start: u32,
        row_count: u32,
        col_start: u32,
        col_count: u32,
    ) {
        self.excluded_rects.push(RectMask {
            row_start,
            row_count,
            col_start,
            col_count,
        });
    }

    pub fn is_row_active(&self, row: u32) -> bool {
        !self.excluded_rows.contains(&row)
    }

    pub fn is_col_active(&self, col: u32) -> bool {
        !self.excluded_cols.contains(&col)
    }

    fn is_cell_excluded_by_rects(&self, row: u32, col: u32) -> bool {
        self.excluded_rects.iter().any(|rect| {
            row >= rect.row_start
                && row < rect.row_start.saturating_add(rect.row_count)
                && col >= rect.col_start
                && col < rect.col_start.saturating_add(rect.col_count)
        })
    }

    pub fn is_cell_active(&self, row: u32, col: u32) -> bool {
        self.is_row_active(row)
            && self.is_col_active(col)
            && !self.is_cell_excluded_by_rects(row, col)
    }

    pub fn active_row_count(&self) -> u32 {
        self.nrows.saturating_sub(self.excluded_rows.len() as u32)
    }

    pub fn active_col_count(&self) -> u32 {
        self.ncols.saturating_sub(self.excluded_cols.len() as u32)
    }

    pub fn active_rows(&self) -> impl Iterator<Item = u32> + '_ {
        (0..self.nrows).filter(|r| self.is_row_active(*r))
    }

    pub fn active_cols(&self) -> impl Iterator<Item = u32> + '_ {
        (0..self.ncols).filter(|c| self.is_col_active(*c))
    }

    pub fn has_excluded_rows(&self) -> bool {
        !self.excluded_rows.is_empty()
    }

    pub fn has_excluded_cols(&self) -> bool {
        !self.excluded_cols.is_empty()
    }

    pub fn has_excluded_rects(&self) -> bool {
        !self.excluded_rects.is_empty()
    }

    pub fn has_exclusions(&self) -> bool {
        self.has_excluded_rows() || self.has_excluded_cols() || self.has_excluded_rects()
    }

    pub fn has_active_cells(&self) -> bool {
        self.active_row_count() > 0 && self.active_col_count() > 0
    }

    #[cfg(any(test, feature = "dev-apis"))]
    pub fn rows_overlap_excluded(&self, start: u32, count: u32) -> bool {
        for row in start..start.saturating_add(count) {
            if self.excluded_rows.contains(&row) {
                return true;
            }
        }
        false
    }

    #[cfg(any(test, feature = "dev-apis"))]
    pub fn cols_overlap_excluded(&self, start: u32, count: u32) -> bool {
        for col in start..start.saturating_add(count) {
            if self.excluded_cols.contains(&col) {
                return true;
            }
        }
        false
    }

    #[cfg(any(test, feature = "dev-apis"))]
    pub fn rect_overlaps_excluded(
        &self,
        row_start: u32,
        row_count: u32,
        col_start: u32,
        col_count: u32,
    ) -> bool {
        self.rows_overlap_excluded(row_start, row_count)
            || self.cols_overlap_excluded(col_start, col_count)
    }

    #[cfg(any(test, feature = "dev-apis"))]
    #[allow(dead_code)]
    pub fn is_row_in_shift_zone(&self, row: u32) -> bool {
        match (self.row_shift_min, self.row_shift_max) {
            (Some(min), Some(max)) => row >= min && row <= max,
            _ => false,
        }
    }

    #[cfg(any(test, feature = "dev-apis"))]
    #[allow(dead_code)]
    pub fn is_col_in_shift_zone(&self, col: u32) -> bool {
        match (self.col_shift_min, self.col_shift_max) {
            (Some(min), Some(max)) => col >= min && col <= max,
            _ => false,
        }
    }

    pub fn row_shift_bounds(&self) -> Option<(u32, u32)> {
        match (self.row_shift_min, self.row_shift_max) {
            (Some(min), Some(max)) => Some((min, max)),
            _ => None,
        }
    }

    pub fn col_shift_bounds(&self) -> Option<(u32, u32)> {
        match (self.col_shift_min, self.col_shift_max) {
            (Some(min), Some(max)) => Some((min, max)),
            _ => None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn all_active_initially() {
        let mask = RegionMask::all_active(10, 5);
        assert!(mask.is_row_active(0));
        assert!(mask.is_row_active(9));
        assert!(mask.is_col_active(0));
        assert!(mask.is_col_active(4));
        assert_eq!(mask.active_row_count(), 10);
        assert_eq!(mask.active_col_count(), 5);
    }

    #[test]
    fn exclude_single_row() {
        let mut mask = RegionMask::all_active(10, 5);
        mask.exclude_row(3);
        assert!(!mask.is_row_active(3));
        assert!(mask.is_row_active(2));
        assert!(mask.is_row_active(4));
        assert_eq!(mask.active_row_count(), 9);
    }

    #[test]
    fn exclude_row_range() {
        let mut mask = RegionMask::all_active(10, 5);
        mask.exclude_rows(2, 4);
        assert!(!mask.is_row_active(2));
        assert!(!mask.is_row_active(5));
        assert!(mask.is_row_active(1));
        assert!(mask.is_row_active(6));
        assert_eq!(mask.active_row_count(), 6);
    }

    #[test]
    fn exclude_rect() {
        let mut mask = RegionMask::all_active(10, 8);
        mask.exclude_rect(2, 3, 4, 2);
        assert!(!mask.is_row_active(2));
        assert!(!mask.is_row_active(4));
        assert!(mask.is_row_active(1));
        assert!(mask.is_row_active(5));
        assert!(!mask.is_col_active(4));
        assert!(!mask.is_col_active(5));
        assert!(mask.is_col_active(3));
        assert!(mask.is_col_active(6));
    }

    #[test]
    fn cell_active_based_on_row_and_col() {
        let mut mask = RegionMask::all_active(10, 10);
        mask.exclude_row(3);
        mask.exclude_col(5);
        assert!(!mask.is_cell_active(3, 5));
        assert!(!mask.is_cell_active(3, 0));
        assert!(!mask.is_cell_active(0, 5));
        assert!(mask.is_cell_active(0, 0));
        assert!(mask.is_cell_active(4, 6));
    }

    #[test]
    fn active_rows_iterator() {
        let mut mask = RegionMask::all_active(5, 3);
        mask.exclude_row(1);
        mask.exclude_row(3);
        let active: Vec<u32> = mask.active_rows().collect();
        assert_eq!(active, vec![0, 2, 4]);
    }

    #[test]
    fn rows_overlap_excluded_detects_overlap() {
        let mut mask = RegionMask::all_active(10, 5);
        mask.exclude_rows(3, 2);
        assert!(mask.rows_overlap_excluded(2, 3));
        assert!(mask.rows_overlap_excluded(4, 2));
        assert!(!mask.rows_overlap_excluded(0, 2));
        assert!(!mask.rows_overlap_excluded(5, 3));
    }

    #[test]
    fn cols_overlap_excluded_detects_overlap() {
        let mut mask = RegionMask::all_active(5, 10);
        mask.exclude_cols(4, 3);
        assert!(mask.cols_overlap_excluded(3, 2));
        assert!(mask.cols_overlap_excluded(6, 2));
        assert!(!mask.cols_overlap_excluded(0, 3));
        assert!(!mask.cols_overlap_excluded(7, 3));
    }

    #[test]
    fn rect_overlaps_excluded_detects_any_overlap() {
        let mut mask = RegionMask::all_active(10, 10);
        mask.exclude_rect(2, 3, 4, 2);
        assert!(mask.rect_overlaps_excluded(1, 2, 0, 3));
        assert!(mask.rect_overlaps_excluded(0, 2, 3, 2));
        assert!(!mask.rect_overlaps_excluded(6, 2, 7, 2));
    }

    #[test]
    fn exclude_rect_cells_masks_only_that_region() {
        let mut mask = RegionMask::all_active(6, 6);
        mask.exclude_rect_cells(2, 2, 2, 2);

        assert!(!mask.is_cell_active(2, 2));
        assert!(!mask.is_cell_active(3, 3));

        assert!(mask.is_cell_active(1, 2));
        assert!(mask.is_cell_active(2, 1));
        assert!(mask.is_cell_active(4, 4));

        assert!(mask.has_exclusions());
        assert!(mask.has_excluded_rects());
    }
}

```

---

### File: `core\src\row_alignment.rs`

```rust
//! Legacy row alignment algorithms (pre-AMR).
//!
//! This module contains the original row alignment implementation that predates
//! the Anchor-Move-Refine (AMR) algorithm in `alignment/`. These functions are
//! retained for:
//!
//! 1. **Fallback scenarios**: The engine may use these when AMR cannot produce
//!    a useful alignment (e.g., heavily repetitive data).
//!
//! 2. **Move detection helpers**: Some functions (`detect_exact_row_block_move`,
//!    `detect_fuzzy_row_block_move`) are still used by the engine's
//!    masked move detection logic.
//!
//! 3. **Test coverage**: Unit tests validate these algorithms work correctly.
//!
//! ## Migration Status
//!
//! The primary alignment path now uses `alignment::align_rows_amr`. The legacy
//! functions are invoked only when:
//! - AMR returns `None` (fallback to `align_row_changes`)
//! - Explicit move detection in masked regions
//!
//! Functions marked `#[cfg(any(test, feature = "dev-apis"))]` are retained for
//! testing but not called from production code paths.

use std::collections::HashSet;

use crate::config::DiffConfig;
use crate::grid_view::{GridView, HashStats, RowHash, RowMeta};
use crate::workbook::Grid;

pub(crate) use crate::alignment_types::{RowAlignment, RowBlockMove};

const _HASH_COLLISION_NOTE: &str = "128-bit xxHash3 collision probability ~10^-29 at 50K rows (birthday bound); \
     secondary verification not required; see hashing.rs for detailed rationale.";

pub(crate) fn detect_exact_row_block_move(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RowBlockMove> {
    if old.nrows != new.nrows || old.ncols != new.ncols {
        return None;
    }

    if old.nrows == 0 {
        return None;
    }

    if !is_within_size_bounds(old, new, config) {
        return None;
    }

    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);

    if view_a.is_low_info_dominated() || view_b.is_low_info_dominated() {
        return None;
    }

    let stats = HashStats::from_row_meta(&view_a.row_meta, &view_b.row_meta);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    let meta_a = &view_a.row_meta;
    let meta_b = &view_b.row_meta;
    let n = meta_a.len();

    if meta_a
        .iter()
        .zip(meta_b.iter())
        .all(|(a, b)| a.signature == b.signature)
    {
        return None;
    }

    let prefix = (0..n).find(|&idx| meta_a[idx].signature != meta_b[idx].signature)?;

    let mut suffix_len = 0usize;
    while suffix_len < n.saturating_sub(prefix) {
        let idx_a = n - 1 - suffix_len;
        let idx_b = n - 1 - suffix_len;
        if meta_a[idx_a].signature == meta_b[idx_b].signature {
            suffix_len += 1;
        } else {
            break;
        }
    }
    let tail_start = n - suffix_len;

    let try_candidate = |src_start: usize, dst_start: usize| -> Option<RowBlockMove> {
        if src_start >= tail_start || dst_start >= tail_start {
            return None;
        }

        let mut len = 0usize;
        while src_start + len < tail_start && dst_start + len < tail_start {
            if meta_a[src_start + len].signature != meta_b[dst_start + len].signature {
                break;
            }
            len += 1;
        }

        if len == 0 {
            return None;
        }

        let src_end = src_start + len;
        let dst_end = dst_start + len;

        if !(src_end <= dst_start || dst_end <= src_start) {
            return None;
        }

        let mut idx_a = 0usize;
        let mut idx_b = 0usize;

        loop {
            if idx_a == src_start {
                idx_a = src_end;
            }
            if idx_b == dst_start {
                idx_b = dst_end;
            }

            if idx_a >= n && idx_b >= n {
                break;
            }

            if idx_a >= n || idx_b >= n {
                return None;
            }

            if meta_a[idx_a].signature != meta_b[idx_b].signature {
                return None;
            }

            idx_a += 1;
            idx_b += 1;
        }

        for meta in &meta_a[src_start..src_end] {
            if stats.freq_a.get(&meta.signature).copied().unwrap_or(0) != 1
                || stats.freq_b.get(&meta.signature).copied().unwrap_or(0) != 1
            {
                return None;
            }
        }

        Some(RowBlockMove {
            src_start_row: meta_a[src_start].row_idx,
            dst_start_row: meta_b[dst_start].row_idx,
            row_count: len as u32,
        })
    };

    if let Some(src_start) =
        (prefix..tail_start).find(|&idx| meta_a[idx].signature == meta_b[prefix].signature)
        && let Some(mv) = try_candidate(src_start, prefix)
    {
        return Some(mv);
    }

    if let Some(dst_start) =
        (prefix..tail_start).find(|&idx| meta_b[idx].signature == meta_a[prefix].signature)
        && let Some(mv) = try_candidate(prefix, dst_start)
    {
        return Some(mv);
    }

    None
}

pub(crate) fn detect_fuzzy_row_block_move(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RowBlockMove> {
    if old.nrows != new.nrows || old.ncols != new.ncols {
        return None;
    }

    if old.nrows == 0 {
        return None;
    }

    if !is_within_size_bounds(old, new, config) {
        return None;
    }

    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);

    if view_a.is_low_info_dominated() || view_b.is_low_info_dominated() {
        return None;
    }

    let stats = HashStats::from_row_meta(&view_a.row_meta, &view_b.row_meta);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    let meta_a = &view_a.row_meta;
    let meta_b = &view_b.row_meta;

    if meta_a
        .iter()
        .zip(meta_b.iter())
        .all(|(a, b)| a.signature == b.signature)
    {
        return None;
    }

    let n = meta_a.len();
    let mut prefix = 0usize;
    while prefix < n && meta_a[prefix].signature == meta_b[prefix].signature {
        prefix += 1;
    }
    if prefix == n {
        return None;
    }

    let mut suffix_len = 0usize;
    while suffix_len < n.saturating_sub(prefix) {
        let idx_a = n - 1 - suffix_len;
        let idx_b = idx_a;
        if meta_a[idx_a].signature == meta_b[idx_b].signature {
            suffix_len += 1;
        } else {
            break;
        }
    }

    let mismatch_end = n - suffix_len;
    if mismatch_end <= prefix {
        return None;
    }

    let mid_len = mismatch_end - prefix;
    if mid_len <= 1 {
        return None;
    }

    let max_block_len = mid_len
        .saturating_sub(1)
        .min(config.moves.max_fuzzy_block_rows as usize);
    if max_block_len == 0 {
        return None;
    }

    let mut candidate: Option<RowBlockMove> = None;

    for block_len in 1..=max_block_len {
        let remaining = mid_len - block_len;

        // Block moved upward: [middle][block] -> [block'][middle]
        if hashes_match(
            &meta_a[prefix..prefix + remaining],
            &meta_b[prefix + block_len..mismatch_end],
        ) {
            let src_block = &meta_a[prefix + remaining..mismatch_end];
            let dst_block = &meta_b[prefix..prefix + block_len];

            if block_similarity(src_block, dst_block) >= config.moves.fuzzy_similarity_threshold {
                let mv = RowBlockMove {
                    src_start_row: src_block[0].row_idx,
                    dst_start_row: dst_block[0].row_idx,
                    row_count: block_len as u32,
                };
                if mv.src_start_row != mv.dst_start_row {
                    if candidate.is_some() {
                        return None;
                    }
                    candidate = Some(mv);
                }
            }
        }

        // Block moved downward: [block][middle] -> [middle][block']
        if hashes_match(
            &meta_a[prefix + block_len..mismatch_end],
            &meta_b[prefix..prefix + remaining],
        ) {
            let src_block = &meta_a[prefix..prefix + block_len];
            let dst_block = &meta_b[prefix + remaining..mismatch_end];

            if block_similarity(src_block, dst_block) >= config.moves.fuzzy_similarity_threshold {
                let mv = RowBlockMove {
                    src_start_row: src_block[0].row_idx,
                    dst_start_row: dst_block[0].row_idx,
                    row_count: block_len as u32,
                };
                if mv.src_start_row != mv.dst_start_row {
                    if candidate.is_some() {
                        return None;
                    }
                    candidate = Some(mv);
                }
            }
        }
    }

    candidate
}

#[cfg(test)]
pub(crate) fn align_row_changes(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RowAlignment> {
    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);
    align_row_changes_from_views(&view_a, &view_b, config)
}

pub(crate) fn align_row_changes_from_views(
    old_view: &GridView,
    new_view: &GridView,
    config: &DiffConfig,
) -> Option<RowAlignment> {
    let row_diff = new_view.source.nrows as i64 - old_view.source.nrows as i64;
    if row_diff.abs() == 1 {
        return align_single_row_change_from_views(old_view, new_view, config);
    }

    align_rows_internal(old_view, new_view, true, config)
}

#[cfg(any(test, feature = "dev-apis"))]
pub(crate) fn align_single_row_change(
    old: &Grid,
    new: &Grid,
    config: &DiffConfig,
) -> Option<RowAlignment> {
    let view_a = GridView::from_grid_with_config(old, config);
    let view_b = GridView::from_grid_with_config(new, config);
    align_single_row_change_from_views(&view_a, &view_b, config)
}

pub(crate) fn align_single_row_change_from_views(
    old_view: &GridView,
    new_view: &GridView,
    config: &DiffConfig,
) -> Option<RowAlignment> {
    align_rows_internal(old_view, new_view, false, config)
}

fn align_rows_internal(
    old_view: &GridView,
    new_view: &GridView,
    allow_blocks: bool,
    config: &DiffConfig,
) -> Option<RowAlignment> {
    if !is_within_size_bounds(old_view.source, new_view.source, config) {
        return None;
    }

    if old_view.source.ncols != new_view.source.ncols {
        return None;
    }

    let row_diff = new_view.source.nrows as i64 - old_view.source.nrows as i64;
    if row_diff == 0 {
        return None;
    }

    let abs_diff = row_diff.unsigned_abs() as u32;

    if !allow_blocks && abs_diff != 1 {
        return None;
    }

    if abs_diff != 1 && (!allow_blocks || abs_diff > config.alignment.max_block_gap) {
        return None;
    }

    if old_view.is_low_info_dominated() || new_view.is_low_info_dominated() {
        return None;
    }

    let stats = HashStats::from_row_meta(&old_view.row_meta, &new_view.row_meta);
    if stats.has_heavy_repetition(config.alignment.max_hash_repeat) {
        return None;
    }

    if row_diff == 1 {
        find_single_gap_alignment(
            &old_view.row_meta,
            &new_view.row_meta,
            &stats,
            RowChange::Insert,
        )
    } else if row_diff == -1 {
        find_single_gap_alignment(
            &old_view.row_meta,
            &new_view.row_meta,
            &stats,
            RowChange::Delete,
        )
    } else if !allow_blocks {
        None
    } else if row_diff > 0 {
        find_block_gap_alignment(
            &old_view.row_meta,
            &new_view.row_meta,
            &stats,
            RowChange::Insert,
            abs_diff,
        )
    } else {
        find_block_gap_alignment(
            &old_view.row_meta,
            &new_view.row_meta,
            &stats,
            RowChange::Delete,
            abs_diff,
        )
    }
}

enum RowChange {
    Insert,
    Delete,
}

fn find_single_gap_alignment(
    rows_a: &[crate::grid_view::RowMeta],
    rows_b: &[crate::grid_view::RowMeta],
    stats: &HashStats<RowHash>,
    change: RowChange,
) -> Option<RowAlignment> {
    let mut matched = Vec::new();
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();
    let mut skipped = false;

    let mut idx_a = 0usize;
    let mut idx_b = 0usize;

    while idx_a < rows_a.len() && idx_b < rows_b.len() {
        let meta_a = rows_a[idx_a];
        let meta_b = rows_b[idx_b];

        if meta_a.signature == meta_b.signature {
            matched.push((meta_a.row_idx, meta_b.row_idx));
            idx_a += 1;
            idx_b += 1;
            continue;
        }

        if skipped {
            return None;
        }

        match change {
            RowChange::Insert => {
                if !stats.is_unique_to_b(meta_b.signature) {
                    return None;
                }
                inserted.push(meta_b.row_idx);
                idx_b += 1;
            }
            RowChange::Delete => {
                if !stats.is_unique_to_a(meta_a.signature) {
                    return None;
                }
                deleted.push(meta_a.row_idx);
                idx_a += 1;
            }
        }

        skipped = true;
    }

    if idx_a < rows_a.len() || idx_b < rows_b.len() {
        if skipped {
            return None;
        }

        match change {
            RowChange::Insert if idx_a == rows_a.len() && rows_b.len() == idx_b + 1 => {
                let meta_b = rows_b[idx_b];
                if !stats.is_unique_to_b(meta_b.signature) {
                    return None;
                }
                inserted.push(meta_b.row_idx);
            }
            RowChange::Delete if idx_b == rows_b.len() && rows_a.len() == idx_a + 1 => {
                let meta_a = rows_a[idx_a];
                if !stats.is_unique_to_a(meta_a.signature) {
                    return None;
                }
                deleted.push(meta_a.row_idx);
            }
            _ => return None,
        }
    }

    if inserted.len() + deleted.len() != 1 {
        return None;
    }

    let alignment = RowAlignment {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    };

    debug_assert!(
        is_monotonic(&alignment.matched),
        "matched pairs must be strictly increasing in both dimensions"
    );

    Some(alignment)
}

fn find_block_gap_alignment(
    rows_a: &[crate::grid_view::RowMeta],
    rows_b: &[crate::grid_view::RowMeta],
    stats: &HashStats<RowHash>,
    change: RowChange,
    gap: u32,
) -> Option<RowAlignment> {
    let gap = gap as usize;
    if gap == 0 {
        return None;
    }

    let (shorter_len, longer_len) = match change {
        RowChange::Insert => (rows_a.len(), rows_b.len()),
        RowChange::Delete => (rows_b.len(), rows_a.len()),
    };

    if longer_len.saturating_sub(shorter_len) != gap {
        return None;
    }

    let mut prefix = 0usize;
    while prefix < rows_a.len()
        && prefix < rows_b.len()
        && rows_a[prefix].signature == rows_b[prefix].signature
    {
        prefix += 1;
    }

    let mut suffix = 0usize;
    while suffix < shorter_len.saturating_sub(prefix) {
        let idx_a = rows_a.len() - 1 - suffix;
        let idx_b = rows_b.len() - 1 - suffix;
        if rows_a[idx_a].signature == rows_b[idx_b].signature {
            suffix += 1;
        } else {
            break;
        }
    }

    if prefix + suffix != shorter_len {
        return None;
    }

    let mut matched = Vec::with_capacity(shorter_len);
    let mut inserted = Vec::new();
    let mut deleted = Vec::new();

    match change {
        RowChange::Insert => {
            let block_start = prefix;
            let block_end = block_start + gap;
            if block_end > rows_b.len() {
                return None;
            }

            for meta in &rows_b[block_start..block_end] {
                if !stats.is_unique_to_b(meta.signature) {
                    return None;
                }
                inserted.push(meta.row_idx);
            }

            for (idx, meta_a) in rows_a.iter().enumerate() {
                let b_idx = if idx < block_start { idx } else { idx + gap };
                matched.push((meta_a.row_idx, rows_b[b_idx].row_idx));
            }
        }
        RowChange::Delete => {
            let block_start = prefix;
            let block_end = block_start + gap;
            if block_end > rows_a.len() {
                return None;
            }

            for meta in &rows_a[block_start..block_end] {
                if !stats.is_unique_to_a(meta.signature) {
                    return None;
                }
                deleted.push(meta.row_idx);
            }

            for (idx_b, meta_b) in rows_b.iter().enumerate() {
                let a_idx = if idx_b < block_start {
                    idx_b
                } else {
                    idx_b + gap
                };
                matched.push((rows_a[a_idx].row_idx, meta_b.row_idx));
            }
        }
    }

    let alignment = RowAlignment {
        matched,
        inserted,
        deleted,
        moves: Vec::new(),
    };

    debug_assert!(
        is_monotonic(&alignment.matched),
        "matched pairs must be strictly increasing in both dimensions"
    );

    Some(alignment)
}

fn is_monotonic(pairs: &[(u32, u32)]) -> bool {
    pairs.windows(2).all(|w| w[0].0 < w[1].0 && w[0].1 < w[1].1)
}

fn is_within_size_bounds(old: &Grid, new: &Grid, config: &DiffConfig) -> bool {
    let rows = old.nrows.max(new.nrows);
    let cols = old.ncols.max(new.ncols);
    rows <= config.alignment.max_align_rows && cols <= config.alignment.max_align_cols
}

fn hashes_match(slice_a: &[RowMeta], slice_b: &[RowMeta]) -> bool {
    slice_a.len() == slice_b.len()
        && slice_a
            .iter()
            .zip(slice_b.iter())
            .all(|(a, b)| a.signature == b.signature)
}

fn block_similarity(slice_a: &[RowMeta], slice_b: &[RowMeta]) -> f64 {
    let tokens_a: HashSet<RowHash> = slice_a.iter().map(|m| m.signature).collect();
    let tokens_b: HashSet<RowHash> = slice_b.iter().map(|m| m.signature).collect();

    let intersection = tokens_a.intersection(&tokens_b).count();
    let union = tokens_a.union(&tokens_b).count();
    let jaccard = if union == 0 {
        0.0
    } else {
        intersection as f64 / union as f64
    };

    let positional_matches = slice_a
        .iter()
        .zip(slice_b.iter())
        .filter(|(a, b)| a.signature == b.signature)
        .count();
    let positional_ratio = (positional_matches as f64 + 1.0) / (slice_a.len() as f64 + 1.0);

    jaccard.max(positional_ratio)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::workbook::CellValue;

    fn grid_from_rows(rows: &[&[i32]]) -> Grid {
        let nrows = rows.len() as u32;
        let ncols = if nrows == 0 { 0 } else { rows[0].len() as u32 };
        let mut grid = Grid::new(nrows, ncols);

        for (r_idx, row_vals) in rows.iter().enumerate() {
            for (c_idx, value) in row_vals.iter().enumerate() {
                grid.insert_cell(
                    r_idx as u32,
                    c_idx as u32,
                    Some(CellValue::Number(*value as f64)),
                    None,
                );
            }
        }

        grid
    }

    #[test]
    fn detects_exact_row_block_move() {
        let base: Vec<Vec<i32>> = (1..=20)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
        rows_b.splice(12..12, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        let mv = detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("expected block move to be found");
        assert_eq!(
            mv,
            RowBlockMove {
                src_start_row: 4,
                dst_start_row: 12,
                row_count: 4
            }
        );
    }

    #[test]
    fn block_move_detection_rejects_internal_edits() {
        let base: Vec<Vec<i32>> = (1..=12)
            .map(|r| (1..=2).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_b.drain(2..5).collect();
        moved_block[1][0] = 9_999;
        rows_b.splice(6..6, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn detects_fuzzy_row_block_move_with_single_internal_edit() {
        let base: Vec<Vec<i32>> = (1..=18)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
        moved_block[1][1] = 9_999;
        rows_b.splice(12..12, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(
            detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "internal edits should prevent exact move detection"
        );

        let mv = detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("expected fuzzy row block move to be detected");
        assert_eq!(
            mv,
            RowBlockMove {
                src_start_row: 4,
                dst_start_row: 12,
                row_count: 4
            }
        );
    }

    #[test]
    fn fuzzy_move_rejects_low_similarity_block() {
        let base: Vec<Vec<i32>> = (1..=16)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_b.drain(3..7).collect();
        for row in &mut moved_block {
            for value in row.iter_mut() {
                *value += 50_000;
            }
        }
        rows_b.splice(10..10, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
        assert!(
            detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "similarity below threshold should bail out"
        );
    }

    #[test]
    fn fuzzy_move_bails_on_heavy_repetition_or_ambiguous_candidates() {
        let repeated_row = [1, 2];
        let rows_a: Vec<Vec<i32>> = (0..10).map(|_| repeated_row.to_vec()).collect();
        let mut rows_b = rows_a.clone();

        let block: Vec<Vec<i32>> = rows_b.drain(0..3).collect();
        rows_b.splice(5..5, block);

        let rows_a_refs: Vec<&[i32]> = rows_a.iter().map(|row| row.as_slice()).collect();
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&rows_a_refs);
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(
            detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "heavy repetition or ambiguous candidates should not emit a move"
        );
    }

    #[test]
    fn fuzzy_move_noop_when_grids_identical() {
        let base: Vec<Vec<i32>> = (1..=6)
            .map(|r| (1..=2).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);
        let grid_b = grid_from_rows(&base_refs);

        assert!(detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
        assert!(detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn detects_fuzzy_row_block_move_upward_with_single_internal_edit() {
        let base: Vec<Vec<i32>> = (1..=18)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_b.drain(12..16).collect();
        moved_block[1][1] = 9_999;
        rows_b.splice(4..4, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(
            detect_exact_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "internal edits should prevent exact move detection"
        );

        let mv = detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default())
            .expect("expected fuzzy row block move upward to be detected");
        assert_eq!(
            mv,
            RowBlockMove {
                src_start_row: 12,
                dst_start_row: 4,
                row_count: 4
            }
        );
    }

    #[test]
    fn fuzzy_move_bails_on_ambiguous_candidates_below_repetition_threshold() {
        let base: Vec<Vec<i32>> = (1..=16)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_baseline_a = grid_from_rows(&base_refs);

        let mut rows_baseline_b = base.clone();
        let mut moved: Vec<Vec<i32>> = rows_baseline_b.drain(3..7).collect();
        moved[1][1] = 9999;
        rows_baseline_b.splice(10..10, moved);
        let refs_baseline_b: Vec<&[i32]> =
            rows_baseline_b.iter().map(|row| row.as_slice()).collect();
        let grid_baseline_b = grid_from_rows(&refs_baseline_b);

        assert!(
            detect_fuzzy_row_block_move(&grid_baseline_a, &grid_baseline_b, &DiffConfig::default())
                .is_some(),
            "baseline: non-ambiguous fuzzy move should be detected"
        );

        let rows_a: Vec<Vec<i32>> = vec![
            vec![1, 2, 3],
            vec![4, 5, 6],
            vec![100, 200, 300],
            vec![101, 201, 301],
            vec![102, 202, 302],
            vec![103, 203, 303],
            vec![100, 200, 300],
            vec![101, 201, 301],
            vec![102, 202, 302],
            vec![103, 203, 999],
            vec![31, 32, 33],
            vec![34, 35, 36],
        ];

        let mut rows_b = rows_a.clone();
        let block1: Vec<Vec<i32>> = rows_b.drain(2..6).collect();
        rows_b.splice(6..6, block1);

        let refs_a: Vec<&[i32]> = rows_a.iter().map(|r| r.as_slice()).collect();
        let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
        let grid_a = grid_from_rows(&refs_a);
        let grid_b = grid_from_rows(&refs_b);

        assert!(
            detect_fuzzy_row_block_move(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "ambiguous candidates: two similar blocks swapped should trigger ambiguity bail-out"
        );
    }

    #[test]
    fn fuzzy_move_at_max_block_rows_threshold() {
        let config = DiffConfig::default();
        let base: Vec<Vec<i32>> = (1..=70)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_b.drain(4..36).collect();
        moved_block[15][1] = 9_999;
        rows_b.splice(36..36, moved_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(
            detect_exact_row_block_move(&grid_a, &grid_b, &config).is_none(),
            "internal edits should prevent exact move detection"
        );

        let mv = detect_fuzzy_row_block_move(&grid_a, &grid_b, &config)
            .expect("expected fuzzy move at configured max_fuzzy_block_rows to be detected");
        assert_eq!(
            mv,
            RowBlockMove {
                src_start_row: 4,
                dst_start_row: 36,
                row_count: config.moves.max_fuzzy_block_rows
            }
        );
    }

    #[test]
    fn fuzzy_move_at_max_hash_repeat_boundary() {
        let base: Vec<Vec<i32>> = (1..=18)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_base = grid_from_rows(&base_refs);

        let mut rows_moved = base.clone();
        let mut moved_block: Vec<Vec<i32>> = rows_moved.drain(4..8).collect();
        moved_block[1][1] = 9_999;
        rows_moved.splice(12..12, moved_block);
        let moved_refs: Vec<&[i32]> = rows_moved.iter().map(|row| row.as_slice()).collect();
        let grid_moved = grid_from_rows(&moved_refs);

        assert!(
            detect_fuzzy_row_block_move(&grid_base, &grid_moved, &DiffConfig::default()).is_some(),
            "baseline: fuzzy move should work with unique rows"
        );

        let mut base_9_repeat: Vec<Vec<i32>> = (1..=18)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        for row in base_9_repeat.iter_mut().take(9) {
            *row = vec![999, 888, 777];
        }
        let refs_9a: Vec<&[i32]> = base_9_repeat.iter().map(|r| r.as_slice()).collect();
        let grid_9a = grid_from_rows(&refs_9a);

        let mut rows_9b = base_9_repeat.clone();
        let mut moved_9: Vec<Vec<i32>> = rows_9b.drain(10..14).collect();
        moved_9[1][1] = 8_888;
        rows_9b.splice(14..14, moved_9);
        let refs_9b: Vec<&[i32]> = rows_9b.iter().map(|r| r.as_slice()).collect();
        let grid_9b = grid_from_rows(&refs_9b);

        assert!(
            detect_fuzzy_row_block_move(&grid_9a, &grid_9b, &DiffConfig::default()).is_none(),
            "repetition guard should trigger when repeat count exceeds max_hash_repeat"
        );

        let mut base_8_repeat: Vec<Vec<i32>> = (1..=18)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        for row in base_8_repeat.iter_mut().take(8) {
            *row = vec![999, 888, 777];
        }
        let refs_8a: Vec<&[i32]> = base_8_repeat.iter().map(|r| r.as_slice()).collect();
        let grid_8a = grid_from_rows(&refs_8a);

        let mut rows_8b = base_8_repeat.clone();
        let mut moved_8: Vec<Vec<i32>> = rows_8b.drain(9..13).collect();
        moved_8[1][1] = 8_888;
        rows_8b.splice(14..14, moved_8);
        let refs_8b: Vec<&[i32]> = rows_8b.iter().map(|r| r.as_slice()).collect();
        let grid_8b = grid_from_rows(&refs_8b);

        assert!(
            detect_fuzzy_row_block_move(&grid_8a, &grid_8b, &DiffConfig::default()).is_some(),
            "repeat count equal to max_hash_repeat should not trigger heavy repetition guard"
        );
    }

    #[test]
    fn aligns_contiguous_block_insert_middle() {
        let base: Vec<Vec<i32>> = (1..=10)
            .map(|r| (1..=4).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let inserted_block: Vec<Vec<i32>> = (0..4)
            .map(|idx| vec![1_000 + idx, 2_000 + idx, 3_000 + idx, 4_000 + idx])
            .collect();
        let mut rows_b = base.clone();
        rows_b.splice(3..3, inserted_block);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        let alignment = align_row_changes(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.inserted, vec![3, 4, 5, 6]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 10);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[2], (2, 2));
        assert_eq!(alignment.matched[3], (3, 7));
        assert_eq!(alignment.matched.last(), Some(&(9, 13)));
        assert!(is_monotonic(&alignment.matched));
    }

    #[test]
    fn aligns_contiguous_block_delete_middle() {
        let base: Vec<Vec<i32>> = (1..=10)
            .map(|r| (1..=4).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        rows_b.drain(3..7);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        let alignment = align_row_changes(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.deleted, vec![3, 4, 5, 6]);
        assert!(alignment.inserted.is_empty());
        assert_eq!(alignment.matched.len(), 6);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[2], (2, 2));
        assert_eq!(alignment.matched[3], (7, 3));
        assert_eq!(alignment.matched.last(), Some(&(9, 5)));
        assert!(is_monotonic(&alignment.matched));
    }

    #[test]
    fn block_alignment_bails_on_noncontiguous_changes() {
        let base: Vec<Vec<i32>> = (1..=8)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base.clone();
        rows_b.insert(1, vec![999, 1_000, 1_001]);
        rows_b.insert(5, vec![2_000, 2_001, 2_002]);
        let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
        let grid_b = grid_from_rows(&rows_b_refs);

        assert!(align_row_changes(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn align_row_changes_rejects_column_insert_mismatch() {
        let grid_a = grid_from_rows(&[&[10, 11, 12], &[20, 21, 22]]);
        let grid_b = grid_from_rows(&[&[0, 10, 11, 12], &[0, 20, 21, 22], &[0, 30, 31, 32]]);

        assert!(
            align_row_changes(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "column insertion changing column count should skip row alignment"
        );
    }

    #[test]
    fn align_row_changes_rejects_column_delete_mismatch() {
        let grid_a = grid_from_rows(&[&[10, 11, 12, 13], &[20, 21, 22, 23], &[30, 31, 32, 33]]);
        let grid_b = grid_from_rows(&[&[10, 12, 13], &[30, 32, 33]]);

        assert!(
            align_row_changes(&grid_a, &grid_b, &DiffConfig::default()).is_none(),
            "column deletion changing column count should skip row alignment"
        );
    }

    #[test]
    fn aligns_single_insert_with_unique_row() {
        let base = (1..=10)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect::<Vec<_>>())
            .collect::<Vec<_>>();
        let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let mut rows_b = base_refs.clone();
        rows_b.insert(
            5,
            &[999, 1000, 1001], // inserted at position 6 (1-based)
        );
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.inserted, vec![5]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 10);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[4], (4, 4));
        assert_eq!(alignment.matched[5], (5, 6));
        assert_eq!(alignment.matched.last(), Some(&(9, 10)));
    }

    #[test]
    fn rejects_non_monotonic_alignment_with_extra_mismatch() {
        let base_rows = [[11, 12, 13], [21, 22, 23], [31, 32, 33], [41, 42, 43]];
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let rows_b: Vec<&[i32]> = vec![
            base_refs[0],       // same
            &[999, 1000, 1001], // inserted unique row
            base_refs[2],       // move row 3 before row 2 to break monotonicity
            base_refs[1],
            base_refs[3],
        ];
        let grid_b = grid_from_rows(&rows_b);

        assert!(align_single_row_change(&grid_a, &grid_b, &DiffConfig::default()).is_none());
    }

    #[test]
    fn aligns_insert_at_row_zero() {
        let base_rows: Vec<Vec<i32>> = (1..=5)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let new_first_row = [999, 1000, 1001];
        let mut rows_b = vec![new_first_row.as_slice()];
        rows_b.extend(base_refs.iter().copied());
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.inserted, vec![0]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 5);
        assert_eq!(alignment.matched[0], (0, 1));
        assert_eq!(alignment.matched[4], (4, 5));
    }

    #[test]
    fn aligns_insert_at_last_row() {
        let base_rows: Vec<Vec<i32>> = (1..=5)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let new_last_row = [999, 1000, 1001];
        let mut rows_b: Vec<&[i32]> = base_refs.clone();
        rows_b.push(new_last_row.as_slice());
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.inserted, vec![5]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 5);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[4], (4, 4));
    }

    #[test]
    fn aligns_delete_at_row_zero() {
        let base_rows: Vec<Vec<i32>> = (1..=5)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let rows_b: Vec<&[i32]> = base_refs[1..].to_vec();
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert!(alignment.inserted.is_empty());
        assert_eq!(alignment.deleted, vec![0]);
        assert_eq!(alignment.matched.len(), 4);
        assert_eq!(alignment.matched[0], (1, 0));
        assert_eq!(alignment.matched[3], (4, 3));
    }

    #[test]
    fn aligns_delete_at_last_row() {
        let base_rows: Vec<Vec<i32>> = (1..=5)
            .map(|r| (1..=3).map(|c| r * 10 + c).collect())
            .collect();
        let base_refs: Vec<&[i32]> = base_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&base_refs);

        let rows_b: Vec<&[i32]> = base_refs[..4].to_vec();
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert!(alignment.inserted.is_empty());
        assert_eq!(alignment.deleted, vec![4]);
        assert_eq!(alignment.matched.len(), 4);
        assert_eq!(alignment.matched[0], (0, 0));
        assert_eq!(alignment.matched[3], (3, 3));
    }

    #[test]
    fn aligns_single_row_to_two_rows_via_insert() {
        let single_row = [[42, 43, 44]];
        let single_refs: Vec<&[i32]> = single_row.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&single_refs);

        let new_row = [999, 1000, 1001];
        let rows_b: Vec<&[i32]> = vec![single_refs[0], new_row.as_slice()];
        let grid_b = grid_from_rows(&rows_b);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert_eq!(alignment.inserted, vec![1]);
        assert!(alignment.deleted.is_empty());
        assert_eq!(alignment.matched.len(), 1);
        assert_eq!(alignment.matched[0], (0, 0));
    }

    #[test]
    fn aligns_two_rows_to_single_row_via_delete() {
        let two_rows = [[42, 43, 44], [99, 100, 101]];
        let two_refs: Vec<&[i32]> = two_rows.iter().map(|row| row.as_slice()).collect();
        let grid_a = grid_from_rows(&two_refs);

        let single_refs: Vec<&[i32]> = vec![two_refs[0]];
        let grid_b = grid_from_rows(&single_refs);

        let alignment = align_single_row_change(&grid_a, &grid_b, &DiffConfig::default())
            .expect("alignment should succeed");
        assert!(alignment.inserted.is_empty());
        assert_eq!(alignment.deleted, vec![1]);
        assert_eq!(alignment.matched.len(), 1);
        assert_eq!(alignment.matched[0], (0, 0));
    }

    #[test]
    fn monotonicity_helper_accepts_valid_sequence() {
        let valid: Vec<(u32, u32)> = vec![(0, 0), (1, 2), (3, 4), (5, 7)];
        assert!(super::is_monotonic(&valid));
    }

    #[test]
    fn monotonicity_helper_rejects_non_increasing_a() {
        let invalid: Vec<(u32, u32)> = vec![(0, 0), (2, 2), (1, 4)];
        assert!(!super::is_monotonic(&invalid));
    }

    #[test]
    fn monotonicity_helper_rejects_non_increasing_b() {
        let invalid: Vec<(u32, u32)> = vec![(0, 3), (1, 2), (2, 4)];
        assert!(!super::is_monotonic(&invalid));
    }

    #[test]
    fn monotonicity_helper_accepts_empty_and_single() {
        assert!(super::is_monotonic(&[]));
        assert!(super::is_monotonic(&[(5, 10)]));
    }
}

```

---

### File: `core\src\session.rs`

```rust
use crate::string_pool::StringPool;

/// Holds shared diffing state such as the string pool.
pub struct DiffSession {
    pub strings: StringPool,
}

impl DiffSession {
    pub fn new() -> Self {
        Self {
            strings: StringPool::new(),
        }
    }

    pub fn strings(&self) -> &StringPool {
        &self.strings
    }

    pub fn strings_mut(&mut self) -> &mut StringPool {
        &mut self.strings
    }
}

```

---

### File: `core\src\sink.rs`

```rust
use crate::diff::{DiffError, DiffOp};
use crate::string_pool::StringPool;
use std::marker::PhantomData;

/// Trait for streaming diff operations to a consumer.
///
/// See `docs/streaming_contract.md` for determinism, lifecycle, and string table rules.
///
/// Streaming entry points call sinks in this order:
///
/// 1. `begin(pool)` once (before any ops)
/// 2. `emit(op)` zero or more times
/// 3. `finish()` once (even on most error paths)
///
/// Sinks can use `begin` to access the string table (via `pool.strings()`), e.g. to write a
/// header before streaming ops.
pub trait DiffSink {
    /// Called once before any ops are emitted.
    ///
    /// Default is a no-op so sinks that don't need setup can ignore it.
    fn begin(&mut self, _pool: &StringPool) -> Result<(), DiffError> {
        Ok(())
    }

    /// Emit one diff operation.
    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError>;

    /// Finish the stream (flush/close output destinations).
    fn finish(&mut self) -> Result<(), DiffError> {
        Ok(())
    }
}

pub(crate) struct SinkFinishGuard<S: DiffSink> {
    sink: *mut S,
    armed: bool,
    _marker: PhantomData<*mut S>,
}

impl<S: DiffSink> SinkFinishGuard<S> {
    pub(crate) fn new(sink: &mut S) -> Self {
        Self {
            sink,
            armed: true,
            _marker: PhantomData,
        }
    }

    pub(crate) fn finish_and_disarm(&mut self) -> Result<(), DiffError> {
        self.armed = false;
        // Safety: the guard is created from an exclusive borrow of `sink` and
        // tied to its lifetime via PhantomData, so the pointer remains valid.
        unsafe { (&mut *self.sink).finish() }
    }
}

impl<S: DiffSink> Drop for SinkFinishGuard<S> {
    fn drop(&mut self) {
        if !self.armed {
            return;
        }
        // Best-effort finish; ignore errors to avoid masking the original error.
        unsafe {
            let _ = (&mut *self.sink).finish();
        }
    }
}

pub(crate) struct NoFinishSink<'a, S: DiffSink> {
    inner: &'a mut S,
}

impl<'a, S: DiffSink> NoFinishSink<'a, S> {
    pub(crate) fn new(inner: &'a mut S) -> Self {
        Self { inner }
    }
}

impl<S: DiffSink> DiffSink for NoFinishSink<'_, S> {
    fn begin(&mut self, pool: &StringPool) -> Result<(), DiffError> {
        self.inner.begin(pool)
    }

    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        self.inner.emit(op)
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        Ok(())
    }
}

/// A sink that collects ops into a Vec for compatibility.
pub struct VecSink {
    ops: Vec<DiffOp>,
}

impl VecSink {
    pub fn new() -> Self {
        Self { ops: Vec::new() }
    }

    pub fn op_capacity(&self) -> usize {
        self.ops.capacity()
    }

    pub fn op_len(&self) -> usize {
        self.ops.len()
    }

    pub fn into_ops(self) -> Vec<DiffOp> {
        self.ops
    }
}

impl DiffSink for VecSink {
    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        self.ops.push(op);
        Ok(())
    }
}

/// A sink that forwards ops to a callback.
pub struct CallbackSink<F: FnMut(DiffOp)> {
    f: F,
}

impl<F: FnMut(DiffOp)> CallbackSink<F> {
    pub fn new(f: F) -> Self {
        Self { f }
    }
}

impl<F: FnMut(DiffOp)> DiffSink for CallbackSink<F> {
    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        (self.f)(op);
        Ok(())
    }
}

```

---

### File: `core\src\string_pool.rs`

```rust
use rustc_hash::FxHashMap;
use serde::{Deserialize, Serialize};

#[repr(transparent)]
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord, Serialize, Deserialize)]
pub struct StringId(pub u32);

impl std::fmt::Display for StringId {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

#[derive(Debug, Default)]
pub struct StringPool {
    strings: Vec<String>,
    index: FxHashMap<String, StringId>,
}

impl StringPool {
    pub fn new() -> Self {
        let mut pool = Self::default();
        pool.intern("");
        pool
    }

    pub fn intern(&mut self, s: &str) -> StringId {
        if let Some(&id) = self.index.get(s) {
            return id;
        }

        let id = StringId(self.strings.len() as u32);
        let owned = s.to_owned();
        self.strings.push(owned.clone());
        self.index.insert(owned, id);
        id
    }

    pub fn resolve(&self, id: StringId) -> &str {
        &self.strings[id.0 as usize]
    }

    pub fn strings(&self) -> &[String] {
        &self.strings
    }

    pub fn into_strings(self) -> Vec<String> {
        self.strings
    }

    pub fn len(&self) -> usize {
        self.strings.len()
    }

    pub fn estimated_bytes(&self) -> u64 {
        use std::mem::size_of;

        let strings_overhead = self.strings.capacity().saturating_mul(size_of::<String>());
        let strings_payload: usize = self.strings.iter().map(|s| s.capacity()).sum();
        let index_overhead = self.index.capacity().saturating_mul(size_of::<(String, StringId)>());

        (strings_overhead + strings_payload + index_overhead) as u64
    }
}

```

---

### File: `core\src\tabular_schema.rs`

```rust
use serde_json::Value;

use crate::excel_open_xml::PackageError;
use crate::model::{Measure, Model, ModelColumn, ModelRelationship, ModelTable};
use crate::string_pool::StringPool;

#[derive(Debug, Clone, Default)]
pub(crate) struct RawTabularModel {
    pub tables: Vec<RawTable>,
    pub relationships: Vec<RawRelationship>,
    pub measures: Vec<RawMeasure>,
}

#[derive(Debug, Clone)]
pub(crate) struct RawTable {
    pub name: String,
    pub columns: Vec<RawColumn>,
}

#[derive(Debug, Clone)]
pub(crate) struct RawColumn {
    pub name: String,
    pub data_type: Option<String>,
    pub is_hidden: Option<bool>,
    pub format_string: Option<String>,
    pub sort_by: Option<String>,
    pub summarize_by: Option<String>,
    pub expression: Option<String>,
}

#[derive(Debug, Clone)]
pub(crate) struct RawRelationship {
    pub from_table: String,
    pub from_column: String,
    pub to_table: String,
    pub to_column: String,
    pub cross_filtering_behavior: Option<String>,
    pub cardinality: Option<String>,
    pub is_active: Option<bool>,
    pub name: Option<String>,
}

#[derive(Debug, Clone)]
pub(crate) struct RawMeasure {
    pub full_name: String,
    pub expression: String,
}

fn strip_bom(s: &str) -> &str {
    s.strip_prefix('\u{FEFF}').unwrap_or(s)
}

pub(crate) fn parse_data_model_schema(bytes: &[u8]) -> Result<RawTabularModel, PackageError> {
    let text = std::str::from_utf8(bytes).map_err(|e| PackageError::UnsupportedFormat {
        message: format!("DataModelSchema is not UTF-8: {}", e),
    })?;
    let text = strip_bom(text);

    let v: Value = serde_json::from_str(text).map_err(|e| PackageError::UnsupportedFormat {
        message: format!("DataModelSchema JSON parse error: {}", e),
    })?;

    let mut out = RawTabularModel::default();

    if try_collect_from_model_tables(&v, &mut out) {
        normalize(&mut out);
        return Ok(out);
    }

    collect_measures_anywhere(&v, "", &mut out);
    normalize(&mut out);
    Ok(out)
}

fn try_collect_from_model_tables(v: &Value, out: &mut RawTabularModel) -> bool {
    let model = match v.get("model") {
        Some(m) => m,
        None => return false,
    };
    let tables = match model.get("tables").and_then(|t| t.as_array()) {
        Some(t) => t,
        None => return false,
    };

    for t in tables {
        let table_name = t.get("name").and_then(|x| x.as_str()).unwrap_or("");
        let mut raw_table = RawTable {
            name: table_name.to_string(),
            columns: Vec::new(),
        };

        if let Some(columns) = t.get("columns").and_then(|c| c.as_array()) {
            for c in columns {
                if let Some(col) = parse_column_obj(c) {
                    raw_table.columns.push(col);
                }
            }
        }

        if !raw_table.name.is_empty() {
            out.tables.push(raw_table);
        }

        if let Some(measures) = t.get("measures").and_then(|m| m.as_array()) {
            for m in measures {
                if let Some(rm) = parse_measure_obj(m, table_name) {
                    out.measures.push(rm);
                }
            }
        }
    }

    if let Some(relationships) = model.get("relationships").and_then(|r| r.as_array()) {
        for rel in relationships {
            if let Some(raw_rel) = parse_relationship_obj(rel) {
                out.relationships.push(raw_rel);
            }
        }
    }
    true
}

fn parse_measure_obj(v: &Value, table_name: &str) -> Option<RawMeasure> {
    let name = v.get("name").and_then(|x| x.as_str())?;
    let expr = v.get("expression").and_then(|x| x.as_str()).unwrap_or("");

    let full_name = if table_name.is_empty() {
        name.to_string()
    } else {
        format!("{}/{}", table_name, name)
    };

    Some(RawMeasure {
        full_name,
        expression: expr.to_string(),
    })
}

fn parse_column_obj(v: &Value) -> Option<RawColumn> {
    let name = v.get("name").and_then(|x| x.as_str())?;
    let data_type = opt_string_field(v, "dataType");
    let is_hidden = v.get("isHidden").and_then(|x| x.as_bool());
    let format_string = opt_string_field(v, "formatString");
    let sort_by = opt_string_field(v, "sortByColumn");
    let summarize_by = opt_string_field(v, "summarizeBy");
    let expression = opt_string_field(v, "expression");

    Some(RawColumn {
        name: name.to_string(),
        data_type,
        is_hidden,
        format_string,
        sort_by,
        summarize_by,
        expression,
    })
}

fn parse_relationship_obj(v: &Value) -> Option<RawRelationship> {
    let from_table = v.get("fromTable").and_then(|x| x.as_str())?;
    let from_column = v.get("fromColumn").and_then(|x| x.as_str())?;
    let to_table = v.get("toTable").and_then(|x| x.as_str())?;
    let to_column = v.get("toColumn").and_then(|x| x.as_str())?;

    Some(RawRelationship {
        from_table: from_table.to_string(),
        from_column: from_column.to_string(),
        to_table: to_table.to_string(),
        to_column: to_column.to_string(),
        cross_filtering_behavior: opt_string_field(v, "crossFilteringBehavior"),
        cardinality: opt_string_field(v, "cardinality"),
        is_active: v.get("isActive").and_then(|x| x.as_bool()),
        name: opt_string_field(v, "name"),
    })
}

fn collect_measures_anywhere(v: &Value, table_name: &str, out: &mut RawTabularModel) {
    match v {
        Value::Object(map) => {
            if let Some(measures) = map.get("measures").and_then(|m| m.as_array()) {
                for m in measures {
                    if let Some(rm) = parse_measure_obj(m, table_name) {
                        out.measures.push(rm);
                    }
                }
            }

            let next_table = map.get("name").and_then(|x| x.as_str()).unwrap_or(table_name);

            for (_k, child) in map {
                collect_measures_anywhere(child, next_table, out);
            }
        }
        Value::Array(arr) => {
            for child in arr {
                collect_measures_anywhere(child, table_name, out);
            }
        }
        _ => {}
    }
}

fn normalize(out: &mut RawTabularModel) {
    out.measures
        .sort_by(|a, b| cmp_case_insensitive(&a.full_name, &b.full_name));
    out.measures
        .dedup_by(|a, b| a.full_name == b.full_name && a.expression == b.expression);

    out.tables.sort_by(|a, b| cmp_case_insensitive(&a.name, &b.name));
    for table in &mut out.tables {
        table
            .columns
            .sort_by(|a, b| cmp_case_insensitive(&a.name, &b.name));
    }

    out.relationships.sort_by(|a, b| cmp_relationship_key(a, b));
}

pub(crate) fn build_model(raw: &RawTabularModel, pool: &mut StringPool) -> Model {
    let mut m = Model::default();

    for rt in &raw.tables {
        let name = pool.intern(&rt.name);
        let mut columns = Vec::with_capacity(rt.columns.len());
        for rc in &rt.columns {
            let col_name = pool.intern(&rc.name);
            let data_type = rc.data_type.as_deref().map(|s| pool.intern(s));
            let format_string = rc.format_string.as_deref().map(|s| pool.intern(s));
            let sort_by = rc.sort_by.as_deref().map(|s| pool.intern(s));
            let summarize_by = rc.summarize_by.as_deref().map(|s| pool.intern(s));
            let expression = rc.expression.as_deref().map(|s| pool.intern(s));
            columns.push(ModelColumn {
                name: col_name,
                data_type,
                is_hidden: rc.is_hidden,
                format_string,
                sort_by,
                summarize_by,
                expression,
            });
        }
        m.tables.push(ModelTable { name, columns });
    }

    for rr in &raw.relationships {
        let from_table = pool.intern(&rr.from_table);
        let from_column = pool.intern(&rr.from_column);
        let to_table = pool.intern(&rr.to_table);
        let to_column = pool.intern(&rr.to_column);
        let cross_filtering_behavior = rr
            .cross_filtering_behavior
            .as_deref()
            .map(|s| pool.intern(s));
        let cardinality = rr.cardinality.as_deref().map(|s| pool.intern(s));
        let name = rr.name.as_deref().map(|s| pool.intern(s));
        m.relationships.push(ModelRelationship {
            from_table,
            from_column,
            to_table,
            to_column,
            cross_filtering_behavior,
            cardinality,
            is_active: rr.is_active,
            name,
        });
    }

    for rm in &raw.measures {
        let name = pool.intern(&rm.full_name);
        let expr = pool.intern(&rm.expression);
        m.measures.push(Measure {
            name,
            expression: expr,
        });
    }
    m
}

fn opt_string_field(v: &Value, key: &str) -> Option<String> {
    v.get(key)
        .and_then(|x| x.as_str())
        .filter(|s| !s.is_empty())
        .map(|s| s.to_string())
}

fn cmp_case_insensitive(a: &str, b: &str) -> std::cmp::Ordering {
    let al = a.to_lowercase();
    let bl = b.to_lowercase();
    let cmp = al.cmp(&bl);
    if cmp == std::cmp::Ordering::Equal {
        a.cmp(b)
    } else {
        cmp
    }
}

fn cmp_relationship_key(a: &RawRelationship, b: &RawRelationship) -> std::cmp::Ordering {
    let fields_a = [
        a.from_table.as_str(),
        a.from_column.as_str(),
        a.to_table.as_str(),
        a.to_column.as_str(),
    ];
    let fields_b = [
        b.from_table.as_str(),
        b.from_column.as_str(),
        b.to_table.as_str(),
        b.to_column.as_str(),
    ];

    for (av, bv) in fields_a.iter().zip(fields_b.iter()) {
        let cmp = cmp_case_insensitive(av, bv);
        if cmp != std::cmp::Ordering::Equal {
            return cmp;
        }
    }

    let extra_a = [
        a.cross_filtering_behavior.as_deref(),
        a.cardinality.as_deref(),
        a.name.as_deref(),
    ];
    let extra_b = [
        b.cross_filtering_behavior.as_deref(),
        b.cardinality.as_deref(),
        b.name.as_deref(),
    ];

    for (av, bv) in extra_a.iter().zip(extra_b.iter()) {
        let cmp = cmp_case_insensitive(av.unwrap_or(""), bv.unwrap_or(""));
        if cmp != std::cmp::Ordering::Equal {
            return cmp;
        }
    }

    a.is_active.cmp(&b.is_active)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn parse_tables_columns_relationships() {
        let json = r#"{
            "model": {
                "tables": [
                    {
                        "name": "Sales",
                        "columns": [
                            {
                                "name": "Amount",
                                "dataType": "decimal",
                                "isHidden": true,
                                "formatString": "0.00",
                                "sortByColumn": "SortCol",
                                "summarizeBy": "sum",
                                "expression": "[Amount] * 2"
                            }
                        ],
                        "measures": [
                            {
                                "name": "Total",
                                "expression": "SUM(Sales[Amount])"
                            }
                        ]
                    }
                ],
                "relationships": [
                    {
                        "fromTable": "Sales",
                        "fromColumn": "CustomerId",
                        "toTable": "Customers",
                        "toColumn": "Id",
                        "crossFilteringBehavior": "oneDirection",
                        "cardinality": "ManyToOne",
                        "isActive": true,
                        "name": "SalesCustomers"
                    }
                ]
            }
        }"#;

        let raw = parse_data_model_schema(json.as_bytes()).expect("parse schema");
        assert_eq!(raw.tables.len(), 1);
        assert_eq!(raw.measures.len(), 1);
        assert_eq!(raw.relationships.len(), 1);

        let table = &raw.tables[0];
        assert_eq!(table.name, "Sales");
        assert_eq!(table.columns.len(), 1);
        let col = &table.columns[0];
        assert_eq!(col.name, "Amount");
        assert_eq!(col.data_type.as_deref(), Some("decimal"));
        assert_eq!(col.is_hidden, Some(true));
        assert_eq!(col.format_string.as_deref(), Some("0.00"));
        assert_eq!(col.sort_by.as_deref(), Some("SortCol"));
        assert_eq!(col.summarize_by.as_deref(), Some("sum"));
        assert_eq!(col.expression.as_deref(), Some("[Amount] * 2"));

        let measure = &raw.measures[0];
        assert_eq!(measure.full_name, "Sales/Total");
        assert_eq!(measure.expression, "SUM(Sales[Amount])");

        let rel = &raw.relationships[0];
        assert_eq!(rel.from_table, "Sales");
        assert_eq!(rel.from_column, "CustomerId");
        assert_eq!(rel.to_table, "Customers");
        assert_eq!(rel.to_column, "Id");
        assert_eq!(rel.cross_filtering_behavior.as_deref(), Some("oneDirection"));
        assert_eq!(rel.cardinality.as_deref(), Some("ManyToOne"));
        assert_eq!(rel.is_active, Some(true));
        assert_eq!(rel.name.as_deref(), Some("SalesCustomers"));

        let mut pool = StringPool::new();
        let model = build_model(&raw, &mut pool);
        assert_eq!(model.tables.len(), 1);
        assert_eq!(pool.resolve(model.tables[0].name), "Sales");
        assert_eq!(model.tables[0].columns.len(), 1);
        assert_eq!(
            pool.resolve(model.tables[0].columns[0].name),
            "Amount"
        );
        assert_eq!(model.relationships.len(), 1);
        assert_eq!(pool.resolve(model.relationships[0].from_table), "Sales");
        assert_eq!(model.measures.len(), 1);
        assert_eq!(pool.resolve(model.measures[0].name), "Sales/Total");
    }
}

```

---

### File: `core\src\vba.rs`

```rust
use crate::string_pool::StringId;

/// The kind of VBA module contained in an `.xlsm` workbook.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum VbaModuleType {
    /// A standard module (e.g., `Module1`).
    Standard,
    /// A class module.
    Class,
    /// A form module.
    Form,
    /// A document module (e.g., `ThisWorkbook`, sheet modules).
    Document,
}

/// A VBA module extracted from a workbook.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct VbaModule {
    /// Module name (interned in the associated string pool).
    pub name: StringId,
    /// Module type (standard/class/form/document).
    pub module_type: VbaModuleType,
    /// Raw module source code.
    pub code: String,
}

```

---

### File: `core\src\workbook.rs`

```rust
//! Workbook, sheet, and grid data structures.
//!
//! This module defines the core intermediate representation (IR) for Excel workbooks:
//! - [`Workbook`]: A collection of sheets
//! - [`Sheet`]: A named sheet with a grid of cells
//! - [`Grid`]: A sparse 2D grid of cell content with optional row/column signatures
//! - [`CellContent`]: Value + formula for a single cell (coordinates stored in the grid key)

use crate::addressing::{AddressParseError, address_to_index, index_to_address};
use crate::hashing::normalize_float_for_hash;
use crate::string_pool::{StringId, StringPool};
use rustc_hash::FxHashMap;
use serde::de::Error as DeError;
use serde::{Deserialize, Deserializer, Serialize, Serializer};
use std::hash::{Hash, Hasher};
use std::str::FromStr;

/// A snapshot of a cell's logical content for comparison purposes.
///
/// Used in [`crate::diff::DiffOp::CellEdited`] to represent the "before" and "after" states.
/// Equality comparison intentionally ignores `addr` and compares only `(value, formula)`.
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CellSnapshot {
    pub addr: CellAddress,
    pub value: Option<CellValue>,
    pub formula: Option<StringId>,
}

impl CellSnapshot {
    pub fn from_cell(row: u32, col: u32, cell: &CellContent) -> CellSnapshot {
        CellSnapshot {
            addr: CellAddress::from_indices(row, col),
            value: cell.value.clone(),
            formula: cell.formula,
        }
    }

    pub fn empty(addr: CellAddress) -> CellSnapshot {
        CellSnapshot {
            addr,
            value: None,
            formula: None,
        }
    }
}

/// An Excel workbook containing one or more sheets.
#[derive(Debug, Clone, PartialEq, Default)]
pub struct Workbook {
    pub sheets: Vec<Sheet>,
    pub named_ranges: Vec<NamedRange>,
    pub charts: Vec<ChartObject>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct NamedRange {
    pub name: StringId,
    pub refers_to: StringId,
    pub scope: Option<StringId>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ChartInfo {
    pub name: StringId,
    pub chart_type: StringId,
    pub data_range: Option<StringId>,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct ChartObject {
    pub sheet: StringId,
    /// Optional workbook-internal sheet id for rename-safe matching.
    pub workbook_sheet_id: Option<u32>,
    pub info: ChartInfo,
    pub xml_hash: u128,
}

/// A single sheet within a workbook.
#[derive(Debug, Clone, PartialEq)]
pub struct Sheet {
    /// The display name of the sheet (e.g., "Sheet1", "Data").
    pub name: StringId,
    /// Optional workbook-internal sheet id for rename-safe matching.
    pub workbook_sheet_id: Option<u32>,
    /// The type of sheet (worksheet, chart, macro, etc.).
    pub kind: SheetKind,
    /// The grid of cell data.
    pub grid: Grid,
}

/// The type of an Excel sheet.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum SheetKind {
    Worksheet,
    Chart,
    Macro,
    Other,
}

/// A 2D grid of cells representing sheet data, stored as sparse or dense data.
///
/// # Invariants
///
/// All cells stored in `cells` must satisfy `row < nrows` and `col < ncols`.
#[derive(Debug, Clone, PartialEq)]
pub struct Grid {
    /// Number of rows in the grid's bounding rectangle.
    pub nrows: u32,
    /// Number of columns in the grid's bounding rectangle.
    pub ncols: u32,
    /// Cell storage (sparse or dense).
    pub cells: GridStorage,
    /// Optional precomputed row signatures for alignment.
    pub row_signatures: Option<Vec<RowSignature>>,
    /// Optional precomputed column signatures for alignment.
    pub col_signatures: Option<Vec<ColSignature>>,
}

/// Storage backend for grid cells.
#[derive(Debug, Clone, PartialEq)]
pub enum GridStorage {
    Sparse(FxHashMap<(u32, u32), CellContent>),
    Dense(DenseGrid),
}

/// Dense row-major storage for grid cells.
#[derive(Debug, Clone, PartialEq)]
pub struct DenseGrid {
    ncols: u32,
    cells: Vec<CellContent>,
    non_empty: usize,
}

/// A single cell's logical content (coordinates live in the `Grid` key).
#[derive(Debug, Clone, PartialEq, Default)]
pub struct CellContent {
    /// The cell's value, if any.
    pub value: Option<CellValue>,
    /// The cell's formula text (without leading '='), if any.
    pub formula: Option<StringId>,
}

pub type Cell = CellContent;

/// A view of a cell's content together with its coordinates.
#[derive(Debug, Clone, Copy)]
pub struct CellRef<'a> {
    pub row: u32,
    pub col: u32,
    pub address: CellAddress,
    pub value: &'a Option<CellValue>,
    pub formula: &'a Option<StringId>,
}

fn cell_is_non_empty(cell: &CellContent) -> bool {
    cell.value.is_some() || cell.formula.is_some()
}

impl DenseGrid {
    fn new(nrows: u32, ncols: u32) -> Self {
        let size = nrows.saturating_mul(ncols) as usize;
        Self {
            ncols,
            cells: vec![CellContent::default(); size],
            non_empty: 0,
        }
    }

    fn capacity(&self) -> usize {
        self.cells.capacity()
    }

    fn index(&self, row: u32, col: u32) -> Option<usize> {
        if self.ncols == 0 {
            return None;
        }
        let idx = row.saturating_mul(self.ncols).saturating_add(col) as usize;
        self.cells.get(idx).map(|_| idx)
    }

    fn get(&self, row: u32, col: u32) -> Option<&CellContent> {
        let idx = self.index(row, col)?;
        let cell = &self.cells[idx];
        cell_is_non_empty(cell).then_some(cell)
    }

    fn get_mut(&mut self, row: u32, col: u32) -> Option<&mut CellContent> {
        let idx = self.index(row, col)?;
        if !cell_is_non_empty(&self.cells[idx]) {
            return None;
        }
        Some(&mut self.cells[idx])
    }

    fn set(&mut self, row: u32, col: u32, value: Option<CellValue>, formula: Option<StringId>) {
        if let Some(idx) = self.index(row, col) {
            let was_non_empty = cell_is_non_empty(&self.cells[idx]);
            let new_cell = CellContent { value, formula };
            let is_non_empty = cell_is_non_empty(&new_cell);
            if !was_non_empty && is_non_empty {
                self.non_empty = self.non_empty.saturating_add(1);
            } else if was_non_empty && !is_non_empty {
                self.non_empty = self.non_empty.saturating_sub(1);
            }
            self.cells[idx] = new_cell;
        }
    }

    fn len(&self) -> usize {
        self.non_empty
    }

    fn iter(&self) -> DenseCellIter<'_> {
        DenseCellIter {
            cells: &self.cells,
            idx: 0,
            ncols: self.ncols,
        }
    }
}

impl GridStorage {
    fn new_sparse() -> Self {
        GridStorage::Sparse(FxHashMap::default())
    }

    fn new_dense(nrows: u32, ncols: u32) -> Self {
        GridStorage::Dense(DenseGrid::new(nrows, ncols))
    }

    pub fn len(&self) -> usize {
        match self {
            GridStorage::Sparse(map) => map.len(),
            GridStorage::Dense(grid) => grid.len(),
        }
    }

    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    pub fn get(&self, row: u32, col: u32) -> Option<&CellContent> {
        match self {
            GridStorage::Sparse(map) => map.get(&(row, col)),
            GridStorage::Dense(grid) => grid.get(row, col),
        }
    }

    pub fn get_mut(&mut self, row: u32, col: u32) -> Option<&mut CellContent> {
        match self {
            GridStorage::Sparse(map) => map.get_mut(&(row, col)),
            GridStorage::Dense(grid) => grid.get_mut(row, col),
        }
    }

    pub fn insert(&mut self, row: u32, col: u32, cell: CellContent) {
        if !cell_is_non_empty(&cell) {
            match self {
                GridStorage::Sparse(map) => {
                    map.remove(&(row, col));
                }
                GridStorage::Dense(grid) => {
                    grid.set(row, col, None, None);
                }
            }
            return;
        }
        match self {
            GridStorage::Sparse(map) => {
                map.insert((row, col), cell);
            }
            GridStorage::Dense(grid) => {
                grid.set(row, col, cell.value, cell.formula);
            }
        }
    }

    pub fn iter(&self) -> GridCellIter<'_> {
        match self {
            GridStorage::Sparse(map) => GridCellIter::Sparse(map.iter()),
            GridStorage::Dense(grid) => GridCellIter::Dense(grid.iter()),
        }
    }
}

pub enum GridCellIter<'a> {
    Sparse(std::collections::hash_map::Iter<'a, (u32, u32), CellContent>),
    Dense(DenseCellIter<'a>),
}

impl<'a> Iterator for GridCellIter<'a> {
    type Item = ((u32, u32), &'a CellContent);

    fn next(&mut self) -> Option<Self::Item> {
        match self {
            GridCellIter::Sparse(iter) => iter.next().map(|(coord, cell)| (*coord, cell)),
            GridCellIter::Dense(iter) => iter.next(),
        }
    }
}

pub struct DenseCellIter<'a> {
    cells: &'a [CellContent],
    idx: usize,
    ncols: u32,
}

impl<'a> Iterator for DenseCellIter<'a> {
    type Item = ((u32, u32), &'a CellContent);

    fn next(&mut self) -> Option<Self::Item> {
        while self.idx < self.cells.len() {
            let idx = self.idx;
            self.idx += 1;
            let cell = &self.cells[idx];
            if !cell_is_non_empty(cell) {
                continue;
            }
            let row = if self.ncols == 0 {
                0
            } else {
                (idx as u32) / self.ncols
            };
            let col = if self.ncols == 0 {
                0
            } else {
                (idx as u32) % self.ncols
            };
            return Some(((row, col), cell));
        }
        None
    }
}

/// A cell address representing a position in a grid.
///
/// Can be parsed from A1-style strings (e.g., "B2", "AA10") and converted back.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct CellAddress {
    /// Zero-based row index.
    pub row: u32,
    /// Zero-based column index.
    pub col: u32,
}

impl CellAddress {
    pub fn from_indices(row: u32, col: u32) -> CellAddress {
        CellAddress { row, col }
    }

    pub fn from_coords(row: u32, col: u32) -> CellAddress {
        Self::from_indices(row, col)
    }

    pub fn to_a1(&self) -> String {
        index_to_address(self.row, self.col)
    }
}

impl std::str::FromStr for CellAddress {
    type Err = AddressParseError;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let (row, col) = address_to_index(s).ok_or_else(|| AddressParseError {
            input: s.to_string(),
        })?;
        Ok(CellAddress { row, col })
    }
}

impl std::fmt::Display for CellAddress {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.to_a1())
    }
}

impl Serialize for CellAddress {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_str(&self.to_a1())
    }
}

impl<'de> Deserialize<'de> for CellAddress {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        let a1 = String::deserialize(deserializer)?;
        CellAddress::from_str(&a1).map_err(|e| DeError::custom(e.to_string()))
    }
}

#[derive(Debug, Clone, Copy, serde::Serialize, serde::Deserialize)]
pub enum CellValue {
    Blank,
    Number(f64),
    Text(StringId),
    Bool(bool),
    Error(StringId),
}

impl PartialEq for CellValue {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (CellValue::Blank, CellValue::Blank) => true,
            (CellValue::Number(a), CellValue::Number(b)) => {
                normalize_float_for_hash(*a) == normalize_float_for_hash(*b)
            }
            (CellValue::Text(a), CellValue::Text(b)) => a == b,
            (CellValue::Bool(a), CellValue::Bool(b)) => a == b,
            (CellValue::Error(a), CellValue::Error(b)) => a == b,
            _ => false,
        }
    }
}

impl Eq for CellValue {}

impl Hash for CellValue {
    fn hash<H: Hasher>(&self, state: &mut H) {
        match self {
            CellValue::Blank => {
                3u8.hash(state);
            }
            CellValue::Number(n) => {
                0u8.hash(state);
                normalize_float_for_hash(*n).hash(state);
            }
            CellValue::Text(id) => {
                1u8.hash(state);
                id.hash(state);
            }
            CellValue::Bool(b) => {
                2u8.hash(state);
                b.hash(state);
            }
            CellValue::Error(id) => {
                4u8.hash(state);
                id.hash(state);
            }
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Default, Serialize, Deserialize)]
pub struct RowSignature {
    #[serde(with = "signature_hex")]
    pub hash: u128,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Default, Serialize, Deserialize)]
pub struct ColSignature {
    #[serde(with = "signature_hex")]
    pub hash: u128,
}

mod signature_hex {
    use serde::de::Error as DeError;
    use serde::{Deserialize, Deserializer, Serializer};

    pub fn serialize<S>(val: &u128, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        let s = format!("{:032x}", val);
        serializer.serialize_str(&s)
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<u128, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        u128::from_str_radix(&s, 16)
            .map_err(|e| DeError::custom(format!("invalid hex hash: {}", e)))
    }
}

impl Grid {
    const DENSE_RATIO_THRESHOLD: f64 = 0.40;
    const DENSE_MIN_CELLS: usize = 4096;

    pub fn new(nrows: u32, ncols: u32) -> Grid {
        Grid::new_with_storage(nrows, ncols, GridStorage::new_sparse())
    }

    pub fn new_dense(nrows: u32, ncols: u32) -> Grid {
        Grid::new_with_storage(nrows, ncols, GridStorage::new_dense(nrows, ncols))
    }

    fn new_with_storage(nrows: u32, ncols: u32, cells: GridStorage) -> Grid {
        Grid {
            nrows,
            ncols,
            cells,
            row_signatures: None,
            col_signatures: None,
        }
    }

    pub fn get(&self, row: u32, col: u32) -> Option<&CellContent> {
        self.cells.get(row, col)
    }

    pub fn get_ref(&self, row: u32, col: u32) -> Option<CellRef<'_>> {
        self.get(row, col).map(|cell| CellRef {
            row,
            col,
            address: CellAddress::from_indices(row, col),
            value: &cell.value,
            formula: &cell.formula,
        })
    }

    pub fn get_mut(&mut self, row: u32, col: u32) -> Option<&mut CellContent> {
        self.row_signatures = None;
        self.col_signatures = None;
        self.cells.get_mut(row, col)
    }

    pub fn insert_cell(
        &mut self,
        row: u32,
        col: u32,
        value: Option<CellValue>,
        formula: Option<StringId>,
    ) {
        debug_assert!(
            row < self.nrows && col < self.ncols,
            "cell coordinates must lie within the grid bounds"
        );
        self.row_signatures = None;
        self.col_signatures = None;
        self.cells
            .insert(row, col, CellContent { value, formula });
        self.maybe_upgrade_to_dense();
    }

    pub fn cell_count(&self) -> usize {
        self.cells.len()
    }

    pub fn estimated_bytes(&self) -> u64 {
        use std::mem::size_of;

        let cell_bytes = match &self.cells {
            GridStorage::Sparse(map) => {
                let cell_entry = size_of::<((u32, u32), CellContent)>();
                map.capacity().saturating_mul(cell_entry)
            }
            GridStorage::Dense(grid) => grid.capacity().saturating_mul(size_of::<CellContent>()),
        };
        let row_sig_bytes = self
            .row_signatures
            .as_ref()
            .map(|v| v.capacity().saturating_mul(size_of::<RowSignature>()))
            .unwrap_or(0);
        let col_sig_bytes = self
            .col_signatures
            .as_ref()
            .map(|v| v.capacity().saturating_mul(size_of::<ColSignature>()))
            .unwrap_or(0);

        (cell_bytes + row_sig_bytes + col_sig_bytes) as u64
    }

    pub fn is_empty(&self) -> bool {
        self.cells.is_empty()
    }

    pub fn cells_equal(&self, other: &GridStorage) -> bool {
        self.cells == *other
    }

    pub fn iter_cells(&self) -> impl Iterator<Item = ((u32, u32), &CellContent)> {
        self.cells.iter()
    }

    pub fn iter_cell_refs(&self) -> impl Iterator<Item = CellRef<'_>> {
        self.cells.iter().map(|((row, col), cell)| CellRef {
            row,
            col,
            address: CellAddress::from_indices(row, col),
            value: &cell.value,
            formula: &cell.formula,
        })
    }

    pub fn rows_iter(&self) -> impl Iterator<Item = u32> + '_ {
        0..self.nrows
    }

    pub fn cols_iter(&self) -> impl Iterator<Item = u32> + '_ {
        0..self.ncols
    }

    pub fn compute_row_signature(&self, row: u32) -> RowSignature {
        use crate::hashing::hash_cell_value;
        use std::hash::Hash;
        use xxhash_rust::xxh3::Xxh3;

        let mut hasher = Xxh3::new();

        match &self.cells {
            GridStorage::Dense(grid) => {
                for col in 0..self.ncols {
                    if let Some(cell) = grid.get(row, col) {
                        hash_cell_value(&cell.value, &mut hasher);
                        cell.formula.hash(&mut hasher);
                    }
                }
            }
            GridStorage::Sparse(map) => {
                if (self.ncols as usize) <= map.len() {
                    for col in 0..self.ncols {
                        if let Some(cell) = map.get(&(row, col)) {
                            if cell.value.is_none() && cell.formula.is_none() {
                                continue;
                            }
                            hash_cell_value(&cell.value, &mut hasher);
                            cell.formula.hash(&mut hasher);
                        }
                    }
                } else {
                    let mut row_cells: Vec<(u32, &CellContent)> = map
                        .iter()
                        .filter(|((r, _), _)| *r == row)
                        .map(|((_, c), cell)| (*c, cell))
                        .collect();
                    row_cells.sort_by_key(|(c, _)| *c);
                    for (_, cell) in row_cells {
                        if cell.value.is_none() && cell.formula.is_none() {
                            continue;
                        }
                        hash_cell_value(&cell.value, &mut hasher);
                        cell.formula.hash(&mut hasher);
                    }
                }
            }
        }

        RowSignature {
            hash: hasher.digest128(),
        }
    }

    pub fn compute_col_signature(&self, col: u32) -> ColSignature {
        use crate::hashing::hash_cell_value;
        use std::hash::Hash;
        use xxhash_rust::xxh3::Xxh3;

        let mut hasher = Xxh3::new();

        match &self.cells {
            GridStorage::Dense(grid) => {
                for row in 0..self.nrows {
                    if let Some(cell) = grid.get(row, col) {
                        hash_cell_value(&cell.value, &mut hasher);
                        cell.formula.hash(&mut hasher);
                    }
                }
            }
            GridStorage::Sparse(map) => {
                if (self.nrows as usize) <= map.len() {
                    for row in 0..self.nrows {
                        if let Some(cell) = map.get(&(row, col)) {
                            if cell.value.is_none() && cell.formula.is_none() {
                                continue;
                            }
                            hash_cell_value(&cell.value, &mut hasher);
                            cell.formula.hash(&mut hasher);
                        }
                    }
                } else {
                    let mut col_cells: Vec<(u32, &CellContent)> = map
                        .iter()
                        .filter(|((_, c), _)| *c == col)
                        .map(|((r, _), cell)| (*r, cell))
                        .collect();
                    col_cells.sort_by_key(|(r, _)| *r);
                    for (_, cell) in col_cells {
                        if cell.value.is_none() && cell.formula.is_none() {
                            continue;
                        }
                        hash_cell_value(&cell.value, &mut hasher);
                        cell.formula.hash(&mut hasher);
                    }
                }
            }
        }

        ColSignature {
            hash: hasher.digest128(),
        }
    }

    pub fn compute_all_signatures(&mut self) {
        use crate::hashing::{hash_cell_value, hash_row_content_128};
        use xxhash_rust::xxh3::Xxh3;

        match &self.cells {
            GridStorage::Dense(grid) => {
                let mut row_signatures = Vec::with_capacity(self.nrows as usize);
                for row in 0..self.nrows {
                    let mut row_cells: Vec<(u32, &CellContent)> = Vec::new();
                    for col in 0..self.ncols {
                        if let Some(cell) = grid.get(row, col) {
                            row_cells.push((col, cell));
                        }
                    }
                    row_signatures.push(RowSignature {
                        hash: hash_row_content_128(&row_cells),
                    });
                }

                let mut col_hashers: Vec<Xxh3> = (0..self.ncols).map(|_| Xxh3::new()).collect();
                for row in 0..self.nrows {
                    for col in 0..self.ncols {
                        if let Some(cell) = grid.get(row, col) {
                            let idx = col as usize;
                            if idx >= col_hashers.len() {
                                continue;
                            }
                            hash_cell_value(&cell.value, &mut col_hashers[idx]);
                            cell.formula.hash(&mut col_hashers[idx]);
                        }
                    }
                }

                let col_signatures: Vec<ColSignature> = col_hashers
                    .into_iter()
                    .map(|hasher| ColSignature {
                        hash: hasher.digest128(),
                    })
                    .collect();

                self.row_signatures = Some(row_signatures);
                self.col_signatures = Some(col_signatures);
            }
            GridStorage::Sparse(map) => {
                let mut row_cells: Vec<Vec<(u32, &CellContent)>> =
                    vec![Vec::new(); self.nrows as usize];

                for ((row, col), cell) in map.iter() {
                    let row_idx = *row as usize;
                    if row_idx >= row_cells.len() || *col >= self.ncols {
                        continue;
                    }
                    row_cells[row_idx].push((*col, cell));
                }

                for row in row_cells.iter_mut() {
                    row.sort_by_key(|(col, _)| *col);
                }

                let row_signatures: Vec<RowSignature> = row_cells
                    .iter()
                    .map(|row| RowSignature {
                        hash: hash_row_content_128(row),
                    })
                    .collect();

                let mut col_hashers: Vec<Xxh3> = (0..self.ncols).map(|_| Xxh3::new()).collect();
                for row in row_cells.iter() {
                    for (col, cell) in row.iter() {
                        let idx = *col as usize;
                        if idx >= col_hashers.len() {
                            continue;
                        }
                        hash_cell_value(&cell.value, &mut col_hashers[idx]);
                        cell.formula.hash(&mut col_hashers[idx]);
                    }
                }

                let col_signatures: Vec<ColSignature> = col_hashers
                    .into_iter()
                    .map(|hasher| ColSignature {
                        hash: hasher.digest128(),
                    })
                    .collect();

                self.row_signatures = Some(row_signatures);
                self.col_signatures = Some(col_signatures);
            }
        }
    }

    pub(crate) fn should_use_dense(nrows: u32, ncols: u32, filled_cells: usize) -> bool {
        let total_cells = nrows.saturating_mul(ncols) as usize;
        if total_cells == 0 || total_cells < Self::DENSE_MIN_CELLS {
            return false;
        }
        let ratio = filled_cells as f64 / total_cells as f64;
        ratio >= Self::DENSE_RATIO_THRESHOLD
    }

    fn maybe_upgrade_to_dense(&mut self) {
        let GridStorage::Sparse(map) = &self.cells else {
            return;
        };

        if !Self::should_use_dense(self.nrows, self.ncols, map.len()) {
            return;
        }

        let mut dense = DenseGrid::new(self.nrows, self.ncols);
        for ((row, col), cell) in map.iter() {
            dense.set(*row, *col, cell.value.clone(), cell.formula);
        }
        self.cells = GridStorage::Dense(dense);
    }
}

impl PartialEq for CellSnapshot {
    fn eq(&self, other: &Self) -> bool {
        self.value == other.value && self.formula == other.formula
    }
}

impl Eq for CellSnapshot {}

impl CellValue {
    pub fn as_text_id(&self) -> Option<StringId> {
        if let CellValue::Text(id) = self {
            Some(*id)
        } else {
            None
        }
    }

    pub fn as_text<'a>(&self, pool: &'a StringPool) -> Option<&'a str> {
        self.as_text_id().map(|id| pool.resolve(id))
    }

    pub fn as_number(&self) -> Option<f64> {
        if let CellValue::Number(n) = self {
            Some(*n)
        } else {
            None
        }
    }

    pub fn as_bool(&self) -> Option<bool> {
        if let CellValue::Bool(b) = self {
            Some(*b)
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::string_pool::StringPool;
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    fn addr(a1: &str) -> CellAddress {
        a1.parse().expect("address should parse")
    }

    fn make_cell(
        pool: &mut StringPool,
        address: &str,
        value: Option<CellValue>,
        formula: Option<&str>,
    ) -> ((u32, u32), CellContent) {
        let (row, col) = address_to_index(address).expect("address should parse");
        let formula_id = formula.map(|s| pool.intern(s));
        (
            (row, col),
            CellContent {
                value,
                formula: formula_id,
            },
        )
    }

    #[test]
    fn snapshot_from_number_cell() {
        let mut pool = StringPool::new();
        let ((row, col), cell) = make_cell(&mut pool, "A1", Some(CellValue::Number(42.0)), None);
        let snap = CellSnapshot::from_cell(row, col, &cell);
        assert_eq!(snap.addr.to_string(), "A1");
        assert_eq!(snap.value, Some(CellValue::Number(42.0)));
        assert!(snap.formula.is_none());
    }

    #[test]
    fn snapshot_from_text_cell() {
        let mut pool = StringPool::new();
        let text_id = pool.intern("hello");
        let ((row, col), cell) = make_cell(&mut pool, "B2", Some(CellValue::Text(text_id)), None);
        let snap = CellSnapshot::from_cell(row, col, &cell);
        assert_eq!(snap.addr.to_string(), "B2");
        assert_eq!(snap.value, Some(CellValue::Text(text_id)));
        assert!(snap.formula.is_none());
    }

    #[test]
    fn snapshot_from_bool_cell() {
        let mut pool = StringPool::new();
        let ((row, col), cell) = make_cell(&mut pool, "C3", Some(CellValue::Bool(true)), None);
        let snap = CellSnapshot::from_cell(row, col, &cell);
        assert_eq!(snap.addr.to_string(), "C3");
        assert_eq!(snap.value, Some(CellValue::Bool(true)));
        assert!(snap.formula.is_none());
    }

    #[test]
    fn snapshot_from_empty_cell() {
        let mut pool = StringPool::new();
        let ((row, col), cell) = make_cell(&mut pool, "D4", None, None);
        let snap = CellSnapshot::from_cell(row, col, &cell);
        assert_eq!(snap.addr.to_string(), "D4");
        assert!(snap.value.is_none());
        assert!(snap.formula.is_none());
    }

    #[test]
    fn snapshot_equality_same_value_and_formula() {
        let mut pool = StringPool::new();
        let formula_id = pool.intern("A1+1");
        let snap1 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Number(1.0)),
            formula: Some(formula_id),
        };
        let snap2 = CellSnapshot {
            addr: addr("B2"),
            value: Some(CellValue::Number(1.0)),
            formula: Some(formula_id),
        };
        assert_eq!(snap1, snap2);
    }

    #[test]
    fn snapshot_inequality_different_value_same_formula() {
        let mut pool = StringPool::new();
        let formula_id = pool.intern("A1+1");
        let snap1 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Number(43.0)),
            formula: Some(formula_id),
        };
        let snap2 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Number(44.0)),
            formula: Some(formula_id),
        };
        assert_ne!(snap1, snap2);
    }

    #[test]
    fn snapshot_inequality_value_vs_formula() {
        let snap1 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Number(42.0)),
            formula: None,
        };
        let mut pool = StringPool::new();
        let formula_id = pool.intern("A1+1");
        let snap2 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Number(42.0)),
            formula: Some(formula_id),
        };
        assert_ne!(snap1, snap2);
    }

    #[test]
    fn snapshot_equality_ignores_address() {
        let mut pool = StringPool::new();
        let text_id = pool.intern("hello");
        let snap1 = CellSnapshot {
            addr: addr("A1"),
            value: Some(CellValue::Text(text_id)),
            formula: None,
        };
        let snap2 = CellSnapshot {
            addr: addr("Z9"),
            value: Some(CellValue::Text(text_id)),
            formula: None,
        };
        assert_eq!(snap1, snap2);
    }

    #[test]
    fn cellvalue_as_text_number_bool_match_variants() {
        let mut pool = StringPool::new();
        let text_id = pool.intern("abc");
        let text = CellValue::Text(text_id);
        let number = CellValue::Number(5.0);
        let boolean = CellValue::Bool(true);

        assert_eq!(text.as_text(&pool), Some("abc"));
        assert_eq!(text.as_number(), None);
        assert_eq!(text.as_bool(), None);

        assert_eq!(number.as_text(&pool), None);
        assert_eq!(number.as_number(), Some(5.0));
        assert_eq!(number.as_bool(), None);

        assert_eq!(boolean.as_text(&pool), None);
        assert_eq!(boolean.as_number(), None);
        assert_eq!(boolean.as_bool(), Some(true));
    }

    fn hash_cell_value(value: &CellValue) -> u64 {
        let mut hasher = DefaultHasher::new();
        value.hash(&mut hasher);
        hasher.finish()
    }

    #[test]
    fn cellvalue_number_hashes_normalize_zero_sign() {
        let h_pos = hash_cell_value(&CellValue::Number(0.0));
        let h_neg = hash_cell_value(&CellValue::Number(-0.0));
        assert_eq!(h_pos, h_neg, "hash should ignore sign of zero");
    }

    #[test]
    fn cellvalue_number_hashes_ignore_ulp_drift() {
        let h_a = hash_cell_value(&CellValue::Number(1.0));
        let h_b = hash_cell_value(&CellValue::Number(1.0000000000000002));
        assert_eq!(h_a, h_b, "minor ULP drift should hash identically");
    }

    #[test]
    fn cellvalue_number_hashes_meaningful_difference() {
        let h_a = hash_cell_value(&CellValue::Number(1.0));
        let h_b = hash_cell_value(&CellValue::Number(1.0001));
        assert_ne!(h_a, h_b, "meaningful numeric changes must alter the hash");
    }

    #[test]
    fn get_mut_clears_cached_signatures() {
        let mut pool = StringPool::new();
        let mut grid = Grid::new(2, 2);
        let id1 = pool.intern("1");
        grid.insert_cell(0, 0, Some(CellValue::Text(id1)), None);
        grid.insert_cell(1, 1, Some(CellValue::Number(2.0)), None);

        grid.compute_all_signatures();
        assert!(grid.row_signatures.is_some());
        assert!(grid.col_signatures.is_some());

        let _ = grid.get_mut(0, 0);

        assert!(grid.row_signatures.is_none());
        assert!(grid.col_signatures.is_none());
    }

    #[test]
    fn insert_clears_cached_signatures() {
        let mut pool = StringPool::new();
        let mut grid = Grid::new(3, 3);
        let id1 = pool.intern("1");
        grid.insert_cell(0, 0, Some(CellValue::Text(id1)), None);

        grid.compute_all_signatures();
        assert!(grid.row_signatures.is_some());
        assert!(grid.col_signatures.is_some());

        let id2 = pool.intern("x");
        grid.insert_cell(1, 1, Some(CellValue::Text(id2)), None);

        assert!(grid.row_signatures.is_none());
        assert!(grid.col_signatures.is_none());
    }

    #[test]
    fn compute_row_signature_matches_cached_for_dense_and_sparse_paths() {
        let mut dense = Grid::new(1, 3);
        dense.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
        dense.insert_cell(0, 1, Some(CellValue::Number(2.0)), None);
        dense.insert_cell(0, 2, Some(CellValue::Number(3.0)), None);
        dense.compute_all_signatures();
        let cached_dense = dense.row_signatures.as_ref().unwrap()[0];
        assert_eq!(dense.compute_row_signature(0), cached_dense);

        let mut sparse = Grid::new(1, 10);
        sparse.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
        sparse.insert_cell(0, 9, Some(CellValue::Number(10.0)), None);
        sparse.compute_all_signatures();
        let cached_sparse = sparse.row_signatures.as_ref().unwrap()[0];
        assert_eq!(sparse.compute_row_signature(0), cached_sparse);
    }

    #[test]
    fn compute_col_signature_matches_cached_for_dense_and_sparse_paths() {
        let mut dense = Grid::new(3, 1);
        dense.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
        dense.insert_cell(1, 0, Some(CellValue::Number(2.0)), None);
        dense.insert_cell(2, 0, Some(CellValue::Number(3.0)), None);
        dense.compute_all_signatures();
        let cached_dense = dense.col_signatures.as_ref().unwrap()[0];
        assert_eq!(dense.compute_col_signature(0), cached_dense);

        let mut sparse = Grid::new(10, 2);
        sparse.insert_cell(0, 1, Some(CellValue::Number(1.0)), None);
        sparse.insert_cell(2, 1, Some(CellValue::Number(3.0)), None);
        sparse.compute_all_signatures();
        let cached_sparse = sparse.col_signatures.as_ref().unwrap()[1];
        assert_eq!(sparse.compute_col_signature(1), cached_sparse);
    }
}

```

---

### File: `core\tests\addressing_pg2_tests.rs`

```rust
mod common;

use common::{open_fixture_workbook, sid};
use excel_diff::{CellValue, address_to_index, index_to_address, with_default_session};

#[test]
fn pg2_addressing_matrix_consistency() {
    let workbook = open_fixture_workbook("pg2_addressing_matrix.xlsx");
    let sheet_names: Vec<String> = with_default_session(|session| {
        workbook
            .sheets
            .iter()
            .map(|s| session.strings.resolve(s.name).to_string())
            .collect()
    });
    let addresses_id = sid("Addresses");
    let sheet = workbook
        .sheets
        .iter()
        .find(|s| s.name == addresses_id)
        .unwrap_or_else(|| panic!("Addresses sheet present; found {:?}", sheet_names));

    for cell in sheet.grid.iter_cell_refs() {
        if let Some(CellValue::Text(text_id)) = cell.value {
            let text =
                with_default_session(|session| session.strings.resolve(*text_id).to_string());
            assert_eq!(cell.address.to_a1(), text.as_str());
            let (r, c) = address_to_index(&text).expect("address strings should parse to indices");
            assert_eq!((r, c), (cell.row, cell.col));
            assert_eq!(index_to_address(cell.row, cell.col), cell.address.to_a1());
        }
    }
}

```

---

### File: `core\tests\amr_multi_gap_tests.rs`

```rust
mod common;

use common::{grid_from_numbers, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn count_ops(ops: &[DiffOp], predicate: impl Fn(&DiffOp) -> bool) -> usize {
    ops.iter().filter(|op| predicate(op)).count()
}

fn count_row_added(ops: &[DiffOp]) -> usize {
    count_ops(ops, |op| matches!(op, DiffOp::RowAdded { .. }))
}

fn count_row_removed(ops: &[DiffOp]) -> usize {
    count_ops(ops, |op| matches!(op, DiffOp::RowRemoved { .. }))
}

fn count_block_moved_rows(ops: &[DiffOp]) -> usize {
    count_ops(ops, |op| matches!(op, DiffOp::BlockMovedRows { .. }))
}

#[test]
fn amr_two_disjoint_insertion_regions() {
    let grid_a = grid_from_numbers(&[
        &[10, 11, 12],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[50, 51, 52],
    ]);

    let grid_b = grid_from_numbers(&[
        &[10, 11, 12],
        &[100, 101, 102],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[200, 201, 202],
        &[201, 202, 203],
        &[50, 51, 52],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);
    let config = DiffConfig::default();

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "diff should be complete without hitting limits"
    );
    assert_eq!(
        count_row_added(&report.ops),
        3,
        "should detect 3 inserted rows across 2 disjoint regions"
    );
    assert_eq!(
        count_row_removed(&report.ops),
        0,
        "should not detect any removed rows"
    );
}

#[test]
fn amr_insertion_and_deletion_in_different_regions() {
    let grid_a = grid_from_numbers(&[
        &[10, 11, 12],
        &[20, 21, 22],
        &[90, 91, 92],
        &[30, 31, 32],
        &[40, 41, 42],
        &[50, 51, 52],
    ]);

    let grid_b = grid_from_numbers(&[
        &[10, 11, 12],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[100, 101, 102],
        &[50, 51, 52],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);
    let config = DiffConfig::default();

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "diff should be complete without hitting limits"
    );
    assert_eq!(
        count_row_added(&report.ops),
        1,
        "should detect 1 inserted row near the tail"
    );
    assert_eq!(
        count_row_removed(&report.ops),
        1,
        "should detect 1 deleted row in the middle"
    );
}

#[test]
fn amr_gap_contains_moved_block_scenario() {
    let grid_a = grid_from_numbers(&[
        &[10, 11, 12],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[50, 51, 52],
        &[60, 61, 62],
        &[70, 71, 72],
        &[80, 81, 82],
    ]);

    let grid_b = grid_from_numbers(&[
        &[10, 11, 12],
        &[60, 61, 62],
        &[70, 71, 72],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[50, 51, 52],
        &[80, 81, 82],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);
    let config = DiffConfig::default();

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "diff should be complete without hitting limits"
    );
    let moves = count_block_moved_rows(&report.ops);
    assert!(
        moves >= 1,
        "should detect at least one block move (rows 60-70 moved up)"
    );
    assert_eq!(
        count_row_added(&report.ops),
        0,
        "should not report spurious insertions when move is detected"
    );
    assert_eq!(
        count_row_removed(&report.ops),
        0,
        "should not report spurious deletions when move is detected"
    );
}

#[test]
fn amr_multiple_anchors_with_gaps() {
    let grid_a = grid_from_numbers(&[
        &[1, 2, 3],
        &[10, 11, 12],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[50, 51, 52],
        &[60, 61, 62],
        &[70, 71, 72],
    ]);

    let grid_b = grid_from_numbers(&[
        &[1, 2, 3],
        &[10, 11, 12],
        &[100, 101, 102],
        &[20, 21, 22],
        &[30, 31, 32],
        &[40, 41, 42],
        &[200, 201, 202],
        &[50, 51, 52],
        &[60, 61, 62],
        &[70, 71, 72],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);
    let config = DiffConfig::default();

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "diff should be complete without hitting limits"
    );
    assert_eq!(
        count_row_added(&report.ops),
        2,
        "should detect both inserted rows in separate gaps between anchors"
    );
}

#[test]
fn amr_recursive_gap_alignment() {
    let values_a: Vec<&[i32]> = (1..=50i32)
        .map(|i| {
            let row: &[i32] = match i {
                1 => &[10, 11, 12],
                2 => &[20, 21, 22],
                3 => &[30, 31, 32],
                4 => &[40, 41, 42],
                5 => &[50, 51, 52],
                6 => &[60, 61, 62],
                7 => &[70, 71, 72],
                8 => &[80, 81, 82],
                9 => &[90, 91, 92],
                10 => &[100, 101, 102],
                11 => &[110, 111, 112],
                12 => &[120, 121, 122],
                13 => &[130, 131, 132],
                14 => &[140, 141, 142],
                15 => &[150, 151, 152],
                16 => &[160, 161, 162],
                17 => &[170, 171, 172],
                18 => &[180, 181, 182],
                19 => &[190, 191, 192],
                20 => &[200, 201, 202],
                21 => &[210, 211, 212],
                22 => &[220, 221, 222],
                23 => &[230, 231, 232],
                24 => &[240, 241, 242],
                25 => &[250, 251, 252],
                26 => &[260, 261, 262],
                27 => &[270, 271, 272],
                28 => &[280, 281, 282],
                29 => &[290, 291, 292],
                30 => &[300, 301, 302],
                31 => &[310, 311, 312],
                32 => &[320, 321, 322],
                33 => &[330, 331, 332],
                34 => &[340, 341, 342],
                35 => &[350, 351, 352],
                36 => &[360, 361, 362],
                37 => &[370, 371, 372],
                38 => &[380, 381, 382],
                39 => &[390, 391, 392],
                40 => &[400, 401, 402],
                41 => &[410, 411, 412],
                42 => &[420, 421, 422],
                43 => &[430, 431, 432],
                44 => &[440, 441, 442],
                45 => &[450, 451, 452],
                46 => &[460, 461, 462],
                47 => &[470, 471, 472],
                48 => &[480, 481, 482],
                49 => &[490, 491, 492],
                50 => &[500, 501, 502],
                _ => &[0, 0, 0],
            };
            row
        })
        .collect();
    let grid_a = grid_from_numbers(&values_a);

    let mut values_b: Vec<&[i32]> = values_a.clone();
    values_b.insert(10, &[1000, 1001, 1002]);
    values_b.insert(25, &[2000, 2001, 2002]);
    values_b.insert(40, &[3000, 3001, 3002]);

    let grid_b = grid_from_numbers(&values_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);
    let config = DiffConfig::default();

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "diff should be complete without hitting limits"
    );
    assert_eq!(
        count_row_added(&report.ops),
        3,
        "should detect all 3 inserted rows distributed across the grid"
    );
}

```

---

### File: `core\tests\branch4_object_diff_tests.rs`

```rust
mod common;

use common::open_fixture_pkg;
use excel_diff::{DiffConfig, DiffOp, DiffReport, StringId, with_default_session};

fn resolve<'a>(report: &'a DiffReport, id: StringId) -> &'a str {
    report.strings[id.0 as usize].as_str()
}

#[test]
fn branch4_named_ranges_emit_add_remove_change() {
    let pkg_a = open_fixture_pkg("named_ranges_a.xlsx");
    let pkg_b = open_fixture_pkg("named_ranges_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());

    let mut saw_added = false;
    let mut saw_removed = false;
    let mut saw_changed = false;

    for op in &report.ops {
        match op {
            DiffOp::NamedRangeAdded { name } => {
                assert_eq!(resolve(&report, *name), "GlobalAdd");
                saw_added = true;
            }
            DiffOp::NamedRangeRemoved { name } => {
                assert_eq!(resolve(&report, *name), "GlobalRemove");
                saw_removed = true;
            }
            DiffOp::NamedRangeChanged {
                name,
                old_ref,
                new_ref,
            } => {
                assert_eq!(resolve(&report, *name), "Sheet1!LocalChange");
                assert_eq!(resolve(&report, *old_ref), "Sheet1!$C$1");
                assert_eq!(resolve(&report, *new_ref), "Sheet1!$C$2");
                saw_changed = true;
            }
            _ => {}
        }
    }

    assert!(saw_added, "expected NamedRangeAdded(GlobalAdd)");
    assert!(saw_removed, "expected NamedRangeRemoved(GlobalRemove)");
    assert!(saw_changed, "expected NamedRangeChanged(Sheet1!LocalChange)");
}

#[test]
fn branch4_charts_emit_added_removed_changed() {
    let pkg_a = open_fixture_pkg("charts_a.xlsx");
    let pkg_b = open_fixture_pkg("charts_b.xlsx");

    let report_ab = pkg_a.diff(&pkg_b, &DiffConfig::default());

    let mut saw_changed_chart1 = false;
    let mut saw_added_chart2 = false;

    for op in &report_ab.ops {
        match op {
            DiffOp::ChartChanged { sheet, name } => {
                assert_eq!(resolve(&report_ab, *sheet), "Sheet1");
                assert_eq!(resolve(&report_ab, *name), "Chart 1");
                saw_changed_chart1 = true;
            }
            DiffOp::ChartAdded { sheet, name } => {
                assert_eq!(resolve(&report_ab, *sheet), "Sheet1");
                assert_eq!(resolve(&report_ab, *name), "Chart 2");
                saw_added_chart2 = true;
            }
            _ => {}
        }
    }

    assert!(saw_changed_chart1, "expected ChartChanged(Sheet1, Chart 1)");
    assert!(saw_added_chart2, "expected ChartAdded(Sheet1, Chart 2)");

    let report_ba = pkg_b.diff(&pkg_a, &DiffConfig::default());
    let mut saw_removed_chart2 = false;
    for op in &report_ba.ops {
        if let DiffOp::ChartRemoved { sheet, name } = op {
            assert_eq!(resolve(&report_ba, *sheet), "Sheet1");
            assert_eq!(resolve(&report_ba, *name), "Chart 2");
            saw_removed_chart2 = true;
        }
    }
    assert!(saw_removed_chart2, "expected ChartRemoved(Sheet1, Chart 2)");
}

#[test]
fn branch4_vba_modules_emit_added_removed_changed() {
    let pkg_base = open_fixture_pkg("vba_base.xlsm");
    let pkg_added = open_fixture_pkg("vba_added.xlsm");
    let pkg_changed = open_fixture_pkg("vba_changed.xlsm");

    let report_added = pkg_base.diff(&pkg_added, &DiffConfig::default());
    let mut saw_module2_added = false;
    for op in &report_added.ops {
        if let DiffOp::VbaModuleAdded { name } = op {
            if resolve(&report_added, *name) == "Module2" {
                saw_module2_added = true;
            }
        }
    }
    assert!(saw_module2_added, "expected VbaModuleAdded(Module2)");

    let report_removed = pkg_added.diff(&pkg_base, &DiffConfig::default());
    let mut saw_module2_removed = false;
    for op in &report_removed.ops {
        if let DiffOp::VbaModuleRemoved { name } = op {
            if resolve(&report_removed, *name) == "Module2" {
                saw_module2_removed = true;
            }
        }
    }
    assert!(saw_module2_removed, "expected VbaModuleRemoved(Module2)");

    let report_changed = pkg_base.diff(&pkg_changed, &DiffConfig::default());
    let mut saw_module1_changed = false;
    for op in &report_changed.ops {
        if let DiffOp::VbaModuleChanged { name } = op {
            if resolve(&report_changed, *name) == "Module1" {
                saw_module1_changed = true;
            }
        }
    }
    assert!(saw_module1_changed, "expected VbaModuleChanged(Module1)");
}

#[test]
fn branch4_vba_modules_open_returns_modules() {
    let pkg_base = open_fixture_pkg("vba_base.xlsm");
    let pkg_added = open_fixture_pkg("vba_added.xlsm");

    let base_modules = pkg_base
        .vba_modules
        .as_ref()
        .expect("expected VBA modules in base fixture");
    let base_names: Vec<String> = with_default_session(|session| {
        base_modules
            .iter()
            .map(|module| session.strings.resolve(module.name).to_string())
            .collect()
    });
    assert!(
        base_names.iter().any(|name| name == "Module1"),
        "expected Module1 in base fixture"
    );

    let added_modules = pkg_added
        .vba_modules
        .as_ref()
        .expect("expected VBA modules in added fixture");
    let added_names: Vec<String> = with_default_session(|session| {
        added_modules
            .iter()
            .map(|module| session.strings.resolve(module.name).to_string())
            .collect()
    });
    assert!(
        added_names.iter().any(|name| name == "Module2"),
        "expected Module2 in added fixture"
    );
}

```

---

### File: `core\tests\common\mod.rs`

```rust
//! Common test utilities shared across integration tests.

#![allow(dead_code)]

use excel_diff::{
    CellSnapshot, CellValue, DiffConfig, DiffOp, DiffReport, DiffSession, DiffSummary,
    ExtractedColumnTypeChanges, ExtractedRenamePairs, ExtractedString, ExtractedStringList, Grid,
    QuerySemanticDetail, RenamePair, Sheet, SheetKind, StepChange, StepDiff, StepParams,
    StepSnapshot, StringId, Workbook, WorkbookPackage, with_default_session,
};
use serde::Deserialize;
use std::fs::File;
use std::io::ErrorKind;
use std::path::PathBuf;

pub fn fixture_path(filename: &str) -> PathBuf {
    let mut path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    path.push("../fixtures/generated");
    path.push(filename);
    path
}

pub fn open_fixture_pkg(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).unwrap_or_else(|e| {
        if e.kind() == ErrorKind::NotFound {
            panic!(
                "missing fixture {}. Run `generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --force --clean` (see fixtures/README.md).",
                path.display()
            );
        }
        panic!("failed to open fixture {}: {e}", path.display());
    });
    WorkbookPackage::open(file).unwrap_or_else(|e| {
        panic!("failed to parse fixture {}: {e}", path.display());
    })
}

pub fn open_fixture_workbook(name: &str) -> Workbook {
    open_fixture_pkg(name).workbook
}

pub fn diff_fixture_pkgs(a: &str, b: &str, config: &DiffConfig) -> DiffReport {
    let pkg_a = open_fixture_pkg(a);
    let pkg_b = open_fixture_pkg(b);
    pkg_a.diff(&pkg_b, config)
}

pub fn grid_from_numbers(values: &[&[i32]]) -> Grid {
    let nrows = values.len() as u32;
    let ncols = if nrows == 0 {
        0
    } else {
        values[0].len() as u32
    };

    let mut grid = Grid::new(nrows, ncols);
    for (r, row_vals) in values.iter().enumerate() {
        for (c, v) in row_vals.iter().enumerate() {
            grid.insert_cell(r as u32, c as u32, Some(CellValue::Number(*v as f64)), None);
        }
    }

    grid
}

pub fn sid(s: &str) -> StringId {
    with_default_session(|session| session.strings.intern(s))
}

pub fn single_sheet_workbook(name: &str, grid: Grid) -> Workbook {
    with_default_session(|session| Workbook {
        sheets: vec![Sheet {
            name: session.strings.intern(name),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    })
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct StructuredOutput {
    pub ops: Vec<DiffOp>,
    pub summary: DiffSummary,
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub struct JsonlOutput {
    pub strings: Vec<String>,
    pub ops: Vec<DiffOp>,
}

fn normalize_summary(summary: DiffSummary) -> DiffSummary {
    #[cfg(feature = "perf-metrics")]
    {
        summary.metrics = None;
    }
    summary
}

fn normalize_structured_output(mut output: StructuredOutput) -> StructuredOutput {
    output.summary = normalize_summary(output.summary);
    output
}

pub fn assert_structured_determinism_with_fresh_sessions<F>(runs: usize, mut f: F)
where
    F: FnMut(&mut DiffSession) -> StructuredOutput,
{
    let mut baseline: Option<StructuredOutput> = None;
    for _ in 0..runs {
        let mut session = DiffSession::new();
        let output = normalize_structured_output(f(&mut session));
        match &baseline {
            None => baseline = Some(output),
            Some(expected) => assert_eq!(
                expected, &output,
                "structured streaming output should be deterministic"
            ),
        }
    }
}

pub fn assert_jsonl_determinism_with_fresh_sessions<F>(runs: usize, mut f: F)
where
    F: FnMut(&mut DiffSession) -> Vec<u8>,
{
    let mut baseline: Option<JsonlOutput> = None;
    for _ in 0..runs {
        let mut session = DiffSession::new();
        let output = parse_jsonl_output(&f(&mut session));
        match &baseline {
            None => baseline = Some(output),
            Some(expected) => assert_eq!(
                expected, &output,
                "JSONL streaming output should be deterministic"
            ),
        }
    }
}

pub fn parse_jsonl_output(bytes: &[u8]) -> JsonlOutput {
    #[derive(Deserialize)]
    struct Header {
        kind: String,
        strings: Vec<String>,
    }

    let text = std::str::from_utf8(bytes).expect("output should be UTF-8");
    let mut lines = text.lines().filter(|l| !l.trim().is_empty());
    let header_line = lines.next().expect("expected a JSON Lines header line");
    let header: Header = serde_json::from_str(header_line).expect("header should parse");
    assert_eq!(header.kind, "Header");

    let mut ops = Vec::new();
    for line in lines {
        let op: DiffOp = serde_json::from_str(line).expect("op line should parse as DiffOp");
        for id in collect_string_ids(&op) {
            assert!(
                (id.0 as usize) < header.strings.len(),
                "StringId {} out of range for header string table (len={})",
                id.0,
                header.strings.len()
            );
        }
        ops.push(op);
    }

    JsonlOutput {
        strings: header.strings,
        ops,
    }
}

pub fn collect_string_ids(op: &DiffOp) -> Vec<StringId> {
    fn collect_cell_value(ids: &mut Vec<StringId>, value: &CellValue) {
        match value {
            CellValue::Text(id) | CellValue::Error(id) => ids.push(*id),
            CellValue::Number(_) | CellValue::Bool(_) | CellValue::Blank => {}
        }
    }

    fn collect_snapshot(ids: &mut Vec<StringId>, snap: &CellSnapshot) {
        if let Some(value) = &snap.value {
            collect_cell_value(ids, value);
        }
        if let Some(formula) = snap.formula {
            ids.push(formula);
        }
    }

    fn collect_extracted_string(ids: &mut Vec<StringId>, value: &ExtractedString) {
        if let ExtractedString::Known { value } = value {
            ids.push(*value);
        }
    }

    fn collect_extracted_string_list(ids: &mut Vec<StringId>, value: &ExtractedStringList) {
        if let ExtractedStringList::Known { values } = value {
            ids.extend(values.iter().copied());
        }
    }

    fn collect_rename_pairs(ids: &mut Vec<StringId>, value: &ExtractedRenamePairs) {
        if let ExtractedRenamePairs::Known { pairs } = value {
            for RenamePair { from, to } in pairs {
                ids.push(*from);
                ids.push(*to);
            }
        }
    }

    fn collect_column_type_changes(ids: &mut Vec<StringId>, value: &ExtractedColumnTypeChanges) {
        if let ExtractedColumnTypeChanges::Known { changes } = value {
            for change in changes {
                ids.push(change.column);
            }
        }
    }

    fn collect_step_params(ids: &mut Vec<StringId>, params: &StepParams) {
        match params {
            StepParams::TableSelectRows { .. } => {}
            StepParams::TableRemoveColumns { columns } => collect_extracted_string_list(ids, columns),
            StepParams::TableRenameColumns { renames } => collect_rename_pairs(ids, renames),
            StepParams::TableTransformColumnTypes { transforms } => {
                collect_column_type_changes(ids, transforms);
            }
            StepParams::TableNestedJoin {
                left_keys,
                right_keys,
                new_column,
                ..
            } => {
                collect_extracted_string_list(ids, left_keys);
                collect_extracted_string_list(ids, right_keys);
                collect_extracted_string(ids, new_column);
            }
            StepParams::TableJoin {
                left_keys,
                right_keys,
                ..
            } => {
                collect_extracted_string_list(ids, left_keys);
                collect_extracted_string_list(ids, right_keys);
            }
            StepParams::Other { .. } => {}
        }
    }

    fn collect_step_snapshot(ids: &mut Vec<StringId>, snapshot: &StepSnapshot) {
        ids.push(snapshot.name);
        ids.extend(snapshot.source_refs.iter().copied());
        if let Some(params) = &snapshot.params {
            collect_step_params(ids, params);
        }
    }

    fn collect_step_diff(ids: &mut Vec<StringId>, diff: &StepDiff) {
        match diff {
            StepDiff::StepAdded { step } | StepDiff::StepRemoved { step } => {
                collect_step_snapshot(ids, step);
            }
            StepDiff::StepReordered { name, .. } => ids.push(*name),
            StepDiff::StepModified { before, after, changes } => {
                collect_step_snapshot(ids, before);
                collect_step_snapshot(ids, after);
                for change in changes {
                    if let StepChange::Renamed { from, to } = change {
                        ids.push(*from);
                        ids.push(*to);
                    }
                    if let StepChange::SourceRefsChanged { removed, added } = change {
                        ids.extend(removed.iter().copied());
                        ids.extend(added.iter().copied());
                    }
                }
            }
        }
    }

    fn collect_semantic_detail(ids: &mut Vec<StringId>, detail: &QuerySemanticDetail) {
        for diff in &detail.step_diffs {
            collect_step_diff(ids, diff);
        }
    }

    let mut ids = Vec::new();
    match op {
        DiffOp::SheetAdded { sheet } | DiffOp::SheetRemoved { sheet } => ids.push(*sheet),
        DiffOp::SheetRenamed { sheet, from, to } => {
            ids.push(*sheet);
            ids.push(*from);
            ids.push(*to);
        }
        DiffOp::RowAdded { sheet, .. }
        | DiffOp::RowRemoved { sheet, .. }
        | DiffOp::RowReplaced { sheet, .. } => ids.push(*sheet),
        DiffOp::DuplicateKeyCluster { sheet, key, .. } => {
            ids.push(*sheet);
            for value in key.iter().flatten() {
                collect_cell_value(&mut ids, value);
            }
        }
        DiffOp::ColumnAdded { sheet, .. } | DiffOp::ColumnRemoved { sheet, .. } => ids.push(*sheet),
        DiffOp::BlockMovedRows { sheet, .. }
        | DiffOp::BlockMovedColumns { sheet, .. }
        | DiffOp::BlockMovedRect { sheet, .. }
        | DiffOp::RectReplaced { sheet, .. } => ids.push(*sheet),
        DiffOp::CellEdited {
            sheet, from, to, ..
        } => {
            ids.push(*sheet);
            collect_snapshot(&mut ids, from);
            collect_snapshot(&mut ids, to);
        }
        DiffOp::VbaModuleAdded { name }
        | DiffOp::VbaModuleRemoved { name }
        | DiffOp::VbaModuleChanged { name } => ids.push(*name),
        DiffOp::NamedRangeAdded { name } | DiffOp::NamedRangeRemoved { name } => ids.push(*name),
        DiffOp::NamedRangeChanged { name, old_ref, new_ref } => {
            ids.push(*name);
            ids.push(*old_ref);
            ids.push(*new_ref);
        }
        DiffOp::ChartAdded { sheet, name }
        | DiffOp::ChartRemoved { sheet, name }
        | DiffOp::ChartChanged { sheet, name } => {
            ids.push(*sheet);
            ids.push(*name);
        }
        DiffOp::QueryAdded { name }
        | DiffOp::QueryRemoved { name }
        | DiffOp::QueryDefinitionChanged { name, .. } => ids.push(*name),
        DiffOp::QueryRenamed { from, to } => {
            ids.push(*from);
            ids.push(*to);
        }
        DiffOp::QueryMetadataChanged { name, old, new, .. } => {
            ids.push(*name);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        #[cfg(feature = "model-diff")]
        DiffOp::TableAdded { name } | DiffOp::TableRemoved { name } => ids.push(*name),
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnAdded {
            table,
            name,
            data_type,
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(data_type) = data_type {
                ids.push(*data_type);
            }
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnRemoved { table, name } => {
            ids.push(*table);
            ids.push(*name);
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnTypeChanged {
            table,
            name,
            old_type,
            new_type,
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(old_type) = old_type {
                ids.push(*old_type);
            }
            if let Some(new_type) = new_type {
                ids.push(*new_type);
            }
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnPropertyChanged {
            table,
            name,
            old,
            new,
            ..
        } => {
            ids.push(*table);
            ids.push(*name);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        #[cfg(feature = "model-diff")]
        DiffOp::CalculatedColumnDefinitionChanged { table, name, .. } => {
            ids.push(*table);
            ids.push(*name);
        }
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipAdded {
            from_table,
            from_column,
            to_table,
            to_column,
        }
        | DiffOp::RelationshipRemoved {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            ids.push(*from_table);
            ids.push(*from_column);
            ids.push(*to_table);
            ids.push(*to_column);
        }
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipPropertyChanged {
            from_table,
            from_column,
            to_table,
            to_column,
            old,
            new,
            ..
        } => {
            ids.push(*from_table);
            ids.push(*from_column);
            ids.push(*to_table);
            ids.push(*to_column);
            if let Some(old) = old {
                ids.push(*old);
            }
            if let Some(new) = new {
                ids.push(*new);
            }
        }
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureAdded { name }
        | DiffOp::MeasureRemoved { name }
        | DiffOp::MeasureDefinitionChanged { name, .. } => ids.push(*name),
        _ => {}
    }

    if let DiffOp::QueryDefinitionChanged { semantic_detail, .. } = op {
        if let Some(detail) = semantic_detail {
            collect_semantic_detail(&mut ids, detail);
        }
    }

    ids
}

```

---

### File: `core\tests\d1_database_mode_tests.rs`

```rust
mod common;

use common::{grid_from_numbers, open_fixture_workbook, sid};
use excel_diff::{
    CellValue, DiffConfig, DiffOp, DiffReport, Grid, Workbook, WorkbookPackage,
    diff_grids_database_mode, with_default_session,
};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn diff_db(grid_a: &Grid, grid_b: &Grid, keys: &[u32]) -> DiffReport {
    with_default_session(|session| {
        diff_grids_database_mode(
            grid_a,
            grid_b,
            keys,
            &mut session.strings,
            &DiffConfig::default(),
        )
    })
}

fn data_grid(workbook: &Workbook) -> &Grid {
    let data_id = sid("Data");
    workbook
        .sheets
        .iter()
        .find(|s| s.name == data_id)
        .map(|s| &s.grid)
        .expect("Data sheet present")
}

fn grid_from_float_rows(rows: &[&[f64]]) -> Grid {
    let nrows = rows.len() as u32;
    let ncols = if nrows == 0 { 0 } else { rows[0].len() as u32 };
    let mut grid = Grid::new(nrows, ncols);

    for (r_idx, row_vals) in rows.iter().enumerate() {
        for (c_idx, value) in row_vals.iter().enumerate() {
            grid.insert_cell(
                r_idx as u32,
                c_idx as u32,
                Some(CellValue::Number(*value)),
                None,
            );
        }
    }

    grid
}

#[test]
fn d1_equal_ordered_database_mode_empty_diff() {
    let workbook = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let grid = data_grid(&workbook);

    let report = diff_db(grid, grid, &[0]);
    assert!(
        report.ops.is_empty(),
        "database mode should ignore row order when keyed rows are identical"
    );
}

#[test]
fn d1_equal_reordered_database_mode_empty_diff() {
    let wb_a = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let wb_b = open_fixture_workbook("db_equal_ordered_b.xlsx");

    let grid_a = data_grid(&wb_a);
    let grid_b = data_grid(&wb_b);

    let report = diff_db(grid_a, grid_b, &[0]);
    assert!(
        report.ops.is_empty(),
        "keyed alignment should match rows by key and ignore reordering"
    );
}

#[test]
fn d1_spreadsheet_mode_sees_reorder_as_changes() {
    let wb_a = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let wb_b = open_fixture_workbook("db_equal_ordered_b.xlsx");

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "Spreadsheet Mode should see structural changes when rows are reordered, \
         demonstrating the semantic difference from Database Mode"
    );
}

#[test]
fn d6_duplicate_keys_emit_cluster_and_match() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100], &[1, 20, 200]]);
    let grid_b = grid_from_numbers(&[&[1, 10, 100], &[1, 20, 250]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let clusters: Vec<_> = report
        .ops
        .iter()
        .filter_map(|op| {
            if let DiffOp::DuplicateKeyCluster {
                key,
                left_rows,
                right_rows,
                ..
            } = op
            {
                Some((key.clone(), left_rows.clone(), right_rows.clone()))
            } else {
                None
            }
        })
        .collect();
    assert_eq!(clusters.len(), 1, "expected one duplicate key cluster");
    assert_eq!(clusters[0].1.len(), 2, "expected two left rows in cluster");
    assert_eq!(clusters[0].2.len(), 2, "expected two right rows in cluster");

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 1,
        "duplicate cluster matching should emit the non-key cell edit"
    );

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(row_added_count, 0, "no row adds expected within cluster");
    assert_eq!(row_removed_count, 0, "no row removals expected within cluster");
}

#[test]
fn d1_database_mode_row_added() {
    let grid_a = grid_from_numbers(&[&[1, 10], &[2, 20]]);
    let grid_b = grid_from_numbers(&[&[1, 10], &[2, 20], &[3, 30]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(
        row_added_count, 1,
        "database mode should emit one RowAdded for key 3"
    );
}

#[test]
fn d1_database_mode_row_removed() {
    let grid_a = grid_from_numbers(&[&[1, 10], &[2, 20], &[3, 30]]);
    let grid_b = grid_from_numbers(&[&[1, 10], &[2, 20]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(
        row_removed_count, 1,
        "database mode should emit one RowRemoved for key 3"
    );
}

#[test]
fn d1_database_mode_cell_edited() {
    let grid_a = grid_from_numbers(&[&[1, 10], &[2, 20]]);
    let grid_b = grid_from_numbers(&[&[1, 99], &[2, 20]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 1,
        "database mode should emit one CellEdited for the changed non-key cell"
    );
}

#[test]
fn d1_database_mode_cell_edited_with_reorder() {
    let grid_a = grid_from_numbers(&[&[1, 10], &[2, 20], &[3, 30]]);
    let grid_b = grid_from_numbers(&[&[3, 30], &[2, 99], &[1, 10]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 1,
        "database mode should ignore reordering and find only the cell edit for key 2"
    );
}

#[test]
fn d1_database_mode_treats_small_float_key_noise_as_equal() {
    let grid_a = grid_from_float_rows(&[&[1.0, 10.0], &[2.0, 20.0], &[3.0, 30.0]]);
    let grid_b = grid_from_float_rows(&[&[1.0000000000000002, 10.0], &[2.0, 20.0], &[3.0, 30.0]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);
    assert!(
        report.ops.is_empty(),
        "ULP-level noise in key column should not break row alignment"
    );
}

#[test]
fn d1_database_mode_detects_meaningful_float_key_change() {
    let grid_a = grid_from_float_rows(&[&[1.0, 10.0], &[2.0, 20.0], &[3.0, 30.0]]);
    let grid_b = grid_from_float_rows(&[&[1.0001, 10.0], &[2.0, 20.0], &[3.0, 30.0]]);

    let report = diff_db(&grid_a, &grid_b, &[0]);

    let row_removed = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    let row_added = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();

    assert_eq!(
        row_removed, 1,
        "meaningful key drift should remove the original keyed row"
    );
    assert_eq!(
        row_added, 1,
        "meaningful key drift should add the new keyed row"
    );
}

#[test]
fn d5_composite_key_equal_reordered_database_mode_empty_diff() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100], &[1, 20, 200], &[2, 10, 300]]);
    let grid_b = grid_from_numbers(&[&[2, 10, 300], &[1, 10, 100], &[1, 20, 200]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1]);
    assert!(
        report.ops.is_empty(),
        "composite keyed alignment should ignore row order differences"
    );
}

#[test]
fn d5_composite_key_row_added_and_cell_edited() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100], &[1, 20, 200]]);
    let grid_b = grid_from_numbers(&[&[1, 10, 150], &[1, 20, 200], &[2, 30, 300]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1]);

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(
        row_added_count, 1,
        "new composite key should produce exactly one RowAdded"
    );

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(
        row_removed_count, 0,
        "no rows should be removed when only a new composite key is introduced"
    );

    let mut cell_edited_iter = report.ops.iter().filter_map(|op| {
        if let DiffOp::CellEdited { addr, .. } = op {
            Some(addr)
        } else {
            None
        }
    });

    let edited_addr = cell_edited_iter
        .next()
        .expect("one cell edit for changed non-key value");
    assert!(
        cell_edited_iter.next().is_none(),
        "only one CellEdited should be present"
    );
    assert_eq!(edited_addr.col, 2, "only non-key column should be edited");
    assert_eq!(
        edited_addr.row, 0,
        "cell edit should reference the row of key (1,10) in the new grid"
    );
}

#[test]
fn d5_composite_key_partial_key_mismatch_yields_add_and_remove() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100]]);
    let grid_b = grid_from_numbers(&[&[1, 20, 100]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1]);

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(
        row_removed_count, 1,
        "changed composite key should remove the old tuple"
    );

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(
        row_added_count, 1,
        "changed composite key should add the new tuple"
    );

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 0,
        "partial key match must not be treated as a cell edit"
    );
}

#[test]
fn d6_composite_key_duplicate_keys_emit_cluster() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100], &[1, 10, 200]]);
    let grid_b = grid_from_numbers(&[&[1, 10, 100]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1]);

    let has_cluster = report.ops.iter().any(|op| {
        matches!(op, DiffOp::DuplicateKeyCluster { .. })
    });
    assert!(
        has_cluster,
        "duplicate composite keys should emit a DuplicateKeyCluster op"
    );
}

#[test]
fn d10_mixed_region_table_and_freeform_cells() {
    let mut grid_a = Grid::new(6, 5);
    grid_a.insert_cell(2, 1, Some(CellValue::Number(1.0)), None);
    grid_a.insert_cell(2, 2, Some(CellValue::Number(100.0)), None);
    grid_a.insert_cell(3, 1, Some(CellValue::Number(2.0)), None);
    grid_a.insert_cell(3, 2, Some(CellValue::Number(200.0)), None);
    grid_a.insert_cell(0, 4, Some(CellValue::Number(10.0)), None);

    let mut grid_b = Grid::new(6, 5);
    grid_b.insert_cell(2, 1, Some(CellValue::Number(2.0)), None);
    grid_b.insert_cell(2, 2, Some(CellValue::Number(200.0)), None);
    grid_b.insert_cell(3, 1, Some(CellValue::Number(1.0)), None);
    grid_b.insert_cell(3, 2, Some(CellValue::Number(150.0)), None);
    grid_b.insert_cell(0, 4, Some(CellValue::Number(20.0)), None);

    let report = diff_db(&grid_a, &grid_b, &[1]);

    let mut edited_addrs = report.ops.iter().filter_map(|op| {
        if let DiffOp::CellEdited { addr, .. } = op {
            Some((addr.row, addr.col))
        } else {
            None
        }
    }).collect::<Vec<_>>();
    edited_addrs.sort_unstable();

    assert_eq!(
        edited_addrs,
        vec![(0, 4), (3, 2)],
        "expected a freeform edit and a keyed table edit"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. })),
        "table reorder should not emit row add/remove ops"
    );
}

#[test]
fn d5_non_contiguous_key_columns_equal_reordered_empty_diff() {
    let grid_a = grid_from_numbers(&[&[1, 999, 10, 100], &[1, 888, 20, 200], &[2, 777, 10, 300]]);
    let grid_b = grid_from_numbers(&[&[2, 777, 10, 300], &[1, 999, 10, 100], &[1, 888, 20, 200]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 2]);
    assert!(
        report.ops.is_empty(),
        "non-contiguous key columns [0,2] should align correctly ignoring row order"
    );
}

#[test]
fn d5_non_contiguous_key_columns_detects_edits_in_skipped_column() {
    let grid_a = grid_from_numbers(&[&[1, 999, 10, 100], &[1, 888, 20, 200], &[2, 777, 10, 300]]);
    let grid_b = grid_from_numbers(&[&[2, 111, 10, 300], &[1, 222, 10, 100], &[1, 333, 20, 200]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 2]);

    let cell_edited_ops: Vec<_> = report
        .ops
        .iter()
        .filter_map(|op| {
            if let DiffOp::CellEdited { addr, .. } = op {
                Some(addr)
            } else {
                None
            }
        })
        .collect();

    assert_eq!(
        cell_edited_ops.len(),
        3,
        "should detect 3 edits in skipped non-key column 1"
    );

    for addr in &cell_edited_ops {
        assert_eq!(
            addr.col, 1,
            "all edits should be in the skipped column 1, not key columns 0 or 2"
        );
    }

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(row_added_count, 0, "no rows should be added");

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(row_removed_count, 0, "no rows should be removed");
}

#[test]
fn d5_non_contiguous_key_columns_row_added_and_cell_edited() {
    let grid_a = grid_from_numbers(&[&[1, 999, 10, 100], &[1, 888, 20, 200]]);
    let grid_b = grid_from_numbers(&[&[1, 999, 10, 150], &[1, 888, 20, 200], &[2, 777, 30, 300]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 2]);

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(
        row_added_count, 1,
        "new non-contiguous composite key should produce exactly one RowAdded"
    );

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(row_removed_count, 0, "no rows should be removed");

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 1,
        "changed non-key column should produce exactly one CellEdited"
    );
}

#[test]
fn d5_three_column_composite_key_equal_reordered_empty_diff() {
    let grid_a = grid_from_numbers(&[
        &[1, 10, 100, 1000],
        &[1, 10, 200, 2000],
        &[1, 20, 100, 3000],
        &[2, 10, 100, 4000],
    ]);
    let grid_b = grid_from_numbers(&[
        &[2, 10, 100, 4000],
        &[1, 20, 100, 3000],
        &[1, 10, 200, 2000],
        &[1, 10, 100, 1000],
    ]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1, 2]);
    assert!(
        report.ops.is_empty(),
        "three-column composite key should align correctly ignoring row order"
    );
}

#[test]
fn d5_three_column_composite_key_partial_match_yields_add_and_remove() {
    let grid_a = grid_from_numbers(&[&[1, 10, 100, 1000]]);
    let grid_b = grid_from_numbers(&[&[1, 10, 200, 1000]]);

    let report = diff_db(&grid_a, &grid_b, &[0, 1, 2]);

    let row_removed_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    assert_eq!(
        row_removed_count, 1,
        "changed third key column should remove the old tuple"
    );

    let row_added_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    assert_eq!(
        row_added_count, 1,
        "changed third key column should add the new tuple"
    );

    let cell_edited_count = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    assert_eq!(
        cell_edited_count, 0,
        "partial three-column key match must not be treated as a cell edit"
    );
}

```

---

### File: `core\tests\d2_d4_database_mode_workbook_tests.rs`

```rust
mod common;

use common::{open_fixture_workbook, sid};
use excel_diff::{
    CellValue, DiffConfig, DiffOp, DiffReport, Grid, Workbook, diff_grids_database_mode,
    with_default_session,
};

fn diff_db(grid_a: &Grid, grid_b: &Grid, keys: &[u32]) -> DiffReport {
    with_default_session(|session| {
        diff_grids_database_mode(
            grid_a,
            grid_b,
            keys,
            &mut session.strings,
            &DiffConfig::default(),
        )
    })
}

fn data_grid(workbook: &Workbook) -> &Grid {
    let data_id = sid("Data");
    workbook
        .sheets
        .iter()
        .find(|s| s.name == data_id)
        .map(|s| &s.grid)
        .expect("Data sheet present")
}

fn find_row_by_id(grid: &Grid, target_id: i64) -> Option<u32> {
    for row in 0..grid.nrows {
        if let Some(cell) = grid.get(row, 0) {
            if let Some(CellValue::Number(n)) = cell.value {
                if (n as i64) == target_id {
                    return Some(row);
                }
            }
        }
    }
    None
}

fn count_ops(report: &DiffReport) -> (usize, usize, usize) {
    let row_added = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .count();
    let row_removed = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .count();
    let cell_edited = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .count();
    (row_added, row_removed, cell_edited)
}

#[test]
fn d2_row_added_emits_row_added_only() {
    let wb_a = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let wb_b = open_fixture_workbook("db_row_added_b.xlsx");

    let grid_a = data_grid(&wb_a);
    let grid_b = data_grid(&wb_b);

    let report = diff_db(grid_a, grid_b, &[0]);
    let (row_added, row_removed, cell_edited) = count_ops(&report);

    assert_eq!(row_added, 1, "should emit exactly 1 RowAdded");
    assert_eq!(row_removed, 0, "should emit 0 RowRemoved");
    assert_eq!(cell_edited, 0, "should emit 0 CellEdited");

    let row_b = find_row_by_id(grid_b, 1001).expect("new row with ID 1001 should exist in B");

    let added_row_idx = report
        .ops
        .iter()
        .find_map(|op| {
            if let DiffOp::RowAdded { row_idx, .. } = op {
                Some(*row_idx)
            } else {
                None
            }
        })
        .expect("RowAdded op should exist");

    assert_eq!(
        added_row_idx, row_b,
        "RowAdded row_idx should match the row of ID 1001 in grid B"
    );
}

#[test]
fn d2_row_removed_emits_row_removed_only() {
    let wb_a = open_fixture_workbook("db_row_added_b.xlsx");
    let wb_b = open_fixture_workbook("db_equal_ordered_a.xlsx");

    let grid_a = data_grid(&wb_a);
    let grid_b = data_grid(&wb_b);

    let report = diff_db(grid_a, grid_b, &[0]);
    let (row_added, row_removed, cell_edited) = count_ops(&report);

    assert_eq!(row_removed, 1, "should emit exactly 1 RowRemoved");
    assert_eq!(row_added, 0, "should emit 0 RowAdded");
    assert_eq!(cell_edited, 0, "should emit 0 CellEdited");

    let row_a = find_row_by_id(grid_a, 1001).expect("removed row with ID 1001 should exist in A");

    let removed_row_idx = report
        .ops
        .iter()
        .find_map(|op| {
            if let DiffOp::RowRemoved { row_idx, .. } = op {
                Some(*row_idx)
            } else {
                None
            }
        })
        .expect("RowRemoved op should exist");

    assert_eq!(
        removed_row_idx, row_a,
        "RowRemoved row_idx should match the row of ID 1001 in grid A"
    );
}

#[test]
fn d3_row_update_emits_cell_edited_only() {
    let wb_a = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let wb_b = open_fixture_workbook("db_row_update_b.xlsx");

    let grid_a = data_grid(&wb_a);
    let grid_b = data_grid(&wb_b);

    let report = diff_db(grid_a, grid_b, &[0]);
    let (row_added, row_removed, cell_edited) = count_ops(&report);

    assert_eq!(cell_edited, 1, "should emit exactly 1 CellEdited");
    assert_eq!(row_added, 0, "should emit 0 RowAdded");
    assert_eq!(row_removed, 0, "should emit 0 RowRemoved");

    let row_b = find_row_by_id(grid_b, 7).expect("row with ID 7 should exist in B");

    let edit = report
        .ops
        .iter()
        .find_map(|op| {
            if let DiffOp::CellEdited { addr, from, to, .. } = op {
                Some((addr, from, to))
            } else {
                None
            }
        })
        .expect("CellEdited op should exist");

    let (addr, from, to) = edit;
    assert_eq!(addr.row, row_b, "CellEdited should target row of ID 7 in B");
    assert_eq!(
        addr.col, 2,
        "CellEdited should target Amount column (col 2)"
    );

    let baseline_amount = 7.0 * 10.5;
    match &from.value {
        Some(CellValue::Number(n)) => {
            assert!(
                (*n - baseline_amount).abs() < 0.001,
                "from.value should be baseline Amount for ID 7 ({baseline_amount}), got {n}"
            );
        }
        other => panic!("from.value should be Number, got {:?}", other),
    }

    match &to.value {
        Some(CellValue::Number(n)) => {
            assert!(
                (*n - 120.0).abs() < 0.001,
                "to.value should be 120.0, got {n}"
            );
        }
        other => panic!("to.value should be Number, got {:?}", other),
    }

    assert!(from.formula.is_none(), "from.formula should be None");
    assert!(to.formula.is_none(), "to.formula should be None");
}

#[test]
fn d4_reorder_and_change_emits_cell_edited_only() {
    let wb_a = open_fixture_workbook("db_equal_ordered_a.xlsx");
    let wb_b = open_fixture_workbook("db_reorder_and_change_b.xlsx");

    let grid_a = data_grid(&wb_a);
    let grid_b = data_grid(&wb_b);

    let report = diff_db(grid_a, grid_b, &[0]);
    let (row_added, row_removed, cell_edited) = count_ops(&report);

    assert_eq!(cell_edited, 1, "should emit exactly 1 CellEdited");
    assert_eq!(row_added, 0, "should emit 0 RowAdded (reorder is ignored)");
    assert_eq!(
        row_removed, 0,
        "should emit 0 RowRemoved (reorder is ignored)"
    );

    let row_b = find_row_by_id(grid_b, 7).expect("row with ID 7 should exist in shuffled B");

    let edit = report
        .ops
        .iter()
        .find_map(|op| {
            if let DiffOp::CellEdited { addr, from, to, .. } = op {
                Some((addr, from, to))
            } else {
                None
            }
        })
        .expect("CellEdited op should exist");

    let (addr, from, to) = edit;
    assert_eq!(
        addr.row, row_b,
        "CellEdited should target row of ID 7 in shuffled B"
    );
    assert_eq!(
        addr.col, 2,
        "CellEdited should target Amount column (col 2)"
    );

    let baseline_amount = 7.0 * 10.5;
    match &from.value {
        Some(CellValue::Number(n)) => {
            assert!(
                (*n - baseline_amount).abs() < 0.001,
                "from.value should be baseline Amount for ID 7 ({baseline_amount}), got {n}"
            );
        }
        other => panic!("from.value should be Number, got {:?}", other),
    }

    match &to.value {
        Some(CellValue::Number(n)) => {
            assert!(
                (*n - 120.0).abs() < 0.001,
                "to.value should be 120.0, got {n}"
            );
        }
        other => panic!("to.value should be Number, got {:?}", other),
    }
}

```

---

### File: `core\tests\data_mashup_tests.rs`

```rust
use std::fs::File;
use std::io::{ErrorKind, Read};

use base64::Engine;
use base64::engine::general_purpose::STANDARD;
use excel_diff::{
    ContainerError, DataMashupError, PackageError, RawDataMashup, build_data_mashup,
    open_data_mashup,
};
use quick_xml::{Reader, events::Event};
use zip::ZipArchive;

mod common;
use common::fixture_path;

fn unwrap_path_error(err: PackageError) -> PackageError {
    match err {
        PackageError::WithPath { source, .. } => *source,
        other => other,
    }
}

fn unwrap_datamashup_part_error(err: PackageError) -> PackageError {
    match err {
        PackageError::DataMashupPartError { source, .. } => PackageError::DataMashup(source),
        other => other,
    }
}

fn unwrap_all(err: PackageError) -> PackageError {
    let err = unwrap_path_error(err);
    unwrap_datamashup_part_error(err)
}

#[test]
fn workbook_without_datamashup_returns_none() {
    let path = fixture_path("minimal.xlsx");
    let result = open_data_mashup(&path).expect("minimal workbook should load");
    assert!(result.is_none());
}

#[test]
fn workbook_with_valid_datamashup_parses() {
    let path = fixture_path("m_change_literal_b.xlsx");
    let raw = open_data_mashup(&path)
        .expect("valid mashup should load")
        .expect("mashup should be present");

    assert_eq!(raw.version, 0);
    assert!(!raw.package_parts.is_empty());
    assert!(!raw.metadata.is_empty());

    let assembled = assemble_top_level_bytes(&raw);
    let expected = datamashup_bytes_from_fixture(&path);
    assert_eq!(assembled, expected);
}

#[test]
fn datamashup_with_base64_whitespace_parses() {
    let path = fixture_path("mashup_base64_whitespace.xlsx");
    let raw = open_data_mashup(&path)
        .expect("whitespace in base64 payload should be tolerated")
        .expect("mashup should be present");
    assert_eq!(raw.version, 0);
    assert!(!raw.package_parts.is_empty());
}

#[test]
fn utf16_le_datamashup_parses() {
    let path = fixture_path("mashup_utf16_le.xlsx");
    let raw = open_data_mashup(&path)
        .expect("UTF-16LE mashup should load")
        .expect("mashup should be present");
    assert_eq!(raw.version, 0);
    assert!(!raw.package_parts.is_empty());
}

#[test]
fn utf16_be_datamashup_parses() {
    let path = fixture_path("mashup_utf16_be.xlsx");
    let raw = open_data_mashup(&path)
        .expect("UTF-16BE mashup should load")
        .expect("mashup should be present");
    assert_eq!(raw.version, 0);
    assert!(!raw.package_parts.is_empty());
}

#[test]
fn corrupt_base64_returns_error() {
    let path = fixture_path("corrupt_base64.xlsx");
    let err = open_data_mashup(&path).expect_err("corrupt base64 should fail");
    let err = unwrap_all(err);
    assert!(
        matches!(err, PackageError::DataMashup(DataMashupError::Base64Invalid)),
        "expected Base64Invalid, got {err:?}"
    );
}

#[test]
fn duplicate_datamashup_parts_are_rejected() {
    let path = fixture_path("duplicate_datamashup_parts.xlsx");
    let err = open_data_mashup(&path).expect_err("duplicate DataMashup parts should be rejected");
    let err = unwrap_all(err);
    assert!(
        matches!(err, PackageError::DataMashup(DataMashupError::FramingInvalid)),
        "expected FramingInvalid, got {err:?}"
    );
}

#[test]
fn duplicate_datamashup_elements_are_rejected() {
    let path = fixture_path("duplicate_datamashup_elements.xlsx");
    let err =
        open_data_mashup(&path).expect_err("duplicate DataMashup elements should be rejected");
    let err = unwrap_all(err);
    assert!(
        matches!(err, PackageError::DataMashup(DataMashupError::FramingInvalid)),
        "expected FramingInvalid, got {err:?}"
    );
}

#[test]
fn nonexistent_file_returns_io() {
    let path = fixture_path("missing_mashup.xlsx");
    let err = open_data_mashup(&path).expect_err("missing file should error");
    let err = unwrap_path_error(err);
    match err {
        PackageError::Container(ContainerError::Io(e)) => {
            assert_eq!(e.kind(), ErrorKind::NotFound)
        }
        other => panic!("expected Io error, got {other:?}"),
    }
}

#[test]
fn non_excel_container_returns_not_excel_error() {
    let path = fixture_path("random_zip.zip");
    let err = open_data_mashup(&path).expect_err("random zip should not parse");
    let err = unwrap_path_error(err);
    assert!(
        matches!(err, PackageError::Container(ContainerError::NotOpcPackage)),
        "expected NotOpcPackage, got {err:?}"
    );
}

#[test]
fn missing_content_types_is_not_excel_error() {
    let path = fixture_path("no_content_types.xlsx");
    let err = open_data_mashup(&path).expect_err("missing [Content_Types].xml should fail");
    let err = unwrap_path_error(err);
    assert!(
        matches!(err, PackageError::Container(ContainerError::NotOpcPackage)),
        "expected NotOpcPackage, got {err:?}"
    );
}

#[test]
fn non_zip_file_returns_not_zip_error() {
    let path = fixture_path("not_a_zip.txt");
    let err = open_data_mashup(&path).expect_err("non-zip input should not parse as Excel");
    let err = unwrap_path_error(err);
    assert!(
        matches!(err, PackageError::Container(ContainerError::NotZipContainer)),
        "expected NotZipContainer, got {err:?}"
    );
}

#[test]
fn build_data_mashup_smoke_from_fixture() {
    let raw = open_data_mashup(fixture_path("one_query.xlsx"))
        .expect("fixture should load")
        .expect("DataMashup should be present");
    let dm = build_data_mashup(&raw).expect("build_data_mashup should succeed");

    assert_eq!(dm.version, 0);
    assert!(
        dm.package_parts
            .main_section
            .source
            .contains("section Section1;")
    );
    assert!(!dm.metadata.formulas.is_empty());

    let non_connection: Vec<_> = dm
        .metadata
        .formulas
        .iter()
        .filter(|m| m.section_name == "Section1" && !m.is_connection_only)
        .collect();
    assert_eq!(non_connection.len(), 1);
    let meta = non_connection[0];
    assert_eq!(
        meta.item_path,
        format!("{}/{}", meta.section_name, meta.formula_name)
    );
    assert_eq!(meta.item_path, "Section1/Query1");
    assert_eq!(meta.section_name, "Section1");
    assert_eq!(meta.formula_name, "Query1");
    assert!(meta.load_to_sheet || meta.load_to_model);
}

fn datamashup_bytes_from_fixture(path: &std::path::Path) -> Vec<u8> {
    let file = File::open(path).expect("fixture should be readable");
    let mut archive = ZipArchive::new(file).expect("fixture should be a zip container");
    for i in 0..archive.len() {
        let mut file = archive.by_index(i).expect("zip entry should be readable");
        let name = file.name().to_string();
        if !name.starts_with("customXml/") || !name.ends_with(".xml") {
            continue;
        }

        let mut buf = Vec::new();
        file.read_to_end(&mut buf).expect("XML part should read");
        if let Some(text) = extract_datamashup_base64(&buf) {
            let cleaned: String = text.split_whitespace().collect();
            return STANDARD
                .decode(cleaned.as_bytes())
                .expect("DataMashup base64 should decode");
        }
    }

    panic!("DataMashup element not found in {}", path.display());
}

fn extract_datamashup_base64(xml: &[u8]) -> Option<String> {
    let mut reader = Reader::from_reader(xml);
    reader.config_mut().trim_text(false);
    let mut buf = Vec::new();
    let mut in_datamashup = false;
    let mut content = String::new();

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(e)) if is_datamashup_element(e.name().as_ref()) => {
                if in_datamashup {
                    return None;
                }
                in_datamashup = true;
                content.clear();
            }
            Ok(Event::Text(t)) if in_datamashup => {
                let text = t.unescape().ok()?.into_owned();
                content.push_str(&text);
            }
            Ok(Event::CData(t)) if in_datamashup => {
                content.push_str(&String::from_utf8_lossy(&t.into_inner()));
            }
            Ok(Event::End(e)) if is_datamashup_element(e.name().as_ref()) => {
                if !in_datamashup {
                    return None;
                }
                return Some(content.clone());
            }
            Ok(Event::Eof) => return None,
            Err(_) => return None,
            _ => {}
        }
        buf.clear();
    }
}

fn is_datamashup_element(name: &[u8]) -> bool {
    match name.iter().rposition(|&b| b == b':') {
        Some(idx) => name.get(idx + 1..) == Some(b"DataMashup".as_slice()),
        None => name == b"DataMashup",
    }
}

fn assemble_top_level_bytes(raw: &RawDataMashup) -> Vec<u8> {
    let mut bytes = Vec::new();
    bytes.extend_from_slice(&raw.version.to_le_bytes());
    bytes.extend_from_slice(&(raw.package_parts.len() as u32).to_le_bytes());
    bytes.extend_from_slice(&raw.package_parts);
    bytes.extend_from_slice(&(raw.permissions.len() as u32).to_le_bytes());
    bytes.extend_from_slice(&raw.permissions);
    bytes.extend_from_slice(&(raw.metadata.len() as u32).to_le_bytes());
    bytes.extend_from_slice(&raw.metadata);
    bytes.extend_from_slice(&(raw.permission_bindings.len() as u32).to_le_bytes());
    bytes.extend_from_slice(&raw.permission_bindings);
    bytes
}

```

---

### File: `core\tests\database_mode_wrapper_tests.rs`

```rust
use excel_diff::{
    CellValue, DiffConfig, Grid, LimitBehavior, diff_grids_database_mode, with_default_session,
};

#[test]
fn database_mode_wrapper_limits_exceeded_returns_incomplete_report() {
    let mut grid_a = Grid::new(2, 1);
    grid_a.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
    grid_a.insert_cell(1, 0, Some(CellValue::Number(1.0)), None);

    let mut grid_b = Grid::new(2, 1);
    grid_b.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
    grid_b.insert_cell(1, 0, Some(CellValue::Number(1.0)), None);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 1;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let result = std::panic::catch_unwind(|| {
        with_default_session(|session| {
            diff_grids_database_mode(&grid_a, &grid_b, &[0], &mut session.strings, &config)
        })
    });
    assert!(result.is_ok(), "database mode wrapper should not panic");
    let report = result.unwrap();

    assert!(!report.complete, "report should be marked incomplete");
    assert!(
        report
            .warnings
            .iter()
            .any(|w| w.contains("alignment limits exceeded")),
        "expected limits exceeded warning; warnings: {:?}",
        report.warnings
    );
}


```

---

### File: `core\tests\e2e_perf_workbook_open.rs`

```rust
#![cfg(feature = "perf-metrics")]

mod common;

use common::fixture_path;
use excel_diff::perf::DiffMetrics;
use excel_diff::{CallbackSink, ContainerLimits, DiffConfig, WorkbookPackage};
use std::fs::File;

fn open_fixture_with_size(name: &str) -> (WorkbookPackage, u64) {
    let path = fixture_path(name);
    let bytes = std::fs::metadata(&path)
        .map(|meta| meta.len())
        .unwrap_or(0);
    let file = File::open(&path).unwrap_or_else(|e| {
        panic!("failed to open fixture {}: {e}", path.display());
    });
    let limits = ContainerLimits {
        max_entries: 10_000,
        max_part_uncompressed_bytes: 512 * 1024 * 1024,
        max_total_uncompressed_bytes: 1024 * 1024 * 1024,
    };
    let pkg = WorkbookPackage::open_with_limits(file, limits).unwrap_or_else(|e| {
        panic!("failed to parse fixture {}: {e}", path.display());
    });
    (pkg, bytes)
}

fn log_perf_metric(name: &str, metrics: &DiffMetrics, old_bytes: u64, new_bytes: u64) {
    let total_input_bytes = old_bytes.saturating_add(new_bytes);
    println!(
        "PERF_METRIC {name} total_time_ms={} parse_time_ms={} diff_time_ms={} signature_build_time_ms={} move_detection_time_ms={} alignment_time_ms={} cell_diff_time_ms={} op_emit_time_ms={} report_serialize_time_ms={} peak_memory_bytes={} grid_storage_bytes={} string_pool_bytes={} op_buffer_bytes={} alignment_buffer_bytes={} rows_processed={} cells_compared={} anchors_found={} moves_detected={} hash_lookups_est={} allocations_est={} old_bytes={} new_bytes={} total_input_bytes={}",
        metrics.total_time_ms,
        metrics.parse_time_ms,
        metrics.diff_time_ms,
        metrics.signature_build_time_ms,
        metrics.move_detection_time_ms,
        metrics.alignment_time_ms,
        metrics.cell_diff_time_ms,
        metrics.op_emit_time_ms,
        metrics.report_serialize_time_ms,
        metrics.peak_memory_bytes,
        metrics.grid_storage_bytes,
        metrics.string_pool_bytes,
        metrics.op_buffer_bytes,
        metrics.alignment_buffer_bytes,
        metrics.rows_processed,
        metrics.cells_compared,
        metrics.anchors_found,
        metrics.moves_detected,
        metrics.hash_lookups_est,
        metrics.allocations_est,
        old_bytes,
        new_bytes,
        total_input_bytes
    );
}

fn run_e2e_case(name: &str, old_name: &str, new_name: &str, expect_ops: bool) {
    let (old_pkg, old_bytes) = open_fixture_with_size(old_name);
    let (new_pkg, new_bytes) = open_fixture_with_size(new_name);

    let mut op_count = 0usize;
    let summary = {
        let mut sink = CallbackSink::new(|_op| op_count += 1);
        old_pkg
            .diff_streaming(&new_pkg, &DiffConfig::default(), &mut sink)
            .expect("diff_streaming should succeed")
    };

    assert!(summary.complete, "expected streaming diff to complete");
    assert_eq!(
        summary.op_count, op_count,
        "summary op_count should match sink-emitted ops"
    );

    if expect_ops {
        assert!(summary.op_count > 0, "expected at least one op");
    } else {
        assert_eq!(summary.op_count, 0, "expected no ops");
    }

    let metrics = summary.metrics.expect("expected perf metrics");
    assert!(
        metrics.parse_time_ms > 0,
        "parse_time_ms should be non-zero for e2e fixtures"
    );
    assert!(
        metrics.total_time_ms >= metrics.parse_time_ms,
        "total_time_ms should include parse_time_ms"
    );
    assert_eq!(
        metrics.diff_time_ms,
        metrics.total_time_ms.saturating_sub(metrics.parse_time_ms)
    );

    log_perf_metric(name, &metrics, old_bytes, new_bytes);
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn e2e_p1_dense_single_edit() {
    run_e2e_case(
        "e2e_p1_dense",
        "e2e_p1_dense_a.xlsx",
        "e2e_p1_dense_b.xlsx",
        true,
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn e2e_p2_noise_single_edit() {
    run_e2e_case(
        "e2e_p2_noise",
        "e2e_p2_noise_a.xlsx",
        "e2e_p2_noise_b.xlsx",
        true,
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn e2e_p3_repetitive_single_edit() {
    run_e2e_case(
        "e2e_p3_repetitive",
        "e2e_p3_repetitive_a.xlsx",
        "e2e_p3_repetitive_b.xlsx",
        true,
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn e2e_p4_sparse_single_edit() {
    run_e2e_case(
        "e2e_p4_sparse",
        "e2e_p4_sparse_a.xlsx",
        "e2e_p4_sparse_b.xlsx",
        true,
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn e2e_p5_identical() {
    run_e2e_case(
        "e2e_p5_identical",
        "e2e_p5_identical_a.xlsx",
        "e2e_p5_identical_b.xlsx",
        false,
    );
}

```

---

### File: `core\tests\engine_tests.rs`

```rust
mod common;

use common::sid;
use excel_diff::{
    CellAddress, CellSnapshot, CellValue, DiffConfig, DiffOp, DiffReport, FormulaDiffResult, Grid,
    Sheet, SheetKind, Workbook, WorkbookPackage,
};

type SheetSpec<'a> = (&'a str, Vec<(u32, u32, f64)>);

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn make_workbook(sheets: Vec<SheetSpec<'_>>) -> Workbook {
    let sheet_ir: Vec<Sheet> = sheets
        .into_iter()
        .map(|(name, cells)| {
            let max_row = cells.iter().map(|(r, _, _)| *r).max().unwrap_or(0);
            let max_col = cells.iter().map(|(_, c, _)| *c).max().unwrap_or(0);
            let mut grid = Grid::new(max_row + 1, max_col + 1);
            for (r, c, val) in cells {
                grid.insert_cell(r, c, Some(CellValue::Number(val)), None);
            }
            Sheet {
                name: sid(name),
                workbook_sheet_id: None,
                kind: SheetKind::Worksheet,
                grid,
            }
        })
        .collect();
    Workbook {
        sheets: sheet_ir,
        ..Default::default()
    }
}

fn make_sheet_with_kind(name: &str, kind: SheetKind, cells: Vec<(u32, u32, f64)>) -> Sheet {
    make_sheet_with_kind_and_id(name, kind, None, cells)
}

fn make_sheet_with_kind_and_id(
    name: &str,
    kind: SheetKind,
    workbook_sheet_id: Option<u32>,
    cells: Vec<(u32, u32, f64)>,
) -> Sheet {
    let (nrows, ncols) = if cells.is_empty() {
        (0, 0)
    } else {
        let max_row = cells.iter().map(|(r, _, _)| *r).max().unwrap_or(0);
        let max_col = cells.iter().map(|(_, c, _)| *c).max().unwrap_or(0);
        (max_row + 1, max_col + 1)
    };

    let mut grid = Grid::new(nrows, ncols);
    for (r, c, val) in cells {
        grid.insert_cell(r, c, Some(CellValue::Number(val)), None);
    }

    Sheet {
        name: sid(name),
        workbook_sheet_id,
        kind,
        grid,
    }
}

#[test]
fn identical_workbooks_produce_empty_report() {
    let wb = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let report = diff_workbooks(&wb, &wb, &DiffConfig::default());
    assert!(report.ops.is_empty());
}

#[test]
fn sheet_added_detected() {
    let old = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let new = make_workbook(vec![
        ("Sheet1", vec![(0, 0, 1.0)]),
        ("Sheet2", vec![(0, 0, 2.0)]),
    ]);
    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::SheetAdded { sheet } if *sheet == sid("Sheet2")))
    );
}

#[test]
fn sheet_removed_detected() {
    let old = make_workbook(vec![
        ("Sheet1", vec![(0, 0, 1.0)]),
        ("Sheet2", vec![(0, 0, 2.0)]),
    ]);
    let new = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::SheetRemoved { sheet } if *sheet == sid("Sheet2")))
    );
}

#[test]
fn cell_edited_detected() {
    let old = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let new = make_workbook(vec![("Sheet1", vec![(0, 0, 2.0)])]);
    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);
    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            assert_eq!(*sheet, sid("Sheet1"));
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(from.value, Some(CellValue::Number(1.0)));
            assert_eq!(to.value, Some(CellValue::Number(2.0)));
        }
        _ => panic!("expected CellEdited"),
    }
}

#[test]
fn diff_report_json_round_trips() {
    let old = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let new = make_workbook(vec![("Sheet1", vec![(0, 0, 2.0)])]);
    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    let json = serde_json::to_string(&report).expect("serialize");
    let parsed: DiffReport = serde_json::from_str(&json).expect("deserialize");
    assert_eq!(report, parsed);
}

#[test]
fn sheet_name_case_insensitive_no_changes() {
    let old = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let new = make_workbook(vec![("sheet1", vec![(0, 0, 1.0)])]);

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert!(report.ops.is_empty());
}

#[test]
fn sheet_name_case_insensitive_cell_edit() {
    let old = make_workbook(vec![("Sheet1", vec![(0, 0, 1.0)])]);
    let new = make_workbook(vec![("sheet1", vec![(0, 0, 2.0)])]);

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            assert_eq!(*sheet, sid("Sheet1"));
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(from.value, Some(CellValue::Number(1.0)));
            assert_eq!(to.value, Some(CellValue::Number(2.0)));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn sheet_identity_includes_kind() {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);

    let worksheet = Sheet {
        name: sid("Sheet1"),
        workbook_sheet_id: None,
        kind: SheetKind::Worksheet,
        grid: grid.clone(),
    };

    let chart = Sheet {
        name: sid("Sheet1"),
        workbook_sheet_id: None,
        kind: SheetKind::Chart,
        grid,
    };

    let old = Workbook {
        sheets: vec![worksheet],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![chart],
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());

    let mut added = 0;
    let mut removed = 0;
    for op in &report.ops {
        match op {
            DiffOp::SheetAdded { sheet } if *sheet == sid("Sheet1") => added += 1,
            DiffOp::SheetRemoved { sheet } if *sheet == sid("Sheet1") => removed += 1,
            _ => {}
        }
    }

    assert_eq!(added, 1, "expected one SheetAdded for Chart 'Sheet1'");
    assert_eq!(
        removed, 1,
        "expected one SheetRemoved for Worksheet 'Sheet1'"
    );
    assert_eq!(report.ops.len(), 2, "no other ops expected");
}

#[test]
fn deterministic_sheet_op_ordering() {
    let budget_old = make_sheet_with_kind("Budget", SheetKind::Worksheet, vec![(0, 0, 1.0)]);
    let budget_new = make_sheet_with_kind("Budget", SheetKind::Worksheet, vec![(0, 0, 2.0)]);
    let sheet1_old = make_sheet_with_kind("Sheet1", SheetKind::Worksheet, vec![(0, 1, 5.0)]);
    let sheet1_chart = make_sheet_with_kind("sheet1", SheetKind::Chart, Vec::new());
    let summary_new = make_sheet_with_kind("Summary", SheetKind::Worksheet, vec![(0, 0, 3.0)]);

    let old = Workbook {
        sheets: vec![budget_old.clone(), sheet1_old],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![budget_new.clone(), sheet1_chart, summary_new],
        ..Default::default()
    };

    let budget_addr = CellAddress::from_indices(0, 0);
    let expected = vec![
        DiffOp::cell_edited(
            sid("Budget"),
            budget_addr,
            CellSnapshot {
                addr: budget_addr,
                value: Some(CellValue::Number(1.0)),
                formula: None,
            },
            CellSnapshot {
                addr: budget_addr,
                value: Some(CellValue::Number(2.0)),
                formula: None,
            },
            FormulaDiffResult::Unchanged,
        ),
        DiffOp::SheetRemoved {
            sheet: sid("Sheet1"),
        },
        DiffOp::SheetAdded {
            sheet: sid("sheet1"),
        },
        DiffOp::SheetAdded {
            sheet: sid("Summary"),
        },
    ];

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(
        report.ops, expected,
        "ops should be ordered by lowercase name then sheet kind"
    );
}

#[test]
fn sheet_rename_with_id_emits_rename_and_uses_new_name_for_grid_ops() {
    let old_sheet = make_sheet_with_kind_and_id(
        "OldName",
        SheetKind::Worksheet,
        Some(7),
        vec![(0, 0, 1.0)],
    );
    let new_sheet = make_sheet_with_kind_and_id(
        "NewName",
        SheetKind::Worksheet,
        Some(7),
        vec![(0, 0, 2.0)],
    );
    let old = Workbook {
        sheets: vec![old_sheet],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![new_sheet],
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 2, "expected rename plus one cell edit");

    let mut saw_rename = false;
    let mut saw_edit = false;
    for op in &report.ops {
        match op {
            DiffOp::SheetRenamed { sheet, from, to } => {
                assert_eq!(sheet, &sid("NewName"));
                assert_eq!(from, &sid("OldName"));
                assert_eq!(to, &sid("NewName"));
                saw_rename = true;
            }
            DiffOp::CellEdited { sheet, .. } => {
                assert_eq!(sheet, &sid("NewName"));
                saw_edit = true;
            }
            other => panic!("unexpected op: {other:?}"),
        }
    }
    assert!(saw_rename, "expected SheetRenamed op");
    assert!(saw_edit, "expected CellEdited op");
}

#[test]
fn sheet_name_swap_prefers_id_matching() {
    let old_a = make_sheet_with_kind_and_id(
        "Alpha",
        SheetKind::Worksheet,
        Some(1),
        vec![(0, 0, 1.0)],
    );
    let old_b = make_sheet_with_kind_and_id(
        "Beta",
        SheetKind::Worksheet,
        Some(2),
        vec![(0, 0, 10.0)],
    );
    let new_a = make_sheet_with_kind_and_id(
        "Beta",
        SheetKind::Worksheet,
        Some(1),
        vec![(0, 0, 2.0)],
    );
    let new_b = make_sheet_with_kind_and_id(
        "Alpha",
        SheetKind::Worksheet,
        Some(2),
        vec![(0, 0, 11.0)],
    );

    let old = Workbook {
        sheets: vec![old_a, old_b],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![new_a, new_b],
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 4, "expected two renames and two edits");

    let mut renames = Vec::new();
    let mut edits = Vec::new();
    for op in &report.ops {
        match op {
            DiffOp::SheetRenamed { from, to, .. } => renames.push((*from, *to)),
            DiffOp::CellEdited { sheet, .. } => edits.push(*sheet),
            other => panic!("unexpected op: {other:?}"),
        }
    }

    assert_eq!(renames.len(), 2);
    assert!(renames.contains(&(sid("Alpha"), sid("Beta"))));
    assert!(renames.contains(&(sid("Beta"), sid("Alpha"))));
    assert!(edits.contains(&sid("Alpha")));
    assert!(edits.contains(&sid("Beta")));
}

#[test]
fn sheet_identity_includes_kind_for_macro_and_other() {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);

    let macro_sheet = Sheet {
        name: sid("Code"),
        workbook_sheet_id: None,
        kind: SheetKind::Macro,
        grid: grid.clone(),
    };

    let other_sheet = Sheet {
        name: sid("Code"),
        workbook_sheet_id: None,
        kind: SheetKind::Other,
        grid,
    };

    let old = Workbook {
        sheets: vec![macro_sheet],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![other_sheet],
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());

    let mut added = 0;
    let mut removed = 0;
    for op in &report.ops {
        match op {
            DiffOp::SheetAdded { sheet } if *sheet == sid("Code") => added += 1,
            DiffOp::SheetRemoved { sheet } if *sheet == sid("Code") => removed += 1,
            _ => {}
        }
    }

    assert_eq!(added, 1, "expected one SheetAdded for Other 'Code'");
    assert_eq!(removed, 1, "expected one SheetRemoved for Macro 'Code'");
    assert_eq!(report.ops.len(), 2, "no other ops expected");
}

#[test]
fn sheet_id_matching_respects_kind() {
    let old_sheet = make_sheet_with_kind_and_id(
        "Sheet1",
        SheetKind::Worksheet,
        Some(42),
        vec![(0, 0, 1.0)],
    );
    let new_sheet = make_sheet_with_kind_and_id(
        "Sheet1",
        SheetKind::Chart,
        Some(42),
        Vec::new(),
    );

    let old = Workbook {
        sheets: vec![old_sheet],
        ..Default::default()
    };
    let new = Workbook {
        sheets: vec![new_sheet],
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 2, "expected add/remove due to kind mismatch");
    assert!(report.ops.iter().any(|op| matches!(op, DiffOp::SheetRemoved { sheet } if *sheet == sid("Sheet1"))));
    assert!(report.ops.iter().any(|op| matches!(op, DiffOp::SheetAdded { sheet } if *sheet == sid("Sheet1"))));
    assert!(!report.ops.iter().any(|op| matches!(op, DiffOp::SheetRenamed { .. })));
}

#[cfg(not(debug_assertions))]
#[test]
fn duplicate_sheet_identity_last_writer_wins_release() {
    let duplicate_a = make_sheet_with_kind("Sheet1", SheetKind::Worksheet, vec![(0, 0, 1.0)]);
    let duplicate_b = make_sheet_with_kind("sheet1", SheetKind::Worksheet, vec![(0, 1, 2.0)]);

    let old = Workbook {
        sheets: vec![duplicate_a, duplicate_b],
        ..Default::default()
    };
    let new = Workbook {
        sheets: Vec::new(),
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1, "expected last writer to win");

    match &report.ops[0] {
        DiffOp::SheetRemoved { sheet } => assert_eq!(
            *sheet,
            sid("sheet1"),
            "duplicate identity should prefer the last sheet in release builds"
        ),
        other => panic!("expected SheetRemoved, got {other:?}"),
    }
}

#[test]
fn move_detection_respects_column_gate() {
    let nrows: u32 = 4;
    let ncols: u32 = 300;
    let src_rows = 1..3;
    let src_cols = 2..7;
    let dst_start_col: u32 = 200;
    let dst_end_col = dst_start_col + (src_cols.end - src_cols.start);

    let mut grid_a = Grid::new(nrows, ncols);
    let mut grid_b = Grid::new(nrows, ncols);

    for r in 0..nrows {
        for c in 0..ncols {
            let base_value = Some(CellValue::Number((r * 1_000 + c) as f64));

            grid_a.insert_cell(r, c, base_value.clone(), None);

            let in_src = src_rows.contains(&r) && src_cols.contains(&c);
            let in_dst = src_rows.contains(&r) && c >= dst_start_col && c < dst_end_col;

            if in_dst {
                let offset = c - dst_start_col;
                let src_c = src_cols.start + offset;
                let moved_value = Some(CellValue::Number((r * 1_000 + src_c) as f64));
                grid_b.insert_cell(r, c, moved_value, None);
            } else if !in_src {
                grid_b.insert_cell(r, c, base_value, None);
            }
        }
    }

    let wb_a = Workbook {
        sheets: vec![Sheet {
            name: sid("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_a,
        }],
        ..Default::default()
    };
    let wb_b = Workbook {
        sheets: vec![Sheet {
            name: sid("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_b,
        }],
        ..Default::default()
    };

    let default_report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());
    assert!(
        !default_report.ops.is_empty(),
        "changes should be detected even when move detection is gated off"
    );
    assert!(
        !default_report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRect { .. })),
        "default gate should skip block move detection on wide sheets"
    );

    let mut wide_gate = DiffConfig::default();
    wide_gate.moves.max_move_detection_cols = 512;
    let wide_report = diff_workbooks(&wb_a, &wb_b, &wide_gate);
    assert!(
        !wide_report.ops.is_empty(),
        "expected diffs when move detection is enabled"
    );
    assert!(
        wide_report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRect { .. })),
        "wider gate should allow block move detection on wide sheets"
    );
}

#[test]
fn duplicate_sheet_identity_emits_warning() {
    let duplicate_a = make_sheet_with_kind("Sheet1", SheetKind::Worksheet, vec![(0, 0, 1.0)]);
    let duplicate_b = make_sheet_with_kind("sheet1", SheetKind::Worksheet, vec![(0, 1, 2.0)]);
    let old = Workbook {
        sheets: vec![duplicate_a, duplicate_b],
        ..Default::default()
    };
    let new = Workbook {
        sheets: Vec::new(),
        ..Default::default()
    };

    let result = std::panic::catch_unwind(|| diff_workbooks(&old, &new, &DiffConfig::default()));
    assert!(result.is_ok(), "duplicate sheet identity should not panic");
    let report = result.unwrap();
    assert!(
        report.warnings.iter().any(|w| w.contains("duplicate sheet identity")),
        "should emit warning about duplicate sheet identity; warnings: {:?}",
        report.warnings
    );
}

#[test]
fn duplicate_workbook_sheet_id_falls_back_to_name_matching() {
    let first = make_sheet_with_kind_and_id(
        "First",
        SheetKind::Worksheet,
        Some(1),
        vec![(0, 0, 1.0)],
    );
    let second = make_sheet_with_kind_and_id(
        "Second",
        SheetKind::Worksheet,
        Some(1),
        vec![(0, 0, 2.0)],
    );
    let old = Workbook {
        sheets: vec![first, second],
        ..Default::default()
    };
    let new = Workbook {
        sheets: Vec::new(),
        ..Default::default()
    };

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert!(
        report.warnings.iter().any(|w| w.contains("duplicate workbook sheetId in old workbook: id=1")),
        "expected duplicate workbook sheetId warning, warnings: {:?}",
        report.warnings
    );

    let removed: Vec<_> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::SheetRemoved { sheet } => Some(*sheet),
            _ => None,
        })
        .collect();
    assert_eq!(removed.len(), 2, "expected both sheets removed via fallback");
    assert!(removed.contains(&sid("First")));
    assert!(removed.contains(&sid("Second")));
}

```

---

### File: `core\tests\excel_open_xml_tests.rs`

```rust
mod common;

use common::{fixture_path, open_fixture_workbook, sid};
use excel_diff::{
    CellAddress, ContainerError, ContainerLimits, DataMashupError, OpcContainer, PackageError,
    SheetKind, WorkbookPackage, open_data_mashup,
};
use std::fs;
use std::io::{Cursor, ErrorKind, Write};
use std::path::Path;
use std::time::SystemTime;
use zip::write::FileOptions;
use zip::{CompressionMethod, ZipWriter};

fn temp_xlsx_path(prefix: &str) -> std::path::PathBuf {
    let mut path = std::env::temp_dir();
    let nanos = SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .expect("system time should be after unix epoch")
        .as_nanos();
    path.push(format!("excel_diff_{prefix}_{nanos}.xlsx"));
    path
}

fn write_zip(entries: &[(&str, &str)], path: &Path) {
    let file = fs::File::create(path).expect("create temp zip");
    let mut writer = ZipWriter::new(file);
    let options = FileOptions::default().compression_method(CompressionMethod::Stored);

    for (name, contents) in entries {
        writer.start_file(*name, options).expect("start zip entry");
        writer
            .write_all(contents.as_bytes())
            .expect("write zip entry");
    }

    writer.finish().expect("finish zip");
}

#[test]
fn open_minimal_workbook_succeeds() {
    let workbook = open_fixture_workbook("minimal.xlsx");
    assert_eq!(workbook.sheets.len(), 1);

    let sheet = &workbook.sheets[0];
    assert_eq!(sheet.name, sid("Sheet1"));
    assert!(matches!(sheet.kind, SheetKind::Worksheet));
    assert_eq!(sheet.grid.nrows, 1);
    assert_eq!(sheet.grid.ncols, 1);

    let cell = sheet.grid.get(0, 0).expect("A1 should be present");
    assert_eq!(CellAddress::from_coords(0, 0).to_a1(), "A1");
    assert!(cell.value.is_some());
}

#[test]
fn open_nonexistent_file_returns_io_error() {
    let path = fixture_path("definitely_missing.xlsx");
    let file = std::fs::File::open(&path);
    assert!(file.is_err(), "missing file should error");
    let io_err = file.unwrap_err();
    assert_eq!(io_err.kind(), ErrorKind::NotFound);
}

#[test]
fn random_zip_is_not_excel() {
    let path = fixture_path("random_zip.zip");
    let file = std::fs::File::open(&path).expect("random zip file exists");
    let err = WorkbookPackage::open(file).expect_err("random zip should not parse");
    assert!(matches!(
        err,
        PackageError::Container(ContainerError::NotOpcPackage)
    ));
}

#[test]
fn no_content_types_is_not_excel() {
    let path = fixture_path("no_content_types.xlsx");
    let file = std::fs::File::open(&path).expect("no content types file exists");
    let err = WorkbookPackage::open(file).expect_err("missing content types should fail");
    assert!(matches!(
        err,
        PackageError::Container(ContainerError::NotOpcPackage)
    ));
}

#[test]
fn not_zip_container_returns_error() {
    let path = std::env::temp_dir().join("excel_diff_not_zip.txt");
    fs::write(&path, "this is not a zip container").expect("write temp file");
    let file = std::fs::File::open(&path).expect("not zip file exists");
    let err = WorkbookPackage::open(file).expect_err("non-zip should fail");
    assert!(matches!(
        err,
        PackageError::Container(ContainerError::NotZipContainer)
    ));
    let _ = fs::remove_file(&path);
}

#[test]
fn missing_workbook_xml_returns_missing_part() {
    let path = temp_xlsx_path("missing_workbook_xml");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="rels" ContentType="application/vnd.openxmlformats-package.relationships+xml"/>
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    write_zip(&[("[Content_Types].xml", content_types)], &path);

    let file = std::fs::File::open(&path).expect("temp file exists");
    let err = WorkbookPackage::open(file).expect_err("missing workbook xml should error");
    match err {
        PackageError::MissingPart { path } => {
            assert_eq!(path, "xl/workbook.xml");
        }
        other => panic!("expected MissingPart for xl/workbook.xml, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn missing_worksheet_xml_returns_missing_part() {
    let path = temp_xlsx_path("missing_worksheet_xml");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="rels" ContentType="application/vnd.openxmlformats-package.relationships+xml"/>
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let workbook_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<workbook xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main"
          xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships">
  <sheets>
    <sheet name="Sheet1" sheetId="1" r:id="rId1"/>
  </sheets>
</workbook>"#;

    let relationships = r#"<?xml version="1.0" encoding="UTF-8"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
  <Relationship Id="rId1"
                Type="http://schemas.openxmlformats.org/officeDocument/2006/relationships/worksheet"
                Target="worksheets/sheet1.xml"/>
</Relationships>"#;

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("xl/workbook.xml", workbook_xml),
            ("xl/_rels/workbook.xml.rels", relationships),
        ],
        &path,
    );

    let file = std::fs::File::open(&path).expect("temp file exists");
    let err = WorkbookPackage::open(file).expect_err("missing worksheet xml should error");
    match err {
        PackageError::MissingPart { path: part_path } => {
            assert!(
                part_path.contains("sheet1.xml"),
                "expected path to contain sheet1.xml, got {part_path}"
            );
        }
        other => panic!("expected MissingPart for worksheet, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn truncated_zip_never_panics() {
    let valid_zip_bytes = {
        let mut buf = Vec::new();
        {
            let cursor = Cursor::new(&mut buf);
            let mut writer = ZipWriter::new(cursor);
            let options = FileOptions::default().compression_method(CompressionMethod::Stored);
            writer.start_file("[Content_Types].xml", options).unwrap();
            writer.write_all(b"<Types/>").unwrap();
            writer.finish().unwrap();
        }
        buf
    };

    for truncate_at in [0, 4, 10, 20, valid_zip_bytes.len() / 2] {
        let truncated = &valid_zip_bytes[..truncate_at.min(valid_zip_bytes.len())];
        let cursor = Cursor::new(truncated.to_vec());
        let result = std::panic::catch_unwind(|| OpcContainer::open_from_reader(cursor));
        assert!(result.is_ok(), "truncated ZIP at byte {} should not panic", truncate_at);
        let inner = result.unwrap();
        assert!(inner.is_err(), "truncated ZIP should return error, not Ok");
    }
}

#[test]
fn zip_bomb_defense_rejects_oversized_part() {
    let path = temp_xlsx_path("oversized_part");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let large_content = "x".repeat(1024);

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("large_file.xml", &large_content),
        ],
        &path,
    );

    let file = std::fs::File::open(&path).expect("temp file exists");
    let limits = ContainerLimits {
        max_entries: 100,
        max_part_uncompressed_bytes: 512,
        max_total_uncompressed_bytes: 10_000,
    };
    let mut container = OpcContainer::open_from_reader_with_limits(file, limits)
        .expect("container should open (content_types is small)");

    let err = container.read_file_checked("large_file.xml")
        .expect_err("oversized part should be rejected");
    assert!(
        matches!(err, ContainerError::PartTooLarge { .. }),
        "expected PartTooLarge error, got {err:?}"
    );

    let _ = fs::remove_file(&path);
}

#[test]
fn oversized_content_types_is_rejected_during_open() {
    let path = temp_xlsx_path("oversized_content_types");
    let huge_content_types = "x".repeat(1024);

    write_zip(&[("[Content_Types].xml", &huge_content_types)], &path);

    let file = std::fs::File::open(&path).expect("temp file exists");
    let limits = ContainerLimits {
        max_entries: 100,
        max_part_uncompressed_bytes: 100,
        max_total_uncompressed_bytes: 10_000,
    };

    let err = match OpcContainer::open_from_reader_with_limits(file, limits) {
        Ok(_) => panic!("expected oversized [Content_Types].xml to be rejected"),
        Err(e) => e,
    };

    match err {
        ContainerError::PartTooLarge { path, size, limit } => {
            assert_eq!(path, "[Content_Types].xml");
            assert_eq!(size, 1024);
            assert_eq!(limit, 100);
        }
        other => panic!("expected PartTooLarge, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn invalid_xml_in_workbook_does_not_panic() {
    let path = temp_xlsx_path("invalid_xml");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let malformed_workbook = "<workbook><sheets><sheet name='x' unclosed";

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("xl/workbook.xml", malformed_workbook),
        ],
        &path,
    );

    let file = std::fs::File::open(&path).expect("temp file exists");
    let result = std::panic::catch_unwind(|| WorkbookPackage::open(file));
    assert!(result.is_ok(), "malformed XML should not panic (catch_unwind succeeded)");
    let err = result
        .unwrap()
        .expect_err("malformed XML should return error, not Ok");
    match err {
        PackageError::InvalidXml {
            part,
            line,
            column,
            ..
        } => {
            assert_eq!(part, "xl/workbook.xml");
            assert!(line > 0, "expected line > 0, got {line}");
            assert!(column > 0, "expected column > 0, got {column}");
        }
        other => panic!("expected InvalidXml, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn invalid_xml_in_relationships_includes_part_and_position() {
    let path = temp_xlsx_path("invalid_rels_xml");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="rels" ContentType="application/vnd.openxmlformats-package.relationships+xml"/>
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let workbook_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<workbook xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main"
          xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships">
  <sheets>
    <sheet name="Sheet1" sheetId="1" r:id="rId1"/>
  </sheets>
</workbook>"#;

    let malformed_rels = r#"<?xml version="1.0" encoding="UTF-8"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
  <Relationship Id="rId1" Target="worksheets/sheet1.xml""#;

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("xl/workbook.xml", workbook_xml),
            ("xl/_rels/workbook.xml.rels", malformed_rels),
        ],
        &path,
    );

    let file = std::fs::File::open(&path).expect("temp file exists");
    let result = std::panic::catch_unwind(|| WorkbookPackage::open(file));
    assert!(result.is_ok(), "malformed relationships XML should not panic");
    let err = result.unwrap().expect_err("malformed relationships XML should error");
    match err {
        PackageError::InvalidXml {
            part,
            line,
            column,
            ..
        } => {
            assert_eq!(part, "xl/_rels/workbook.xml.rels");
            assert!(line > 0, "expected line > 0, got {line}");
            assert!(column > 0, "expected column > 0, got {column}");
        }
        other => panic!("expected InvalidXml, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn invalid_xml_in_worksheet_includes_part_and_position() {
    let path = temp_xlsx_path("invalid_sheet_xml");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="rels" ContentType="application/vnd.openxmlformats-package.relationships+xml"/>
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let workbook_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<workbook xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main"
          xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships">
  <sheets>
    <sheet name="Sheet1" sheetId="1" r:id="rId1"/>
  </sheets>
</workbook>"#;

    let relationships = r#"<?xml version="1.0" encoding="UTF-8"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
  <Relationship Id="rId1"
                Type="http://schemas.openxmlformats.org/officeDocument/2006/relationships/worksheet"
                Target="worksheets/sheet1.xml"/>
</Relationships>"#;

    let malformed_sheet = r#"<?xml version="1.0" encoding="UTF-8"?>
<worksheet xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main">
  <sheetData>
    <row r="1">
      <c r="A1"><v>1</v>
    </row>
  </sheetData>
</worksheet>"#;

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("xl/workbook.xml", workbook_xml),
            ("xl/_rels/workbook.xml.rels", relationships),
            ("xl/worksheets/sheet1.xml", malformed_sheet),
        ],
        &path,
    );

    let file = std::fs::File::open(&path).expect("temp file exists");
    let result = std::panic::catch_unwind(|| WorkbookPackage::open(file));
    assert!(result.is_ok(), "malformed worksheet XML should not panic");
    let err = result.unwrap().expect_err("malformed worksheet XML should error");
    match err {
        PackageError::InvalidXml {
            part,
            line,
            column,
            ..
        } => {
            assert_eq!(part, "xl/worksheets/sheet1.xml");
            assert!(line > 0, "expected line > 0, got {line}");
            assert!(column > 0, "expected column > 0, got {column}");
        }
        other => panic!("expected InvalidXml, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn datamashup_base64_decodes_but_framing_invalid_includes_part() {
    let path = temp_xlsx_path("dm_framing_invalid");
    let content_types = r#"<?xml version="1.0" encoding="UTF-8"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="xml" ContentType="application/xml"/>
</Types>"#;

    let dm_xml = r#"<?xml version="1.0" encoding="UTF-8"?>
<root xmlns:dm="http://schemas.microsoft.com/DataMashup">
  <dm:DataMashup>AAAA</dm:DataMashup>
</root>"#;

    write_zip(
        &[
            ("[Content_Types].xml", content_types),
            ("customXml/item1.xml", dm_xml),
        ],
        &path,
    );

    let err = open_data_mashup(&path).expect_err("expected framing error");
    let err = match err {
        PackageError::WithPath { source, .. } => *source,
        other => other,
    };

    match err {
        PackageError::DataMashupPartError { part, source } => {
            assert_eq!(part, "customXml/item1.xml");
            assert!(matches!(source, DataMashupError::FramingInvalid));
        }
        other => panic!("expected DataMashupPartError, got {other:?}"),
    }

    let _ = fs::remove_file(&path);
}

#[test]
fn corrupt_inputs_never_panic() {
    let test_cases: Vec<(&str, Vec<u8>)> = vec![
        ("empty", vec![]),
        ("garbage", vec![0xFF, 0xFE, 0x00, 0x01, 0x02, 0x03]),
        ("partial_zip_header", vec![0x50, 0x4B, 0x03, 0x04]),
        ("random_bytes", (0..100).map(|i| (i * 17 + 31) as u8).collect()),
    ];

    for (name, bytes) in test_cases {
        let cursor = Cursor::new(bytes);
        let result = std::panic::catch_unwind(|| WorkbookPackage::open(cursor));
        assert!(
            result.is_ok(),
            "corrupt input '{}' should not panic",
            name
        );
    }
}

```

---

### File: `core\tests\f7_formula_canonicalization_tests.rs`

```rust
use excel_diff::parse_formula;

#[test]
fn canonicalizes_commutative_binary_ops() {
    let a = parse_formula("A1+B1").unwrap().canonicalize();
    let b = parse_formula("B1+A1").unwrap().canonicalize();
    assert_eq!(a, b);

    let a = parse_formula("A1*B1").unwrap().canonicalize();
    let b = parse_formula("B1*A1").unwrap().canonicalize();
    assert_eq!(a, b);
}

#[test]
fn canonicalizes_commutative_functions_by_sorting_args() {
    let a = parse_formula("SUM(A1,B1)").unwrap().canonicalize();
    let b = parse_formula("SUM(B1,A1)").unwrap().canonicalize();
    assert_eq!(a, b);

    let a = parse_formula("AND(TRUE,FALSE)").unwrap().canonicalize();
    let b = parse_formula("AND(FALSE,TRUE)").unwrap().canonicalize();
    assert_eq!(a, b);
}

#[test]
fn does_not_canonicalize_non_commutative_ops() {
    let a = parse_formula("A1-B1").unwrap().canonicalize();
    let b = parse_formula("B1-A1").unwrap().canonicalize();
    assert_ne!(a, b);
}

#[test]
fn canonicalizes_range_endpoints() {
    let a = parse_formula("B2:A1").unwrap().canonicalize();
    let b = parse_formula("A1:B2").unwrap().canonicalize();
    assert_eq!(a, b);
}

#[test]
fn structured_refs_parse_and_canonicalize() {
    let a = parse_formula("Table1[Column1]").unwrap().canonicalize();
    let b = parse_formula("TABLE1[COLUMN1]").unwrap().canonicalize();
    assert_eq!(a, b);
}

```

---

### File: `core\tests\f7_formula_diff_integration_tests.rs`

```rust
use excel_diff::{
    CellValue, DiffConfig, DiffOp, FormulaDiffResult, Grid, Sheet, SheetKind, StringPool, Workbook,
    diff_grids_database_mode, diff_workbooks_with_pool,
};

fn workbook_with_formula(
    pool: &mut StringPool,
    sheet: excel_diff::StringId,
    row: u32,
    col: u32,
    formula: &str,
) -> Workbook {
    let mut grid = Grid::new(row + 1, col + 1);
    let formula_id = pool.intern(formula);
    grid.insert_cell(row, col, None, Some(formula_id));

    Workbook {
        sheets: vec![Sheet {
            name: sheet,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn cell_edit_op(report: &excel_diff::DiffReport) -> DiffOp {
    report
        .ops
        .iter()
        .find(|op| matches!(op, DiffOp::CellEdited { .. }))
        .cloned()
        .expect("expected a cell edit in the diff")
}

#[test]
fn formatting_only_vs_text_change_respects_flag() {
    let mut pool = StringPool::new();
    let sheet = pool.intern("Sheet1");
    let old = workbook_with_formula(&mut pool, sheet, 0, 0, "sum(a1,b1)");
    let new = workbook_with_formula(&mut pool, sheet, 0, 0, "SUM(A1,B1)");

    let mut enabled = DiffConfig::default();
    enabled.semantic.enable_formula_semantic_diff = true;
    let disabled = DiffConfig::default();

    let report_enabled = diff_workbooks_with_pool(&old, &new, &mut pool, &enabled);
    let report_disabled = diff_workbooks_with_pool(&old, &new, &mut pool, &disabled);

    match cell_edit_op(&report_enabled) {
        DiffOp::CellEdited { formula_diff, .. } => {
            assert_eq!(formula_diff, FormulaDiffResult::FormattingOnly);
        }
        _ => panic!("expected CellEdited op in enabled diff"),
    }

    match cell_edit_op(&report_disabled) {
        DiffOp::CellEdited { formula_diff, .. } => {
            assert_eq!(formula_diff, FormulaDiffResult::TextChange);
        }
        _ => panic!("expected CellEdited op in disabled diff"),
    }
}

#[test]
fn filled_down_formulas_detect_row_shift() {
    let mut pool = StringPool::new();

    let mut config = DiffConfig::default();
    config.semantic.enable_formula_semantic_diff = true;

    let mut old = Grid::new(1, 2);
    old.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
    old.insert_cell(0, 1, None, Some(pool.intern("A1+B1")));

    let mut new = Grid::new(2, 2);
    new.insert_cell(0, 0, Some(CellValue::Number(0.0)), None);
    new.insert_cell(1, 0, Some(CellValue::Number(1.0)), None);
    new.insert_cell(1, 1, None, Some(pool.intern("A2+B2")));

    let report = diff_grids_database_mode(&old, &new, &[0], &mut pool, &config);

    let cell_edit = cell_edit_op(&report);
    match cell_edit {
        DiffOp::CellEdited {
            addr, formula_diff, ..
        } => {
            assert_eq!(addr.row, 1);
            assert_eq!(addr.col, 1);
            assert_eq!(formula_diff, FormulaDiffResult::Filled);
        }
        _ => panic!("expected CellEdited op"),
    }

    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { row_idx, .. } if *row_idx == 0)),
        "expected a row insertion ahead of the filled-down formula",
    );
}

```

---

### File: `core\tests\f7_formula_parser_tests.rs`

```rust
use excel_diff::{
    BinaryOperator, CellReference, ColRef, ExcelError, FormulaExpr, RangeReference, RowRef,
    UnaryOperator, parse_formula,
};

fn cell(sheet: Option<&str>, row: RowRef, col: ColRef) -> FormulaExpr {
    FormulaExpr::CellRef(CellReference {
        sheet: sheet.map(|s| s.to_string()),
        row,
        col,
        spill: false,
    })
}

#[test]
fn parses_expected_ast_shapes() {
    let cases = vec![
        ("1", FormulaExpr::Number(1.0)),
        ("\"x\"", FormulaExpr::Text("x".to_string())),
        ("TRUE", FormulaExpr::Boolean(true)),
        ("#DIV/0!", FormulaExpr::Error(ExcelError::Div0)),
        ("A1", cell(None, RowRef::Relative(1), ColRef::Relative(1))),
        ("$B$2", cell(None, RowRef::Absolute(2), ColRef::Absolute(2))),
        (
            "R[1]C[-1]",
            cell(None, RowRef::Offset(1), ColRef::Offset(-1)),
        ),
        (
            "SUM(A1,B1)",
            FormulaExpr::FunctionCall {
                name: "SUM".into(),
                args: vec![
                    cell(None, RowRef::Relative(1), ColRef::Relative(1)),
                    cell(None, RowRef::Relative(1), ColRef::Relative(2)),
                ],
            },
        ),
        (
            "{1,2;3,4}",
            FormulaExpr::Array(vec![
                vec![FormulaExpr::Number(1.0), FormulaExpr::Number(2.0)],
                vec![FormulaExpr::Number(3.0), FormulaExpr::Number(4.0)],
            ]),
        ),
        (
            "A1:B2",
            FormulaExpr::RangeRef(RangeReference {
                sheet: None,
                start: CellReference {
                    sheet: None,
                    row: RowRef::Relative(1),
                    col: ColRef::Relative(1),
                    spill: false,
                },
                end: CellReference {
                    sheet: None,
                    row: RowRef::Relative(2),
                    col: ColRef::Relative(2),
                    spill: false,
                },
            }),
        ),
        (
            "A1^-1",
            FormulaExpr::BinaryOp {
                op: BinaryOperator::Pow,
                left: Box::new(cell(None, RowRef::Relative(1), ColRef::Relative(1))),
                right: Box::new(FormulaExpr::UnaryOp {
                    op: UnaryOperator::Minus,
                    operand: Box::new(FormulaExpr::Number(1.0)),
                }),
            },
        ),
    ];

    for (text, expected) in cases {
        let parsed = parse_formula(text).expect("formula should parse");
        assert_eq!(parsed, expected, "mismatched AST for '{text}'");
    }
}

#[test]
fn parses_varied_syntaxes() {
    let samples = [
        "=sum( A1 , B1 )",
        "'My Sheet'!$A$1",
        "[Book1.xlsx]Sheet1!A1",
        "{1,2;3,4}",
        "Table1[Column1]",
        "R1C1",
        "R[2]C[-3]",
    ];

    for text in samples {
        parse_formula(text).unwrap_or_else(|e| panic!("failed to parse {text}: {e}"));
    }
}

```

---

### File: `core\tests\f7_formula_shift_tests.rs`

```rust
use excel_diff::{formulas_equivalent_modulo_shift, parse_formula};

#[test]
fn filled_down_formulas_match_under_row_shift() {
    let old = parse_formula("A1+B1").expect("old formula parses");
    let new = parse_formula("A2+B2").expect("new formula parses");

    assert!(
        formulas_equivalent_modulo_shift(&old, &new, 1, 0),
        "expected formulas to match after row shift",
    );
    assert!(
        !formulas_equivalent_modulo_shift(&old, &new, 0, 0),
        "without shift they should differ",
    );
}

#[test]
fn mismatched_refs_do_not_match_under_zero_shift() {
    let old = parse_formula("A1+B1").expect("old formula parses");
    let new = parse_formula("A1+B2").expect("new formula parses");

    assert!(
        !formulas_equivalent_modulo_shift(&old, &new, 0, 0),
        "different refs should not be equivalent without a shift",
    );
}

```

---

### File: `core\tests\g10_row_block_alignment_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, sid};
use excel_diff::{DiffConfig, DiffOp};

#[test]
fn g10_row_block_insert_middle_emits_four_rowadded_and_no_noise() {
    let report = diff_fixture_pkgs(
        "row_block_insert_a.xlsx",
        "row_block_insert_b.xlsx",
        &DiffConfig::default(),
    );

    let rows_added: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowAdded {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(*sheet, sid("Sheet1"));
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(
        rows_added,
        vec![3, 4, 5, 6],
        "expected four RowAdded ops for the inserted block"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowRemoved { .. })),
        "no rows should be removed for block insert"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned block insert should not emit CellEdited noise"
    );
}

#[test]
fn g10_row_block_delete_middle_emits_four_rowremoved_and_no_noise() {
    let report = diff_fixture_pkgs(
        "row_block_delete_a.xlsx",
        "row_block_delete_b.xlsx",
        &DiffConfig::default(),
    );

    let rows_removed: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowRemoved {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(*sheet, sid("Sheet1"));
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(
        rows_removed,
        vec![3, 4, 5, 6],
        "expected four RowRemoved ops for the deleted block"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. })),
        "no rows should be added for block delete"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned block delete should not emit CellEdited noise"
    );
}

```

---

### File: `core\tests\g11_row_block_move_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, grid_from_numbers, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn g11_row_block_move_emits_single_blockmovedrows() {
    let report = diff_fixture_pkgs(
        "row_block_move_a.xlsx",
        "row_block_move_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(report.ops.len(), 1, "expected a single diff op");
    let strings = &report.strings;

    match &report.ops[0] {
        DiffOp::BlockMovedRows {
            sheet,
            src_start_row,
            row_count,
            dst_start_row,
            block_hash,
        } => {
            assert_eq!(
                strings.get(sheet.0 as usize).map(String::as_str),
                Some("Sheet1")
            );
            assert_eq!(*src_start_row, 4);
            assert_eq!(*row_count, 4);
            assert_eq!(*dst_start_row, 12);
            assert!(block_hash.is_none());
        }
        other => panic!("expected BlockMovedRows op, got {:?}", other),
    }

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. })),
        "pure move should not emit RowAdded"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowRemoved { .. })),
        "pure move should not emit RowRemoved"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "pure move should not emit CellEdited noise"
    );
}

#[test]
fn g11_repeated_rows_do_not_emit_blockmove() {
    let grid_a = grid_from_numbers(&[&[1, 10], &[1, 10], &[2, 20], &[2, 20]]);

    let grid_b = grid_from_numbers(&[&[2, 20], &[2, 20], &[1, 10], &[1, 10]]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRows { .. })),
        "ambiguous repeated rows must not emit BlockMovedRows"
    );

    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "fallback path should emit positional CellEdited noise"
    );
}

```

---

### File: `core\tests\g12_column_block_move_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, grid_from_numbers, sid, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn g12_column_move_emits_single_blockmovedcolumns() {
    let report = diff_fixture_pkgs(
        "column_move_a.xlsx",
        "column_move_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(report.ops.len(), 1, "expected a single diff op");

    match &report.ops[0] {
        DiffOp::BlockMovedColumns {
            sheet,
            src_start_col,
            col_count,
            dst_start_col,
            block_hash,
        } => {
            assert_eq!(sheet, &sid("Data"));
            assert_eq!(*src_start_col, 2);
            assert_eq!(*col_count, 1);
            assert_eq!(*dst_start_col, 5);
            assert!(block_hash.is_none());
        }
        other => panic!("expected BlockMovedColumns op, got {:?}", other),
    }

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::ColumnAdded { .. })),
        "pure move should not emit ColumnAdded"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::ColumnRemoved { .. })),
        "pure move should not emit ColumnRemoved"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. })),
        "pure move should not emit row operations"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "pure move should not emit CellEdited noise"
    );
}

#[test]
fn g12_repeated_columns_do_not_emit_blockmovedcolumns() {
    let grid_a = grid_from_numbers(&[&[1, 1, 2, 2], &[10, 10, 20, 20]]);
    let grid_b = grid_from_numbers(&[&[2, 2, 1, 1], &[20, 20, 10, 10]]);

    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedColumns { .. })),
        "ambiguous repeated columns must not emit BlockMovedColumns"
    );

    assert!(
        report.ops.iter().any(|op| matches!(
            op,
            DiffOp::CellEdited { .. } | DiffOp::ColumnAdded { .. } | DiffOp::ColumnRemoved { .. }
        )),
        "fallback path should emit some other diff operation"
    );
}

#[test]
fn g12_multi_column_block_move_emits_blockmovedcolumns() {
    let grid_a = grid_from_numbers(&[
        &[10, 20, 30, 40, 50, 60],
        &[11, 21, 31, 41, 51, 61],
        &[12, 22, 32, 42, 52, 62],
    ]);

    let grid_b = grid_from_numbers(&[
        &[10, 40, 50, 20, 30, 60],
        &[11, 41, 51, 21, 31, 61],
        &[12, 42, 52, 22, 32, 62],
    ]);

    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert_eq!(
        report.ops.len(),
        1,
        "expected a single diff op for multi-column move"
    );

    match &report.ops[0] {
        DiffOp::BlockMovedColumns {
            sheet,
            src_start_col,
            col_count,
            dst_start_col,
            block_hash,
        } => {
            assert_eq!(sheet, &sid("Data"));
            assert_eq!(*src_start_col, 3);
            assert_eq!(*col_count, 2, "should detect a 2-column block move");
            assert_eq!(*dst_start_col, 1);
            assert!(block_hash.is_none());
        }
        other => panic!("expected BlockMovedColumns op, got {:?}", other),
    }
}

#[test]
fn g12_two_independent_column_moves_do_not_emit_blockmovedcolumns() {
    let grid_a = grid_from_numbers(&[&[10, 20, 30, 40, 50, 60], &[11, 21, 31, 41, 51, 61]]);

    let grid_b = grid_from_numbers(&[&[20, 10, 30, 40, 60, 50], &[21, 11, 31, 41, 61, 51]]);

    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedColumns { .. })),
        "two independent column swaps must not emit BlockMovedColumns"
    );

    assert!(
        !report.ops.is_empty(),
        "fallback path should emit some diff operations"
    );
}

#[test]
fn g12_column_swap_emits_blockmovedcolumns() {
    let grid_a = grid_from_numbers(&[&[10, 20, 30, 40], &[11, 21, 31, 41]]);

    let grid_b = grid_from_numbers(&[&[20, 10, 30, 40], &[21, 11, 31, 41]]);

    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert_eq!(
        report.ops.len(),
        1,
        "swap should produce single BlockMovedColumns op"
    );

    match &report.ops[0] {
        DiffOp::BlockMovedColumns {
            sheet,
            col_count,
            src_start_col,
            dst_start_col,
            ..
        } => {
            assert_eq!(sheet, &sid("Data"));
            assert_eq!(*col_count, 1, "swap is represented as moving one column");
            assert!(
                (*src_start_col == 0 && *dst_start_col == 1)
                    || (*src_start_col == 1 && *dst_start_col == 0),
                "swap should move column 0 or 1 past the other"
            );
        }
        other => panic!("expected BlockMovedColumns, got {:?}", other),
    }
}

```

---

### File: `core\tests\g12_rect_block_move_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, grid_from_numbers, sid, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn g12_rect_block_move_emits_single_blockmovedrect() {
    let report = diff_fixture_pkgs(
        "rect_block_move_a.xlsx",
        "rect_block_move_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(report.ops.len(), 1, "expected a single diff op");

    match &report.ops[0] {
        DiffOp::BlockMovedRect {
            sheet,
            src_start_row,
            src_row_count,
            src_start_col,
            src_col_count,
            dst_start_row,
            dst_start_col,
            block_hash: _,
        } => {
            assert_eq!(*sheet, sid("Data"));
            assert_eq!(*src_start_row, 2);
            assert_eq!(*src_row_count, 3);
            assert_eq!(*src_start_col, 1);
            assert_eq!(*src_col_count, 3);
            assert_eq!(*dst_start_row, 9);
            assert_eq!(*dst_start_col, 6);
        }
        other => panic!("expected BlockMovedRect op, got {:?}", other),
    }
}

#[test]
fn g12_rect_block_move_ambiguous_swap_does_not_emit_blockmovedrect() {
    let (grid_a, grid_b) = swap_two_blocks();
    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRect { .. })),
        "ambiguous block swap must not emit BlockMovedRect"
    );
    assert!(
        !report.ops.is_empty(),
        "fallback path should emit some diff operations"
    );
}

#[test]
fn g12_rect_block_move_with_internal_edit_falls_back() {
    let (grid_a, grid_b) = move_with_edit();
    let wb_a = single_sheet_workbook("Data", grid_a);
    let wb_b = single_sheet_workbook("Data", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRect { .. })),
        "move with internal edit should not be treated as exact rectangular move"
    );
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "edited block should surface as cell edits or structural diffs"
    );
}

fn swap_two_blocks() -> (excel_diff::Grid, excel_diff::Grid) {
    let base: Vec<Vec<i32>> = (0..6)
        .map(|r| (0..6).map(|c| 100 * r + c).collect())
        .collect();
    let mut grid_a = base.clone();
    let mut grid_b = base.clone();

    let block_one = vec![vec![900, 901], vec![902, 903]];
    let block_two = vec![vec![700, 701], vec![702, 703]];

    place_block(&mut grid_a, 0, 0, &block_one);
    place_block(&mut grid_a, 3, 3, &block_two);

    // Swap the two distinct blocks in grid B.
    place_block(&mut grid_b, 0, 0, &block_two);
    place_block(&mut grid_b, 3, 3, &block_one);

    (grid_from_matrix(grid_a), grid_from_matrix(grid_b))
}

fn move_with_edit() -> (excel_diff::Grid, excel_diff::Grid) {
    let mut grid_a = base_background(10, 10);
    let mut grid_b = base_background(10, 10);

    let block = vec![vec![11, 12, 13], vec![21, 22, 23], vec![31, 32, 33]];

    place_block(&mut grid_a, 1, 1, &block);
    place_block(&mut grid_b, 6, 4, &block);
    grid_b[7][5] = 9_999; // edit inside the moved block

    (grid_from_matrix(grid_a), grid_from_matrix(grid_b))
}

fn base_background(rows: usize, cols: usize) -> Vec<Vec<i32>> {
    (0..rows)
        .map(|r| (0..cols).map(|c| (r as i32) * 1_000 + c as i32).collect())
        .collect()
}

fn place_block(target: &mut [Vec<i32>], top: usize, left: usize, block: &[Vec<i32>]) {
    for (r_offset, row_vals) in block.iter().enumerate() {
        for (c_offset, value) in row_vals.iter().enumerate() {
            let row = top + r_offset;
            let col = left + c_offset;
            if let Some(row_slice) = target.get_mut(row)
                && let Some(cell) = row_slice.get_mut(col)
            {
                *cell = *value;
            }
        }
    }
}

fn grid_from_matrix(matrix: Vec<Vec<i32>>) -> excel_diff::Grid {
    let refs: Vec<&[i32]> = matrix.iter().map(|row| row.as_slice()).collect();
    grid_from_numbers(&refs)
}

```

---

### File: `core\tests\g13_fuzzy_row_move_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, grid_from_numbers, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn g13_fuzzy_row_move_emits_blockmovedrows_and_celledited() {
    let report = diff_fixture_pkgs(
        "grid_move_and_edit_a.xlsx",
        "grid_move_and_edit_b.xlsx",
        &DiffConfig::default(),
    );

    let block_moves: Vec<(u32, u32, u32, Option<u64>)> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::BlockMovedRows {
                src_start_row,
                row_count,
                dst_start_row,
                block_hash,
                ..
            } => Some((*src_start_row, *row_count, *dst_start_row, *block_hash)),
            _ => None,
        })
        .collect();

    assert_eq!(block_moves.len(), 1, "expected a single BlockMovedRows op");
    let (src_start_row, row_count, dst_start_row, block_hash) = block_moves[0];
    assert_eq!(src_start_row, 4);
    assert_eq!(row_count, 4);
    assert_eq!(dst_start_row, 13);
    assert!(block_hash.is_none());

    let edited_rows: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::CellEdited { addr, .. } => Some(addr.row),
            _ => None,
        })
        .collect();
    assert!(
        edited_rows
            .iter()
            .any(|r| *r >= dst_start_row && *r < dst_start_row + row_count),
        "expected a CellEdited inside the moved block"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { row_idx, .. } if *row_idx >= dst_start_row && *row_idx < dst_start_row + row_count)),
        "moved rows must not be reported as added"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowRemoved { row_idx, .. } if *row_idx >= src_start_row && *row_idx < src_start_row + row_count)),
        "moved rows must not be reported as removed"
    );
}

#[test]
fn g13_fuzzy_row_move_can_be_disabled() {
    let base: Vec<Vec<i32>> = (1..=18)
        .map(|r| (1..=3).map(|c| r * 10 + c).collect())
        .collect();
    let base_refs: Vec<&[i32]> = base.iter().map(|row| row.as_slice()).collect();
    let grid_a = grid_from_numbers(&base_refs);

    let mut rows_b = base.clone();
    let mut moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
    moved_block[1][1] = 9_999;
    rows_b.splice(12..12, moved_block);
    let rows_b_refs: Vec<&[i32]> = rows_b.iter().map(|row| row.as_slice()).collect();
    let grid_b = grid_from_numbers(&rows_b_refs);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut disabled = DiffConfig::default();
    disabled.moves.enable_fuzzy_moves = false;
    let report_disabled = diff_workbooks(&wb_a, &wb_b, &disabled);
    let disabled_moves = report_disabled
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRows { .. }))
        .count();
    let disabled_block_edits = report_disabled
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::CellEdited { addr, .. }
                if addr.row >= 12 && addr.row < 16
            )
        })
        .count();

    let report_enabled = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());
    let enabled_moves = report_enabled
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRows { .. }))
        .count();
    let enabled_block_edits = report_enabled
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::CellEdited { addr, .. }
                if addr.row >= 12 && addr.row < 16
            )
        })
        .count();

    assert!(
        enabled_moves >= disabled_moves,
        "enabling fuzzy moves should not reduce move detection"
    );
    assert!(
        enabled_block_edits > disabled_block_edits,
        "fuzzy move detection should emit edits within the moved block"
    );
}

#[test]
fn g13_in_place_edits_do_not_emit_blockmovedrows() {
    let rows: Vec<Vec<i32>> = (1..=12)
        .map(|r| (1..=3).map(|c| r * 10 + c).collect())
        .collect();
    let rows_refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&rows_refs);

    let mut edited_rows = rows.clone();
    if let Some(cell) = edited_rows.get_mut(5).and_then(|row| row.get_mut(1)) {
        *cell += 7;
    }
    let edited_refs: Vec<&[i32]> = edited_rows.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&edited_refs);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRows { .. })),
        "in-place edits must not be classified as BlockMovedRows"
    );
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "edits should still be surfaced as CellEdited"
    );
}

#[test]
fn g13_ambiguous_repeated_blocks_do_not_emit_blockmovedrows() {
    let mut rows_a: Vec<Vec<i32>> = vec![vec![1, 1]; 10];
    rows_a.push(vec![99, 99]);
    rows_a.push(vec![2, 2]);

    let mut rows_b = rows_a.clone();
    let moved = rows_b.remove(10);
    rows_b.insert(3, moved);

    let refs_a: Vec<&[i32]> = rows_a.iter().map(|r| r.as_slice()).collect();
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs_a);
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRows { .. })),
        "ambiguous repeated patterns should not emit BlockMovedRows"
    );
    assert!(
        !report.ops.is_empty(),
        "fallback path should produce some diff noise"
    );
}

```

---

### File: `core\tests\g14_move_combination_tests.rs`

```rust
mod common;

use common::{grid_from_numbers, single_sheet_workbook};
use excel_diff::{DiffConfig, DiffOp, DiffReport, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn collect_rect_moves(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRect { .. }))
        .collect()
}

fn collect_row_moves(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRows { .. }))
        .collect()
}

fn collect_col_moves(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedColumns { .. }))
        .collect()
}

fn collect_row_adds(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. }))
        .collect()
}

fn collect_row_removes(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowRemoved { .. }))
        .collect()
}

fn collect_cell_edits(report: &DiffReport) -> Vec<&DiffOp> {
    report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .collect()
}

fn base_grid(rows: usize, cols: usize) -> Vec<Vec<i32>> {
    (0..rows)
        .map(|r| {
            (0..cols)
                .map(|c| (r as i32 + 1) * 100 + c as i32 + 1)
                .collect()
        })
        .collect()
}

fn place_block(target: &mut [Vec<i32>], top: usize, left: usize, block: &[Vec<i32>]) {
    for (r_offset, row_vals) in block.iter().enumerate() {
        for (c_offset, value) in row_vals.iter().enumerate() {
            let row = top + r_offset;
            let col = left + c_offset;
            if let Some(row_slice) = target.get_mut(row)
                && let Some(cell) = row_slice.get_mut(col)
            {
                *cell = *value;
            }
        }
    }
}

fn grid_from_matrix(matrix: &[Vec<i32>]) -> excel_diff::Grid {
    let refs: Vec<&[i32]> = matrix.iter().map(|row| row.as_slice()).collect();
    grid_from_numbers(&refs)
}

#[test]
fn g14_rect_move_no_additional_changes_produces_single_op() {
    let mut grid_a = base_grid(12, 10);
    let mut grid_b = base_grid(12, 10);

    let block = vec![vec![9001, 9002], vec![9003, 9004]];
    place_block(&mut grid_a, 2, 2, &block);
    place_block(&mut grid_b, 8, 6, &block);

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let has_rect_move = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::BlockMovedRect { .. }));

    assert!(has_rect_move, "pure rect move should be detected");

    assert_eq!(
        report.ops.len(),
        1,
        "pure rect move should produce exactly one BlockMovedRect op"
    );
}

#[test]
fn g14_rect_move_plus_cell_edit_no_silent_data_loss() {
    let mut grid_a = base_grid(12, 10);
    let mut grid_b = base_grid(12, 10);

    let block = vec![vec![9001, 9002], vec![9003, 9004]];
    place_block(&mut grid_a, 2, 2, &block);
    place_block(&mut grid_b, 8, 6, &block);
    grid_b[0][0] = 77777;

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let rect_moves = collect_rect_moves(&report);
    let cell_edits = collect_cell_edits(&report);

    assert_eq!(
        rect_moves.len(),
        1,
        "expected single BlockMovedRect for the moved block"
    );

    if let DiffOp::BlockMovedRect {
        src_start_row,
        src_start_col,
        src_row_count,
        src_col_count,
        dst_start_row,
        dst_start_col,
        ..
    } = rect_moves[0]
    {
        assert_eq!(*src_start_row, 2);
        assert_eq!(*src_start_col, 2);
        assert_eq!(*src_row_count, 2);
        assert_eq!(*src_col_count, 2);
        assert_eq!(*dst_start_row, 8);
        assert_eq!(*dst_start_col, 6);
    } else {
        panic!("expected BlockMovedRect");
    }

    assert!(
        !cell_edits.is_empty(),
        "expected cell edits outside the moved block"
    );
}

#[test]
fn g14_pure_row_move_produces_single_op() {
    let rows: Vec<Vec<i32>> = (1..=20)
        .map(|r| (1..=4).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b = rows.clone();
    let moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
    rows_b.splice(12..12, moved_block);
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let has_row_move = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::BlockMovedRows { .. }));

    assert!(has_row_move, "pure row block move should be detected");

    assert_eq!(
        report.ops.len(),
        1,
        "pure row block move should produce exactly one BlockMovedRows op"
    );
}

#[test]
fn g14_row_move_plus_cell_edit_no_silent_data_loss() {
    let rows: Vec<Vec<i32>> = (1..=20)
        .map(|r| (1..=4).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b = rows.clone();
    let moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
    rows_b.splice(12..12, moved_block);
    rows_b[0][0] = 99999;
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "should not have silent data loss - changes must be reported"
    );
}

#[test]
fn g14_pure_column_move_produces_single_op() {
    let rows: Vec<Vec<i32>> = (0..5)
        .map(|r| (0..8).map(|c| (r + 1) * 10 + c + 1).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b: Vec<Vec<i32>> = rows.clone();
    for row in &mut rows_b {
        let moved_col = row.remove(1);
        row.insert(5, moved_col);
    }
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let has_col_move = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::BlockMovedColumns { .. }));

    assert!(has_col_move, "pure column block move should be detected");

    assert_eq!(
        report.ops.len(),
        1,
        "pure column block move should produce exactly one BlockMovedColumns op"
    );
}

#[test]
fn g14_column_move_plus_cell_edit_no_silent_data_loss() {
    let rows: Vec<Vec<i32>> = (0..5)
        .map(|r| (0..8).map(|c| (r + 1) * 10 + c + 1).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b: Vec<Vec<i32>> = rows.clone();
    for row in &mut rows_b {
        let moved_col = row.remove(1);
        row.insert(5, moved_col);
    }
    rows_b[0][0] = 88888;
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "should not have silent data loss - changes must be reported"
    );
}

#[test]
fn g14_two_disjoint_row_block_moves_detected() {
    let rows: Vec<Vec<i32>> = (1..=24)
        .map(|r| (1..=3).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b: Vec<Vec<i32>> = Vec::new();

    rows_b.extend_from_slice(&rows[0..3]);
    rows_b.extend_from_slice(&rows[7..10]);
    rows_b.extend_from_slice(&rows[13..24]);
    rows_b.extend_from_slice(&rows[3..7]);
    rows_b.extend_from_slice(&rows[10..13]);

    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_moves = collect_row_moves(&report);
    assert_eq!(
        row_moves.len(),
        2,
        "expected exactly two BlockMovedRows ops for two disjoint moves"
    );

    let mut actual: Vec<(u32, u32, u32)> = row_moves
        .iter()
        .map(|op| {
            if let DiffOp::BlockMovedRows {
                src_start_row,
                row_count,
                dst_start_row,
                ..
            } = **op
            {
                (src_start_row, row_count, dst_start_row)
            } else {
                unreachable!()
            }
        })
        .collect();
    actual.sort();

    let mut expected = vec![(3u32, 4u32, 17u32), (10u32, 3u32, 21u32)];
    expected.sort();

    assert_eq!(
        actual, expected,
        "row move ops should match the two expected disjoint moves"
    );
}

#[test]
fn g14_row_move_plus_column_move_both_detected() {
    let rows: Vec<Vec<i32>> = (0..15)
        .map(|r| (0..10).map(|c| (r + 1) * 100 + c + 1).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b = rows.clone();

    let moved_rows: Vec<Vec<i32>> = rows_b.drain(2..5).collect();
    rows_b.splice(10..10, moved_rows);

    for row in &mut rows_b {
        let moved_col = row.remove(1);
        row.insert(7, moved_col);
    }

    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_moves = collect_row_moves(&report);
    let col_moves = collect_col_moves(&report);

    assert_eq!(
        row_moves.len(),
        1,
        "expected a single BlockMovedRows op for the moved row block"
    );
    assert_eq!(
        col_moves.len(),
        1,
        "expected a single BlockMovedColumns op for the moved column"
    );

    if let DiffOp::BlockMovedRows {
        src_start_row,
        row_count,
        dst_start_row,
        ..
    } = *row_moves[0]
    {
        assert_eq!(src_start_row, 2);
        assert_eq!(row_count, 3);
        assert_eq!(dst_start_row, 10);
    } else {
        panic!("expected BlockMovedRows op");
    }

    if let DiffOp::BlockMovedColumns {
        src_start_col,
        col_count,
        dst_start_col,
        ..
    } = *col_moves[0]
    {
        assert_eq!(src_start_col, 1);
        assert_eq!(col_count, 1);
        assert_eq!(dst_start_col, 7);
    } else {
        panic!("expected BlockMovedColumns op");
    }
}

#[test]
fn g14_fuzzy_row_move_produces_move_and_internal_edits() {
    let rows: Vec<Vec<i32>> = (1..=20)
        .map(|r| (1..=4).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b = rows.clone();
    let mut moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
    moved_block[1][1] = 5555;
    rows_b.splice(12..12, moved_block);
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let has_row_move = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::BlockMovedRows { .. }));

    let has_internal_edit = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::CellEdited { .. }));

    assert!(has_row_move, "should detect the fuzzy row block move");
    assert!(
        has_internal_edit,
        "should report cell edits inside the moved block"
    );
}

#[test]
fn g14_fuzzy_row_move_plus_outside_edit_no_silent_data_loss() {
    let rows: Vec<Vec<i32>> = (1..=20)
        .map(|r| (1..=4).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b = rows.clone();
    let mut moved_block: Vec<Vec<i32>> = rows_b.drain(4..8).collect();
    moved_block[1][1] = 5555;
    rows_b.splice(12..12, moved_block);
    rows_b[0][0] = 99999;
    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "should not have silent data loss - changes must be reported"
    );
}

#[test]
fn g14_grid_changes_no_silent_data_loss() {
    let mut grid_a = base_grid(15, 12);
    let mut grid_b = base_grid(15, 12);

    let block = vec![vec![7001, 7002], vec![7003, 7004], vec![7005, 7006]];
    place_block(&mut grid_a, 3, 3, &block);
    place_block(&mut grid_b, 10, 8, &block);
    grid_b[0][0] = 11111;
    grid_b[0][11] = 22222;
    grid_b[14][0] = 33333;
    grid_b[14][11] = 44444;

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "should not have silent data loss - changes must be reported"
    );

    let cell_edits: Vec<(u32, u32)> = report
        .ops
        .iter()
        .filter_map(|op| {
            if let DiffOp::CellEdited { addr, .. } = op {
                Some((addr.row, addr.col))
            } else {
                None
            }
        })
        .collect();

    assert!(
        !cell_edits.is_empty() || !report.ops.is_empty(),
        "some form of changes should be reported"
    );
}

#[test]
fn g14_three_disjoint_rect_block_moves_detected() {
    let mut grid_a = base_grid(20, 10);
    let mut grid_b = base_grid(20, 10);

    let block1 = vec![vec![1001, 1002], vec![1003, 1004]];
    let block2 = vec![vec![2001, 2002], vec![2003, 2004]];
    let block3 = vec![vec![3001, 3002], vec![3003, 3004]];

    place_block(&mut grid_a, 2, 1, &block1);
    place_block(&mut grid_a, 6, 3, &block2);
    place_block(&mut grid_a, 12, 5, &block3);

    place_block(&mut grid_b, 10, 1, &block1);
    place_block(&mut grid_b, 4, 6, &block2);
    place_block(&mut grid_b, 16, 2, &block3);

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let rect_moves: Vec<_> = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRect { .. }))
        .collect();

    assert_eq!(
        rect_moves.len(),
        3,
        "expected exactly three rect block moves to be detected"
    );
    assert_eq!(
        report.ops.len(),
        3,
        "multi-rect move scenario should not emit extra structural ops"
    );
}

#[test]
fn g14_two_disjoint_rect_moves_plus_outside_edits_no_silent_data_loss() {
    let mut grid_a = base_grid(20, 12);
    let mut grid_b = base_grid(20, 12);

    let block1 = vec![vec![8001, 8002], vec![8003, 8004]];
    let block2 = vec![vec![9001, 9002], vec![9003, 9004]];

    place_block(&mut grid_a, 2, 2, &block1);
    place_block(&mut grid_a, 10, 7, &block2);

    place_block(&mut grid_b, 8, 4, &block1);
    place_block(&mut grid_b, 14, 1, &block2);

    grid_b[0][0] = 77777;
    grid_b[19][11] = 88888;

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let rect_moves: Vec<_> = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::BlockMovedRect { .. }))
        .collect();
    assert!(
        rect_moves.len() >= 2,
        "should detect both rect block moves in the scenario"
    );

    let rect_regions = [
        (2u32, 2u32, 2u32, 2u32),
        (10u32, 7u32, 2u32, 2u32),
        (8u32, 4u32, 2u32, 2u32),
        (14u32, 1u32, 2u32, 2u32),
    ];

    let outside_cell_edits: Vec<_> = report
        .ops
        .iter()
        .filter_map(|op| {
            if let DiffOp::CellEdited { addr, .. } = op {
                let in_rect = rect_regions.iter().any(|(r, c, h, w)| {
                    addr.row >= *r && addr.row < *r + *h && addr.col >= *c && addr.col < *c + *w
                });
                if !in_rect {
                    return Some((addr.row, addr.col));
                }
            }
            None
        })
        .collect();

    assert!(
        !outside_cell_edits.is_empty(),
        "cell edits outside moved rects should be surfaced"
    );
}

#[allow(clippy::needless_range_loop)]
#[test]
fn g14_rect_move_plus_row_insertion_outside_no_silent_data_loss() {
    let mut grid_a = base_grid(12, 10);
    let block = vec![vec![9001, 9002], vec![9003, 9004]];
    place_block(&mut grid_a, 2, 2, &block);

    let mut grid_b = base_grid(13, 10);

    for col in 0..10 {
        grid_b[0][col] = 50000 + col as i32;
    }

    for row in 1..13 {
        for col in 0..10 {
            grid_b[row][col] = (row as i32) * 100 + col as i32 + 1;
        }
    }

    place_block(&mut grid_b, 9, 6, &block);

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let rect_moves = collect_rect_moves(&report);
    let row_adds = collect_row_adds(&report);

    assert_eq!(
        rect_moves.len(),
        1,
        "expected a single BlockMovedRect for the moved block"
    );
    assert!(
        !row_adds.is_empty(),
        "expected at least one RowAdded for the inserted row"
    );
}

#[test]
fn g14_rect_move_plus_row_deletion_outside_no_silent_data_loss() {
    let mut grid_a = base_grid(14, 10);
    let block = vec![vec![8001, 8002], vec![8003, 8004]];
    place_block(&mut grid_a, 3, 3, &block);

    let mut grid_b = base_grid(13, 10);

    place_block(&mut grid_b, 8, 6, &block);

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let rect_moves = collect_rect_moves(&report);
    let row_removes = collect_row_removes(&report);

    assert_eq!(
        rect_moves.len(),
        1,
        "expected a single BlockMovedRect for the moved block"
    );
    assert!(
        !row_removes.is_empty(),
        "expected at least one RowRemoved for the deleted row"
    );
}

#[test]
fn g14_row_block_move_plus_row_insertion_outside_no_silent_data_loss() {
    let rows: Vec<Vec<i32>> = (1..=20)
        .map(|r| (1..=4).map(|c| r * 10 + c).collect())
        .collect();
    let refs: Vec<&[i32]> = rows.iter().map(|r| r.as_slice()).collect();
    let grid_a = grid_from_numbers(&refs);

    let mut rows_b: Vec<Vec<i32>> = Vec::with_capacity(21);

    rows_b.push(vec![9991, 9992, 9993, 9994]);

    let mut original = rows.clone();
    let moved_block: Vec<Vec<i32>> = original.drain(4..8).collect();
    original.splice(12..12, moved_block);
    rows_b.extend(original);

    let refs_b: Vec<&[i32]> = rows_b.iter().map(|r| r.as_slice()).collect();
    let grid_b = grid_from_numbers(&refs_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "row block move + row insertion should produce operations"
    );
}

#[test]
fn g14_move_detection_disabled_falls_back_to_positional() {
    let grid_a = grid_from_numbers(&[
        &[1, 2, 3],
        &[10, 20, 30],
        &[100, 200, 300],
        &[1000, 2000, 3000],
        &[10000, 20000, 30000],
    ]);

    let grid_b = grid_from_numbers(&[
        &[1, 2, 3],
        &[1000, 2000, 3000],
        &[10000, 20000, 30000],
        &[10, 20, 30],
        &[100, 200, 300],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.moves.max_move_iterations = 0;
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowRemoved { .. })),
        "expected positional fallback when move detection disabled"
    );
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. })),
        "expected positional fallback when move detection disabled"
    );
    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRows { .. })),
        "no block move should be present when move detection disabled"
    );
}

#[test]
fn g14_masked_move_detection_not_gated_by_recursive_align_threshold() {
    let grid_a = grid_from_numbers(&[
        &[1, 2, 3],
        &[10, 20, 30],
        &[100, 200, 300],
        &[1000, 2000, 3000],
        &[10000, 20000, 30000],
    ]);

    let grid_b = grid_from_numbers(&[
        &[1, 2, 3],
        &[1000, 2000, 3000],
        &[10000, 20000, 30000],
        &[10, 20, 30],
        &[100, 200, 300],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.recursive_align_threshold = 1;
    config.moves.max_move_detection_rows = 10;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::BlockMovedRows { .. })),
        "masked move detection should be enabled by max_move_detection_rows, independent of recursion threshold"
    );
}

#[test]
fn g14_max_move_iterations_limits_detected_moves() {
    let mut grid_a = base_grid(50, 10);
    let mut grid_b = base_grid(50, 10);

    let block1 = vec![vec![1001, 1002], vec![1003, 1004]];
    let block2 = vec![vec![2001, 2002], vec![2003, 2004]];
    let block3 = vec![vec![3001, 3002], vec![3003, 3004]];
    let block4 = vec![vec![4001, 4002], vec![4003, 4004]];
    let block5 = vec![vec![5001, 5002], vec![5003, 5004]];

    place_block(&mut grid_a, 2, 1, &block1);
    place_block(&mut grid_a, 8, 1, &block2);
    place_block(&mut grid_a, 14, 1, &block3);
    place_block(&mut grid_a, 20, 1, &block4);
    place_block(&mut grid_a, 26, 1, &block5);

    place_block(&mut grid_b, 40, 7, &block1);
    place_block(&mut grid_b, 34, 7, &block2);
    place_block(&mut grid_b, 28, 7, &block3);
    place_block(&mut grid_b, 22, 7, &block4);
    place_block(&mut grid_b, 16, 7, &block5);

    let wb_a = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_a));
    let wb_b = single_sheet_workbook("Sheet1", grid_from_matrix(&grid_b));

    let mut limited_config = DiffConfig::default();
    limited_config.moves.max_move_iterations = 2;
    let report_limited = diff_workbooks(&wb_a, &wb_b, &limited_config);

    let rect_moves_limited = collect_rect_moves(&report_limited);

    assert!(
        rect_moves_limited.len() <= 2,
        "with max_move_iterations=2, at most 2 rect moves should be detected, got {}",
        rect_moves_limited.len()
    );

    assert!(
        !report_limited.ops.is_empty(),
        "remaining differences should still be surfaced, not silently dropped"
    );

    let full_config = DiffConfig::default();
    let report_full = diff_workbooks(&wb_a, &wb_b, &full_config);

    let rect_moves_full = collect_rect_moves(&report_full);

    assert!(
        rect_moves_full.len() >= 5,
        "with default config, all 5 rect moves should be detected, got {}",
        rect_moves_full.len()
    );
}

```

---

### File: `core\tests\g15_column_structure_row_alignment_tests.rs`

```rust
//! Integration tests verifying column structural changes do not break row alignment when row content is preserved.
//! Covers Branch 1.3 acceptance criteria for column insertion/deletion resilience.

mod common;

use common::single_sheet_workbook;
use excel_diff::{CellValue, DiffConfig, DiffOp, DiffReport, Grid, Workbook, WorkbookPackage};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn make_grid_with_cells(nrows: u32, ncols: u32, cells: &[(u32, u32, i32)]) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for (row, col, val) in cells {
        grid.insert_cell(*row, *col, Some(CellValue::Number(*val as f64)), None);
    }
    grid
}

fn grid_from_row_data(rows: &[Vec<i32>]) -> Grid {
    let nrows = rows.len() as u32;
    let ncols = rows.iter().map(|r| r.len()).max().unwrap_or(0) as u32;
    let mut grid = Grid::new(nrows, ncols);

    for (r, row_vals) in rows.iter().enumerate() {
        for (c, val) in row_vals.iter().enumerate() {
            grid.insert_cell(
                r as u32,
                c as u32,
                Some(CellValue::Number(*val as f64)),
                None,
            );
        }
    }
    grid
}

#[test]
fn g15_blank_column_insert_at_position_zero_preserves_row_alignment() {
    let grid_a = grid_from_row_data(&[
        vec![10, 20, 30],
        vec![11, 21, 31],
        vec![12, 22, 32],
        vec![13, 23, 33],
        vec![14, 24, 34],
    ]);

    let grid_b = make_grid_with_cells(
        5,
        4,
        &[
            (0, 1, 10),
            (0, 2, 20),
            (0, 3, 30),
            (1, 1, 11),
            (1, 2, 21),
            (1, 3, 31),
            (2, 1, 12),
            (2, 2, 22),
            (2, 3, 32),
            (3, 1, 13),
            (3, 2, 23),
            (3, 3, 33),
            (4, 1, 14),
            (4, 2, 24),
            (4, 3, 34),
        ],
    );

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let column_adds: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnAdded { col_idx, .. } => Some(*col_idx),
            _ => None,
        })
        .collect();

    let row_changes: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. } | DiffOp::BlockMovedRows { .. }
            )
        })
        .collect();

    assert!(
        column_adds.contains(&0) || !report.ops.is_empty(),
        "blank column insert at position 0 should be detected as ColumnAdded or produce some diff"
    );

    assert!(
        row_changes.is_empty(),
        "blank column insert should NOT produce spurious row add/remove operations; got {:?}",
        row_changes
    );
}

#[test]
fn g15_blank_column_insert_middle_preserves_row_alignment() {
    let grid_a = grid_from_row_data(&[
        vec![1, 2, 3, 4],
        vec![5, 6, 7, 8],
        vec![9, 10, 11, 12],
        vec![13, 14, 15, 16],
    ]);

    let grid_b = make_grid_with_cells(
        4,
        5,
        &[
            (0, 0, 1),
            (0, 1, 2),
            (0, 3, 3),
            (0, 4, 4),
            (1, 0, 5),
            (1, 1, 6),
            (1, 3, 7),
            (1, 4, 8),
            (2, 0, 9),
            (2, 1, 10),
            (2, 3, 11),
            (2, 4, 12),
            (3, 0, 13),
            (3, 1, 14),
            (3, 3, 15),
            (3, 4, 16),
        ],
    );

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_structural_ops: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. } | DiffOp::BlockMovedRows { .. }
            )
        })
        .collect();

    assert!(
        row_structural_ops.is_empty(),
        "blank column insert in middle should not cause row structural changes; got {:?}",
        row_structural_ops
    );

    let has_column_op = report.ops.iter().any(|op| {
        matches!(
            op,
            DiffOp::ColumnAdded { .. } | DiffOp::ColumnRemoved { .. }
        )
    });

    assert!(
        has_column_op || !report.ops.is_empty(),
        "column structure change should be detected"
    );
}

#[test]
fn g15_column_delete_preserves_row_alignment_when_content_order_maintained() {
    let grid_a = grid_from_row_data(&[
        vec![1, 2, 3, 4, 5],
        vec![6, 7, 8, 9, 10],
        vec![11, 12, 13, 14, 15],
        vec![16, 17, 18, 19, 20],
    ]);

    let grid_b = grid_from_row_data(&[
        vec![1, 2, 4, 5],
        vec![6, 7, 9, 10],
        vec![11, 12, 14, 15],
        vec![16, 17, 19, 20],
    ]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let column_removes: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnRemoved { col_idx, .. } => Some(*col_idx),
            _ => None,
        })
        .collect();

    let row_structural_ops: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. } | DiffOp::BlockMovedRows { .. }
            )
        })
        .collect();

    assert!(
        row_structural_ops.is_empty(),
        "column deletion should not cause spurious row changes; got {:?}",
        row_structural_ops
    );

    assert!(
        !column_removes.is_empty() || !report.ops.is_empty(),
        "column deletion should be detected"
    );
}

#[test]
fn g15_row_insert_with_column_structure_change_both_detected() {
    let grid_a = grid_from_row_data(&[vec![1, 2, 3], vec![4, 5, 6], vec![7, 8, 9]]);

    let grid_b = make_grid_with_cells(
        4,
        4,
        &[
            (0, 0, 1000),
            (0, 1, 1),
            (0, 2, 2),
            (0, 3, 3),
            (1, 0, 1001),
            (1, 1, 100),
            (1, 2, 200),
            (1, 3, 300),
            (2, 0, 1002),
            (2, 1, 4),
            (2, 2, 5),
            (2, 3, 6),
            (3, 0, 1003),
            (3, 1, 7),
            (3, 2, 8),
            (3, 3, 9),
        ],
    );

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    assert!(
        !report.ops.is_empty(),
        "row insert + column change should produce diff operations"
    );

    let has_row_op = report.ops.iter().any(|op| {
        matches!(
            op,
            DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. } | DiffOp::CellEdited { .. }
        )
    });

    let has_col_op = report.ops.iter().any(|op| {
        matches!(
            op,
            DiffOp::ColumnAdded { .. } | DiffOp::ColumnRemoved { .. } | DiffOp::CellEdited { .. }
        )
    });

    assert!(
        has_row_op || has_col_op,
        "at least one structural change type should be detected"
    );
}

#[test]
fn g15_single_row_grid_column_insert_no_spurious_row_ops() {
    let grid_a = grid_from_row_data(&[vec![10, 20]]);

    let grid_b = make_grid_with_cells(1, 3, &[(0, 0, 10), (0, 2, 20)]);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_ops: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. }))
        .collect();

    assert!(
        row_ops.is_empty(),
        "single row grid with column insert should not have row ops; got {:?}",
        row_ops
    );
}

#[test]
fn g15_all_blank_column_insert_no_content_change_minimal_diff() {
    let grid_a = grid_from_row_data(&[vec![1, 2], vec![3, 4], vec![5, 6]]);

    let grid_b = make_grid_with_cells(
        3,
        3,
        &[
            (0, 0, 1),
            (0, 1, 2),
            (1, 0, 3),
            (1, 1, 4),
            (2, 0, 5),
            (2, 1, 6),
        ],
    );

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_ops: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. }))
        .collect();

    assert!(
        row_ops.is_empty(),
        "appending blank column should not cause row operations; got {:?}",
        row_ops
    );
}

#[test]
fn g15_large_grid_column_insert_row_alignment_preserved() {
    let rows: Vec<Vec<i32>> = (0..50)
        .map(|r| (0..10).map(|c| r * 100 + c).collect())
        .collect();
    let grid_a = grid_from_row_data(&rows);

    let mut cells_b: Vec<(u32, u32, i32)> = Vec::with_capacity(50 * 10);
    for r in 0..50 {
        for c in 0..10 {
            let new_col = if c < 5 { c } else { c + 1 };
            cells_b.push((r, new_col, r as i32 * 100 + c as i32));
        }
    }
    let grid_b = make_grid_with_cells(50, 11, &cells_b);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let report = diff_workbooks(&wb_a, &wb_b, &DiffConfig::default());

    let row_structural_ops: Vec<&DiffOp> = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. } | DiffOp::BlockMovedRows { .. }
            )
        })
        .collect();

    assert!(
        row_structural_ops.is_empty(),
        "large grid column insert should not cause row changes; got {} row ops",
        row_structural_ops.len()
    );

    let column_adds = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::ColumnAdded { .. }))
        .count();

    assert!(
        column_adds > 0 || !report.ops.is_empty(),
        "column insertion should be detected in large grid"
    );
}

```

---

### File: `core\tests\g1_g2_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, sid};
use excel_diff::{
    CellValue, DiffConfig, DiffOp, DiffReport, Grid, Sheet, SheetKind, Workbook, WorkbookPackage,
};

fn workbook_with_number(value: f64) -> Workbook {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(value)), None);

    Workbook {
        sheets: vec![Sheet {
            name: sid("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn g1_equal_sheet_produces_empty_diff() {
    let report = diff_fixture_pkgs(
        "equal_sheet_a.xlsx",
        "equal_sheet_b.xlsx",
        &DiffConfig::default(),
    );

    assert!(
        report.ops.is_empty(),
        "identical 5x5 sheet should produce an empty diff"
    );
}

#[test]
fn g2_single_cell_literal_change_produces_one_celledited() {
    let report = diff_fixture_pkgs(
        "single_cell_value_a.xlsx",
        "single_cell_value_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(
        report.ops.len(),
        1,
        "expected exactly one diff op for a single edited cell"
    );

    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            assert_eq!(*sheet, sid("Sheet1"));
            assert_eq!(addr.to_a1(), "C3");
            assert_eq!(from.value, Some(CellValue::Number(1.0)));
            assert_eq!(to.value, Some(CellValue::Number(2.0)));
            assert_eq!(from.formula, to.formula, "no formula changes expected");
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::RowAdded { .. }
                | DiffOp::RowRemoved { .. }
                | DiffOp::ColumnAdded { .. }
                | DiffOp::ColumnRemoved { .. }
        )),
        "single cell change should not produce row/column structure ops"
    );
}

#[test]
fn g2_float_ulp_noise_is_ignored_in_diff() {
    let old = workbook_with_number(1.0);
    let new = workbook_with_number(1.0000000000000002);

    let report = diff_workbooks(&old, &new, &DiffConfig::default());

    assert!(
        report.ops.is_empty(),
        "ULP-level float drift should not produce a diff op"
    );
}

#[test]
fn g2_meaningful_float_change_emits_cell_edit() {
    let old = workbook_with_number(1.0);
    let new = workbook_with_number(1.0001);

    let report = diff_workbooks(&old, &new, &DiffConfig::default());

    assert_eq!(
        report.ops.len(),
        1,
        "meaningful float change should produce exactly one diff op"
    );

    match &report.ops[0] {
        DiffOp::CellEdited { addr, from, to, .. } => {
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(from.value, Some(CellValue::Number(1.0)));
            assert_eq!(to.value, Some(CellValue::Number(1.0001)));
        }
        other => panic!("expected CellEdited diff op, got {other:?}"),
    }
}

#[test]
fn g2_nan_values_are_treated_as_equal() {
    let signaling_nan = f64::from_bits(0x7ff8_0000_0000_0000);
    let quiet_nan = f64::NAN;

    let old = workbook_with_number(signaling_nan);
    let new = workbook_with_number(quiet_nan);

    let report = diff_workbooks(&old, &new, &DiffConfig::default());

    assert!(
        report.ops.is_empty(),
        "different NaN bit patterns should be considered equal in diffing"
    );
}

```

---

### File: `core\tests\g5_g7_grid_workbook_tests.rs`

```rust
mod common;

use common::diff_fixture_pkgs;
use excel_diff::{CellValue, DiffConfig, DiffOp, with_default_session};
use std::collections::BTreeSet;

#[test]
fn g5_multi_cell_edits_produces_only_celledited_ops() {
    let report = diff_fixture_pkgs(
        "multi_cell_edits_a.xlsx",
        "multi_cell_edits_b.xlsx",
        &DiffConfig::default(),
    );

    let (text_x, text_y) = with_default_session(|session| {
        let x = session.strings.intern("x");
        let y = session.strings.intern("y");
        (CellValue::Text(x), CellValue::Text(y))
    });
    let expected = vec![
        ("B2", CellValue::Number(1.0), CellValue::Number(42.0)),
        ("D5", CellValue::Number(2.0), CellValue::Number(99.0)),
        ("H7", CellValue::Number(3.0), CellValue::Number(3.5)),
        ("J10", text_x, text_y),
    ];

    assert_eq!(
        report.ops.len(),
        expected.len(),
        "expected one DiffOp per configured edit"
    );
    assert!(
        report
            .ops
            .iter()
            .all(|op| matches!(op, DiffOp::CellEdited { .. })),
        "multi-cell edits should produce only CellEdited ops"
    );

    for (addr, expected_from, expected_to) in expected {
        let (sheet, from, to) = report
            .ops
            .iter()
            .find_map(|op| match op {
                DiffOp::CellEdited {
                    sheet,
                    addr: a,
                    from,
                    to,
                    ..
                } if a.to_a1() == addr => Some((sheet, from, to)),
                _ => None,
            })
            .unwrap_or_else(|| panic!("missing CellEdited for {addr}"));

        let sheet_name = report.strings[sheet.0 as usize].as_str();
        assert_eq!(sheet_name, "Sheet1");
        assert_eq!(from.value, Some(expected_from));
        assert_eq!(to.value, Some(expected_to));
        assert_eq!(from.formula, to.formula, "no formula changes expected");
    }

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::RowAdded { .. }
                | DiffOp::RowRemoved { .. }
                | DiffOp::ColumnAdded { .. }
                | DiffOp::ColumnRemoved { .. }
        )),
        "multi-cell edits should not produce row/column structure ops"
    );
}

#[test]
fn g6_row_append_bottom_emits_two_rowadded_and_no_celledited() {
    let report = diff_fixture_pkgs(
        "row_append_bottom_a.xlsx",
        "row_append_bottom_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(
        report.ops.len(),
        2,
        "expected exactly two RowAdded ops for appended rows"
    );

    let rows_added: BTreeSet<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowAdded {
                sheet,
                row_idx,
                row_signature,
            } => {
                let sheet_name = report.strings[sheet.0 as usize].as_str();
                assert_eq!(sheet_name, "Sheet1");
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    let expected: BTreeSet<u32> = [10u32, 11u32].into_iter().collect();
    assert_eq!(rows_added, expected);

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::RowRemoved { .. }
                | DiffOp::ColumnAdded { .. }
                | DiffOp::ColumnRemoved { .. }
                | DiffOp::CellEdited { .. }
        )),
        "row append should not emit removals, column ops, or cell edits"
    );
}

#[test]
fn g6_row_delete_bottom_emits_two_rowremoved_and_no_celledited() {
    let report = diff_fixture_pkgs(
        "row_delete_bottom_a.xlsx",
        "row_delete_bottom_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(
        report.ops.len(),
        2,
        "expected exactly two RowRemoved ops for deleted rows"
    );

    let rows_removed: BTreeSet<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowRemoved {
                sheet,
                row_idx,
                row_signature,
            } => {
                let sheet_name = report.strings[sheet.0 as usize].as_str();
                assert_eq!(sheet_name, "Sheet1");
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    let expected: BTreeSet<u32> = [10u32, 11u32].into_iter().collect();
    assert_eq!(rows_removed, expected);

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::RowAdded { .. }
                | DiffOp::ColumnAdded { .. }
                | DiffOp::ColumnRemoved { .. }
                | DiffOp::CellEdited { .. }
        )),
        "row delete should not emit additions, column ops, or cell edits"
    );
}

#[test]
fn g7_col_append_right_emits_two_columnadded_and_no_celledited() {
    let report = diff_fixture_pkgs(
        "col_append_right_a.xlsx",
        "col_append_right_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(
        report.ops.len(),
        2,
        "expected exactly two ColumnAdded ops for appended columns"
    );

    let cols_added: BTreeSet<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnAdded {
                sheet,
                col_idx,
                col_signature,
            } => {
                let sheet_name = report.strings[sheet.0 as usize].as_str();
                assert_eq!(sheet_name, "Sheet1");
                assert!(col_signature.is_none());
                Some(*col_idx)
            }
            _ => None,
        })
        .collect();

    let expected: BTreeSet<u32> = [4u32, 5u32].into_iter().collect();
    assert_eq!(cols_added, expected);

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::ColumnRemoved { .. }
                | DiffOp::RowAdded { .. }
                | DiffOp::RowRemoved { .. }
                | DiffOp::CellEdited { .. }
        )),
        "column append should not emit removals, row ops, or cell edits"
    );
}

#[test]
fn g7_col_delete_right_emits_two_columnremoved_and_no_celledited() {
    let report = diff_fixture_pkgs(
        "col_delete_right_a.xlsx",
        "col_delete_right_b.xlsx",
        &DiffConfig::default(),
    );

    assert_eq!(
        report.ops.len(),
        2,
        "expected exactly two ColumnRemoved ops for deleted columns"
    );

    let cols_removed: BTreeSet<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnRemoved {
                sheet,
                col_idx,
                col_signature,
            } => {
                let sheet_name = report.strings[sheet.0 as usize].as_str();
                assert_eq!(sheet_name, "Sheet1");
                assert!(col_signature.is_none());
                Some(*col_idx)
            }
            _ => None,
        })
        .collect();

    let expected: BTreeSet<u32> = [4u32, 5u32].into_iter().collect();
    assert_eq!(cols_removed, expected);

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::ColumnAdded { .. }
                | DiffOp::RowAdded { .. }
                | DiffOp::RowRemoved { .. }
                | DiffOp::CellEdited { .. }
        )),
        "column delete should not emit additions, row ops, or cell edits"
    );
}

```

---

### File: `core\tests\g8_row_alignment_grid_workbook_tests.rs`

```rust
mod common;

use common::diff_fixture_pkgs;
use excel_diff::{DiffConfig, DiffOp};

#[test]
fn single_row_insert_middle_produces_one_row_added() {
    let report = diff_fixture_pkgs(
        "row_insert_middle_a.xlsx",
        "row_insert_middle_b.xlsx",
        &DiffConfig::default(),
    );

    let strings = &report.strings;

    let rows_added: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowAdded {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(
                    strings.get(sheet.0 as usize).map(String::as_str),
                    Some("Sheet1")
                );
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(rows_added, vec![5], "expected single RowAdded at index 5");

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowRemoved { .. })),
        "no rows should be removed for middle insert"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned insert should not emit CellEdited noise"
    );
}

#[test]
fn single_row_delete_middle_produces_one_row_removed() {
    let report = diff_fixture_pkgs(
        "row_delete_middle_a.xlsx",
        "row_delete_middle_b.xlsx",
        &DiffConfig::default(),
    );

    let strings = &report.strings;

    let rows_removed: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowRemoved {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(
                    strings.get(sheet.0 as usize).map(String::as_str),
                    Some("Sheet1")
                );
                assert!(row_signature.is_none());
                Some(*row_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(
        rows_removed,
        vec![5],
        "expected single RowRemoved at index 5"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::RowAdded { .. })),
        "no rows should be added for middle delete"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned delete should not emit CellEdited noise"
    );
}

#[test]
fn alignment_bails_out_when_additional_edits_present() {
    let report = diff_fixture_pkgs(
        "row_insert_with_edit_a.xlsx",
        "row_insert_with_edit_b.xlsx",
        &DiffConfig::default(),
    );

    let rows_added: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::RowAdded { row_idx, .. } => Some(*row_idx),
            _ => None,
        })
        .collect();

    assert!(
        rows_added.contains(&10),
        "fallback positional diff should add the tail row"
    );
    assert!(
        !rows_added.contains(&5),
        "mid-sheet RowAdded at 5 would indicate the alignment path was taken"
    );

    let edited_rows: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::CellEdited { addr, .. } => Some(addr.row),
            _ => None,
        })
        .collect();

    assert!(
        !edited_rows.is_empty(),
        "fallback positional diff should surface cell edits after the inserted row"
    );
    assert!(
        edited_rows.iter().any(|row| *row >= 5),
        "cell edits should include rows at or below the insertion point"
    );
}

```

---

### File: `core\tests\g9_column_alignment_grid_workbook_tests.rs`

```rust
mod common;

use common::{diff_fixture_pkgs, open_fixture_workbook, sid};
use excel_diff::{CellValue, DiffConfig, DiffOp, Workbook};

#[test]
fn g9_col_insert_middle_emits_one_columnadded_and_no_noise() {
    let report = diff_fixture_pkgs(
        "col_insert_middle_a.xlsx",
        "col_insert_middle_b.xlsx",
        &DiffConfig::default(),
    );

    let cols_added: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnAdded {
                sheet,
                col_idx,
                col_signature,
            } => {
                assert_eq!(sheet, &sid("Data"));
                assert!(col_signature.is_none());
                Some(*col_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(
        cols_added,
        vec![3],
        "expected single ColumnAdded at inserted position"
    );

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::ColumnRemoved { .. } | DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. }
        )),
        "column insert should not emit row ops or ColumnRemoved"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned insert should not emit CellEdited noise"
    );
}

#[test]
fn g9_col_delete_middle_emits_one_columnremoved_and_no_noise() {
    let report = diff_fixture_pkgs(
        "col_delete_middle_a.xlsx",
        "col_delete_middle_b.xlsx",
        &DiffConfig::default(),
    );

    let cols_removed: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::ColumnRemoved {
                sheet,
                col_idx,
                col_signature,
            } => {
                assert_eq!(sheet, &sid("Data"));
                assert!(col_signature.is_none());
                Some(*col_idx)
            }
            _ => None,
        })
        .collect();

    assert_eq!(
        cols_removed,
        vec![3],
        "expected single ColumnRemoved at deleted position"
    );

    assert!(
        !report.ops.iter().any(|op| matches!(
            op,
            DiffOp::ColumnAdded { .. } | DiffOp::RowAdded { .. } | DiffOp::RowRemoved { .. }
        )),
        "column delete should not emit ColumnAdded or row ops"
    );

    assert!(
        !report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "aligned delete should not emit CellEdited noise"
    );
}

#[test]
fn g9_alignment_bails_out_when_additional_edits_present() {
    let wb_b = open_fixture_workbook("col_insert_with_edit_b.xlsx");
    let report = diff_fixture_pkgs(
        "col_insert_with_edit_a.xlsx",
        "col_insert_with_edit_b.xlsx",
        &DiffConfig::default(),
    );
    let inserted_idx = find_header_col(&wb_b, "Inserted");

    let has_middle_column_add = report.ops.iter().any(|op| match op {
        DiffOp::ColumnAdded { col_idx, .. } => *col_idx == inserted_idx,
        _ => false,
    });
    assert!(
        !has_middle_column_add,
        "alignment should bail out; no ColumnAdded at the inserted index"
    );

    let edited_cols: Vec<u32> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::CellEdited { addr, .. } => Some(addr.col),
            _ => None,
        })
        .collect();

    assert!(
        !edited_cols.is_empty(),
        "fallback positional diff should emit CellEdited ops"
    );
    assert!(
        edited_cols.iter().any(|col| *col > inserted_idx),
        "CellEdited ops should appear in columns to the right of the insertion"
    );
}

fn find_header_col(workbook: &Workbook, header: &str) -> u32 {
    let header_id = sid(header);
    workbook
        .sheets
        .iter()
        .flat_map(|sheet| sheet.grid.iter_cells())
        .find_map(|((row, col), cell)| match &cell.value {
            Some(CellValue::Text(text)) if row == 0 && *text == header_id => Some(col),
            _ => None,
        })
        .expect("header column should exist in fixture")
}

```

---

### File: `core\tests\grid_view_hashstats_tests.rs`

```rust
use excel_diff::{
    ColHash, ColMeta, ColSignature, FrequencyClass, HashStats, RowHash, RowMeta, RowSignature,
};

fn row_meta(row_idx: u32, hash: RowHash) -> RowMeta {
    RowMeta {
        row_idx,
        signature: hash,
        non_blank_count: 0,
        first_non_blank_col: 0,
        frequency_class: FrequencyClass::Common,
        is_low_info: false,
    }
}

fn col_meta(col_idx: u32, hash: ColHash) -> ColMeta {
    ColMeta {
        col_idx,
        hash,
        non_blank_count: 0,
        first_non_blank_row: 0,
    }
}

#[test]
fn hashstats_counts_and_positions_basic() {
    let h1: RowHash = RowSignature { hash: 1 };
    let h2: RowHash = RowSignature { hash: 2 };
    let h3: RowHash = RowSignature { hash: 3 };
    let h4: RowHash = RowSignature { hash: 4 };

    let rows_a = vec![
        row_meta(0, h1),
        row_meta(1, h2),
        row_meta(2, h2),
        row_meta(3, h3),
    ];
    let rows_b = vec![row_meta(0, h2), row_meta(1, h3), row_meta(2, h4)];

    let stats = HashStats::from_row_meta(&rows_a, &rows_b);

    assert_eq!(stats.freq_a.get(&h1).copied().unwrap_or(0), 1);
    assert_eq!(stats.freq_b.get(&h1).copied().unwrap_or(0), 0);

    assert_eq!(stats.freq_a.get(&h2).copied().unwrap_or(0), 2);
    assert_eq!(stats.freq_b.get(&h2).copied().unwrap_or(0), 1);

    assert_eq!(stats.freq_a.get(&h3).copied().unwrap_or(0), 1);
    assert_eq!(stats.freq_b.get(&h3).copied().unwrap_or(0), 1);

    assert_eq!(stats.freq_a.get(&h4).copied().unwrap_or(0), 0);
    assert_eq!(stats.freq_b.get(&h4).copied().unwrap_or(0), 1);

    assert_eq!(
        stats.hash_to_positions_b.get(&h2).cloned().unwrap(),
        vec![0]
    );
    assert_eq!(
        stats.hash_to_positions_b.get(&h3).cloned().unwrap(),
        vec![1]
    );
    assert_eq!(
        stats.hash_to_positions_b.get(&h4).cloned().unwrap(),
        vec![2]
    );

    let threshold = 1;
    assert!(stats.is_unique(h3));
    assert!(stats.is_common(h2, threshold));
    assert!(!stats.is_rare(h3, threshold));
    assert!(stats.appears_in_both(h3));
    assert!(!stats.appears_in_both(h1));
    assert!(!stats.appears_in_both(h4));
}

#[test]
fn hashstats_rare_but_not_common_boundary() {
    let h: RowHash = RowSignature { hash: 42 };
    let rows_a = vec![row_meta(0, h), row_meta(1, h)];
    let rows_b = vec![row_meta(0, h)];

    let stats = HashStats::from_row_meta(&rows_a, &rows_b);
    let threshold = 2;

    assert!(stats.is_rare(h, threshold));
    assert!(!stats.is_common(h, threshold));
    assert!(stats.appears_in_both(h));
    assert!(!stats.is_unique(h));
}

#[test]
fn hashstats_equal_to_threshold_behavior() {
    let h: RowHash = RowSignature { hash: 99 };
    let rows_a = vec![row_meta(0, h), row_meta(1, h), row_meta(2, h)];
    let rows_b = vec![row_meta(0, h), row_meta(1, h), row_meta(2, h)];

    let stats = HashStats::from_row_meta(&rows_a, &rows_b);
    let threshold = 3;

    assert!(stats.is_rare(h, threshold));
    assert!(!stats.is_common(h, threshold));
    assert!(stats.appears_in_both(h));
    assert!(!stats.is_unique(h));
}

#[test]
fn hashstats_empty_inputs() {
    let stats = HashStats::from_row_meta(&[], &[]);
    let dummy_hash: RowHash = RowSignature { hash: 123 };

    assert!(stats.freq_a.is_empty());
    assert!(stats.freq_b.is_empty());
    assert!(stats.hash_to_positions_b.is_empty());

    assert!(!stats.is_unique(dummy_hash));
    assert!(!stats.is_rare(dummy_hash, 1));
    assert!(!stats.is_common(dummy_hash, 0));
    assert!(!stats.appears_in_both(dummy_hash));
}

#[test]
fn hashstats_from_col_meta_tracks_positions() {
    let h1: ColHash = ColSignature { hash: 10 };
    let h2: ColHash = ColSignature { hash: 20 };
    let h3: ColHash = ColSignature { hash: 30 };

    let cols_a = vec![col_meta(0, h1), col_meta(1, h2), col_meta(2, h2)];
    let cols_b = vec![col_meta(0, h2), col_meta(1, h3)];

    let stats = HashStats::from_col_meta(&cols_a, &cols_b);

    assert_eq!(stats.freq_a.get(&h1).copied().unwrap_or(0), 1);
    assert_eq!(stats.freq_b.get(&h1).copied().unwrap_or(0), 0);

    assert_eq!(stats.freq_a.get(&h2).copied().unwrap_or(0), 2);
    assert_eq!(stats.freq_b.get(&h2).copied().unwrap_or(0), 1);

    assert_eq!(stats.freq_b.get(&h3).copied().unwrap_or(0), 1);
    assert_eq!(stats.freq_a.get(&h3).copied().unwrap_or(0), 0);

    assert_eq!(
        stats
            .hash_to_positions_b
            .get(&h2)
            .cloned()
            .unwrap_or_default(),
        vec![0]
    );
    assert_eq!(
        stats
            .hash_to_positions_b
            .get(&h3)
            .cloned()
            .unwrap_or_default(),
        vec![1]
    );
}

```

---

### File: `core\tests\grid_view_tests.rs`

```rust
use excel_diff::{CellValue, DiffConfig, Grid, GridView, with_default_session};

mod common;
use common::grid_from_numbers;

fn insert_cell(
    grid: &mut Grid,
    row: u32,
    col: u32,
    value: Option<CellValue>,
    formula: Option<&str>,
) {
    let formula_id = formula.map(|s| with_default_session(|session| session.strings.intern(s)));
    grid.insert_cell(row, col, value, formula_id);
}

fn text(value: &str) -> CellValue {
    with_default_session(|session| CellValue::Text(session.strings.intern(value)))
}

#[test]
fn gridview_dense_3x3_layout_and_metadata() {
    let grid = grid_from_numbers(&[&[1, 2, 3], &[4, 5, 6], &[7, 8, 9]]);

    let view = GridView::from_grid(&grid);

    assert_eq!(view.rows.len(), 3);
    assert_eq!(view.row_meta.len(), 3);
    assert_eq!(view.col_meta.len(), 3);

    for (row_idx, row_view) in view.rows.iter().enumerate() {
        assert_eq!(row_view.cells.len(), 3);
        for (col_idx, (col, _cell)) in row_view.cells.iter().enumerate() {
            assert_eq!(*col as usize, col_idx);
        }

        let meta = &view.row_meta[row_idx];
        assert_eq!(meta.non_blank_count, 3);
        assert_eq!(meta.first_non_blank_col, 0);
        assert!(!meta.is_low_info);
    }

    for (col_idx, col_meta) in view.col_meta.iter().enumerate() {
        assert_eq!(col_meta.non_blank_count, 3);
        assert_eq!(col_meta.first_non_blank_row, 0);
        assert_eq!(col_meta.col_idx as usize, col_idx);
    }
}

#[test]
fn gridview_sparse_rows_low_info_classification() {
    let mut grid = Grid::new(4, 4);
    insert_cell(&mut grid, 0, 0, Some(text("Header")), None);
    insert_cell(&mut grid, 2, 2, Some(CellValue::Number(10.0)), None);
    insert_cell(&mut grid, 3, 1, Some(text("   ")), None);

    let view = GridView::from_grid(&grid);

    assert_eq!(view.row_meta[0].non_blank_count, 1);
    assert!(view.row_meta[0].is_low_info);
    assert_eq!(view.row_meta[0].first_non_blank_col, 0);

    assert_eq!(view.row_meta[1].non_blank_count, 0);
    assert!(view.row_meta[1].is_low_info);
    assert_eq!(view.row_meta[1].first_non_blank_col, 0);

    assert_eq!(view.row_meta[2].non_blank_count, 1);
    assert!(view.row_meta[2].is_low_info);
    assert_eq!(view.row_meta[2].first_non_blank_col, 2);

    assert_eq!(view.row_meta[3].non_blank_count, 1);
    assert!(view.row_meta[3].is_low_info);
    assert_eq!(view.row_meta[3].first_non_blank_col, 1);
}

#[allow(clippy::field_reassign_with_default)]
#[test]
fn gridview_formula_only_row_respects_threshold() {
    let mut grid = Grid::new(2, 2);
    insert_cell(&mut grid, 0, 0, None, Some("=A1+1"));

    let view_default = GridView::from_grid(&grid);
    assert_eq!(view_default.row_meta[0].non_blank_count, 1);
    assert!(view_default.row_meta[0].is_low_info);

    let mut config = DiffConfig::default();
    config.alignment.low_info_threshold = 1;
    let view_tuned = GridView::from_grid_with_config(&grid, &config);
    assert_eq!(view_tuned.row_meta[0].non_blank_count, 1);
    assert!(!view_tuned.row_meta[0].is_low_info);
}

#[test]
fn gridview_column_metadata_matches_signatures() {
    let mut grid = Grid::new(4, 4);
    insert_cell(&mut grid, 0, 1, Some(text("a")), Some("=B1"));
    insert_cell(&mut grid, 1, 3, Some(CellValue::Number(2.0)), Some("=1+1"));
    insert_cell(&mut grid, 2, 0, Some(CellValue::Bool(true)), None);
    insert_cell(&mut grid, 2, 2, Some(text("mid")), None);
    insert_cell(&mut grid, 3, 0, None, Some("=A1"));

    grid.compute_all_signatures();
    let row_signatures = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should be computed")
        .clone();
    let col_signatures = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should be computed")
        .clone();

    let view = GridView::from_grid(&grid);

    for (idx, meta) in view.col_meta.iter().enumerate() {
        assert_eq!(meta.hash, col_signatures[idx]);
    }

    for (idx, meta) in view.row_meta.iter().enumerate() {
        assert_eq!(meta.signature, row_signatures[idx]);
    }

    assert_eq!(view.col_meta[0].non_blank_count, 2);
    assert_eq!(view.col_meta[0].first_non_blank_row, 2);
    assert_eq!(view.col_meta[1].non_blank_count, 1);
    assert_eq!(view.col_meta[1].first_non_blank_row, 0);
    assert_eq!(view.col_meta[2].non_blank_count, 1);
    assert_eq!(view.col_meta[2].first_non_blank_row, 2);
    assert_eq!(view.col_meta[3].non_blank_count, 1);
    assert_eq!(view.col_meta[3].first_non_blank_row, 1);
}

#[test]
fn gridview_empty_grid_is_stable() {
    let grid = Grid::new(0, 0);

    let view = GridView::from_grid(&grid);

    assert!(view.rows.is_empty());
    assert!(view.row_meta.is_empty());
    assert!(view.col_meta.is_empty());
}

#[test]
fn gridview_large_sparse_grid_constructs_without_panic() {
    let nrows = 10_000;
    let ncols = 10;
    let mut grid = Grid::new(nrows, ncols);

    for r in (0..nrows).step_by(100) {
        let col = (r / 100) % ncols;
        insert_cell(
            &mut grid,
            r,
            col,
            Some(CellValue::Number((r / 100) as f64)),
            None,
        );
    }

    let view = GridView::from_grid(&grid);

    assert_eq!(view.rows.len(), nrows as usize);
    assert_eq!(view.col_meta.len(), ncols as usize);

    assert_eq!(view.row_meta[1].non_blank_count, 0);
    assert_eq!(view.row_meta[100].non_blank_count, 1);
    assert_eq!(view.row_meta[100].first_non_blank_col, 1);

    assert!(
        view.col_meta
            .iter()
            .any(|meta| meta.non_blank_count > 0 && meta.first_non_blank_row == 0)
    );
}

#[test]
fn gridview_row_hashes_ignore_small_float_drift() {
    let mut grid_a = Grid::new(1, 1);
    insert_cell(&mut grid_a, 0, 0, Some(CellValue::Number(1.0)), None);

    let mut grid_b = Grid::new(1, 1);
    insert_cell(
        &mut grid_b,
        0,
        0,
        Some(CellValue::Number(1.0000000000000002)),
        None,
    );

    let view_a = GridView::from_grid(&grid_a);
    let view_b = GridView::from_grid(&grid_b);

    assert_eq!(
        view_a.row_meta[0].signature, view_b.row_meta[0].signature,
        "row signatures should be stable under ULP-level float differences"
    );
}

```

---

### File: `core\tests\hardening_tests.rs`

```rust
mod common;

use common::single_sheet_workbook;
use excel_diff::{CellValue, DiffConfig, DiffOp, Grid, ProgressCallback, WorkbookPackage};
use std::sync::Mutex;

fn create_simple_grid(nrows: u32, ncols: u32, base_value: i32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number(
                    (base_value as i64 + row as i64 * 1000 + col as i64) as f64,
                )),
                None,
            );
        }
    }
    grid
}

#[test]
fn memory_budget_forces_positional_fallback_and_warning() {
    let grid_a = create_simple_grid(10, 3, 0);
    let mut grid_b = create_simple_grid(10, 3, 0);
    grid_b.insert_cell(5, 1, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.hardening.max_memory_mb = Some(0);

    let report = WorkbookPackage::from(wb_a).diff(&WorkbookPackage::from(wb_b), &config);

    assert!(!report.complete, "memory fallback should mark report incomplete");
    assert!(
        report
            .warnings
            .iter()
            .any(|w| w.to_lowercase().contains("memory")),
        "expected a memory warning: {:?}",
        report.warnings
    );
    assert!(
        report
            .warnings
            .iter()
            .any(|w| w.to_lowercase().contains("positional")),
        "expected warning to mention positional diff: {:?}",
        report.warnings
    );
    assert!(
        report.ops.iter().any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "should still emit ops via positional diff"
    );
}

#[test]
fn timeout_yields_partial_report_and_warning() {
    let grid_a = create_simple_grid(10, 3, 0);
    let mut grid_b = create_simple_grid(10, 3, 0);
    grid_b.insert_cell(5, 1, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.hardening.timeout_seconds = Some(0);

    let report = WorkbookPackage::from(wb_a).diff(&WorkbookPackage::from(wb_b), &config);

    assert!(!report.complete, "timeout should mark report incomplete");
    assert!(
        report
            .warnings
            .iter()
            .any(|w| w.to_lowercase().contains("timeout")),
        "expected a timeout warning: {:?}",
        report.warnings
    );
}

#[derive(Default)]
struct CollectProgress {
    events: Mutex<Vec<(String, f32)>>,
}

impl ProgressCallback for CollectProgress {
    fn on_progress(&self, phase: &str, percent: f32) {
        let mut events = match self.events.lock() {
            Ok(guard) => guard,
            Err(poisoned) => poisoned.into_inner(),
        };
        events.push((phase.to_string(), percent));
    }
}

#[test]
fn progress_callback_fires_for_cell_diff() {
    let grid_a = create_simple_grid(512, 10, 0);
    let mut grid_b = create_simple_grid(512, 10, 0);
    grid_b.insert_cell(500, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let config = DiffConfig::default();
    let progress = CollectProgress::default();

    let _report =
        WorkbookPackage::from(wb_a).diff_with_progress(&WorkbookPackage::from(wb_b), &config, &progress);

    let events = match progress.events.lock() {
        Ok(guard) => guard,
        Err(poisoned) => poisoned.into_inner(),
    };

    assert!(
        events.iter().any(|(phase, _)| phase == "cell_diff"),
        "expected at least one cell_diff progress event: {:?}",
        *events
    );
    assert!(
        events.iter().all(|(_, pct)| *pct >= 0.0 && *pct <= 1.0),
        "percent should stay within [0.0, 1.0]: {:?}",
        *events
    );
    assert!(
        events.len() < 10_000,
        "progress callbacks should be throttled: got {} callbacks",
        events.len()
    );
}


```

---

### File: `core\tests\integration_test.rs`

```rust
use std::path::PathBuf;

fn get_fixture_path(filename: &str) -> PathBuf {
    let mut d = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    // Go up one level from 'core', then into 'fixtures/generated'
    d.push("../fixtures/generated");
    d.push(filename);
    d
}

#[test]
fn test_locate_fixture() {
    let path = get_fixture_path("minimal.xlsx");
    // This test confirms that the Rust code can locate the Python-generated fixtures
    // using the relative path strategy from the monorepo root.
    assert!(
        path.exists(),
        "Fixture minimal.xlsx should exist at {:?}",
        path
    );
}

```

---

### File: `core\tests\leaf_diff_equivalence_tests.rs`

```rust
use excel_diff::advanced::{diff_grids_with_pool, diff_sheets_with_pool, diff_workbooks_with_pool};
use excel_diff::{DiffConfig, Grid, Sheet, SheetKind, StringPool, Workbook};

fn make_grid(values: &[f64]) -> Grid {
    let mut grid = Grid::new(values.len() as u32, 1);
    for (idx, val) in values.iter().enumerate() {
        grid.insert_cell(idx as u32, 0, Some(excel_diff::CellValue::Number(*val)), None);
    }
    grid
}

#[test]
fn grid_leaf_diff_matches_single_sheet_workbook() {
    let mut pool = StringPool::new();
    let sheet_id = pool.intern("<grid>");

    let grid_a = make_grid(&[1.0, 2.0]);
    let grid_b = make_grid(&[1.0, 3.0]);

    let wb_a = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_a.clone(),
        }],
        ..Default::default()
    };
    let wb_b = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_b.clone(),
        }],
        ..Default::default()
    };

    let config = DiffConfig::default();
    let leaf_report = diff_grids_with_pool(&grid_a, &grid_b, &mut pool, &config);
    let wb_report = diff_workbooks_with_pool(&wb_a, &wb_b, &mut pool, &config);

    assert_eq!(leaf_report.ops, wb_report.ops);
    assert_eq!(leaf_report.complete, wb_report.complete);
    assert_eq!(leaf_report.warnings, wb_report.warnings);
}

#[test]
fn sheet_leaf_diff_matches_single_sheet_workbook() {
    let mut pool = StringPool::new();
    let sheet_id = pool.intern("Sheet1");

    let grid_a = make_grid(&[1.0, 2.0]);
    let grid_b = make_grid(&[1.0, 3.0]);

    let sheet_a = Sheet {
        name: sheet_id,
        workbook_sheet_id: None,
        kind: SheetKind::Worksheet,
        grid: grid_a.clone(),
    };
    let sheet_b = Sheet {
        name: sheet_id,
        workbook_sheet_id: None,
        kind: SheetKind::Worksheet,
        grid: grid_b.clone(),
    };

    let wb_a = Workbook {
        sheets: vec![sheet_a.clone()],
        ..Default::default()
    };
    let wb_b = Workbook {
        sheets: vec![sheet_b.clone()],
        ..Default::default()
    };

    let config = DiffConfig::default();
    let leaf_report = diff_sheets_with_pool(&sheet_a, &sheet_b, &mut pool, &config);
    let wb_report = diff_workbooks_with_pool(&wb_a, &wb_b, &mut pool, &config);

    assert_eq!(leaf_report.ops, wb_report.ops);
    assert_eq!(leaf_report.complete, wb_report.complete);
    assert_eq!(leaf_report.warnings, wb_report.warnings);
}

```

---

### File: `core\tests\limit_behavior_tests.rs`

```rust
mod common;

use common::{sid, single_sheet_workbook};
use excel_diff::{
    CellValue, DiffConfig, DiffError, DiffOp, DiffReport, Grid, LimitBehavior, Workbook,
    WorkbookPackage, try_diff_workbooks_with_pool, with_default_session,
};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn try_diff_workbooks(
    old: &Workbook,
    new: &Workbook,
    config: &DiffConfig,
) -> Result<DiffReport, DiffError> {
    with_default_session(|session| {
        try_diff_workbooks_with_pool(old, new, &mut session.strings, config)
    })
}

fn create_simple_grid(nrows: u32, ncols: u32, base_value: i32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number(
                    (base_value as i64 + row as i64 * 1000 + col as i64) as f64,
                )),
                None,
            );
        }
    }
    grid
}

fn count_ops(ops: &[DiffOp], predicate: impl Fn(&DiffOp) -> bool) -> usize {
    ops.iter().filter(|op| predicate(op)).count()
}

#[test]
fn large_grid_completes_within_default_limits() {
    let grid_a = create_simple_grid(1000, 10, 0);
    let mut grid_b = create_simple_grid(1000, 10, 0);
    grid_b.insert_cell(500, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "1000-row grid should complete within default limits"
    );
    assert!(
        report.warnings.is_empty(),
        "should have no warnings for successful diff"
    );
    assert!(
        count_ops(&report.ops, |op| matches!(op, DiffOp::CellEdited { .. })) >= 1,
        "should detect the cell edit"
    );
}

#[test]
fn limit_exceeded_fallback_to_positional() {
    let grid_a = create_simple_grid(100, 10, 0);
    let mut grid_b = create_simple_grid(100, 10, 0);
    grid_b.insert_cell(50, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::FallbackToPositional;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "FallbackToPositional should still produce a complete result"
    );
    assert!(
        report.warnings.is_empty(),
        "FallbackToPositional should not add warnings"
    );
    assert!(
        count_ops(&report.ops, |op| matches!(op, DiffOp::CellEdited { .. })) >= 1,
        "should detect the cell edit via positional diff"
    );
}

#[test]
fn limit_exceeded_return_partial_result() {
    let grid_a = create_simple_grid(100, 10, 0);
    let mut grid_b = create_simple_grid(100, 10, 0);
    grid_b.insert_cell(50, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnPartialResult;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        !report.complete,
        "ReturnPartialResult should mark report as incomplete"
    );
    assert!(
        !report.warnings.is_empty(),
        "ReturnPartialResult should add a warning about limits exceeded"
    );
    assert!(
        report.warnings[0].contains("limits exceeded"),
        "warning should mention limits exceeded"
    );
    assert!(
        !report.ops.is_empty(),
        "ReturnPartialResult should still produce ops via positional diff"
    );
}

#[test]
fn limit_exceeded_return_error_returns_structured_error() {
    let grid_a = create_simple_grid(100, 10, 0);
    let grid_b = create_simple_grid(100, 10, 0);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let result = try_diff_workbooks(&wb_a, &wb_b, &config);
    assert!(result.is_err(), "should return error when limits exceeded");

    let err = result.unwrap_err();
    match err {
        DiffError::LimitsExceeded {
            sheet,
            rows,
            cols,
            max_rows,
            max_cols,
        } => {
            assert_eq!(sheet, sid("Sheet1"));
            assert_eq!(rows, 100);
            assert_eq!(cols, 10);
            assert_eq!(max_rows, 50);
            assert_eq!(max_cols, 16384);
        }
        _ => panic!("unexpected error variant: {err:?}"),
    }
}

#[test]
fn limit_exceeded_return_error_produces_warning_via_legacy_api() {
    let grid_a = create_simple_grid(100, 10, 0);
    let grid_b = create_simple_grid(100, 10, 0);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let report = diff_workbooks(&wb_a, &wb_b, &config);
    assert!(!report.complete, "report should be incomplete");
    assert!(
        report.warnings.iter().any(|w| w.contains("limits exceeded")),
        "should have limits exceeded warning; warnings: {:?}",
        report.warnings
    );
}

#[test]
fn column_limit_exceeded() {
    let grid_a = create_simple_grid(10, 100, 0);
    let mut grid_b = create_simple_grid(10, 100, 0);
    grid_b.insert_cell(5, 50, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_cols = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnPartialResult;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        !report.complete,
        "should be marked incomplete when column limit exceeded"
    );
    assert!(
        !report.warnings.is_empty(),
        "should have warning about column limit"
    );
}

#[test]
fn within_limits_no_warning() {
    let grid_a = create_simple_grid(45, 10, 0);
    let mut grid_b = create_simple_grid(45, 10, 0);
    grid_b.insert_cell(20, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Sheet1", grid_a);
    let wb_b = single_sheet_workbook("Sheet1", grid_b);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnPartialResult;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "should be complete when within limits");
    assert!(
        report.warnings.is_empty(),
        "should have no warnings when within limits"
    );
}

#[test]
fn multiple_sheets_limit_warning_includes_sheet_name() {
    let grid_small = create_simple_grid(10, 5, 0);
    let grid_large_a = create_simple_grid(100, 10, 1000);
    let grid_large_b = create_simple_grid(100, 10, 2000);

    let wb_a = excel_diff::Workbook {
        sheets: vec![
            excel_diff::Sheet {
                name: sid("SmallSheet"),
                workbook_sheet_id: None,
                kind: excel_diff::SheetKind::Worksheet,
                grid: grid_small.clone(),
            },
            excel_diff::Sheet {
                name: sid("LargeSheet"),
                workbook_sheet_id: None,
                kind: excel_diff::SheetKind::Worksheet,
                grid: grid_large_a,
            },
        ],
        ..Default::default()
    };

    let wb_b = excel_diff::Workbook {
        sheets: vec![
            excel_diff::Sheet {
                name: sid("SmallSheet"),
                workbook_sheet_id: None,
                kind: excel_diff::SheetKind::Worksheet,
                grid: grid_small,
            },
            excel_diff::Sheet {
                name: sid("LargeSheet"),
                workbook_sheet_id: None,
                kind: excel_diff::SheetKind::Worksheet,
                grid: grid_large_b,
            },
        ],
        ..Default::default()
    };

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 50;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnPartialResult;

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(!report.complete, "should be incomplete due to large sheet");
    assert!(
        report.warnings.iter().any(|w| w.contains("LargeSheet")),
        "warning should reference the sheet that exceeded limits"
    );
}

#[test]
fn large_grid_5k_rows_completes_within_default_limits() {
    let grid_a = create_simple_grid(5000, 10, 0);
    let mut grid_b = create_simple_grid(5000, 10, 0);
    grid_b.insert_cell(2500, 5, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("LargeSheet", grid_a);
    let wb_b = single_sheet_workbook("LargeSheet", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "5000-row grid should complete within default limits (max_align_rows=500000)"
    );
    assert!(
        report.warnings.is_empty(),
        "should have no warnings for successful large grid diff"
    );
    assert!(
        count_ops(&report.ops, |op| matches!(op, DiffOp::CellEdited { .. })) >= 1,
        "should detect the cell edit in large grid"
    );
}

#[test]
fn wide_grid_500_cols_completes_within_default_limits() {
    let grid_a = create_simple_grid(100, 500, 0);
    let mut grid_b = create_simple_grid(100, 500, 0);
    grid_b.insert_cell(50, 250, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("WideSheet", grid_a);
    let wb_b = single_sheet_workbook("WideSheet", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "500-column grid should complete within default limits (max_align_cols=16384)"
    );
    assert!(
        report.warnings.is_empty(),
        "should have no warnings for successful wide grid diff"
    );
    assert!(
        count_ops(&report.ops, |op| matches!(op, DiffOp::CellEdited { .. })) >= 1,
        "should detect the cell edit in wide grid"
    );
}

```

---

### File: `core\tests\m10_embedded_m_diff_tests.rs`

```rust
use excel_diff::{DiffConfig, DiffOp, DiffReport, QueryChangeKind, WorkbookPackage};
use std::fs::File;

mod common;
use common::fixture_path;

fn load_package(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).expect("fixture file should open");
    WorkbookPackage::open(file).expect("fixture should parse as WorkbookPackage")
}

fn m_ops(report: &DiffReport) -> Vec<&DiffOp> {
    report.m_ops().collect()
}

fn resolve_name<'a>(report: &'a DiffReport, op: &DiffOp) -> &'a str {
    let name_id = match op {
        DiffOp::QueryAdded { name } => *name,
        DiffOp::QueryRemoved { name } => *name,
        DiffOp::QueryRenamed { from, .. } => *from,
        DiffOp::QueryDefinitionChanged { name, .. } => *name,
        DiffOp::QueryMetadataChanged { name, .. } => *name,
        _ => panic!("not a query op"),
    };
    &report.strings[name_id.0 as usize]
}

#[test]
fn embedded_only_change_produces_embedded_definitionchanged() {
    let pkg_a = load_package("m_embedded_change_a.xlsx");
    let pkg_b = load_package("m_embedded_change_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    assert_eq!(ops.len(), 1, "expected exactly one diff for embedded change");

    let def_changed: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }))
        .collect();

    assert_eq!(def_changed.len(), 1, "expected one definition change");

    match def_changed[0] {
        DiffOp::QueryDefinitionChanged { change_kind, .. } => {
            assert_eq!(*change_kind, QueryChangeKind::Semantic);
        }
        _ => unreachable!(),
    }

    assert_eq!(
        resolve_name(&report, def_changed[0]),
        "Embedded/Content/efgh.package/Section1/Inner"
    );
}

```

---

### File: `core\tests\m10_m_parser_tier2_tests.rs`

```rust
use excel_diff::{MAstKind, ast_semantically_equal, canonicalize_m_ast, parse_m_expression};

fn kind(expr: &str) -> MAstKind {
    let mut ast = parse_m_expression(expr).expect("parse should succeed");
    canonicalize_m_ast(&mut ast);
    ast.root_kind_for_testing()
}

fn canon(expr: &str) -> excel_diff::MModuleAst {
    let mut ast = parse_m_expression(expr).expect("parse should succeed");
    canonicalize_m_ast(&mut ast);
    ast
}

#[test]
fn parse_function_literals() {
    assert_eq!(
        kind("(x) => x"),
        MAstKind::FunctionLiteral { param_count: 1 }
    );
    assert_eq!(
        kind("(x, y) => x + y"),
        MAstKind::FunctionLiteral { param_count: 2 }
    );
}

#[test]
fn parse_unary_ops() {
    assert_eq!(kind("not true"), MAstKind::UnaryOp);
    assert_eq!(kind("-1"), MAstKind::Primitive);
    assert_eq!(kind("-(1)"), MAstKind::UnaryOp);
}

#[test]
fn precedence_mul_binds_tighter_than_add() {
    let a = canon("1 + 2 * 3");
    let b = canon("1 + (2 * 3)");
    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn precedence_and_binds_tighter_than_or() {
    let a = canon("a or b and c");
    let b = canon("a or (b and c)");
    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn formatting_only_function_literal_equal() {
    let a = canon("(x)=>x");
    let b = canon("( x ) => x");
    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn formatting_only_type_ascription_equal() {
    let a = canon("x as Number");
    let b = canon("x as number");
    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn formatting_only_precedence_equal() {
    let a = canon("1+(2*3)");
    let b = canon("1 + 2 * 3");
    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn parse_try_otherwise() {
    assert_eq!(kind("try 1 otherwise 0"), MAstKind::TryOtherwise);
    let a = canon("try 1 otherwise 0");
    let b = canon("try (1) otherwise (0)");
    assert!(ast_semantically_equal(&a, &b));
}

```

---

### File: `core\tests\m4_package_parts_tests.rs`

```rust
use std::io::{Cursor, Write};

use excel_diff::{
    DataMashupError, DataMashupLimits, open_data_mashup, parse_package_parts,
    parse_package_parts_with_limits, parse_section_members,
};
use zip::write::FileOptions;
use zip::{CompressionMethod, ZipWriter};

mod common;
use common::fixture_path;

const MIN_PACKAGE_XML: &str = "<Package></Package>";
const MIN_SECTION: &str = "section Section1;\nshared Foo = 1;";
const BOM_SECTION: &str = "\u{FEFF}section Section1;\nshared Foo = 1;";

#[test]
fn package_parts_contains_expected_entries() {
    let path = fixture_path("one_query.xlsx");
    let raw = open_data_mashup(&path)
        .expect("fixture should open")
        .expect("mashup should be present");

    let parts = parse_package_parts(&raw.package_parts).expect("PackageParts should parse");

    assert!(!parts.package_xml.raw_xml.is_empty());
    assert!(
        parts.main_section.source.contains("section Section1;"),
        "main Section1.m should be present"
    );
    assert!(
        parts.main_section.source.contains("shared"),
        "at least one shared query should be present"
    );
    assert!(
        parts.embedded_contents.is_empty(),
        "one_query.xlsx should not contain embedded contents"
    );
}

#[test]
fn embedded_content_detection() {
    let path = fixture_path("multi_query_with_embedded.xlsx");
    let raw = open_data_mashup(&path)
        .expect("fixture should open")
        .expect("mashup should be present");

    let parts = parse_package_parts(&raw.package_parts).expect("PackageParts should parse");

    assert!(
        !parts.embedded_contents.is_empty(),
        "multi_query_with_embedded.xlsx should expose at least one embedded content"
    );

    for embedded in &parts.embedded_contents {
        assert!(
            embedded.section.source.contains("section Section1"),
            "embedded Section1.m should be present for {}",
            embedded.name
        );
        assert!(
            embedded.section.source.contains("shared"),
            "embedded Section1.m should contain at least one shared member for {}",
            embedded.name
        );
    }
}

#[test]
fn parse_package_parts_rejects_non_zip() {
    let bogus = b"this is not a zip file";
    let err = parse_package_parts(bogus).expect_err("non-zip bytes should fail");
    assert!(matches!(err, DataMashupError::FramingInvalid));
}

#[test]
fn missing_config_package_xml_errors() {
    let bytes = build_zip(vec![(
        "Formulas/Section1.m",
        MIN_SECTION.as_bytes().to_vec(),
    )]);
    let err = parse_package_parts(&bytes)
        .expect_err("missing Config/Package.xml should be framing invalid");
    assert!(matches!(err, DataMashupError::FramingInvalid));
}

#[test]
fn missing_section1_errors() {
    let bytes = build_zip(vec![(
        "Config/Package.xml",
        MIN_PACKAGE_XML.as_bytes().to_vec(),
    )]);
    let err = parse_package_parts(&bytes)
        .expect_err("missing Formulas/Section1.m should be framing invalid");
    assert!(matches!(err, DataMashupError::FramingInvalid));
}

#[test]
fn invalid_utf8_in_package_xml_errors() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", vec![0xFF, 0xFF, 0xFF]),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
    ]);
    let err = parse_package_parts(&bytes).expect_err("invalid UTF-8 in Package.xml should error");
    assert!(matches!(err, DataMashupError::FramingInvalid));
}

#[test]
fn invalid_utf8_in_section1_errors() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", vec![0xFF, 0xFF]),
    ]);

    let err = parse_package_parts(&bytes).expect_err("invalid UTF-8 in Section1.m should error");
    assert!(matches!(err, DataMashupError::FramingInvalid));
}

#[test]
fn embedded_content_invalid_zip_is_skipped() {
    let bytes =
        build_minimal_package_parts_with(vec![("Content/bogus.package", b"not a zip".to_vec())]);
    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert!(parts.embedded_contents.is_empty());
}

#[test]
fn embedded_content_missing_section1_is_skipped() {
    let nested = build_zip(vec![("Config/Formulas.xml", b"<Formulas/>".to_vec())]);
    let bytes = build_minimal_package_parts_with(vec![("Content/no_section1.package", nested)]);
    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert!(parts.embedded_contents.is_empty());
}

#[test]
fn embedded_content_invalid_utf8_is_skipped() {
    let nested = build_zip(vec![("Formulas/Section1.m", vec![0xFF, 0xFF])]);
    let bytes = build_minimal_package_parts_with(vec![("Content/bad_utf8.package", nested)]);
    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert!(parts.embedded_contents.is_empty());
}

#[test]
fn embedded_content_partial_failure_retains_valid_entries() {
    let good_nested = build_embedded_section_zip(MIN_SECTION.as_bytes().to_vec());
    let bytes = build_minimal_package_parts_with(vec![
        ("Content/good.package", good_nested),
        ("Content/bad.package", b"not a zip".to_vec()),
    ]);

    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert_eq!(parts.embedded_contents.len(), 1);
    let embedded = &parts.embedded_contents[0];
    assert_eq!(embedded.name, "Content/good.package");
    assert!(embedded.section.source.contains("section Section1;"));
    assert!(embedded.section.source.contains("shared"));
}

#[test]
fn unpacked_embedded_section_is_extracted() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/abcd/Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
    ]);

    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert_eq!(parts.embedded_contents.len(), 1);
    assert_eq!(parts.embedded_contents[0].name, "Content/abcd");
    assert!(parts.embedded_contents[0]
        .section
        .source
        .contains("shared Foo = 1;"));
}

#[test]
fn leading_slash_paths_are_accepted() {
    let embedded =
        build_embedded_section_zip("section Section1;\nshared Bar = 2;".as_bytes().to_vec());
    let bytes = build_zip(vec![
        (
            "/Config/Package.xml",
            br#"<Package from="leading"/>"#.to_vec(),
        ),
        ("/Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("/Content/abcd.package", embedded),
        (
            "Config/Package.xml",
            br#"<Package from="canonical"/>"#.to_vec(),
        ),
    ]);

    let parts = parse_package_parts(&bytes).expect("leading slash entries should parse");
    assert!(
        parts.package_xml.raw_xml.contains(r#"from="leading""#),
        "first encountered Package.xml should win"
    );
    assert!(parts.main_section.source.contains("shared Foo = 1;"));
    assert_eq!(parts.embedded_contents.len(), 1);
    assert!(
        parts.embedded_contents[0]
            .section
            .source
            .contains("shared Bar = 2;")
    );
}

#[test]
fn backslash_paths_are_normalized_for_embedded() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        (
            "Content\\abcd\\Formulas\\Section1.m",
            MIN_SECTION.as_bytes().to_vec(),
        ),
    ]);

    let parts = parse_package_parts(&bytes).expect("backslash paths should parse");
    assert_eq!(parts.embedded_contents.len(), 1);
    assert_eq!(parts.embedded_contents[0].name, "Content/abcd");
}

#[test]
fn embedded_content_name_is_canonicalized() {
    let nested = build_embedded_section_zip(MIN_SECTION.as_bytes().to_vec());
    let bytes = build_minimal_package_parts_with(vec![("/Content/efgh.package", nested)]);

    let parts =
        parse_package_parts(&bytes).expect("embedded content with leading slash should parse");
    assert_eq!(parts.embedded_contents.len(), 1);
    assert_eq!(parts.embedded_contents[0].name, "Content/efgh.package");
}

#[test]
fn empty_content_directory_is_ignored() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/", Vec::new()),
    ]);

    let parts = parse_package_parts(&bytes).expect("package with empty Content/ directory parses");
    assert!(!parts.package_xml.raw_xml.is_empty());
    assert!(!parts.main_section.source.is_empty());
    assert!(
        parts.embedded_contents.is_empty(),
        "bare Content/ directory should not produce embedded contents"
    );
}

#[test]
fn parse_package_parts_never_panics_on_random_bytes() {
    for seed in 0u64..64 {
        let len = (seed as usize * 13 % 256) + (seed as usize % 7);
        let bytes = random_bytes(seed, len);
        let _ = parse_package_parts(&bytes);
    }
}

#[test]
fn package_parts_section1_with_bom_parses_via_parse_section_members() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", BOM_SECTION.as_bytes().to_vec()),
    ]);

    let parts = parse_package_parts(&bytes).expect("BOM-prefixed Section1.m should parse");
    assert!(
        !parts.main_section.source.starts_with('\u{FEFF}'),
        "PackageParts should strip a single leading BOM from Section1.m"
    );
    let members = parse_section_members(&parts.main_section.source)
        .expect("parse_section_members should accept BOM-prefixed Section1");
    assert_eq!(members.len(), 1);
    assert_eq!(members[0].member_name, "Foo");
    assert_eq!(members[0].section_name, "Section1");
}

#[test]
fn embedded_content_section1_with_bom_parses_via_parse_section_members() {
    let embedded = build_embedded_section_zip(BOM_SECTION.as_bytes().to_vec());
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/bom_embedded.package", embedded),
    ]);

    let parts = parse_package_parts(&bytes).expect("outer package should parse");
    assert!(
        !parts.embedded_contents.is_empty(),
        "embedded package should be detected"
    );

    let embedded = parts
        .embedded_contents
        .iter()
        .find(|entry| entry.name == "Content/bom_embedded.package")
        .expect("expected embedded package to round-trip name");

    assert!(
        !embedded.section.source.starts_with('\u{FEFF}'),
        "embedded Section1.m should strip leading BOM"
    );

    let members = parse_section_members(&embedded.section.source)
        .expect("parse_section_members should accept embedded BOM Section1");
    assert!(
        !members.is_empty(),
        "embedded Section1.m should contain members"
    );
    assert!(
        members.iter().any(|member| {
            member.section_name == "Section1"
                && member.member_name == "Foo"
                && member.expression_m == "1"
        }),
        "embedded Section1.m should parse shared Foo = 1"
    );
}

#[test]
fn package_parts_with_limits_rejects_oversized_required_part() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", vec![b'a'; 200]),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 100,
        max_inner_part_bytes: 100,
        max_inner_total_bytes: 10_000,
    };

    let err =
        parse_package_parts_with_limits(&bytes, limits).expect_err("expected oversized part error");

    match err {
        DataMashupError::InnerPartTooLarge { path, size, limit } => {
            assert_eq!(path, "Formulas/Section1.m");
            assert_eq!(size, 200);
            assert_eq!(limit, 100);
        }
        other => panic!("expected InnerPartTooLarge, got {other:?}"),
    }
}

#[test]
fn package_parts_with_limits_rejects_too_many_entries() {
    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Foo.txt", b"1".to_vec()),
        ("Bar.txt", b"2".to_vec()),
        ("Baz.txt", b"3".to_vec()),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 4,
        max_inner_part_bytes: 10_000,
        max_inner_total_bytes: 10_000,
    };

    let err =
        parse_package_parts_with_limits(&bytes, limits).expect_err("expected too-many-entries error");
    match err {
        DataMashupError::InnerTooManyEntries {
            entries,
            max_entries,
        } => {
            assert_eq!(entries, 5);
            assert_eq!(max_entries, 4);
        }
        other => panic!("expected InnerTooManyEntries, got {other:?}"),
    }
}

#[test]
fn package_parts_with_limits_rejects_total_size() {
    let base = (MIN_PACKAGE_XML.len() + MIN_SECTION.len()) as u64;
    let limit = base + 10;

    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/too_much.package", vec![0u8; 11]),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 100,
        max_inner_part_bytes: 10_000,
        max_inner_total_bytes: limit,
    };

    let err =
        parse_package_parts_with_limits(&bytes, limits).expect_err("expected total-size error");
    match err {
        DataMashupError::InnerTotalTooLarge { limit: got } => {
            assert_eq!(got, limit);
        }
        other => panic!("expected InnerTotalTooLarge, got {other:?}"),
    }
}

#[test]
fn package_parts_with_limits_accepts_part_at_limit() {
    let section_bytes = MIN_SECTION.as_bytes().to_vec();
    let max_part = section_bytes.len() as u64;
    let total = max_part + MIN_PACKAGE_XML.len() as u64;

    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", section_bytes),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 100,
        max_inner_part_bytes: max_part,
        max_inner_total_bytes: total,
    };

    let parts = parse_package_parts_with_limits(&bytes, limits)
        .expect("part size equal to limit should pass");
    assert!(parts.main_section.source.contains("shared Foo = 1;"));
}

#[test]
fn package_parts_with_limits_accepts_total_at_limit() {
    let extra = vec![b'a'; 10];
    let total =
        MIN_PACKAGE_XML.len() as u64 + MIN_SECTION.len() as u64 + extra.len() as u64;

    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/extra.package", extra),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 100,
        max_inner_part_bytes: 10_000,
        max_inner_total_bytes: total,
    };

    parse_package_parts_with_limits(&bytes, limits)
        .expect("total size equal to limit should pass");
}

#[test]
fn embedded_content_limits_apply_to_nested_packages() {
    let huge_section = vec![b'A'; 50_000];
    let nested = {
        let cursor = Cursor::new(Vec::new());
        let mut writer = ZipWriter::new(cursor);
        let options = FileOptions::default().compression_method(CompressionMethod::Deflated);
        writer
            .start_file("Formulas/Section1.m", options)
            .expect("start zip entry");
        writer
            .write_all(&huge_section)
            .expect("write zip entry");
        writer.finish().expect("finish zip").into_inner()
    };

    let bytes = build_zip(vec![
        ("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()),
        ("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()),
        ("Content/oversized.package", nested),
    ]);

    let limits = DataMashupLimits {
        max_inner_entries: 100,
        max_inner_part_bytes: 10_000,
        max_inner_total_bytes: 100_000,
    };

    let err =
        parse_package_parts_with_limits(&bytes, limits).expect_err("expected nested size error");
    match err {
        DataMashupError::InnerPartTooLarge { path, size, limit } => {
            assert_eq!(path, "Content/oversized.package/Formulas/Section1.m");
            assert_eq!(size, 50_000);
            assert_eq!(limit, 10_000);
        }
        other => panic!("expected InnerPartTooLarge, got {other:?}"),
    }
}

fn build_minimal_package_parts_with(entries: Vec<(&str, Vec<u8>)>) -> Vec<u8> {
    let mut all_entries = Vec::with_capacity(entries.len() + 2);
    all_entries.push(("Config/Package.xml", MIN_PACKAGE_XML.as_bytes().to_vec()));
    all_entries.push(("Formulas/Section1.m", MIN_SECTION.as_bytes().to_vec()));
    all_entries.extend(entries);
    build_zip(all_entries)
}

fn build_embedded_section_zip(section_bytes: Vec<u8>) -> Vec<u8> {
    build_zip(vec![("Formulas/Section1.m", section_bytes)])
}

fn build_zip(entries: Vec<(&str, Vec<u8>)>) -> Vec<u8> {
    let cursor = Cursor::new(Vec::new());
    let mut writer = ZipWriter::new(cursor);
    let options = FileOptions::default().compression_method(CompressionMethod::Stored);

    for (name, bytes) in entries {
        if name.ends_with('/') {
            writer
                .add_directory(name, options)
                .expect("start zip directory");
        } else {
            writer.start_file(name, options).expect("start zip entry");
            writer.write_all(&bytes).expect("write zip entry");
        }
    }

    writer.finish().expect("finish zip").into_inner()
}

fn random_bytes(seed: u64, len: usize) -> Vec<u8> {
    let mut bytes = Vec::with_capacity(len);
    let mut state = seed.wrapping_mul(6364136223846793005).wrapping_add(1);
    for _ in 0..len {
        state = state
            .wrapping_mul(2862933555777941757)
            .wrapping_add(3037000493);
        bytes.push((state >> 32) as u8);
    }
    bytes
}

```

---

### File: `core\tests\m4_permissions_metadata_tests.rs`

```rust
use excel_diff::{
    DataMashupError, Permissions, PermissionBindingsStatus, RawDataMashup, build_data_mashup,
    build_queries, open_data_mashup, parse_metadata, parse_package_parts, parse_section_members,
};

mod common;
use common::fixture_path;

fn load_datamashup(path: &str) -> excel_diff::DataMashup {
    let raw = open_data_mashup(fixture_path(path))
        .expect("fixture should load")
        .expect("DataMashup should be present");
    build_data_mashup(&raw).expect("DataMashup should build")
}

#[test]
fn permissions_parsed_flags_default_vs_firewall_off() {
    let defaults = load_datamashup("permissions_defaults.xlsx");
    let firewall_off = load_datamashup("permissions_firewall_off.xlsx");

    assert_eq!(defaults.version, 0);
    assert_eq!(firewall_off.version, 0);

    assert!(defaults.permissions.firewall_enabled);
    assert!(!defaults.permissions.can_evaluate_future_packages);
    assert!(!firewall_off.permissions.firewall_enabled);
    assert_eq!(
        defaults.permissions.workbook_group_type,
        firewall_off.permissions.workbook_group_type
    );
}

#[test]
fn permissions_missing_or_malformed_yields_defaults() {
    let base_raw = open_data_mashup(fixture_path("one_query.xlsx"))
        .expect("fixture should load")
        .expect("DataMashup should be present");

    let mut missing = base_raw.clone();
    missing.permissions = Vec::new();
    missing.permission_bindings = Vec::new();
    let dm = build_data_mashup(&missing).expect("missing permissions should default");
    assert_eq!(dm.permissions, Permissions::default());

    let mut malformed = base_raw.clone();
    malformed.permissions = b"<not-xml".to_vec();
    let dm = build_data_mashup(&malformed).expect("malformed permissions should default");
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn permissions_invalid_entities_yield_defaults() {
    let base_raw = open_data_mashup(fixture_path("one_query.xlsx"))
        .expect("fixture should load")
        .expect("DataMashup should be present");

    let invalid_permissions = br#"
        <Permissions>
            <CanEvaluateFuturePackages>&bad;</CanEvaluateFuturePackages>
            <FirewallEnabled>true</FirewallEnabled>
        </Permissions>
    "#;
    let mut raw = base_raw.clone();
    raw.permissions = invalid_permissions.to_vec();

    let dm = build_data_mashup(&raw).expect("invalid permissions entities should default");
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn metadata_empty_bytes_returns_empty_struct() {
    let metadata = parse_metadata(&[]).expect("empty metadata should parse");
    assert!(metadata.formulas.is_empty());
}

#[test]
fn metadata_invalid_header_too_short_errors() {
    let err = parse_metadata(&[0x01]).expect_err("short metadata should error");
    match err {
        DataMashupError::XmlError(msg) => {
            assert!(msg.contains("metadata XML not found"));
        }
        other => panic!("expected XmlError, got {other:?}"),
    }
}

#[test]
fn metadata_invalid_length_prefix_errors() {
    let mut bytes = Vec::new();
    bytes.extend_from_slice(&0u32.to_le_bytes());
    bytes.extend_from_slice(&100u32.to_le_bytes());
    bytes.extend_from_slice(&[0u8; 10]);

    let err = parse_metadata(&bytes).expect_err("invalid length prefix should error");
    match err {
        DataMashupError::InvalidHeader(msg) => {
            assert!(msg.contains("metadata length prefix invalid"));
        }
        other => panic!("expected InvalidHeader, got {other:?}"),
    }
}

#[test]
fn metadata_invalid_utf8_errors() {
    let mut bytes = Vec::new();
    bytes.extend_from_slice(&0u32.to_le_bytes());
    bytes.extend_from_slice(&2u32.to_le_bytes());
    bytes.extend_from_slice(&[0xFF, 0xFF]);

    let err = parse_metadata(&bytes).expect_err("invalid utf-8 should error");
    match err {
        DataMashupError::XmlError(msg) => {
            assert!(msg.contains("metadata is not valid UTF-8"));
        }
        other => panic!("expected XmlError, got {other:?}"),
    }
}

#[test]
fn metadata_malformed_xml_errors() {
    let xml = b"<LocalPackageMetadataFile><foo";
    let mut bytes = Vec::new();
    bytes.extend_from_slice(&0u32.to_le_bytes());
    bytes.extend_from_slice(&(xml.len() as u32).to_le_bytes());
    bytes.extend_from_slice(xml);

    let err = parse_metadata(&bytes).expect_err("malformed xml should error");
    match err {
        DataMashupError::XmlError(_) => {}
        other => panic!("expected XmlError, got {other:?}"),
    }
}

#[test]
fn metadata_formulas_match_section_members() {
    let raw = open_data_mashup(fixture_path("metadata_simple.xlsx"))
        .expect("fixture should load")
        .expect("DataMashup should be present");
    let package = parse_package_parts(&raw.package_parts).expect("package parts should parse");
    let metadata = parse_metadata(&raw.metadata).expect("metadata should parse");
    let members =
        parse_section_members(&package.main_section.source).expect("section members should parse");

    let section1_formulas: Vec<_> = metadata
        .formulas
        .iter()
        .filter(|m| m.section_name == "Section1" && !m.is_connection_only)
        .collect();

    assert_eq!(section1_formulas.len(), members.len());
    for meta in section1_formulas {
        assert!(!meta.formula_name.is_empty());
    }
}

#[test]
fn metadata_load_destinations_simple() {
    let dm = load_datamashup("metadata_simple.xlsx");
    let load_to_sheet = dm
        .metadata
        .formulas
        .iter()
        .find(|m| m.item_path == "Section1/LoadToSheet")
        .expect("LoadToSheet metadata missing");
    assert!(load_to_sheet.load_to_sheet);
    assert!(!load_to_sheet.load_to_model);
    assert!(!load_to_sheet.is_connection_only);

    let load_to_model = dm
        .metadata
        .formulas
        .iter()
        .find(|m| m.item_path == "Section1/LoadToModel")
        .expect("LoadToModel metadata missing");
    assert!(!load_to_model.load_to_sheet);
    assert!(load_to_model.load_to_model);
    assert!(!load_to_model.is_connection_only);
}

#[test]
fn metadata_groups_basic_hierarchy() {
    let dm = load_datamashup("metadata_query_groups.xlsx");
    let grouped = dm
        .metadata
        .formulas
        .iter()
        .find(|m| m.item_path == "Section1/GroupedFoo")
        .expect("GroupedFoo metadata missing");
    assert_eq!(grouped.group_path.as_deref(), Some("Inputs/DimTables"));

    let root = dm
        .metadata
        .formulas
        .iter()
        .find(|m| m.item_path == "Section1/RootQuery")
        .expect("RootQuery metadata missing");
    assert!(root.group_path.is_none());
}

#[test]
fn metadata_hidden_queries_connection_only() {
    let dm = load_datamashup("metadata_hidden_queries.xlsx");
    let has_connection_only = dm
        .metadata
        .formulas
        .iter()
        .any(|m| !m.load_to_sheet && !m.load_to_model && m.is_connection_only);
    assert!(has_connection_only);
}

#[test]
fn metadata_itempath_decodes_percent_encoded_utf8() {
    let xml = r#"
        <LocalPackageMetadataFile>
            <Formulas>
                <Item>
                    <ItemType>Formula</ItemType>
                    <ItemPath>Section1/Foo%20Bar%C3%A9</ItemPath>
                    <Entry Type="FillEnabled" Value="l1" />
                </Item>
            </Formulas>
        </LocalPackageMetadataFile>
    "#;

    let metadata = parse_metadata(xml.as_bytes()).expect("metadata should parse");
    assert_eq!(metadata.formulas.len(), 1);
    let item = &metadata.formulas[0];
    assert_eq!(item.item_path, "Section1/Foo Bar\u{00e9}");
    assert_eq!(item.section_name, "Section1");
    assert_eq!(item.formula_name, "Foo Bar\u{00e9}");
    assert!(item.load_to_sheet);
    assert!(!item.is_connection_only);
}

#[test]
fn metadata_itempath_decodes_space_and_slash() {
    let xml = r#"
        <LocalPackageMetadataFile>
            <Formulas>
                <Item>
                    <ItemType>Formula</ItemType>
                    <ItemPath>Section1/Foo%20Bar%2FInner</ItemPath>
                    <Entry Type="FillEnabled" Value="l1" />
                </Item>
            </Formulas>
        </LocalPackageMetadataFile>
    "#;

    let metadata = parse_metadata(xml.as_bytes()).expect("metadata should parse");
    assert_eq!(metadata.formulas.len(), 1);
    let item = &metadata.formulas[0];
    assert_eq!(item.item_path, "Section1/Foo Bar/Inner");
    assert_eq!(item.section_name, "Section1");
    assert_eq!(item.formula_name, "Foo Bar/Inner");
}

#[test]
fn permission_bindings_present_flag() {
    let dm = load_datamashup("permissions_defaults.xlsx");
    assert!(!dm.permission_bindings_raw.is_empty());
}

#[test]
fn permission_bindings_dpapi_blob_defaults_permissions() {
    let dm = load_datamashup("dpapi_blob_present.xlsx");
    assert_eq!(dm.permission_bindings_status, PermissionBindingsStatus::Unverifiable);
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn permission_bindings_missing_ok() {
    let base_raw = open_data_mashup(fixture_path("one_query.xlsx"))
        .expect("fixture should load")
        .expect("DataMashup should be present");

    let mut synthetic = RawDataMashup {
        permission_bindings: Vec::new(),
        ..base_raw.clone()
    };
    synthetic.permissions = Vec::new();
    synthetic.metadata = Vec::new();

    let dm = build_data_mashup(&synthetic).expect("empty bindings should build");
    assert!(dm.permission_bindings_raw.is_empty());
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn build_queries_is_compatible_with_metadata_simple() {
    let dm = load_datamashup("metadata_simple.xlsx");
    let queries = build_queries(&dm).expect("queries should build");
    assert!(!queries.is_empty());
}

```

---

### File: `core\tests\m5_query_domain_tests.rs`

```rust
use std::collections::HashSet;

use excel_diff::{
    build_data_mashup, build_embedded_queries, build_queries, open_data_mashup,
    parse_section_members,
};

mod common;
use common::fixture_path;

fn load_datamashup(path: &str) -> excel_diff::DataMashup {
    let raw = open_data_mashup(fixture_path(path))
        .expect("fixture should load")
        .expect("DataMashup should be present");
    build_data_mashup(&raw).expect("DataMashup should build")
}

#[test]
fn metadata_join_simple() {
    let dm = load_datamashup("metadata_simple.xlsx");
    let queries = build_queries(&dm).expect("queries should build");

    assert_eq!(queries.len(), 2);
    let names: HashSet<_> = queries.iter().map(|q| q.name.as_str()).collect();
    assert_eq!(
        names,
        HashSet::from(["Section1/LoadToSheet", "Section1/LoadToModel"])
    );

    let sheet = queries
        .iter()
        .find(|q| q.section_member == "LoadToSheet")
        .expect("LoadToSheet query missing");
    assert!(sheet.metadata.load_to_sheet);
    assert!(!sheet.metadata.load_to_model);

    let model = queries
        .iter()
        .find(|q| q.section_member == "LoadToModel")
        .expect("LoadToModel query missing");
    assert!(!model.metadata.load_to_sheet);
    assert!(model.metadata.load_to_model);
}

#[test]
fn metadata_join_url_encoding() {
    let dm = load_datamashup("metadata_url_encoding.xlsx");
    let queries = build_queries(&dm).expect("queries should build");

    assert_eq!(queries.len(), 1);
    let q = &queries[0];
    assert_eq!(q.name, "Section1/Query with space & #");
    assert_eq!(q.section_member, "Query with space & #");
    assert!(q.metadata.load_to_sheet || q.metadata.load_to_model);
}

#[test]
fn member_without_metadata_is_preserved() {
    let dm = load_datamashup("metadata_missing_entry.xlsx");
    assert!(dm.metadata.formulas.is_empty());
    let queries = build_queries(&dm).expect("queries should build");

    assert_eq!(queries.len(), 1);
    let q = &queries[0];
    assert_eq!(q.name, "Section1/MissingMetadata");
    assert_eq!(q.section_member, "MissingMetadata");
    assert_eq!(q.metadata.item_path, "Section1/MissingMetadata");
    assert!(!q.metadata.load_to_sheet);
    assert!(!q.metadata.load_to_model);
    assert!(q.metadata.is_connection_only);
    assert_eq!(q.metadata.group_path, None);
}

#[test]
fn query_names_unique() {
    let dm = load_datamashup("metadata_simple.xlsx");
    let queries = build_queries(&dm).expect("queries should build");

    let mut seen = HashSet::new();
    for q in &queries {
        assert!(seen.insert(&q.name));
    }
}

#[test]
fn metadata_orphan_entries() {
    let dm = load_datamashup("metadata_orphan_entries.xlsx");
    let queries = build_queries(&dm).expect("queries should build");

    assert_eq!(queries.len(), 1);
    assert_eq!(queries[0].name, "Section1/Foo");
    assert!(
        dm.metadata
            .formulas
            .iter()
            .any(|m| m.item_path == "Section1/Nonexistent")
    );
}

#[test]
fn queries_preserve_section_member_order() {
    let dm = load_datamashup("metadata_simple.xlsx");
    let members = parse_section_members(&dm.package_parts.main_section.source)
        .expect("Section1 should parse");
    let queries = build_queries(&dm).expect("queries should build");

    assert_eq!(members.len(), queries.len());
    for (idx, (member, query)) in members.iter().zip(queries.iter()).enumerate() {
        assert_eq!(
            query.section_member, member.member_name,
            "query at position {} should match Section1 member order",
            idx
        );
    }
}

#[test]
fn build_embedded_queries_include_expected_name() {
    let dm = load_datamashup("m_embedded_change_a.xlsx");
    let queries = build_embedded_queries(&dm);

    assert!(queries.iter().any(|q| {
        q.name == "Embedded/Content/efgh.package/Section1/Inner"
    }));
}

#[test]
fn build_queries_excludes_embedded_prefix() {
    let dm = load_datamashup("m_embedded_change_a.xlsx");
    let queries = build_queries(&dm).expect("queries should build");

    assert!(queries.iter().all(|q| !q.name.starts_with("Embedded/")));
}

```

---

### File: `core\tests\m6_textual_m_diff_tests.rs`

```rust
use excel_diff::{
    DiffConfig, DiffOp, DiffReport, QueryChangeKind, QueryMetadataField, WorkbookPackage,
};
use std::fs::File;

mod common;
use common::fixture_path;

fn load_package(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).expect("fixture file should open");
    WorkbookPackage::open(file).expect("fixture should parse as WorkbookPackage")
}

fn m_ops(report: &DiffReport) -> Vec<&DiffOp> {
    report.m_ops().collect()
}

fn resolve_name<'a>(report: &'a DiffReport, op: &DiffOp) -> &'a str {
    let name_id = match op {
        DiffOp::QueryAdded { name } => *name,
        DiffOp::QueryRemoved { name } => *name,
        DiffOp::QueryRenamed { from, .. } => *from,
        DiffOp::QueryDefinitionChanged { name, .. } => *name,
        DiffOp::QueryMetadataChanged { name, .. } => *name,
        _ => panic!("not a query op"),
    };
    &report.strings[name_id.0 as usize]
}

#[test]
fn basic_add_query_diff() {
    let pkg_a = load_package("m_add_query_a.xlsx");
    let pkg_b = load_package("m_add_query_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    assert_eq!(ops.len(), 1, "expected exactly one diff for added query");
    assert!(
        matches!(ops[0], DiffOp::QueryAdded { .. }),
        "expected QueryAdded"
    );
    assert_eq!(resolve_name(&report, ops[0]), "Section1/Bar");
}

#[test]
fn basic_remove_query_diff() {
    let pkg_a = load_package("m_remove_query_a.xlsx");
    let pkg_b = load_package("m_remove_query_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    assert_eq!(ops.len(), 1, "expected exactly one diff for removed query");
    assert!(
        matches!(ops[0], DiffOp::QueryRemoved { .. }),
        "expected QueryRemoved"
    );
    assert_eq!(resolve_name(&report, ops[0]), "Section1/Bar");
}

#[test]
fn literal_change_produces_definitionchanged() {
    let pkg_a = load_package("m_change_literal_a.xlsx");
    let pkg_b = load_package("m_change_literal_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    assert_eq!(ops.len(), 1, "expected one diff for changed literal");
    match ops[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(
                *change_kind,
                QueryChangeKind::Semantic,
                "literal change is semantic"
            );
            assert_ne!(
                old_hash, new_hash,
                "hashes should differ for semantic change"
            );
        }
        _ => panic!("expected QueryDefinitionChanged, got {:?}", ops[0]),
    }
    assert_eq!(resolve_name(&report, ops[0]), "Section1/Foo");
}

#[test]
fn metadata_change_produces_metadata_ops() {
    let pkg_a = load_package("m_metadata_only_change_a.xlsx");
    let pkg_b = load_package("m_metadata_only_change_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    assert!(
        !ops.is_empty(),
        "expected at least one diff for metadata change"
    );
    for op in &ops {
        match op {
            DiffOp::QueryMetadataChanged { field, .. } => {
                assert!(
                    matches!(
                        field,
                        QueryMetadataField::LoadToSheet
                            | QueryMetadataField::LoadToModel
                            | QueryMetadataField::GroupPath
                            | QueryMetadataField::ConnectionOnly
                    ),
                    "expected a recognized metadata field"
                );
            }
            _ => panic!("expected only QueryMetadataChanged ops, got {:?}", op),
        }
    }
}

#[test]
fn definition_and_metadata_change_produces_both() {
    let pkg_a = load_package("m_def_and_metadata_change_a.xlsx");
    let pkg_b = load_package("m_def_and_metadata_change_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    let has_definition_change = ops
        .iter()
        .any(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }));
    assert!(
        has_definition_change,
        "expected QueryDefinitionChanged when definition changes"
    );
}

#[test]
fn identical_workbooks_produce_no_diffs() {
    let pkg = load_package("one_query.xlsx");

    let report = pkg.diff(&pkg, &DiffConfig::default());
    let ops = m_ops(&report);

    assert!(
        ops.is_empty(),
        "identical WorkbookPackage should produce no M diffs"
    );
}

#[test]
fn rename_produces_query_renamed() {
    let pkg_a = load_package("m_rename_query_a.xlsx");
    let pkg_b = load_package("m_rename_query_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    let renamed_ops: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryRenamed { .. }))
        .collect();

    assert_eq!(
        renamed_ops.len(),
        1,
        "expected exactly one QueryRenamed op for rename scenario"
    );

    match renamed_ops[0] {
        DiffOp::QueryRenamed { from, to } => {
            let from_name = &report.strings[from.0 as usize];
            let to_name = &report.strings[to.0 as usize];
            assert_eq!(from_name, "Section1/Foo");
            assert_eq!(to_name, "Section1/Bar");
        }
        _ => unreachable!(),
    }
}

```

---

### File: `core\tests\m7_ast_canonicalization_tests.rs`

```rust
use excel_diff::{
    DataMashup, MAstKind, MParseError, MTokenDebug, ast_semantically_equal, build_data_mashup,
    build_queries, canonicalize_m_ast, open_data_mashup, parse_m_expression, tokenize_for_testing,
};

mod common;
use common::fixture_path;

fn load_datamashup(name: &str) -> DataMashup {
    let raw = open_data_mashup(fixture_path(name))
        .expect("fixture should open")
        .expect("DataMashup should be present");
    build_data_mashup(&raw).expect("DataMashup should build")
}

fn load_single_query_expression(workbook: &str) -> String {
    let dm = load_datamashup(workbook);
    let queries = build_queries(&dm).expect("queries should parse");
    queries
        .first()
        .expect("fixture should contain a query")
        .expression_m
        .clone()
}

fn load_query_expression(workbook: &str, query_name: &str) -> String {
    let dm = load_datamashup(workbook);
    let queries = build_queries(&dm).expect("queries should parse");
    queries
        .into_iter()
        .find(|q| q.name == query_name)
        .expect("expected query to exist")
        .expression_m
}

#[test]
fn parse_basic_let_query_succeeds() {
    let expr = load_single_query_expression("one_query.xlsx");

    let result = parse_m_expression(&expr);

    assert!(result.is_ok(), "expected parse to succeed");
}

#[test]
fn basic_let_query_ast_is_let() {
    let expr = load_single_query_expression("one_query.xlsx");

    let ast = parse_m_expression(&expr).expect("expected parse to succeed");
    match ast.root_kind_for_testing() {
        MAstKind::Let { binding_count } => {
            assert!(
                binding_count >= 1,
                "expected at least one binding in basic let query"
            );
        }
        other => panic!("expected let root, got {:?}", other),
    }
}

#[test]
fn nested_let_in_binding_parses_successfully() {
    let expr = r#"
        let
            Source = let x = 1 in x,
            Result = Source
        in
            Result
    "#;

    let mut ast = parse_m_expression(expr).expect("nested let should parse");
    let mut ast_again = ast.clone();

    canonicalize_m_ast(&mut ast);
    canonicalize_m_ast(&mut ast_again);

    assert!(
        ast_semantically_equal(&ast, &ast_again),
        "canonicalization should not change equality for nested lets"
    );
}

#[test]
fn nested_let_formatting_only_equal() {
    let expr_a = r#"
        let
            Source = let x = 1 in x,
            Result = Source
        in
            Result
    "#;
    let expr_b = r#"let Source = let x = 1 in x, Result = Source in Result"#;

    let mut ast_a = parse_m_expression(expr_a).expect("first nested let should parse");
    let mut ast_b = parse_m_expression(expr_b).expect("second nested let should parse");

    canonicalize_m_ast(&mut ast_a);
    canonicalize_m_ast(&mut ast_b);

    assert!(
        ast_semantically_equal(&ast_a, &ast_b),
        "formatting-only differences with nested lets should compare equal"
    );
}

#[test]
fn formatting_only_queries_semantically_equal() {
    let expr_a = load_query_expression("m_formatting_only_a.xlsx", "Section1/FormatTest");
    let expr_b = load_query_expression("m_formatting_only_b.xlsx", "Section1/FormatTest");

    let mut ast_a = parse_m_expression(&expr_a).expect("formatting-only A should parse");
    let mut ast_b = parse_m_expression(&expr_b).expect("formatting-only B should parse");

    canonicalize_m_ast(&mut ast_a);
    canonicalize_m_ast(&mut ast_b);

    assert!(
        ast_semantically_equal(&ast_a, &ast_b),
        "formatting-only variants should be equal after canonicalization"
    );
}

#[test]
fn formatting_only_variant_detects_semantic_change() {
    let expr_b = load_query_expression("m_formatting_only_b.xlsx", "Section1/FormatTest");
    let expr_variant =
        load_query_expression("m_formatting_only_b_variant.xlsx", "Section1/FormatTest");

    let mut ast_b = parse_m_expression(&expr_b).expect("formatting-only B should parse");
    let mut ast_variant =
        parse_m_expression(&expr_variant).expect("formatting-only B variant should parse");

    canonicalize_m_ast(&mut ast_b);
    canonicalize_m_ast(&mut ast_variant);

    assert!(
        !ast_semantically_equal(&ast_b, &ast_variant),
        "semantic change should be detected even after canonicalization"
    );
}

#[test]
fn malformed_query_yields_parse_error() {
    let malformed = "let\n    Source = 1\n// missing 'in' and expression";

    let result = parse_m_expression(malformed);

    assert!(
        matches!(
            result,
            Err(MParseError::MissingInClause | MParseError::InvalidLetBinding)
        ),
        "missing 'in' should produce a parse error"
    );
}

#[test]
fn empty_expression_is_error() {
    let cases = ["", "   // only comment", "/* only block comment */"];

    for case in cases {
        let result = parse_m_expression(case);
        assert!(
            matches!(result, Err(MParseError::Empty)),
            "empty or comment-only input should return Empty, got {:?}",
            result
        );
    }
}

#[test]
fn unterminated_string_yields_error() {
    let result = parse_m_expression("\"unterminated");

    assert!(
        matches!(result, Err(MParseError::UnterminatedString)),
        "unterminated string should surface the correct error"
    );
}

#[test]
fn unterminated_block_comment_yields_error() {
    let result = parse_m_expression("let Source = 1 /* unterminated");

    assert!(
        matches!(result, Err(MParseError::UnterminatedBlockComment)),
        "unterminated block comment should surface the correct error"
    );
}

#[test]
fn unbalanced_delimiter_yields_error() {
    let cases = [
        "let Source = (1",
        "let Source = [1",
        "let Source = {1",
        "let Source = (1]",
    ];

    for case in cases {
        let result = parse_m_expression(case);
        assert!(
            matches!(result, Err(MParseError::UnbalancedDelimiter)),
            "unbalanced delimiters should error, got {:?}",
            result
        );
    }
}

#[test]
fn canonicalization_is_idempotent() {
    let expr = load_query_expression("m_formatting_only_b.xlsx", "Section1/FormatTest");

    let mut ast_once = parse_m_expression(&expr).expect("formatting-only B should parse");
    let mut ast_twice = ast_once.clone();

    canonicalize_m_ast(&mut ast_once);
    canonicalize_m_ast(&mut ast_twice);
    canonicalize_m_ast(&mut ast_twice);

    assert_eq!(
        ast_once, ast_twice,
        "canonicalization should produce a stable AST"
    );
}

#[test]
fn hash_date_tokenization_is_atomic() {
    let tokens = tokenize_for_testing(r#"#"Foo" = #date(2020,1,1)"#)
        .expect("hash literal tokenization should succeed");

    let expected = vec![
        MTokenDebug::Identifier("Foo".to_string()),
        MTokenDebug::Symbol('='),
        MTokenDebug::Identifier("#date".to_string()),
        MTokenDebug::Symbol('('),
        MTokenDebug::Number("2020".to_string()),
        MTokenDebug::Symbol(','),
        MTokenDebug::Number("1".to_string()),
        MTokenDebug::Symbol(','),
        MTokenDebug::Number("1".to_string()),
        MTokenDebug::Symbol(')'),
    ];

    assert_eq!(
        expected, tokens,
        "hash-prefixed literals should be lexed as single identifiers"
    );
}

```

---

### File: `core\tests\m7_semantic_m_diff_tests.rs`

```rust
use excel_diff::{
    DiffConfig, DiffOp, DiffReport, QueryChangeKind, SemanticNoisePolicy, WorkbookPackage,
};
use std::fs::File;

mod common;
use common::fixture_path;

fn load_package(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).expect("fixture file should open");
    WorkbookPackage::open(file).expect("fixture should parse as WorkbookPackage")
}

fn m_ops(report: &DiffReport) -> Vec<&DiffOp> {
    report.m_ops().collect()
}

fn resolve_name<'a>(report: &'a DiffReport, op: &DiffOp) -> &'a str {
    let name_id = match op {
        DiffOp::QueryAdded { name } => *name,
        DiffOp::QueryRemoved { name } => *name,
        DiffOp::QueryRenamed { from, .. } => *from,
        DiffOp::QueryDefinitionChanged { name, .. } => *name,
        DiffOp::QueryMetadataChanged { name, .. } => *name,
        _ => panic!("not a query op"),
    };
    &report.strings[name_id.0 as usize]
}

#[test]
fn formatting_only_diff_produces_formatting_only_change() {
    let pkg_a = load_package("m_formatting_only_a.xlsx");
    let pkg_b = load_package("m_formatting_only_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    let def_changed: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }))
        .collect();

    assert_eq!(
        def_changed.len(),
        1,
        "formatting-only changes should produce QueryDefinitionChanged with FormattingOnly kind"
    );

    match def_changed[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(
                *change_kind,
                QueryChangeKind::FormattingOnly,
                "formatting-only diff should have FormattingOnly change kind"
            );
            assert_eq!(
                old_hash, new_hash,
                "formatting-only changes have equal canonical hashes"
            );
        }
        _ => unreachable!(),
    }
    assert_eq!(resolve_name(&report, def_changed[0]), "Section1/FormatTest");
}

#[test]
fn semantic_gate_disabled_produces_semantic_change() {
    let pkg_a = load_package("m_formatting_only_a.xlsx");
    let pkg_b = load_package("m_formatting_only_b.xlsx");

    let mut config = DiffConfig::default();
    config.semantic.enable_m_semantic_diff = false;

    let report = pkg_a.diff(&pkg_b, &config);
    let ops = m_ops(&report);

    let def_changed: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }))
        .collect();

    assert_eq!(
        def_changed.len(),
        1,
        "disabling semantic gate should surface formatting-only differences as Semantic"
    );

    match def_changed[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(
                *change_kind,
                QueryChangeKind::Semantic,
                "with semantic diff disabled, changes are reported as Semantic"
            );
            assert_ne!(
                old_hash, new_hash,
                "textual hashes should differ when semantic diff is disabled"
            );
        }
        _ => unreachable!(),
    }
    assert_eq!(resolve_name(&report, def_changed[0]), "Section1/FormatTest");
}

#[test]
fn formatting_variant_with_real_change_still_reports_semantic() {
    let pkg_b = load_package("m_formatting_only_b.xlsx");
    let pkg_b_variant = load_package("m_formatting_only_b_variant.xlsx");

    let report = pkg_b.diff(&pkg_b_variant, &DiffConfig::default());
    let ops = m_ops(&report);

    let def_changed: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }))
        .collect();

    assert_eq!(
        def_changed.len(),
        1,
        "expected exactly one diff for semantic change"
    );

    match def_changed[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(
                *change_kind,
                QueryChangeKind::Semantic,
                "real change should be reported as Semantic"
            );
            assert_ne!(
                old_hash, new_hash,
                "semantic changes should have different hashes"
            );
        }
        _ => unreachable!(),
    }
    assert_eq!(resolve_name(&report, def_changed[0]), "Section1/FormatTest");
}

#[test]
fn formatting_only_can_be_suppressed_by_policy() {
    let pkg_a = load_package("m_formatting_only_a.xlsx");
    let pkg_b = load_package("m_formatting_only_b.xlsx");

    let mut config = DiffConfig::default();
    config.semantic.semantic_noise_policy = SemanticNoisePolicy::SuppressFormattingOnly;

    let report = pkg_a.diff(&pkg_b, &config);
    let ops = m_ops(&report);

    let def_changed: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }))
        .collect();

    assert!(
        def_changed.is_empty(),
        "formatting-only changes should be suppressed under SuppressFormattingOnly"
    );
    assert!(
        ops.is_empty(),
        "suppressed formatting-only diff should emit no M ops"
    );
}

#[test]
fn semantic_gate_does_not_mask_metadata_only_change() {
    let pkg_a = load_package("m_metadata_only_change_a.xlsx");
    let pkg_b = load_package("m_metadata_only_change_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    let metadata_ops: Vec<_> = ops
        .iter()
        .filter(|op| matches!(op, DiffOp::QueryMetadataChanged { .. }))
        .collect();

    assert!(
        !metadata_ops.is_empty(),
        "expected metadata changes to be reported"
    );
    assert_eq!(resolve_name(&report, metadata_ops[0]), "Section1/Foo");
}

#[test]
fn semantic_gate_does_not_mask_definition_plus_metadata_change() {
    let pkg_a = load_package("m_def_and_metadata_change_a.xlsx");
    let pkg_b = load_package("m_def_and_metadata_change_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let ops = m_ops(&report);

    let has_def_change = ops
        .iter()
        .any(|op| matches!(op, DiffOp::QueryDefinitionChanged { .. }));

    assert!(
        has_def_change,
        "expected QueryDefinitionChanged for definition+metadata change"
    );
}

```

---

### File: `core\tests\m8_m_canonicalize_tokens_tests.rs`

```rust
use excel_diff::{ast_semantically_equal, canonicalize_m_ast, parse_m_expression};

#[test]
fn opaque_boolean_literal_case_is_canonicalized() {
    let a = "if TRUE then 1 else 0";
    let b = "if true then 1 else 0";

    let mut ast_a = parse_m_expression(a).expect("a should parse");
    let mut ast_b = parse_m_expression(b).expect("b should parse");

    canonicalize_m_ast(&mut ast_a);
    canonicalize_m_ast(&mut ast_b);

    assert!(ast_semantically_equal(&ast_a, &ast_b));
}

#[test]
fn opaque_null_literal_case_is_canonicalized() {
    let a = "if NULL then 1 else 0";
    let b = "if null then 1 else 0";

    let mut ast_a = parse_m_expression(a).expect("a should parse");
    let mut ast_b = parse_m_expression(b).expect("b should parse");

    canonicalize_m_ast(&mut ast_a);
    canonicalize_m_ast(&mut ast_b);

    assert!(ast_semantically_equal(&ast_a, &ast_b));
}

```

---

### File: `core\tests\m8_m_parser_coverage_audit_tests.rs`

```rust
use excel_diff::{MAstAccessKind, MAstKind, canonicalize_m_ast, parse_m_expression};

fn parse_kind(expr: &str) -> MAstKind {
    let mut ast = parse_m_expression(expr).expect("expression should parse into an AST container");
    canonicalize_m_ast(&mut ast);
    ast.root_kind_for_testing()
}

fn assert_kind(expr: &str, expected: MAstKind) {
    let got = parse_kind(expr);
    assert_eq!(got, expected);
}

#[test]
fn coverage_audit_tier1_cases_are_structured() {
    assert_kind(
        "Source",
        MAstKind::Ident {
            name: "Source".to_string(),
        },
    );
    assert_kind(
        "#\"Previous Step\"",
        MAstKind::Ident {
            name: "Previous Step".to_string(),
        },
    );

    assert_kind("if true then 1 else 0", MAstKind::If);
    assert_kind("each _ + 1", MAstKind::Each);

    assert_kind(
        "Source[Field]",
        MAstKind::Access {
            kind: MAstAccessKind::Field,
            chain_len: 1,
        },
    );
    assert_kind(
        "Source{0}",
        MAstKind::Access {
            kind: MAstAccessKind::Item,
            chain_len: 1,
        },
    );
    assert_kind(
        "Source{0}[Content]",
        MAstKind::Access {
            kind: MAstAccessKind::Field,
            chain_len: 2,
        },
    );
}

#[test]
fn coverage_audit_tier2_cases_are_structured() {
    assert_kind(
        "(x) => x",
        MAstKind::FunctionLiteral { param_count: 1 },
    );
    assert_kind("1 + 2", MAstKind::BinaryOp);
    assert_kind("not true", MAstKind::UnaryOp);
    assert_kind("x as number", MAstKind::TypeAscription);
    assert_kind("try 1 otherwise 0", MAstKind::TryOtherwise);
}

```

---

### File: `core\tests\m8_m_parser_expansion_tests.rs`

```rust
use excel_diff::{MAstKind, ast_semantically_equal, canonicalize_m_ast, parse_m_expression};

#[test]
fn record_literal_parses_as_record() {
    let ast = parse_m_expression("[Field1 = 1, Field2 = 2]").unwrap();
    assert_eq!(
        ast.root_kind_for_testing(),
        MAstKind::Record { field_count: 2 }
    );
}

#[test]
fn list_literal_parses_as_list() {
    let ast = parse_m_expression("{1,2,3}").unwrap();
    assert_eq!(
        ast.root_kind_for_testing(),
        MAstKind::List { item_count: 3 }
    );
}

#[test]
fn function_call_parses_as_call() {
    let ast = parse_m_expression("Table.FromRows(.)").unwrap();
    assert_eq!(
        ast.root_kind_for_testing(),
        MAstKind::FunctionCall {
            name: "Table.FromRows".to_string(),
            arg_count: 1
        }
    );
}

#[test]
fn primitive_string_parses() {
    let ast = parse_m_expression(r#""hello""#).unwrap();
    assert_eq!(ast.root_kind_for_testing(), MAstKind::Primitive);
}

#[test]
fn primitive_number_parses() {
    let ast = parse_m_expression("42").unwrap();
    assert_eq!(ast.root_kind_for_testing(), MAstKind::Primitive);
}

#[test]
fn record_field_order_is_semantically_equivalent() {
    let mut a = parse_m_expression("[B=2, A=1]").unwrap();
    let mut b = parse_m_expression("[A=1, B=2]").unwrap();

    canonicalize_m_ast(&mut a);
    canonicalize_m_ast(&mut b);

    assert!(ast_semantically_equal(&a, &b));
}

#[test]
fn list_order_is_not_semantically_equivalent() {
    let mut a = parse_m_expression("{1,2}").unwrap();
    let mut b = parse_m_expression("{2,1}").unwrap();

    canonicalize_m_ast(&mut a);
    canonicalize_m_ast(&mut b);

    assert!(!ast_semantically_equal(&a, &b));
}

```

---

### File: `core\tests\m8_semantic_m_diff_nonlet_tests.rs`

```rust
use excel_diff::{DiffConfig, DiffOp, QueryChangeKind, WorkbookPackage};
use std::fs::File;

mod common;
use common::fixture_path;

fn load_pkg(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).expect("fixture file should open");
    WorkbookPackage::open(file).expect("fixture should parse")
}

fn m_ops<'a>(ops: &'a [DiffOp]) -> Vec<&'a DiffOp> {
    ops.iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::QueryAdded { .. }
                    | DiffOp::QueryRemoved { .. }
                    | DiffOp::QueryRenamed { .. }
                    | DiffOp::QueryDefinitionChanged { .. }
            )
        })
        .collect()
}

#[test]
fn record_reorder_is_masked_by_semantic_canonicalization() {
    let a = load_pkg("m_record_equiv_a.xlsx");
    let b = load_pkg("m_record_equiv_b.xlsx");

    let mut cfg = DiffConfig::default();
    cfg.semantic.enable_m_semantic_diff = true;
    let diff = a.diff(&b, &cfg);

    let ops = m_ops(&diff.ops);
    assert_eq!(ops.len(), 1);

    match ops[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(*change_kind, QueryChangeKind::FormattingOnly);
            assert_eq!(old_hash, new_hash);
        }
        _ => panic!("expected QueryDefinitionChanged"),
    }
}

#[test]
fn list_formatting_only_is_masked() {
    let a = load_pkg("m_list_formatting_a.xlsx");
    let b = load_pkg("m_list_formatting_b.xlsx");

    let mut cfg = DiffConfig::default();
    cfg.semantic.enable_m_semantic_diff = true;
    let diff = a.diff(&b, &cfg);

    let ops = m_ops(&diff.ops);
    assert_eq!(ops.len(), 1);

    match ops[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(*change_kind, QueryChangeKind::FormattingOnly);
            assert_eq!(old_hash, new_hash);
        }
        _ => panic!("expected QueryDefinitionChanged"),
    }
}

#[test]
fn call_formatting_only_is_masked() {
    let a = load_pkg("m_call_formatting_a.xlsx");
    let b = load_pkg("m_call_formatting_b.xlsx");

    let mut cfg = DiffConfig::default();
    cfg.semantic.enable_m_semantic_diff = true;
    let diff = a.diff(&b, &cfg);

    let ops = m_ops(&diff.ops);
    assert_eq!(ops.len(), 1);

    match ops[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(*change_kind, QueryChangeKind::FormattingOnly);
            assert_eq!(old_hash, new_hash);
        }
        _ => panic!("expected QueryDefinitionChanged"),
    }
}

#[test]
fn primitive_formatting_only_is_masked() {
    let a = load_pkg("m_primitive_formatting_a.xlsx");
    let b = load_pkg("m_primitive_formatting_b.xlsx");

    let mut cfg = DiffConfig::default();
    cfg.semantic.enable_m_semantic_diff = true;
    let diff = a.diff(&b, &cfg);

    let ops = m_ops(&diff.ops);
    assert_eq!(ops.len(), 1);

    match ops[0] {
        DiffOp::QueryDefinitionChanged {
            change_kind,
            old_hash,
            new_hash,
            ..
        } => {
            assert_eq!(*change_kind, QueryChangeKind::FormattingOnly);
            assert_eq!(old_hash, new_hash);
        }
        _ => panic!("expected QueryDefinitionChanged"),
    }
}

```

---

### File: `core\tests\m9_composed_end_to_end_tests.rs`

```rust
use excel_diff::{DiffConfig, DiffOp, StepChange, StepDiff, WorkbookPackage};
use std::fs::File;

mod common;
use common::fixture_path;

fn load_package(name: &str) -> WorkbookPackage {
    let path = fixture_path(name);
    let file = File::open(&path).expect("fixture file should open");
    WorkbookPackage::open(file).expect("fixture should parse as WorkbookPackage")
}

fn has_params_changed(detail: &excel_diff::QuerySemanticDetail) -> bool {
    detail.step_diffs.iter().any(|diff| match diff {
        StepDiff::StepModified { changes, .. } => {
            changes.iter().any(|c| matches!(c, StepChange::ParamsChanged))
        }
        _ => false,
    })
}

#[test]
fn composed_grid_mashup_reports_grid_and_query_changes() {
    let pkg_a = load_package("composed_grid_mashup_a.xlsx");
    let pkg_b = load_package("composed_grid_mashup_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());

    let has_grid_op = report.grid_ops().any(|op| {
        matches!(
            op,
            DiffOp::RowAdded { .. }
                | DiffOp::RowRemoved { .. }
                | DiffOp::RowReplaced { .. }
                | DiffOp::CellEdited { .. }
        )
    });
    assert!(has_grid_op, "expected at least one grid op in composed diff");

    let mut saw_definition_change = false;
    let mut saw_metadata_change = false;
    let mut saw_params_change = false;

    for op in report.m_ops() {
        match op {
            DiffOp::QueryDefinitionChanged {
                name,
                semantic_detail: Some(detail),
                ..
            } => {
                if report.resolve(*name) == Some("Section1/SalesWithRegions") {
                    saw_definition_change = true;
                    if has_params_changed(detail) {
                        saw_params_change = true;
                    }
                }
            }
            DiffOp::QueryMetadataChanged { name, .. } => {
                if report.resolve(*name) == Some("Section1/SalesWithRegions") {
                    saw_metadata_change = true;
                }
            }
            _ => {}
        }
    }

    assert!(
        saw_definition_change,
        "expected QueryDefinitionChanged for Section1/SalesWithRegions"
    );
    assert!(
        saw_params_change,
        "expected ParamsChanged in semantic detail for Section1/SalesWithRegions"
    );
    assert!(
        saw_metadata_change,
        "expected QueryMetadataChanged for Section1/SalesWithRegions"
    );
}

#[test]
fn adversarial_steps_report_param_changes() {
    let pkg_a = load_package("m_adversarial_steps_a.xlsx");
    let pkg_b = load_package("m_adversarial_steps_b.xlsx");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());

    let mut saw_params_change = false;
    for op in report.m_ops() {
        if let DiffOp::QueryDefinitionChanged {
            name,
            semantic_detail: Some(detail),
            ..
        } = op
        {
            if report.resolve(*name) == Some("Section1/Adversarial") {
                if has_params_changed(detail) {
                    saw_params_change = true;
                }
            }
        }
    }

    assert!(
        saw_params_change,
        "expected ParamsChanged for Section1/Adversarial diff"
    );
}

```

---

### File: `core\tests\m9_m_parser_tier1_tests.rs`

```rust
use excel_diff::{MAstAccessKind, MAstKind, canonicalize_m_ast, parse_m_expression};

fn kind(expr: &str) -> MAstKind {
    let mut ast = parse_m_expression(expr).expect("parse should succeed");
    canonicalize_m_ast(&mut ast);
    ast.root_kind_for_testing()
}

#[test]
fn parse_ident_ref() {
    assert_eq!(
        kind("Source"),
        MAstKind::Ident {
            name: "Source".to_string()
        }
    );

    assert_eq!(
        kind("#\"Previous Step\""),
        MAstKind::Ident {
            name: "Previous Step".to_string()
        }
    );
}

#[test]
fn parse_field_access() {
    assert_eq!(
        kind("Source[Field]"),
        MAstKind::Access {
            kind: MAstAccessKind::Field,
            chain_len: 1
        }
    );
}

#[test]
fn parse_item_access() {
    assert_eq!(
        kind("Source{0}"),
        MAstKind::Access {
            kind: MAstAccessKind::Item,
            chain_len: 1
        }
    );
}

#[test]
fn parse_access_chain() {
    assert_eq!(
        kind("Source{0}[Content]"),
        MAstKind::Access {
            kind: MAstAccessKind::Field,
            chain_len: 2
        }
    );
}

#[test]
fn parse_if_then_else() {
    assert_eq!(kind("if true then 1 else 0"), MAstKind::If);
}

#[test]
fn parse_each_expr() {
    assert_eq!(kind("each _ + 1"), MAstKind::Each);
}

#[test]
fn quoted_identifier_named_then_does_not_confuse_if_parser() {
    let expr = r##"if #"then" then 1 else 0"##;
    assert_eq!(kind(expr), MAstKind::If);
}

```

---

### File: `core\tests\m_section_splitting_tests.rs`

```rust
use excel_diff::{SectionParseError, parse_section_members};

const SECTION_SINGLE: &str = r#"
    section Section1;

    shared Foo = 1;
"#;

const SECTION_MULTI: &str = r#"
    section Section1;

    shared Foo = 1;
    shared Bar = 2;
    Baz = 3;
"#;

const SECTION_NOISY: &str = r#"

// Leading comment

section Section1;

// Comment before Foo
shared Foo = 1;

// Another comment

    shared   Bar   =    2    ;

"#;

const SECTION_WITH_BOM: &str = "\u{FEFF}section Section1;\nshared Foo = 1;";

const SECTION_WITH_QUOTED_IDENTIFIER: &str = r#"
    section Section1;

    shared #"Query with space & #" = 1;
"#;

const SECTION_INVALID_SHARED: &str = r#"
    section Section1;

    shared Broken // missing '=' and ';'
"#;

#[test]
fn parse_single_member_section() {
    let members = parse_section_members(SECTION_SINGLE).expect("single member section parses");
    assert_eq!(members.len(), 1);

    let foo = &members[0];
    assert_eq!(foo.section_name, "Section1");
    assert_eq!(foo.member_name, "Foo");
    assert_eq!(foo.expression_m, "1");
    assert!(foo.is_shared);
}

#[test]
fn parse_multiple_members() {
    let members = parse_section_members(SECTION_MULTI).expect("multi-member section parses");
    assert_eq!(members.len(), 2);

    assert_eq!(members[0].member_name, "Foo");
    assert_eq!(members[0].section_name, "Section1");
    assert_eq!(members[0].expression_m, "1");
    assert!(members[0].is_shared);

    assert_eq!(members[1].member_name, "Bar");
    assert_eq!(members[1].section_name, "Section1");
    assert_eq!(members[1].expression_m, "2");
    assert!(members[1].is_shared);
}

#[test]
fn tolerate_whitespace_comments() {
    let members = parse_section_members(SECTION_NOISY).expect("noisy section still parses");
    assert_eq!(members.len(), 2);

    assert_eq!(members[0].member_name, "Foo");
    assert_eq!(members[0].expression_m, "1");
    assert!(members[0].is_shared);
    assert_eq!(members[0].section_name, "Section1");

    assert_eq!(members[1].member_name, "Bar");
    assert_eq!(members[1].expression_m, "2");
    assert!(members[1].is_shared);
    assert_eq!(members[1].section_name, "Section1");
}

#[test]
fn error_on_missing_section_header() {
    const NO_SECTION: &str = r#"
        shared Foo = 1;
    "#;

    let result = parse_section_members(NO_SECTION);
    assert_eq!(result, Err(SectionParseError::MissingSectionHeader));
}

#[test]
fn section_parsing_tolerates_utf8_bom() {
    let members =
        parse_section_members(SECTION_WITH_BOM).expect("BOM-prefixed section should parse");
    assert_eq!(members.len(), 1);

    let member = &members[0];
    assert_eq!(member.member_name, "Foo");
    assert_eq!(member.section_name, "Section1");
    assert_eq!(member.expression_m, "1");
    assert!(member.is_shared);
}

#[test]
fn parse_quoted_identifier_member() {
    let members = parse_section_members(SECTION_WITH_QUOTED_IDENTIFIER)
        .expect("quoted identifier should parse");
    assert_eq!(members.len(), 1);

    let member = &members[0];
    assert_eq!(member.section_name, "Section1");
    assert_eq!(member.member_name, "Query with space & #");
    assert_eq!(member.expression_m, "1");
    assert!(member.is_shared);
}

#[test]
fn error_on_invalid_shared_member_syntax() {
    let result = parse_section_members(SECTION_INVALID_SHARED);
    assert_eq!(result, Err(SectionParseError::InvalidMemberSyntax));
}

```

---

### File: `core\tests\metrics_unit_tests.rs`

```rust
#![cfg(feature = "perf-metrics")]

use excel_diff::perf::{DiffMetrics, Phase};

#[test]
fn metrics_starts_with_zero_counts() {
    let metrics = DiffMetrics::default();
    assert_eq!(metrics.rows_processed, 0);
    assert_eq!(metrics.cells_compared, 0);
    assert_eq!(metrics.anchors_found, 0);
    assert_eq!(metrics.moves_detected, 0);
    assert_eq!(metrics.parse_time_ms, 0);
    assert_eq!(metrics.signature_build_time_ms, 0);
    assert_eq!(metrics.alignment_time_ms, 0);
    assert_eq!(metrics.move_detection_time_ms, 0);
    assert_eq!(metrics.cell_diff_time_ms, 0);
    assert_eq!(metrics.op_emit_time_ms, 0);
    assert_eq!(metrics.report_serialize_time_ms, 0);
    assert_eq!(metrics.total_time_ms, 0);
    assert_eq!(metrics.diff_time_ms, 0);
    assert_eq!(metrics.peak_memory_bytes, 0);
    assert_eq!(metrics.grid_storage_bytes, 0);
    assert_eq!(metrics.string_pool_bytes, 0);
    assert_eq!(metrics.op_buffer_bytes, 0);
    assert_eq!(metrics.alignment_buffer_bytes, 0);
    assert_eq!(metrics.hash_lookups_est, 0);
    assert_eq!(metrics.allocations_est, 0);
}

#[test]
fn metrics_add_cells_compared_accumulates() {
    let mut metrics = DiffMetrics::default();
    metrics.add_cells_compared(100);
    assert_eq!(metrics.cells_compared, 100);
    metrics.add_cells_compared(50);
    assert_eq!(metrics.cells_compared, 150);
    metrics.add_cells_compared(1000);
    assert_eq!(metrics.cells_compared, 1150);
}

#[test]
fn metrics_add_cells_compared_saturates() {
    let mut metrics = DiffMetrics::default();
    metrics.cells_compared = u64::MAX - 10;
    metrics.add_cells_compared(100);
    assert_eq!(metrics.cells_compared, u64::MAX);
}

#[test]
fn metrics_phase_timing_accumulates() {
    let mut metrics = DiffMetrics::default();

    metrics.start_phase(Phase::Alignment);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Alignment);

    assert!(
        metrics.alignment_time_ms > 0,
        "alignment_time_ms should be non-zero after timed phase"
    );

    let first_alignment = metrics.alignment_time_ms;

    metrics.start_phase(Phase::Alignment);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Alignment);

    assert!(
        metrics.alignment_time_ms > first_alignment,
        "alignment_time_ms should accumulate across multiple phases"
    );
}

#[test]
fn metrics_different_phases_tracked_separately() {
    let mut metrics = DiffMetrics::default();

    metrics.start_phase(Phase::Alignment);
    std::thread::sleep(std::time::Duration::from_millis(5));
    metrics.end_phase(Phase::Alignment);

    metrics.start_phase(Phase::MoveDetection);
    std::thread::sleep(std::time::Duration::from_millis(5));
    metrics.end_phase(Phase::MoveDetection);

    metrics.start_phase(Phase::CellDiff);
    std::thread::sleep(std::time::Duration::from_millis(5));
    metrics.end_phase(Phase::CellDiff);

    assert!(metrics.alignment_time_ms > 0, "alignment should be tracked");
    assert!(
        metrics.move_detection_time_ms > 0,
        "move detection should be tracked"
    );
    assert!(metrics.cell_diff_time_ms > 0, "cell diff should be tracked");
}

#[test]
fn metrics_total_phase_separate_from_components() {
    let mut metrics = DiffMetrics::default();

    metrics.start_phase(Phase::Total);
    metrics.start_phase(Phase::Alignment);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Alignment);
    metrics.end_phase(Phase::Total);

    assert!(metrics.alignment_time_ms > 0);
    assert!(metrics.total_time_ms > 0);
    assert!(
        metrics.total_time_ms >= metrics.alignment_time_ms,
        "total should be >= alignment since it wraps alignment"
    );
}

#[test]
fn metrics_end_phase_without_start_is_safe() {
    let mut metrics = DiffMetrics::default();
    metrics.end_phase(Phase::Alignment);
    assert_eq!(metrics.alignment_time_ms, 0);
}

#[test]
fn metrics_parse_phase_tracks_time() {
    let mut metrics = DiffMetrics::default();
    metrics.start_phase(Phase::Parse);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Parse);
    assert!(metrics.parse_time_ms > 0, "parse_time_ms should be non-zero");
}

#[test]
fn metrics_diff_time_derived_from_total_minus_parse() {
    let mut metrics = DiffMetrics::default();
    metrics.start_phase(Phase::Total);
    metrics.start_phase(Phase::Parse);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Parse);
    std::thread::sleep(std::time::Duration::from_millis(10));
    metrics.end_phase(Phase::Total);

    assert!(
        metrics.total_time_ms >= metrics.parse_time_ms,
        "total should include parse time"
    );
    assert_eq!(
        metrics.diff_time_ms,
        metrics.total_time_ms.saturating_sub(metrics.parse_time_ms)
    );
}

#[test]
fn metrics_rows_processed_can_be_set_directly() {
    let mut metrics = DiffMetrics::default();
    metrics.rows_processed = 5000;
    assert_eq!(metrics.rows_processed, 5000);
    metrics.rows_processed = metrics.rows_processed.saturating_add(3000);
    assert_eq!(metrics.rows_processed, 8000);
}

#[test]
fn metrics_anchors_and_moves_can_be_set() {
    let mut metrics = DiffMetrics::default();
    metrics.anchors_found = 150;
    metrics.moves_detected = 3;
    assert_eq!(metrics.anchors_found, 150);
    assert_eq!(metrics.moves_detected, 3);
}

#[test]
fn metrics_clone_creates_independent_copy() {
    let mut metrics = DiffMetrics::default();
    metrics.rows_processed = 1000;
    metrics.cells_compared = 500;

    let cloned = metrics.clone();
    metrics.rows_processed = 2000;

    assert_eq!(cloned.rows_processed, 1000);
    assert_eq!(metrics.rows_processed, 2000);
}

#[test]
fn metrics_default_equality() {
    let m1 = DiffMetrics::default();
    let m2 = DiffMetrics::default();
    assert_eq!(m1, m2);
}

```

---

### File: `core\tests\output_tests.rs`

```rust
mod common;

use common::{fixture_path, open_fixture_workbook};
use excel_diff::{
    CellAddress, CellDiff, CellSnapshot, CellValue, ContainerError, DiffConfig, DiffOp, DiffReport,
    FormulaDiffResult, LimitBehavior, PackageError, WorkbookPackage, diff_report_to_cell_diffs,
    diff_workbooks_to_json, serialize_cell_diffs, serialize_diff_report,
};
use serde_json::Value;
#[cfg(feature = "perf-metrics")]
use std::collections::BTreeSet;

fn sid_local(pool: &mut excel_diff::StringPool, value: &str) -> excel_diff::StringId {
    pool.intern(value)
}

fn attach_strings(mut report: DiffReport, pool: excel_diff::StringPool) -> DiffReport {
    report.strings = pool.into_strings();
    report
}

fn render_value(report: &DiffReport, value: &Option<excel_diff::CellValue>) -> Option<String> {
    match value {
        Some(excel_diff::CellValue::Number(n)) => Some(n.to_string()),
        Some(excel_diff::CellValue::Text(id)) => report.strings.get(id.0 as usize).cloned(),
        Some(excel_diff::CellValue::Bool(b)) => Some(b.to_string()),
        Some(excel_diff::CellValue::Error(id)) => report.strings.get(id.0 as usize).cloned(),
        Some(excel_diff::CellValue::Blank) => Some(String::new()),
        None => None,
    }
}

fn make_cell_snapshot(addr: CellAddress, value: Option<CellValue>) -> CellSnapshot {
    CellSnapshot {
        addr,
        value,
        formula: None,
    }
}

fn cell_edit(
    sheet: excel_diff::StringId,
    addr: CellAddress,
    from: CellSnapshot,
    to: CellSnapshot,
) -> DiffOp {
    DiffOp::cell_edited(sheet, addr, from, to, FormulaDiffResult::Unchanged)
}

fn numeric_report(addr: CellAddress, from: f64, to: f64) -> DiffReport {
    let mut pool = excel_diff::StringPool::new();
    let sheet = sid_local(&mut pool, "Sheet1");
    attach_strings(
        DiffReport::new(vec![cell_edit(
            sheet,
            addr,
            make_cell_snapshot(addr, Some(CellValue::Number(from))),
            make_cell_snapshot(addr, Some(CellValue::Number(to))),
        )]),
        pool,
    )
}

#[test]
fn diff_report_to_cell_diffs_filters_non_cell_ops() {
    let mut pool = excel_diff::StringPool::new();
    let sheet_added = sid_local(&mut pool, "SheetAdded");
    let sheet1 = sid_local(&mut pool, "Sheet1");
    let sheet2 = sid_local(&mut pool, "Sheet2");
    let old_sheet = sid_local(&mut pool, "OldSheet");
    let old_text = sid_local(&mut pool, "old");
    let new_text = sid_local(&mut pool, "new");
    let addr1 = CellAddress::from_indices(0, 0);
    let addr2 = CellAddress::from_indices(1, 1);

    let report = attach_strings(
        DiffReport::new(vec![
            DiffOp::SheetAdded { sheet: sheet_added },
            cell_edit(
                sheet1,
                addr1,
                make_cell_snapshot(addr1, Some(CellValue::Number(1.0))),
                make_cell_snapshot(addr1, Some(CellValue::Number(2.0))),
            ),
            DiffOp::RowAdded {
                sheet: sheet1,
                row_idx: 5,
                row_signature: None,
            },
            cell_edit(
                sheet2,
                addr2,
                make_cell_snapshot(addr2, Some(CellValue::Text(old_text))),
                make_cell_snapshot(addr2, Some(CellValue::Text(new_text))),
            ),
            DiffOp::SheetRemoved { sheet: old_sheet },
        ]),
        pool,
    );

    let cell_diffs = diff_report_to_cell_diffs(&report);
    assert_eq!(
        cell_diffs.len(),
        2,
        "only CellEdited ops should be projected"
    );

    assert_eq!(cell_diffs[0].coords, addr1.to_a1());
    assert_eq!(cell_diffs[0].value_file1, Some("1".into()));
    assert_eq!(cell_diffs[0].value_file2, Some("2".into()));

    assert_eq!(cell_diffs[1].coords, addr2.to_a1());
    assert_eq!(cell_diffs[1].value_file1, Some("old".into()));
    assert_eq!(cell_diffs[1].value_file2, Some("new".into()));
}

#[test]
fn diff_report_to_cell_diffs_ignores_block_moved_rect() {
    let mut pool = excel_diff::StringPool::new();
    let sheet1 = sid_local(&mut pool, "Sheet1");
    let addr = CellAddress::from_indices(2, 2);

    let report = attach_strings(
        DiffReport::new(vec![
            DiffOp::block_moved_rect(sheet1, 2, 3, 1, 3, 9, 6, Some(0xCAFEBABE)),
            cell_edit(
                sheet1,
                addr,
                make_cell_snapshot(addr, Some(CellValue::Number(10.0))),
                make_cell_snapshot(addr, Some(CellValue::Number(20.0))),
            ),
            DiffOp::BlockMovedRows {
                sheet: sheet1,
                src_start_row: 0,
                row_count: 2,
                dst_start_row: 5,
                block_hash: None,
            },
            DiffOp::BlockMovedColumns {
                sheet: sheet1,
                src_start_col: 0,
                col_count: 2,
                dst_start_col: 5,
                block_hash: None,
            },
        ]),
        pool,
    );

    let cell_diffs = diff_report_to_cell_diffs(&report);
    assert_eq!(
        cell_diffs.len(),
        1,
        "only CellEdited should be projected; BlockMovedRect and other block moves should be ignored"
    );

    assert_eq!(cell_diffs[0].coords, addr.to_a1());
    assert_eq!(cell_diffs[0].value_file1, Some("10".into()));
    assert_eq!(cell_diffs[0].value_file2, Some("20".into()));
}

#[test]
fn diff_report_to_cell_diffs_maps_values_correctly() {
    let mut pool = excel_diff::StringPool::new();
    let sheet_id = sid_local(&mut pool, "SheetX");
    let addr_num = CellAddress::from_indices(2, 2); // C3
    let addr_bool = CellAddress::from_indices(3, 3); // D4

    let report = attach_strings(
        DiffReport::new(vec![
            cell_edit(
                sheet_id,
                addr_num,
                make_cell_snapshot(addr_num, Some(CellValue::Number(42.5))),
                make_cell_snapshot(addr_num, Some(CellValue::Number(43.5))),
            ),
            cell_edit(
                sheet_id,
                addr_bool,
                make_cell_snapshot(addr_bool, Some(CellValue::Bool(true))),
                make_cell_snapshot(addr_bool, Some(CellValue::Bool(false))),
            ),
        ]),
        pool,
    );

    let cell_diffs = diff_report_to_cell_diffs(&report);
    assert_eq!(cell_diffs.len(), 2);

    let number_diff = &cell_diffs[0];
    assert_eq!(number_diff.coords, addr_num.to_a1());
    assert_eq!(number_diff.value_file1, Some("42.5".into()));
    assert_eq!(number_diff.value_file2, Some("43.5".into()));

    let bool_diff = &cell_diffs[1];
    assert_eq!(bool_diff.coords, addr_bool.to_a1());
    assert_eq!(bool_diff.value_file1, Some("true".into()));
    assert_eq!(bool_diff.value_file2, Some("false".into()));
}

#[test]
fn diff_report_to_cell_diffs_filters_no_op_cell_edits() {
    let mut pool = excel_diff::StringPool::new();
    let sheet = sid_local(&mut pool, "Sheet1");
    let addr_a1 = CellAddress::from_indices(0, 0);
    let addr_a2 = CellAddress::from_indices(1, 0);

    let report = attach_strings(
        DiffReport::new(vec![
            cell_edit(
                sheet,
                addr_a1,
                make_cell_snapshot(addr_a1, Some(CellValue::Number(1.0))),
                make_cell_snapshot(addr_a1, Some(CellValue::Number(1.0))),
            ),
            cell_edit(
                sheet,
                addr_a2,
                make_cell_snapshot(addr_a2, Some(CellValue::Number(1.0))),
                make_cell_snapshot(addr_a2, Some(CellValue::Number(2.0))),
            ),
        ]),
        pool,
    );

    let diffs = diff_report_to_cell_diffs(&report);

    assert_eq!(diffs.len(), 1);
    assert_eq!(diffs[0].coords, "A2");
    assert_eq!(diffs[0].value_file1, Some("1".to_string()));
    assert_eq!(diffs[0].value_file2, Some("2".to_string()));
}

#[test]
fn test_json_format() {
    let diffs = vec![
        CellDiff {
            coords: "A1".into(),
            value_file1: Some("100".into()),
            value_file2: Some("200".into()),
        },
        CellDiff {
            coords: "B2".into(),
            value_file1: Some("true".into()),
            value_file2: Some("false".into()),
        },
        CellDiff {
            coords: "C3".into(),
            value_file1: Some("#DIV/0!".into()),
            value_file2: None,
        },
    ];

    let json = serialize_cell_diffs(&diffs).expect("serialization should succeed");
    let value: Value = serde_json::from_str(&json).expect("json should parse");

    assert!(value.is_array(), "expected an array of cell diffs");
    let arr = value
        .as_array()
        .expect("top-level json should be an array of cell diffs");
    assert_eq!(arr.len(), 3);

    let first = &arr[0];
    assert_eq!(first["coords"], Value::String("A1".into()));
    assert_eq!(first["value_file1"], Value::String("100".into()));
    assert_eq!(first["value_file2"], Value::String("200".into()));

    let second = &arr[1];
    assert_eq!(second["coords"], Value::String("B2".into()));
    assert_eq!(second["value_file1"], Value::String("true".into()));
    assert_eq!(second["value_file2"], Value::String("false".into()));

    let third = &arr[2];
    assert_eq!(third["coords"], Value::String("C3".into()));
    assert_eq!(third["value_file1"], Value::String("#DIV/0!".into()));
    assert_eq!(third["value_file2"], Value::Null);
}

#[test]
fn test_json_empty_diff() {
    let fixture = fixture_path("pg1_basic_two_sheets.xlsx");
    let json = diff_workbooks_to_json(&fixture, &fixture, &DiffConfig::default())
        .expect("diffing identical files should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert!(
        report.ops.is_empty(),
        "identical files should produce no diff ops"
    );
}

#[test]
fn test_json_non_empty_diff() {
    let a = fixture_path("json_diff_single_cell_a.xlsx");
    let b = fixture_path("json_diff_single_cell_b.xlsx");

    let json = diff_workbooks_to_json(&a, &b, &DiffConfig::default())
        .expect("diffing different files should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert_eq!(report.ops.len(), 1, "expected a single diff op");
    match &report.ops[0] {
        DiffOp::CellEdited { addr, from, to, .. } => {
            assert_eq!(addr.to_a1(), "C3");
            assert_eq!(render_value(&report, &from.value), Some("1".into()));
            assert_eq!(render_value(&report, &to.value), Some("2".into()));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn test_json_non_empty_diff_bool() {
    let a = fixture_path("json_diff_bool_a.xlsx");
    let b = fixture_path("json_diff_bool_b.xlsx");

    let json = diff_workbooks_to_json(&a, &b, &DiffConfig::default())
        .expect("diffing different files should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert_eq!(report.ops.len(), 1, "expected a single diff op");
    match &report.ops[0] {
        DiffOp::CellEdited { addr, from, to, .. } => {
            assert_eq!(addr.to_a1(), "C3");
            assert_eq!(render_value(&report, &from.value), Some("true".into()));
            assert_eq!(render_value(&report, &to.value), Some("false".into()));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn test_json_diff_value_to_empty() {
    let a = fixture_path("json_diff_value_to_empty_a.xlsx");
    let b = fixture_path("json_diff_value_to_empty_b.xlsx");

    let json = diff_workbooks_to_json(&a, &b, &DiffConfig::default())
        .expect("diffing different files should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert_eq!(report.ops.len(), 1, "expected a single diff op");
    match &report.ops[0] {
        DiffOp::CellEdited { addr, from, to, .. } => {
            assert_eq!(addr.to_a1(), "C3");
            assert_eq!(render_value(&report, &from.value), Some("1".into()));
            assert_eq!(render_value(&report, &to.value), None);
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn json_diff_case_only_sheet_name_no_changes() {
    let old = open_fixture_workbook("sheet_case_only_rename_a.xlsx");
    let new = open_fixture_workbook("sheet_case_only_rename_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());
    assert!(
        report.ops.is_empty(),
        "case-only sheet rename with identical content should produce no diff ops"
    );
}

#[test]
fn json_diff_case_only_sheet_name_cell_edit() {
    let old = open_fixture_workbook("sheet_case_only_rename_edit_a.xlsx");
    let new = open_fixture_workbook("sheet_case_only_rename_edit_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());
    assert_eq!(report.ops.len(), 1, "expected a single cell edit");
    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            let sheet_name = report
                .strings
                .get(sheet.0 as usize)
                .map(|s| s.to_lowercase());
            assert_eq!(sheet_name, Some("sheet1".to_string()));
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(render_value(&report, &from.value), Some("1".into()));
            assert_eq!(render_value(&report, &to.value), Some("2".into()));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn test_json_case_only_sheet_name_no_changes() {
    let a = fixture_path("sheet_case_only_rename_a.xlsx");
    let b = fixture_path("sheet_case_only_rename_b.xlsx");

    let json = diff_workbooks_to_json(&a, &b, &DiffConfig::default())
        .expect("diffing case-only sheet rename should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert!(
        report.ops.is_empty(),
        "case-only sheet rename with identical content should serialize to no ops"
    );
}

#[test]
fn test_json_case_only_sheet_name_cell_edit_via_helper() {
    let a = fixture_path("sheet_case_only_rename_edit_a.xlsx");
    let b = fixture_path("sheet_case_only_rename_edit_b.xlsx");

    let json = diff_workbooks_to_json(&a, &b, &DiffConfig::default())
        .expect("diffing case-only sheet rename with cell edit should succeed");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert_eq!(report.ops.len(), 1, "expected a single cell edit");

    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            let sheet_name = report
                .strings
                .get(sheet.0 as usize)
                .map(|s| s.to_lowercase());
            assert_eq!(sheet_name, Some("sheet1".to_string()));
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(render_value(&report, &from.value), Some("1".into()));
            assert_eq!(render_value(&report, &to.value), Some("2".into()));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn diff_workbooks_to_json_converts_diff_error_to_warning_report() {
    let path = fixture_path("pg1_basic_two_sheets.xlsx");

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 1;
    config.alignment.max_align_cols = 1;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let json = diff_workbooks_to_json(&path, &path, &config)
        .expect("diff errors should be converted into warning reports");
    let report: DiffReport = serde_json::from_str(&json).expect("json should parse");

    assert!(
        !report.complete,
        "limit errors should mark the report as incomplete"
    );
    assert!(
        report
            .warnings
            .iter()
            .any(|warning| warning.contains("limits exceeded")),
        "expected limits-exceeded warning"
    );
    assert!(
        report.ops.is_empty(),
        "diff errors should not leak partial grid ops into the report"
    );
}

#[test]
fn test_diff_workbooks_to_json_reports_invalid_zip() {
    let path = fixture_path("not_a_zip.txt");
    let err = diff_workbooks_to_json(&path, &path, &DiffConfig::default())
        .expect_err("diffing invalid containers should return an error");

    let inner = match err {
        PackageError::WithPath { source, .. } => *source,
        other => other,
    };
    assert!(
        matches!(inner, PackageError::Container(ContainerError::NotZipContainer)),
        "expected container error, got {inner:?}"
    );
}

#[test]
fn serialize_diff_report_nan_maps_to_serialization_error() {
    let addr = CellAddress::from_indices(0, 0);
    let report = numeric_report(addr, f64::NAN, 1.0);

    let err = serialize_diff_report(&report).expect_err("NaN should fail to serialize");
    let wrapped = PackageError::SerializationError(err.to_string());

    match wrapped {
        PackageError::SerializationError(msg) => {
            assert!(
                msg.to_lowercase().contains("nan"),
                "error message should mention NaN for clarity"
            );
        }
        other => panic!("expected SerializationError, got {other:?}"),
    }
}

#[test]
fn serialize_diff_report_infinity_maps_to_serialization_error() {
    let addr = CellAddress::from_indices(0, 0);
    let report = numeric_report(addr, f64::INFINITY, 1.0);

    let err = serialize_diff_report(&report).expect_err("Infinity should fail to serialize");
    let wrapped = PackageError::SerializationError(err.to_string());
    match wrapped {
        PackageError::SerializationError(msg) => {
            assert!(
                msg.to_lowercase().contains("infinity"),
                "error message should mention infinity for clarity"
            );
        }
        other => panic!("expected SerializationError, got {other:?}"),
    }
}

#[test]
fn serialize_diff_report_neg_infinity_maps_to_serialization_error() {
    let addr = CellAddress::from_indices(0, 0);
    let report = numeric_report(addr, f64::NEG_INFINITY, 1.0);

    let err = serialize_diff_report(&report).expect_err("NEG_INFINITY should fail to serialize");
    let wrapped = PackageError::SerializationError(err.to_string());
    match wrapped {
        PackageError::SerializationError(msg) => {
            assert!(
                msg.to_lowercase().contains("infinity"),
                "error message should mention infinity for clarity"
            );
        }
        other => panic!("expected SerializationError, got {other:?}"),
    }
}

#[test]
fn serialize_diff_report_with_finite_numbers_succeeds() {
    let addr = CellAddress::from_indices(1, 1);
    let report = numeric_report(addr, 2.5, 3.5);

    let json = serialize_diff_report(&report).expect("finite values should serialize");
    let parsed: DiffReport = serde_json::from_str(&json).expect("json should parse");
    assert_eq!(parsed.ops.len(), 1);
}

#[test]
fn serialize_full_diff_report_has_complete_true_and_no_warnings() {
    let addr = CellAddress::from_indices(0, 0);
    let report = numeric_report(addr, 1.0, 2.0);

    let json = serialize_diff_report(&report).expect("full report should serialize");
    let value: Value = serde_json::from_str(&json).expect("json should parse");
    let obj = value.as_object().expect("should be object");

    assert_eq!(
        obj.get("complete").and_then(Value::as_bool),
        Some(true),
        "full result should have complete=true"
    );

    let has_warnings = obj
        .get("warnings")
        .map(|v| v.as_array().map(|arr| !arr.is_empty()).unwrap_or(false))
        .unwrap_or(false);
    assert!(
        !has_warnings,
        "full result should have no warnings or empty warnings array"
    );
}

#[test]
fn serialize_partial_diff_report_includes_complete_false_and_warnings() {
    let addr = CellAddress::from_indices(0, 0);
    let mut pool = excel_diff::StringPool::new();
    let sheet = sid_local(&mut pool, "Sheet1");
    let ops = vec![cell_edit(
        sheet,
        addr,
        make_cell_snapshot(addr, Some(CellValue::Number(1.0))),
        make_cell_snapshot(addr, Some(CellValue::Number(2.0))),
    )];
    let report = attach_strings(
        DiffReport::with_partial_result(
            ops,
            "Sheet 'LargeSheet': alignment limits exceeded".to_string(),
        ),
        pool,
    );

    let json = serialize_diff_report(&report).expect("partial report should serialize");
    let value: Value = serde_json::from_str(&json).expect("json should parse");
    let obj = value.as_object().expect("should be object");

    assert_eq!(
        obj.get("complete").and_then(Value::as_bool),
        Some(false),
        "partial result should have complete=false"
    );

    let warnings = obj
        .get("warnings")
        .and_then(Value::as_array)
        .expect("warnings should be present");
    assert!(!warnings.is_empty(), "warnings array should not be empty");
    assert!(
        warnings[0]
            .as_str()
            .unwrap_or("")
            .contains("limits exceeded"),
        "warning should mention limits exceeded"
    );
}

#[test]
#[cfg(feature = "perf-metrics")]
fn serialize_diff_report_with_metrics_includes_metrics_object() {
    use excel_diff::perf::DiffMetrics;

    let addr = CellAddress::from_indices(0, 0);
    let mut pool = excel_diff::StringPool::new();
    let sheet = sid_local(&mut pool, "Sheet1");
    let ops = vec![cell_edit(
        sheet,
        addr,
        make_cell_snapshot(addr, Some(CellValue::Number(1.0))),
        make_cell_snapshot(addr, Some(CellValue::Number(2.0))),
    )];

    let mut report = attach_strings(DiffReport::new(ops), pool);
    let mut metrics = DiffMetrics::default();
    metrics.parse_time_ms = 5;
    metrics.move_detection_time_ms = 5;
    metrics.alignment_time_ms = 10;
    metrics.cell_diff_time_ms = 15;
    metrics.total_time_ms = 30;
    metrics.diff_time_ms = 25;
    metrics.peak_memory_bytes = 12345;
    metrics.rows_processed = 500;
    metrics.cells_compared = 2500;
    metrics.anchors_found = 25;
    metrics.moves_detected = 1;
    report.metrics = Some(metrics);

    let json = serialize_diff_report(&report).expect("report with metrics should serialize");
    let value: Value = serde_json::from_str(&json).expect("json should parse");
    let obj = value.as_object().expect("should be object");

    let keys: BTreeSet<String> = obj.keys().cloned().collect();
    assert!(
        keys.contains("metrics"),
        "serialized report should include metrics key"
    );

    let metrics_obj = obj
        .get("metrics")
        .and_then(Value::as_object)
        .expect("metrics should be an object");

    assert!(
        metrics_obj.contains_key("parse_time_ms"),
        "metrics should contain parse_time_ms"
    );
    assert!(
        metrics_obj.contains_key("move_detection_time_ms"),
        "metrics should contain move_detection_time_ms"
    );
    assert!(
        metrics_obj.contains_key("alignment_time_ms"),
        "metrics should contain alignment_time_ms"
    );
    assert!(
        metrics_obj.contains_key("cell_diff_time_ms"),
        "metrics should contain cell_diff_time_ms"
    );
    assert!(
        metrics_obj.contains_key("total_time_ms"),
        "metrics should contain total_time_ms"
    );
    assert!(
        metrics_obj.contains_key("diff_time_ms"),
        "metrics should contain diff_time_ms"
    );
    assert!(
        metrics_obj.contains_key("peak_memory_bytes"),
        "metrics should contain peak_memory_bytes"
    );
    assert!(
        metrics_obj.contains_key("rows_processed"),
        "metrics should contain rows_processed"
    );
    assert!(
        metrics_obj.contains_key("cells_compared"),
        "metrics should contain cells_compared"
    );
    assert!(
        metrics_obj.contains_key("anchors_found"),
        "metrics should contain anchors_found"
    );
    assert!(
        metrics_obj.contains_key("moves_detected"),
        "metrics should contain moves_detected"
    );

    assert_eq!(
        metrics_obj.get("rows_processed").and_then(Value::as_u64),
        Some(500)
    );
    assert_eq!(
        metrics_obj.get("parse_time_ms").and_then(Value::as_u64),
        Some(5)
    );
    assert_eq!(
        metrics_obj.get("diff_time_ms").and_then(Value::as_u64),
        Some(25)
    );
    assert_eq!(
        metrics_obj.get("cells_compared").and_then(Value::as_u64),
        Some(2500)
    );
}

```

---

### File: `core\tests\package_streaming_tests.rs`

```rust
mod common;

use common::collect_string_ids;
use excel_diff::{
    DataMashup, DiffConfig, DiffError, DiffOp, DiffSink, Grid, JsonLinesSink, Metadata,
    PackageParts, PackageXml, Permissions, SectionDocument, Sheet, SheetKind, Workbook,
    WorkbookPackage, PermissionBindingsStatus,
};
#[cfg(feature = "perf-metrics")]
use excel_diff::{CallbackSink, CellValue};
use serde::Deserialize;

#[derive(Default)]
struct StrictSink {
    finished: bool,
    finish_calls: usize,
    ops: Vec<DiffOp>,
}

impl DiffSink for StrictSink {
    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        if self.finished {
            return Err(DiffError::SinkError {
                message: "emit called after finish".to_string(),
            });
        }
        self.ops.push(op);
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        self.finish_calls += 1;
        self.finished = true;
        Ok(())
    }
}

fn make_dm(section_source: &str) -> DataMashup {
    DataMashup::new(
        0,
        PackageParts {
            package_xml: PackageXml {
                raw_xml: "<Package/>".to_string(),
            },
            main_section: SectionDocument {
                source: section_source.to_string(),
            },
            embedded_contents: Vec::new(),
        },
        Permissions::default(),
        Metadata { formulas: Vec::new() },
        Vec::new(),
        PermissionBindingsStatus::Missing,
    )
}

fn make_workbook(sheet_name: &str) -> Workbook {
    let sheet_id = excel_diff::with_default_session(|session| session.strings.intern(sheet_name));

    Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: Grid::new(0, 0),
        }],
        ..Default::default()
    }
}

#[test]
fn package_diff_streaming_does_not_emit_after_finish_and_finishes_once() {
    let wb = make_workbook("Sheet1");

    let dm_a = make_dm("section Section1;\nshared Foo = 1;");
    let dm_b = make_dm("section Section1;\nshared Bar = 1;");

    let pkg_a = WorkbookPackage {
        workbook: wb.clone(),
        data_mashup: Some(dm_a),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb,
        data_mashup: Some(dm_b),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    let mut sink = StrictSink::default();
    let summary = pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("diff_streaming should succeed");

    assert!(sink.finished, "sink should be finished at end");
    assert_eq!(
        sink.finish_calls, 1,
        "sink.finish() should be called exactly once"
    );

    assert!(
        sink.ops.iter().any(|op| op.is_m_op()),
        "expected at least one M diff op in streaming output"
    );

    assert_eq!(
        summary.op_count,
        sink.ops.len(),
        "summary.op_count should match ops actually emitted"
    );
}

#[test]
fn package_diff_streaming_finishes_on_error() {
    struct FailingSink {
        calls: usize,
        finish_called: bool,
    }

    impl DiffSink for FailingSink {
        fn emit(&mut self, _op: DiffOp) -> Result<(), DiffError> {
            self.calls += 1;
            if self.calls > 2 {
                return Err(DiffError::SinkError {
                    message: "intentional failure".to_string(),
                });
            }
            Ok(())
        }

        fn finish(&mut self) -> Result<(), DiffError> {
            self.finish_called = true;
            Ok(())
        }
    }

    let sheet_id = excel_diff::with_default_session(|session| session.strings.intern("Sheet1"));

    let mut grid_a = Grid::new(10, 1);
    let mut grid_b = Grid::new(10, 1);
    for i in 0..10 {
        grid_a.insert_cell(i, 0, Some(excel_diff::CellValue::Number(i as f64)), None);
        grid_b.insert_cell(
            i,
            0,
            Some(excel_diff::CellValue::Number((i + 100) as f64)),
            None,
        );
    }

    let wb_a = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_a,
        }],
        ..Default::default()
    };
    let wb_b = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_b,
        }],
        ..Default::default()
    };

    let pkg_a = WorkbookPackage {
        workbook: wb_a,
        data_mashup: None,
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb_b,
        data_mashup: None,
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    let mut sink = FailingSink {
        calls: 0,
        finish_called: false,
    };

    let result = pkg_a.diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink);
    assert!(result.is_err(), "diff_streaming should return error");
    assert!(
        sink.finish_called,
        "sink.finish() should be called on error"
    );
}

#[test]
fn package_diff_streaming_finishes_on_m_emit_error() {
    struct FailOnMOpSink {
        finish_called: bool,
        finish_calls: usize,
    }

    impl DiffSink for FailOnMOpSink {
        fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
            if op.is_m_op() {
                return Err(DiffError::SinkError {
                    message: "fail on m op".to_string(),
                });
            }
            Ok(())
        }

        fn finish(&mut self) -> Result<(), DiffError> {
            self.finish_calls += 1;
            self.finish_called = true;
            Ok(())
        }
    }

    let wb = make_workbook("Sheet1");

    let dm_a = make_dm("section Section1;\nshared Foo = 1;");
    let dm_b = make_dm("section Section1;\nshared Bar = 1;");

    let pkg_a = WorkbookPackage {
        workbook: wb.clone(),
        data_mashup: Some(dm_a),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb,
        data_mashup: Some(dm_b),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    let mut sink = FailOnMOpSink {
        finish_called: false,
        finish_calls: 0,
    };

    let result = pkg_a.diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink);

    assert!(result.is_err(), "expected sink error during M op emission");
    assert!(
        sink.finish_called,
        "sink.finish() should be called on M emit error"
    );
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
}

#[cfg(feature = "perf-metrics")]
#[test]
fn package_diff_streaming_includes_package_parse_time_in_total() {
    let mut grid_a = Grid::new(1, 1);
    let mut grid_b = Grid::new(1, 1);
    grid_a.insert_cell(0, 0, Some(CellValue::Number(1.0)), None);
    grid_b.insert_cell(0, 0, Some(CellValue::Number(2.0)), None);

    let sheet_id = excel_diff::with_default_session(|session| session.strings.intern("Sheet1"));

    let wb_a = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_a,
        }],
        ..Default::default()
    };
    let wb_b = Workbook {
        sheets: vec![Sheet {
            name: sheet_id,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid: grid_b,
        }],
        ..Default::default()
    };

    let pkg_a = WorkbookPackage {
        workbook: wb_a,
        data_mashup: None,
        vba_modules: None,
        parse_time_ms: 15,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb_b,
        data_mashup: None,
        vba_modules: None,
        parse_time_ms: 25,
    };

    let mut sink = CallbackSink::new(|_op| {});
    let summary = pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("diff_streaming should succeed");
    let metrics = summary.metrics.expect("expected perf metrics");

    let added = 15u64.saturating_add(25u64);
    assert!(
        metrics.parse_time_ms >= added,
        "parse_time_ms should include package parse time (>= {}), got {}",
        added,
        metrics.parse_time_ms
    );
    assert!(
        metrics.total_time_ms >= metrics.parse_time_ms,
        "total_time_ms should include parse_time_ms (total={}, parse={})",
        metrics.total_time_ms,
        metrics.parse_time_ms
    );
    assert_eq!(
        metrics.diff_time_ms,
        metrics.total_time_ms.saturating_sub(metrics.parse_time_ms)
    );
}

#[test]
fn package_streaming_json_lines_header_includes_m_strings() {
    #[derive(Deserialize)]
    struct Header {
        kind: String,
        strings: Vec<String>,
    }

    let wb = make_workbook("Sheet1");

    let dm_a = make_dm("section Section1;\nshared Foo = 1;");
    let dm_b = make_dm("section Section1;\nshared Bar = 1;");

    let pkg_a = WorkbookPackage {
        workbook: wb.clone(),
        data_mashup: Some(dm_a),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb,
        data_mashup: Some(dm_b),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    let mut out = Vec::<u8>::new();
    let mut sink = JsonLinesSink::new(&mut out);

    let summary = pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("diff_streaming should succeed");

    let text = std::str::from_utf8(&out).expect("output should be valid UTF-8");
    let mut lines = text.lines().filter(|l| !l.trim().is_empty());
    let header_line = lines.next().expect("expected a JSON Lines header line");
    let header: Header = serde_json::from_str(header_line).expect("header should parse");

    assert_eq!(header.kind, "Header");
    assert!(
        header.strings.iter().any(|s| s == "Section1/Foo"),
        "expected header string table to include query name Section1/Foo"
    );
    assert!(
        header.strings.iter().any(|s| s == "Section1/Bar"),
        "expected header string table to include query name Section1/Bar"
    );

    let mut op_lines = 0usize;
    for line in lines {
        let op: DiffOp = serde_json::from_str(line).expect("op line should parse as DiffOp");
        for id in collect_string_ids(&op) {
            assert!(
                (id.0 as usize) < header.strings.len(),
                "StringId {} out of range for header string table (len={})",
                id.0,
                header.strings.len()
            );
        }
        op_lines += 1;
    }

    assert!(op_lines > 0, "expected at least one op line after header");
    assert_eq!(
        summary.op_count, op_lines,
        "summary op_count should match number of ops written after the header"
    );
}

```

---

### File: `core\tests\parallel_determinism_tests.rs`

```rust
#![cfg(feature = "parallel")]

use excel_diff::{
    CellValue, DiffConfig, DiffContext, DiffSession, Diffable, Grid, Sheet, SheetKind, StringPool,
    VecSink, Workbook, try_diff_grids_database_mode_streaming, try_diff_workbooks_streaming,
    with_default_session,
};
use rayon::ThreadPoolBuilder;

fn run_in_pool<T>(threads: usize, f: impl FnOnce() -> T + Send) -> T
where
    T: Send,
{
    let pool = ThreadPoolBuilder::new()
        .num_threads(threads)
        .build()
        .expect("build pool");
    pool.install(f)
}

fn normalize_summary(summary: excel_diff::DiffSummary) -> excel_diff::DiffSummary {
    #[cfg(feature = "perf-metrics")]
    {
        summary.metrics = None;
    }
    summary
}

fn make_workbook(pool: &mut StringPool, value: f64) -> Workbook {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(value)), None);

    Workbook {
        sheets: vec![Sheet {
            name: pool.intern("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn make_keyed_grid(keys: &[i32], values: &[i32]) -> Grid {
    let rows = keys.len().max(values.len());
    let mut grid = Grid::new(rows as u32, 2);
    for row in 0..rows {
        let key = keys.get(row).copied().unwrap_or_default() as f64;
        let value = values.get(row).copied().unwrap_or_default() as f64;
        grid.insert_cell(row as u32, 0, Some(CellValue::Number(key)), None);
        grid.insert_cell(row as u32, 1, Some(CellValue::Number(value)), None);
    }
    grid
}

#[test]
fn ops_are_identical_across_thread_counts() {
    let rows = 10_000u32;
    let cols = 50u32;

    let mut a = Grid::new_dense(rows, cols);
    let mut b = Grid::new_dense(rows, cols);

    for r in 0..rows {
        for c in 0..cols {
            let value = Some(CellValue::Number((r * 100 + c) as f64));
            a.insert_cell(r, c, value.clone(), None);
            b.insert_cell(r, c, value, None);
        }
    }

    b.insert_cell(5000, 25, Some(CellValue::Number(123456.0)), None);

    let config = DiffConfig::default();

    let ops_1 = run_in_pool(1, || {
        with_default_session(|session| {
            let mut ctx = DiffContext::new(&mut session.strings, &config);
            let report = a.diff(&b, &mut ctx);
            report.ops
        })
    });

    let ops_4 = run_in_pool(4, || {
        with_default_session(|session| {
            let mut ctx = DiffContext::new(&mut session.strings, &config);
            let report = a.diff(&b, &mut ctx);
            report.ops
        })
    });

    assert_eq!(ops_1, ops_4);
}

#[test]
fn ops_are_identical_across_thread_counts_block_move() {
    let rows = 1000u32;
    let cols = 20u32;
    let block_size = 100u32;

    let mut a = Grid::new_dense(rows, cols);
    let mut b = Grid::new_dense(rows, cols);

    for r in 0..rows {
        for c in 0..cols {
            let value = Some(CellValue::Number((r * 100 + c) as f64));
            a.insert_cell(r, c, value.clone(), None);
            b.insert_cell(r, c, value, None);
        }
    }

    for r in 0..block_size {
        for c in 0..cols {
            let src_row = 200 + r;
            let dst_row = 600 + r;
            let value = a.get(src_row, c).and_then(|cell| cell.value.clone());
            b.insert_cell(dst_row, c, value, None);
        }
    }

    let config = DiffConfig::default();

    let ops_1 = run_in_pool(1, || {
        with_default_session(|session| {
            let mut ctx = DiffContext::new(&mut session.strings, &config);
            let report = a.diff(&b, &mut ctx);
            report.ops
        })
    });

    let ops_4 = run_in_pool(4, || {
        with_default_session(|session| {
            let mut ctx = DiffContext::new(&mut session.strings, &config);
            let report = a.diff(&b, &mut ctx);
            report.ops
        })
    });

    assert_eq!(ops_1, ops_4);
}

#[test]
fn streaming_workbook_ops_are_identical_across_thread_counts() {
    let config = DiffConfig::default();

    let output_1 = run_in_pool(1, || {
        let mut session = DiffSession::new();
        let wb_a = make_workbook(&mut session.strings, 1.0);
        let wb_b = make_workbook(&mut session.strings, 2.0);
        let mut sink = VecSink::new();
        let summary = try_diff_workbooks_streaming(
            &wb_a,
            &wb_b,
            &mut session.strings,
            &config,
            &mut sink,
        )
        .expect("streaming diff should succeed");
        (sink.into_ops(), summary)
    });

    let output_4 = run_in_pool(4, || {
        let mut session = DiffSession::new();
        let wb_a = make_workbook(&mut session.strings, 1.0);
        let wb_b = make_workbook(&mut session.strings, 2.0);
        let mut sink = VecSink::new();
        let summary = try_diff_workbooks_streaming(
            &wb_a,
            &wb_b,
            &mut session.strings,
            &config,
            &mut sink,
        )
        .expect("streaming diff should succeed");
        (sink.into_ops(), summary)
    });

    assert_eq!(output_1.0, output_4.0);
    assert_eq!(normalize_summary(output_1.1), normalize_summary(output_4.1));
}

#[test]
fn streaming_database_mode_ops_are_identical_across_thread_counts() {
    let config = DiffConfig::default();

    let output_1 = run_in_pool(1, || {
        let mut session = DiffSession::new();
        let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
        let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);
        let sheet_id = session.strings.intern("Data");
        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let summary = try_diff_grids_database_mode_streaming(
            sheet_id,
            &grid_a,
            &grid_b,
            &[0],
            &mut session.strings,
            &config,
            &mut sink,
            &mut op_count,
        )
        .expect("database streaming diff should succeed");
        (sink.into_ops(), summary)
    });

    let output_4 = run_in_pool(4, || {
        let mut session = DiffSession::new();
        let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
        let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);
        let sheet_id = session.strings.intern("Data");
        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let summary = try_diff_grids_database_mode_streaming(
            sheet_id,
            &grid_a,
            &grid_b,
            &[0],
            &mut session.strings,
            &config,
            &mut sink,
            &mut op_count,
        )
        .expect("database streaming diff should succeed");
        (sink.into_ops(), summary)
    });

    assert_eq!(output_1.0, output_4.0);
    assert_eq!(normalize_summary(output_1.1), normalize_summary(output_4.1));
}

```

---

### File: `core\tests\pbix_host_support_tests.rs`

```rust
mod common;

use common::fixture_path;
use excel_diff::{DiffConfig, DiffOp, PbixPackage, PackageError};
use std::fs::File;

#[test]
fn open_pbix_loads_datamashup() {
    let path = fixture_path("pbix_legacy_one_query_a.pbix");
    let file = File::open(&path).expect("fixture should exist");
    let pkg = PbixPackage::open(file).expect("pbix should parse");
    assert!(pkg.data_mashup().is_some(), "DataMashup should be present");
}

#[test]
fn diff_pbix_emits_query_ops() {
    let path_a = fixture_path("pbix_legacy_multi_query_a.pbix");
    let path_b = fixture_path("pbix_legacy_multi_query_b.pbix");
    let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
        .expect("pbix A should parse");
    let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
        .expect("pbix B should parse");

    let report = pkg_a.diff(&pkg_b, &DiffConfig::default());
    let has_m_ops = report.ops.iter().any(DiffOp::is_m_op);
    assert!(has_m_ops, "expected at least one Power Query op");
}

#[test]
#[cfg(feature = "model-diff")]
fn pbix_missing_datamashup_uses_model_schema() {
    let path = fixture_path("pbix_no_datamashup.pbix");
    let file = File::open(&path).expect("fixture should exist");
    let pkg = PbixPackage::open(file).expect("pbix should parse with DataModelSchema");
    assert!(pkg.data_mashup().is_none(), "DataMashup should be missing");
}

#[test]
fn pbix_missing_datamashup_and_schema_returns_dedicated_error() {
    let path = fixture_path("pbix_no_datamashup_no_schema.pbix");
    let file = File::open(&path).expect("fixture should exist");
    let err = PbixPackage::open(file).expect_err("expected missing DataMashup error");
    assert!(matches!(err, PackageError::NoDataMashupUseTabularModel));
}

```

---

### File: `core\tests\perf_large_grid_tests.rs`

```rust
#![cfg(feature = "perf-metrics")]

mod common;

use common::single_sheet_workbook;
use excel_diff::perf::DiffMetrics;
use excel_diff::{
    CellValue, DiffConfig, DiffConfigBuilder, DiffOp, DiffReport, Grid, Workbook, WorkbookPackage,
};

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

fn create_large_grid(nrows: u32, ncols: u32, base_value: i32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number(
                    (base_value as i64 + row as i64 * 1000 + col as i64) as f64,
                )),
                None,
            );
        }
    }
    grid
}

fn create_repetitive_grid(nrows: u32, ncols: u32, pattern_length: u32) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        let pattern_idx = row % pattern_length;
        for col in 0..ncols {
            grid.insert_cell(
                row,
                col,
                Some(CellValue::Number((pattern_idx * 1000 + col) as f64)),
                None,
            );
        }
    }
    grid
}

fn create_sparse_grid(nrows: u32, ncols: u32, fill_percent: u32, seed: u64) -> Grid {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            let mut hasher = DefaultHasher::new();
            (row, col, seed).hash(&mut hasher);
            let hash = hasher.finish();
            if (hash % 100) < fill_percent as u64 {
                grid.insert_cell(
                    row,
                    col,
                    Some(CellValue::Number((row * 1000 + col) as f64)),
                    None,
                );
            }
        }
    }
    grid
}

fn create_grid_with_row_block_move(
    nrows: u32,
    ncols: u32,
    src_start: u32,
    row_count: u32,
    dst_start: u32,
) -> (Grid, Grid) {
    let grid_a = create_large_grid(nrows, ncols, 0);

    let mut order: Vec<u32> = (0..nrows).collect();
    let src_end = src_start.saturating_add(row_count).min(nrows);
    let block: Vec<u32> = order
        .drain(src_start as usize..src_end as usize)
        .collect();

    let mut insert_at = dst_start;
    if insert_at > src_start {
        insert_at = insert_at.saturating_sub(block.len() as u32);
    }
    insert_at = insert_at.min(order.len() as u32);

    order.splice(
        insert_at as usize..insert_at as usize,
        block.into_iter(),
    );

    let mut grid_b = Grid::new(nrows, ncols);
    for (new_row, &old_row) in order.iter().enumerate() {
        let old_row = old_row as u32;
        for col in 0..ncols {
            if let Some(cell) = grid_a.get(old_row, col) {
                grid_b.insert_cell(new_row as u32, col, cell.value.clone(), cell.formula);
            }
        }
    }

    (grid_a, grid_b)
}

fn log_perf_metric(name: &str, metrics: &DiffMetrics, tail: &str) {
    println!(
        "PERF_METRIC {name} total_time_ms={} parse_time_ms={} diff_time_ms={} signature_build_time_ms={} move_detection_time_ms={} alignment_time_ms={} cell_diff_time_ms={} op_emit_time_ms={} report_serialize_time_ms={} peak_memory_bytes={} grid_storage_bytes={} string_pool_bytes={} op_buffer_bytes={} alignment_buffer_bytes={} rows_processed={} cells_compared={} anchors_found={} moves_detected={} hash_lookups_est={} allocations_est={}{}",
        metrics.total_time_ms,
        metrics.parse_time_ms,
        metrics.diff_time_ms,
        metrics.signature_build_time_ms,
        metrics.move_detection_time_ms,
        metrics.alignment_time_ms,
        metrics.cell_diff_time_ms,
        metrics.op_emit_time_ms,
        metrics.report_serialize_time_ms,
        metrics.peak_memory_bytes,
        metrics.grid_storage_bytes,
        metrics.string_pool_bytes,
        metrics.op_buffer_bytes,
        metrics.alignment_buffer_bytes,
        metrics.rows_processed,
        metrics.cells_compared,
        metrics.anchors_found,
        metrics.moves_detected,
        metrics.hash_lookups_est,
        metrics.allocations_est,
        tail
    );
}

#[test]
fn perf_p1_large_dense() {
    let grid_a = create_large_grid(1000, 20, 0);
    let mut grid_b = create_large_grid(1000, 20, 0);
    grid_b.insert_cell(500, 10, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "P1 dense grid should complete successfully"
    );
    assert!(report.warnings.is_empty(), "P1 should have no warnings");
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "P1 should detect the cell edit"
    );
    assert!(
        report.metrics.is_some(),
        "P1 should have metrics when perf-metrics enabled"
    );
    let metrics = report.metrics.unwrap();
    assert!(metrics.rows_processed > 0, "P1 should process rows");
    assert!(metrics.cells_compared > 0, "P1 should compare cells");
    log_perf_metric("perf_p1_large_dense", &metrics, "");
}

#[test]
fn perf_p2_large_noise() {
    let grid_a = create_large_grid(1000, 20, 0);
    let grid_b = create_large_grid(1000, 20, 1);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "P2 noise grid should complete successfully"
    );
    assert!(report.metrics.is_some(), "P2 should have metrics");
    let metrics = report.metrics.unwrap();
    assert!(metrics.rows_processed > 0, "P2 should process rows");
    log_perf_metric("perf_p2_large_noise", &metrics, "");
}

#[test]
fn perf_p3_adversarial_repetitive() {
    let grid_a = create_repetitive_grid(1000, 50, 100);
    let mut grid_b = create_repetitive_grid(1000, 50, 100);
    grid_b.insert_cell(500, 25, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "P3 repetitive grid should complete");
    assert!(report.metrics.is_some(), "P3 should have metrics");
    let metrics = report.metrics.unwrap();
    assert!(metrics.rows_processed > 0, "P3 should process rows");
    log_perf_metric("perf_p3_adversarial_repetitive", &metrics, "");
}

#[test]
fn perf_p4_99_percent_blank() {
    let grid_a = create_sparse_grid(1000, 100, 1, 12345);
    let mut grid_b = create_sparse_grid(1000, 100, 1, 12345);
    grid_b.insert_cell(500, 50, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "P4 sparse grid should complete");
    assert!(report.metrics.is_some(), "P4 should have metrics");
    let metrics = report.metrics.unwrap();
    assert!(metrics.rows_processed > 0, "P4 should process rows");
    log_perf_metric("perf_p4_99_percent_blank", &metrics, "");
}

#[test]
fn perf_p5_identical() {
    let grid_a = create_large_grid(1000, 100, 0);
    let grid_b = create_large_grid(1000, 100, 0);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "P5 identical grid should complete");
    assert!(
        report.ops.is_empty(),
        "P5 identical grids should produce no ops"
    );
    assert!(report.metrics.is_some(), "P5 should have metrics");
    let metrics = report.metrics.unwrap();
    assert!(metrics.rows_processed > 0, "P5 should process rows");
    log_perf_metric("perf_p5_identical", &metrics, "");
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_dense_single_edit() {
    let grid_a = create_large_grid(50000, 100, 0);
    let mut grid_b = create_large_grid(50000, 100, 0);
    grid_b.insert_cell(25000, 50, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(
        report.complete,
        "50k dense grid should complete successfully"
    );
    assert!(
        report.warnings.is_empty(),
        "50k dense should have no warnings"
    );
    assert!(
        report
            .ops
            .iter()
            .any(|op| matches!(op, DiffOp::CellEdited { .. })),
        "50k dense should detect the cell edit"
    );
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric(
        "perf_50k_dense_single_edit",
        &metrics,
        " (enforced: <30s; target: <5s)",
    );
    assert!(
        metrics.total_time_ms < 30000,
        "50k dense grid should complete in <30s, took {}ms",
        metrics.total_time_ms
    );
    assert_eq!(
        metrics.move_detection_time_ms, 0,
        "50k dense single edit should skip move detection (preflight bailout), got {}ms",
        metrics.move_detection_time_ms
    );
    assert_eq!(
        metrics.alignment_time_ms, 0,
        "50k dense single edit should skip alignment (preflight bailout), got {}ms",
        metrics.alignment_time_ms
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_completely_different() {
    let grid_a = create_large_grid(50000, 100, 0);
    let grid_b = create_large_grid(50000, 100, 1);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "50k different grids should complete");
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric(
        "perf_50k_completely_different",
        &metrics,
        " (enforced: <60s; target: <10s)",
    );
    assert!(
        metrics.total_time_ms < 60000,
        "50k completely different should complete in <60s, took {}ms",
        metrics.total_time_ms
    );
    assert_eq!(
        metrics.move_detection_time_ms, 0,
        "50k completely different should skip move detection (preflight bailout), got {}ms",
        metrics.move_detection_time_ms
    );
    assert_eq!(
        metrics.alignment_time_ms, 0,
        "50k completely different should skip alignment (preflight bailout), got {}ms",
        metrics.alignment_time_ms
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_adversarial_repetitive() {
    let grid_a = create_repetitive_grid(50000, 50, 100);
    let mut grid_b = create_repetitive_grid(50000, 50, 100);
    grid_b.insert_cell(25000, 25, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "50k repetitive should complete");
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric(
        "perf_50k_adversarial_repetitive",
        &metrics,
        " (enforced: <120s; target: <15s)",
    );
    assert!(
        metrics.total_time_ms < 120000,
        "50k adversarial repetitive should complete in <120s, took {}ms",
        metrics.total_time_ms
    );
    assert_eq!(
        metrics.move_detection_time_ms, 0,
        "50k adversarial repetitive should skip move detection (preflight bailout), got {}ms",
        metrics.move_detection_time_ms
    );
    assert_eq!(
        metrics.alignment_time_ms, 0,
        "50k adversarial repetitive should skip alignment (preflight bailout), got {}ms",
        metrics.alignment_time_ms
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_alignment_block_move() {
    let (grid_a, grid_b) = create_grid_with_row_block_move(50000, 50, 1000, 200, 40000);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::builder()
        .preflight_min_rows(u32::MAX)
        .max_move_detection_rows(100_000)
        .build()
        .expect("valid config");

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "50k block move should complete");
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric(
        "perf_50k_alignment_block_move",
        &metrics,
        " (alignment/move coverage)",
    );
    assert!(
        metrics.move_detection_time_ms > 0 || metrics.alignment_time_ms > 0,
        "expected alignment or move detection time to be non-zero (move_detection_time_ms={}, alignment_time_ms={})",
        metrics.move_detection_time_ms,
        metrics.alignment_time_ms
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_99_percent_blank() {
    let grid_a = create_sparse_grid(50000, 100, 1, 12345);
    let mut grid_b = create_sparse_grid(50000, 100, 1, 12345);
    grid_b.insert_cell(25000, 50, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "50k sparse should complete");
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric("perf_50k_99_percent_blank", &metrics, " (target: <2s)");
    assert!(
        metrics.total_time_ms < 30000,
        "50k 99% blank should complete in <30s, took {}ms",
        metrics.total_time_ms
    );
}

#[test]
#[ignore = "Long-running test: run with `cargo test --features perf-metrics -- --ignored` to execute"]
fn perf_50k_identical() {
    let grid_a = create_large_grid(50000, 100, 0);
    let grid_b = create_large_grid(50000, 100, 0);

    let wb_a = single_sheet_workbook("Performance", grid_a);
    let wb_b = single_sheet_workbook("Performance", grid_b);

    let config = DiffConfig::default();
    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "50k identical should complete");
    assert!(
        report.ops.is_empty(),
        "50k identical grids should have no ops"
    );
    let metrics = report.metrics.expect("should have metrics");
    log_perf_metric("perf_50k_identical", &metrics, " (target: <1s)");
    assert!(
        metrics.total_time_ms < 15000,
        "50k identical should complete in <15s, took {}ms",
        metrics.total_time_ms
    );
}

#[test]
fn preflight_skips_move_and_alignment_for_single_cell_edit_same_shape() {
    let grid_a = create_large_grid(6000, 50, 0);
    let mut grid_b = create_large_grid(6000, 50, 0);
    grid_b.insert_cell(3000, 25, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Preflight", grid_a);
    let wb_b = single_sheet_workbook("Preflight", grid_b);

    let config = DiffConfig::builder()
        .preflight_min_rows(5000)
        .preflight_in_order_mismatch_max(32)
        .preflight_in_order_match_ratio_min(0.995)
        .bailout_similarity_threshold(0.05)
        .build()
        .expect("valid config");

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "report should complete");

    let cell_edits: Vec<_> = report
        .ops
        .iter()
        .filter(|op| matches!(op, DiffOp::CellEdited { .. }))
        .collect();
    assert_eq!(
        cell_edits.len(),
        1,
        "should have exactly 1 CellEdited op, got {}",
        cell_edits.len()
    );

    let structural_ops = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::RowAdded { .. }
                    | DiffOp::RowRemoved { .. }
                    | DiffOp::ColumnAdded { .. }
                    | DiffOp::ColumnRemoved { .. }
                    | DiffOp::BlockMovedRows { .. }
                    | DiffOp::BlockMovedColumns { .. }
                    | DiffOp::BlockMovedRect { .. }
            )
        })
        .count();
    assert_eq!(structural_ops, 0, "should have no structural/move ops");

    let metrics = report.metrics.expect("should have metrics");
    assert_eq!(
        metrics.move_detection_time_ms, 0,
        "move_detection_time_ms should be 0 (skipped), got {}",
        metrics.move_detection_time_ms
    );
    assert_eq!(
        metrics.alignment_time_ms, 0,
        "alignment_time_ms should be 0 (skipped), got {}",
        metrics.alignment_time_ms
    );

    log_perf_metric(
        "preflight_single_cell_edit",
        &metrics,
        " (preflight bailout)",
    );
}

#[test]
fn perf_preflight_low_similarity() {
    let grid_a = create_large_grid(6000, 50, 0);
    let grid_b = create_large_grid(6000, 50, 100_000_000);

    let wb_a = single_sheet_workbook("Preflight", grid_a);
    let wb_b = single_sheet_workbook("Preflight", grid_b);

    let config = DiffConfig::builder()
        .preflight_min_rows(5000)
        .bailout_similarity_threshold(0.05)
        .build()
        .expect("valid config");

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "report should complete");

    let has_cell_edit = report
        .ops
        .iter()
        .any(|op| matches!(op, DiffOp::CellEdited { .. }));
    assert!(has_cell_edit, "should have at least one CellEdited op");

    let move_ops = report
        .ops
        .iter()
        .filter(|op| {
            matches!(
                op,
                DiffOp::BlockMovedRows { .. }
                    | DiffOp::BlockMovedColumns { .. }
                    | DiffOp::BlockMovedRect { .. }
            )
        })
        .count();
    assert_eq!(move_ops, 0, "should have no move ops");

    let metrics = report.metrics.expect("should have metrics");
    assert_eq!(
        metrics.move_detection_time_ms, 0,
        "move_detection_time_ms should be 0 (skipped), got {}",
        metrics.move_detection_time_ms
    );
    assert_eq!(
        metrics.alignment_time_ms, 0,
        "alignment_time_ms should be 0 (skipped), got {}",
        metrics.alignment_time_ms
    );

    log_perf_metric(
        "perf_preflight_low_similarity",
        &metrics,
        " (dissimilar bailout)",
    );
}

#[test]
fn preflight_does_not_skip_when_multiset_equal_but_order_differs() {
    let mut grid_a = Grid::new(6000, 10);
    for row in 0..6000u32 {
        for col in 0..10u32 {
            grid_a.insert_cell(
                row,
                col,
                Some(CellValue::Number((row * 100 + col) as f64)),
                None,
            );
        }
    }

    let mut grid_b = grid_a.clone();

    for col in 0..10u32 {
        let tmp_row_50 = grid_b.get(50, col).cloned();
        let tmp_row_51 = grid_b.get(51, col).cloned();
        let tmp_row_52 = grid_b.get(52, col).cloned();

        let row_80 = grid_b.get(80, col).cloned();
        let row_81 = grid_b.get(81, col).cloned();
        let row_82 = grid_b.get(82, col).cloned();

        if let Some(c) = row_80 {
            grid_b.insert_cell(50, col, c.value, c.formula);
        }
        if let Some(c) = row_81 {
            grid_b.insert_cell(51, col, c.value, c.formula);
        }
        if let Some(c) = row_82 {
            grid_b.insert_cell(52, col, c.value, c.formula);
        }

        if let Some(c) = tmp_row_50 {
            grid_b.insert_cell(80, col, c.value, c.formula);
        }
        if let Some(c) = tmp_row_51 {
            grid_b.insert_cell(81, col, c.value, c.formula);
        }
        if let Some(c) = tmp_row_52 {
            grid_b.insert_cell(82, col, c.value, c.formula);
        }
    }

    let wb_a = single_sheet_workbook("MoveTest", grid_a);
    let wb_b = single_sheet_workbook("MoveTest", grid_b);

    let config = DiffConfig::builder()
        .preflight_min_rows(5000)
        .preflight_in_order_mismatch_max(32)
        .preflight_in_order_match_ratio_min(0.995)
        .build()
        .expect("valid config");

    let report = diff_workbooks(&wb_a, &wb_b, &config);

    assert!(report.complete, "report should complete");

    let metrics = report.metrics.expect("should have metrics");

    assert!(
        metrics.alignment_time_ms > 0 || metrics.cell_diff_time_ms > 0,
        "preflight should NOT short-circuit when rows are reordered (multiset equal); expected pipeline to proceed, but alignment_time_ms={}, cell_diff_time_ms={}",
        metrics.alignment_time_ms,
        metrics.cell_diff_time_ms
    );
}

#[test]
fn preflight_cells_compared_skips_unchanged_rows() {
    let nrows = 60u32;
    let ncols = 12u32;
    let grid_a = create_large_grid(nrows, ncols, 0);
    let mut grid_b = create_large_grid(nrows, ncols, 0);
    grid_b.insert_cell(25, 7, Some(CellValue::Number(999999.0)), None);

    let wb_a = single_sheet_workbook("Preflight", grid_a);
    let wb_b = single_sheet_workbook("Preflight", grid_b);

    let config = DiffConfig::builder()
        .preflight_min_rows(0)
        .preflight_in_order_mismatch_max(4)
        .preflight_in_order_match_ratio_min(0.9)
        .build()
        .expect("valid config");

    let report = diff_workbooks(&wb_a, &wb_b, &config);
    assert!(report.complete, "report should complete");

    let metrics = report.metrics.expect("should have metrics");
    let expected_max = (ncols as u64).saturating_mul(8);
    assert!(
        metrics.cells_compared <= expected_max,
        "cells_compared should reflect skipped rows (<= {}), got {}",
        expected_max,
        metrics.cells_compared
    );
}

```

---

### File: `core\tests\permission_bindings_tests.rs`

```rust
use excel_diff::{
    DpapiDecryptError, DpapiDecryptor, PermissionBindingsStatus, Permissions, RawDataMashup,
    build_data_mashup_with_decryptor,
};
use sha2::{Digest, Sha256};
use std::io::Write;
use zip::CompressionMethod;
use zip::write::FileOptions;
use zip::ZipWriter;

struct StaticDecryptor {
    result: Result<Vec<u8>, DpapiDecryptError>,
}

impl StaticDecryptor {
    fn new(result: Result<Vec<u8>, DpapiDecryptError>) -> Self {
        Self { result }
    }
}

impl DpapiDecryptor for StaticDecryptor {
    fn decrypt(&self, _blob: &[u8], _entropy: &[u8]) -> Result<Vec<u8>, DpapiDecryptError> {
        self.result.clone()
    }
}

fn make_raw_datamashup(permissions_xml: &[u8], permission_bindings: Vec<u8>) -> RawDataMashup {
    RawDataMashup {
        version: 0,
        package_parts: minimal_package_parts(),
        permissions: permissions_xml.to_vec(),
        metadata: Vec::new(),
        permission_bindings,
    }
}

fn minimal_package_parts() -> Vec<u8> {
    let cursor = std::io::Cursor::new(Vec::new());
    let mut writer = ZipWriter::new(cursor);
    let options = FileOptions::default().compression_method(CompressionMethod::Stored);

    writer
        .start_file("Config/Package.xml", options)
        .expect("start Config/Package.xml");
    writer
        .write_all(b"<Package/>")
        .expect("write Config/Package.xml");

    writer
        .start_file("Formulas/Section1.m", options)
        .expect("start Formulas/Section1.m");
    writer
        .write_all(b"section Section1;\nshared Foo = 1;")
        .expect("write Formulas/Section1.m");

    let cursor = writer.finish().expect("finish zip");
    cursor.into_inner()
}

fn permissions_firewall_off_xml() -> Vec<u8> {
    br#"
        <PermissionList>
            <FirewallEnabled>false</FirewallEnabled>
        </PermissionList>
    "#
    .to_vec()
}

#[test]
fn null_byte_sentinel_preserves_permissions() {
    let raw = make_raw_datamashup(&permissions_firewall_off_xml(), vec![0x00]);
    let decryptor = StaticDecryptor::new(Err(DpapiDecryptError::Unavailable));

    let dm =
        build_data_mashup_with_decryptor(&raw, &decryptor).expect("DataMashup should build");

    assert_eq!(
        dm.permission_bindings_status,
        PermissionBindingsStatus::Disabled
    );
    assert!(!dm.permissions.firewall_enabled);
}

#[test]
fn dpapi_blob_unverifiable_defaults_permissions() {
    let raw = make_raw_datamashup(&permissions_firewall_off_xml(), vec![0x01, 0x02, 0x03]);
    let decryptor = StaticDecryptor::new(Err(DpapiDecryptError::Unavailable));

    let dm =
        build_data_mashup_with_decryptor(&raw, &decryptor).expect("DataMashup should build");

    assert_eq!(
        dm.permission_bindings_status,
        PermissionBindingsStatus::Unverifiable
    );
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn malformed_decrypted_plaintext_defaults_permissions() {
    let raw = make_raw_datamashup(&permissions_firewall_off_xml(), vec![0x01, 0x02, 0x03]);
    let decryptor = StaticDecryptor::new(Ok(vec![0x01, 0x02, 0x03]));

    let dm =
        build_data_mashup_with_decryptor(&raw, &decryptor).expect("DataMashup should build");

    assert_eq!(
        dm.permission_bindings_status,
        PermissionBindingsStatus::InvalidOrTampered
    );
    assert_eq!(dm.permissions, Permissions::default());
}

#[test]
fn verified_hashes_preserve_permissions() {
    let permissions_xml = permissions_firewall_off_xml();
    let raw = make_raw_datamashup(&permissions_xml, vec![0x10, 0x20, 0x30]);

    let package_hash = sha256(&raw.package_parts);
    let permissions_hash = sha256(&raw.permissions);
    let plaintext = build_plaintext(package_hash, permissions_hash);
    let decryptor = StaticDecryptor::new(Ok(plaintext));

    let dm =
        build_data_mashup_with_decryptor(&raw, &decryptor).expect("DataMashup should build");

    assert_eq!(
        dm.permission_bindings_status,
        PermissionBindingsStatus::Verified
    );
    assert!(!dm.permissions.firewall_enabled);
}

fn sha256(bytes: &[u8]) -> [u8; 32] {
    let mut hasher = Sha256::new();
    hasher.update(bytes);
    hasher.finalize().into()
}

fn build_plaintext(package_hash: [u8; 32], permissions_hash: [u8; 32]) -> Vec<u8> {
    let mut buf = Vec::new();
    buf.extend_from_slice(&(package_hash.len() as u32).to_le_bytes());
    buf.extend_from_slice(&package_hash);
    buf.extend_from_slice(&(permissions_hash.len() as u32).to_le_bytes());
    buf.extend_from_slice(&permissions_hash);
    buf
}

```

---

### File: `core\tests\pg1_ir_tests.rs`

```rust
mod common;

use common::{open_fixture_workbook, sid};
use excel_diff::{CellAddress, CellValue, Sheet, with_default_session};

#[test]
fn pg1_basic_two_sheets_structure() {
    let workbook = open_fixture_workbook("pg1_basic_two_sheets.xlsx");
    assert_eq!(workbook.sheets.len(), 2);
    assert_eq!(workbook.sheets[0].name, sid("Sheet1"));
    assert_eq!(workbook.sheets[1].name, sid("Sheet2"));
    assert!(matches!(
        workbook.sheets[0].kind,
        excel_diff::SheetKind::Worksheet
    ));
    assert!(matches!(
        workbook.sheets[1].kind,
        excel_diff::SheetKind::Worksheet
    ));

    let sheet1 = &workbook.sheets[0];
    assert_eq!(sheet1.grid.nrows, 3);
    assert_eq!(sheet1.grid.ncols, 3);
    with_default_session(|session| {
        assert_eq!(
            sheet1.grid.get(0, 0).and_then(|cell| cell
                .value
                .as_ref()
                .and_then(|v| v.as_text(session.strings()))),
            Some("R1C1")
        );
    });

    let sheet2 = &workbook.sheets[1];
    assert_eq!(sheet2.grid.nrows, 5);
    assert_eq!(sheet2.grid.ncols, 2);
    with_default_session(|session| {
        assert_eq!(
            sheet2.grid.get(0, 0).and_then(|cell| {
                cell.value
                    .as_ref()
                    .and_then(|v| v.as_text(session.strings()))
            }),
            Some("S2_R1C1")
        );
    });
}

#[test]
fn pg1_sparse_used_range_extents() {
    let workbook = open_fixture_workbook("pg1_sparse_used_range.xlsx");
    let sheet = workbook
        .sheets
        .iter()
        .find(|s| s.name == sid("Sparse"))
        .expect("Sparse sheet present");

    assert_eq!(sheet.grid.nrows, 10);
    assert_eq!(sheet.grid.ncols, 7);

    assert_cell_text(sheet, 0, 0, "A1");
    assert_cell_text(sheet, 1, 1, "B2");
    assert_cell_text(sheet, 9, 6, "G10");
    assert_eq!(sheet.grid.cell_count(), 3);
}

#[test]
fn pg1_empty_and_mixed_sheets() {
    let workbook = open_fixture_workbook("pg1_empty_and_mixed_sheets.xlsx");

    let empty = sheet_by_name(&workbook, "Empty");
    assert_eq!(empty.grid.nrows, 0);
    assert_eq!(empty.grid.ncols, 0);
    assert_eq!(empty.grid.cell_count(), 0);

    let values_only = sheet_by_name(&workbook, "ValuesOnly");
    assert_eq!(values_only.grid.nrows, 10);
    assert_eq!(values_only.grid.ncols, 10);
    let values: Vec<_> = values_only
        .grid
        .iter_cells()
        .map(|(_, cell)| cell)
        .collect();
    assert!(
        values
            .iter()
            .all(|c| c.value.is_some() && c.formula.is_none()),
        "ValuesOnly cells should have values and no formulas"
    );
    assert_eq!(
        values_only
            .grid
            .get(0, 0)
            .and_then(|cell| cell.value.as_ref().and_then(CellValue::as_number)),
        Some(1.0)
    );

    let formulas = sheet_by_name(&workbook, "FormulasOnly");
    assert_eq!(formulas.grid.nrows, 10);
    assert_eq!(formulas.grid.ncols, 10);
    let first = formulas.grid.get(0, 0).expect("A1 should exist");
    with_default_session(|session| {
        assert_eq!(
            first.formula.map(|id| session.strings.resolve(id)),
            Some("ValuesOnly!A1")
        );
    });
    assert!(
        first.value.is_some(),
        "Formulas should surface cached values when present"
    );
    assert!(
        formulas
            .grid
            .iter_cells()
            .all(|(_, cell)| cell.formula.is_some()),
        "All cells should carry formulas in FormulasOnly"
    );
}

fn sheet_by_name<'a>(workbook: &'a excel_diff::Workbook, name: &str) -> &'a Sheet {
    workbook
        .sheets
        .iter()
        .find(|s| s.name == sid(name))
        .unwrap_or_else(|| panic!("sheet {name} not found"))
}

fn assert_cell_text(sheet: &Sheet, row: u32, col: u32, expected: &str) {
    let cell = sheet
        .grid
        .get(row, col)
        .unwrap_or_else(|| panic!("cell {expected} should exist"));
    assert_eq!(CellAddress::from_coords(row, col).to_a1(), expected);
    with_default_session(|session| {
        assert_eq!(
            cell.value
                .as_ref()
                .and_then(|v| v.as_text(session.strings()))
                .unwrap_or(""),
            expected
        );
    });
}

```

---

### File: `core\tests\pg3_snapshot_tests.rs`

```rust
mod common;

use common::{open_fixture_workbook, sid};
use excel_diff::{
    Cell, CellAddress, CellSnapshot, CellValue, Sheet, Workbook, address_to_index,
    with_default_session,
};

fn sheet_by_name<'a>(workbook: &'a Workbook, name: &str) -> &'a Sheet {
    with_default_session(|session| {
        let id = session.strings.intern(name);
        workbook
            .sheets
            .iter()
            .find(|s| s.name == id)
            .expect("sheet should exist")
    })
}

fn find_cell<'a>(sheet: &'a Sheet, addr: &str) -> Option<&'a Cell> {
    let (row, col) = address_to_index(addr).expect("address should parse");
    sheet.grid.get(row, col)
}

fn snapshot(sheet: &Sheet, addr: &str) -> CellSnapshot {
    let (row, col) = address_to_index(addr).expect("address should parse");
    if let Some(cell) = find_cell(sheet, addr) {
        CellSnapshot::from_cell(row, col, cell)
    } else {
        CellSnapshot {
            addr: CellAddress::from_indices(row, col),
            value: None,
            formula: None,
        }
    }
}

fn resolve_text(id: excel_diff::StringId) -> String {
    with_default_session(|session| session.strings.resolve(id).to_string())
}

#[test]
fn pg3_value_and_formula_cells_snapshot_from_excel() {
    let workbook = open_fixture_workbook("pg3_value_and_formula_cells.xlsx");
    let sheet = sheet_by_name(&workbook, "Types");

    let a1 = snapshot(sheet, "A1");
    assert_eq!(a1.addr.to_string(), "A1");
    assert_eq!(a1.value, Some(CellValue::Number(42.0)));
    assert!(a1.formula.is_none());

    let a2 = snapshot(sheet, "A2");
    let a2_text = match a2.value {
        Some(CellValue::Text(id)) => resolve_text(id),
        other => panic!("expected text cell, got {:?}", other),
    };
    assert_eq!(a2_text, "hello");
    assert!(a2.formula.is_none());

    let a3 = snapshot(sheet, "A3");
    assert_eq!(a3.value, Some(CellValue::Bool(true)));
    assert!(a3.formula.is_none());

    let a4 = snapshot(sheet, "A4");
    assert!(a4.value.is_none());
    assert!(a4.formula.is_none());

    let b1 = snapshot(sheet, "B1");
    assert!(matches!(
        b1.value,
        Some(CellValue::Number(n)) if (n - 43.0).abs() < 1e-6
    ));
    assert_eq!(b1.addr.to_string(), "B1");
    let b1_formula = b1
        .formula
        .map(resolve_text)
        .expect("B1 should have a formula");
    assert!(b1_formula.contains("A1+1"));

    let b2 = snapshot(sheet, "B2");
    let b2_text = match b2.value {
        Some(CellValue::Text(id)) => resolve_text(id),
        other => panic!("expected text cell, got {:?}", other),
    };
    assert_eq!(b2_text, "hello world");
    assert_eq!(b2.addr.to_string(), "B2");
    let b2_formula = b2
        .formula
        .map(resolve_text)
        .expect("B2 should have a formula");
    assert!(b2_formula.contains("hello"));
    assert!(b2_formula.contains("world"));

    let b3 = snapshot(sheet, "B3");
    assert_eq!(b3.value, Some(CellValue::Bool(true)));
    assert_eq!(b3.addr.to_string(), "B3");
    let b3_formula = b3
        .formula
        .map(resolve_text)
        .expect("B3 should have a formula");
    assert!(
        b3_formula.contains(">0"),
        "B3 formula should include comparison: {b3_formula:?}"
    );
}

#[test]
fn snapshot_json_roundtrip() {
    let workbook = open_fixture_workbook("pg3_value_and_formula_cells.xlsx");
    let sheet = sheet_by_name(&workbook, "Types");

    let snapshots = vec![
        snapshot(sheet, "A1"),
        snapshot(sheet, "A2"),
        snapshot(sheet, "B1"),
        snapshot(sheet, "B2"),
        snapshot(sheet, "B3"),
    ];

    for snap in snapshots {
        let addr = snap.addr.to_string();
        let json = serde_json::to_string(&snap).expect("snapshot should serialize");
        let as_value: serde_json::Value =
            serde_json::from_str(&json).expect("snapshot JSON should parse to value");
        assert_eq!(as_value["addr"], serde_json::Value::String(addr));
        let snap_back: CellSnapshot = serde_json::from_str(&json).expect("snapshot should parse");
        assert_eq!(snap.addr, snap_back.addr);
        assert_eq!(snap, snap_back);
    }
}

#[test]
fn snapshot_json_roundtrip_detects_tampered_addr() {
    let snap = CellSnapshot {
        addr: "Z9".parse().expect("address should parse"),
        value: Some(CellValue::Number(1.0)),
        formula: Some(sid("A1+1")),
    };

    let mut value: serde_json::Value =
        serde_json::from_str(&serde_json::to_string(&snap).expect("serialize should work"))
            .expect("serialized JSON should parse");
    value["addr"] = serde_json::Value::String("A1".into());

    let tampered_json = serde_json::to_string(&value).expect("tampered JSON should serialize");
    let tampered: CellSnapshot =
        serde_json::from_str(&tampered_json).expect("tampered JSON should parse");

    assert_ne!(snap.addr, tampered.addr);
    assert_eq!(snap, tampered, "value/formula equality ignores addr");
}

#[test]
fn snapshot_json_rejects_invalid_addr_1a() {
    let json = r#"{"addr":"1A","value":null,"formula":null}"#;
    let result: Result<CellSnapshot, _> = serde_json::from_str(json);
    let err = result
        .expect_err("invalid addr should fail to deserialize")
        .to_string();

    assert!(
        err.contains("invalid cell address"),
        "error should mention invalid cell address: {err}"
    );
    assert!(
        err.contains("1A"),
        "error should include the offending address: {err}"
    );
}

#[test]
fn snapshot_json_rejects_invalid_addr_a0() {
    let json = r#"{"addr":"A0","value":null,"formula":null}"#;
    let result: Result<CellSnapshot, _> = serde_json::from_str(json);
    let err = result
        .expect_err("invalid addr should fail to deserialize")
        .to_string();

    assert!(
        err.contains("invalid cell address"),
        "error should mention invalid cell address: {err}"
    );
    assert!(
        err.contains("A0"),
        "error should include the offending address: {err}"
    );
}

```

---

### File: `core\tests\pg4_diffop_tests.rs`

```rust
mod common;

use common::sid;
use excel_diff::{
    CellAddress, CellSnapshot, CellValue, ColSignature, DiffOp, DiffReport, FormulaDiffResult,
    QueryChangeKind, QueryMetadataField, RowSignature,
};
#[cfg(feature = "model-diff")]
use excel_diff::{ExpressionChangeKind, ModelColumnProperty, RelationshipProperty};
use serde_json::Value;
use std::collections::BTreeSet;

fn addr(a1: &str) -> CellAddress {
    a1.parse().expect("address should parse")
}

fn sid_json(s: &str) -> Value {
    Value::Number(sid(s).0.into())
}

fn snapshot(a1: &str, value: Option<CellValue>, formula: Option<&str>) -> CellSnapshot {
    CellSnapshot {
        addr: addr(a1),
        value,
        formula: formula.map(|s| sid(s)),
    }
}

fn sample_cell_edited() -> DiffOp {
    DiffOp::CellEdited {
        sheet: sid("Sheet1"),
        addr: addr("C3"),
        from: snapshot("C3", Some(CellValue::Number(1.0)), None),
        to: snapshot("C3", Some(CellValue::Number(2.0)), None),
        formula_diff: FormulaDiffResult::Unchanged,
    }
}

// Enforces the invariant documented on DiffOp::CellEdited.
fn assert_cell_edited_invariants(op: &DiffOp, expected_sheet: &str, expected_addr: &str) {
    let expected_addr_parsed: CellAddress =
        expected_addr.parse().expect("expected_addr should parse");
    if let DiffOp::CellEdited {
        sheet,
        addr,
        from,
        to,
        ..
    } = op
    {
        assert_eq!(sheet, &sid(expected_sheet));
        assert_eq!(*addr, expected_addr_parsed);
        assert_eq!(from.addr, expected_addr_parsed);
        assert_eq!(to.addr, expected_addr_parsed);
    } else {
        panic!("expected CellEdited");
    }
}

fn op_kind(op: &DiffOp) -> &'static str {
    match op {
        DiffOp::SheetAdded { .. } => "SheetAdded",
        DiffOp::SheetRemoved { .. } => "SheetRemoved",
        DiffOp::SheetRenamed { .. } => "SheetRenamed",
        DiffOp::RowAdded { .. } => "RowAdded",
        DiffOp::RowRemoved { .. } => "RowRemoved",
        DiffOp::RowReplaced { .. } => "RowReplaced",
        DiffOp::DuplicateKeyCluster { .. } => "DuplicateKeyCluster",
        DiffOp::ColumnAdded { .. } => "ColumnAdded",
        DiffOp::ColumnRemoved { .. } => "ColumnRemoved",
        DiffOp::BlockMovedRows { .. } => "BlockMovedRows",
        DiffOp::BlockMovedColumns { .. } => "BlockMovedColumns",
        DiffOp::BlockMovedRect { .. } => "BlockMovedRect",
        DiffOp::RectReplaced { .. } => "RectReplaced",
        DiffOp::CellEdited { .. } => "CellEdited",
        _ => "Unknown",
    }
}

fn json_keys(json: &Value) -> BTreeSet<String> {
    json.as_object()
        .expect("object json")
        .keys()
        .cloned()
        .collect()
}

#[test]
fn pg4_construct_cell_edited_diffop() {
    let op = sample_cell_edited();

    assert_cell_edited_invariants(&op, "Sheet1", "C3");
    if let DiffOp::CellEdited { from, to, .. } = &op {
        assert_ne!(from.value, to.value);
    }
}

#[test]
fn pg4_construct_row_and_column_diffops() {
    let row_added_with_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 10,
        row_signature: Some(RowSignature { hash: 0xDEADBEEF }),
    };
    let row_added_without_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 11,
        row_signature: None,
    };
    let row_removed_with_sig = DiffOp::RowRemoved {
        sheet: sid("Sheet1"),
        row_idx: 9,
        row_signature: Some(RowSignature { hash: 0x1234 }),
    };
    let row_removed_without_sig = DiffOp::RowRemoved {
        sheet: sid("Sheet1"),
        row_idx: 8,
        row_signature: None,
    };
    let col_added_with_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet2"),
        col_idx: 2,
        col_signature: Some(ColSignature { hash: 0xABCDEF }),
    };
    let col_added_without_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet2"),
        col_idx: 3,
        col_signature: None,
    };
    let col_removed_with_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 1,
        col_signature: Some(ColSignature { hash: 0x123456 }),
    };
    let col_removed_without_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 0,
        col_signature: None,
    };

    if let DiffOp::RowAdded {
        sheet,
        row_idx,
        row_signature,
    } = &row_added_with_sig
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*row_idx, 10);
        assert_eq!(row_signature.as_ref().unwrap().hash, 0xDEADBEEF);
    } else {
        panic!("expected RowAdded with signature");
    }

    if let DiffOp::RowAdded {
        sheet,
        row_idx,
        row_signature,
    } = &row_added_without_sig
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*row_idx, 11);
        assert!(row_signature.is_none());
    } else {
        panic!("expected RowAdded without signature");
    }

    if let DiffOp::RowRemoved {
        sheet,
        row_idx,
        row_signature,
    } = &row_removed_with_sig
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*row_idx, 9);
        assert_eq!(row_signature.as_ref().unwrap().hash, 0x1234);
    } else {
        panic!("expected RowRemoved with signature");
    }

    if let DiffOp::RowRemoved {
        sheet,
        row_idx,
        row_signature,
    } = &row_removed_without_sig
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*row_idx, 8);
        assert!(row_signature.is_none());
    } else {
        panic!("expected RowRemoved without signature");
    }

    if let DiffOp::ColumnAdded {
        sheet,
        col_idx,
        col_signature,
    } = &col_added_with_sig
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*col_idx, 2);
        assert_eq!(col_signature.as_ref().unwrap().hash, 0xABCDEF);
    } else {
        panic!("expected ColumnAdded with signature");
    }

    if let DiffOp::ColumnAdded {
        sheet,
        col_idx,
        col_signature,
    } = &col_added_without_sig
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*col_idx, 3);
        assert!(col_signature.is_none());
    } else {
        panic!("expected ColumnAdded without signature");
    }

    if let DiffOp::ColumnRemoved {
        sheet,
        col_idx,
        col_signature,
    } = &col_removed_with_sig
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*col_idx, 1);
        assert_eq!(col_signature.as_ref().unwrap().hash, 0x123456);
    } else {
        panic!("expected ColumnRemoved with signature");
    }

    if let DiffOp::ColumnRemoved {
        sheet,
        col_idx,
        col_signature,
    } = &col_removed_without_sig
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*col_idx, 0);
        assert!(col_signature.is_none());
    } else {
        panic!("expected ColumnRemoved without signature");
    }

    assert_ne!(row_added_with_sig, row_added_without_sig);
    assert_ne!(row_removed_with_sig, row_removed_without_sig);
    assert_ne!(col_added_with_sig, col_added_without_sig);
    assert_ne!(col_removed_with_sig, col_removed_without_sig);
}

#[test]
fn pg4_construct_block_move_diffops() {
    let block_rows_with_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 10,
        row_count: 3,
        dst_start_row: 5,
        block_hash: Some(0x12345678),
    };
    let block_rows_without_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 20,
        row_count: 2,
        dst_start_row: 0,
        block_hash: None,
    };
    let block_cols_with_hash = DiffOp::BlockMovedColumns {
        sheet: sid("Sheet2"),
        src_start_col: 7,
        col_count: 2,
        dst_start_col: 3,
        block_hash: Some(0xCAFEBABE),
    };
    let block_cols_without_hash = DiffOp::BlockMovedColumns {
        sheet: sid("Sheet2"),
        src_start_col: 4,
        col_count: 1,
        dst_start_col: 9,
        block_hash: None,
    };

    if let DiffOp::BlockMovedRows {
        sheet,
        src_start_row,
        row_count,
        dst_start_row,
        block_hash,
    } = &block_rows_with_hash
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*src_start_row, 10);
        assert_eq!(*row_count, 3);
        assert_eq!(*dst_start_row, 5);
        assert_eq!(block_hash.unwrap(), 0x12345678);
    } else {
        panic!("expected BlockMovedRows with hash");
    }

    if let DiffOp::BlockMovedRows {
        sheet,
        src_start_row,
        row_count,
        dst_start_row,
        block_hash,
    } = &block_rows_without_hash
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*src_start_row, 20);
        assert_eq!(*row_count, 2);
        assert_eq!(*dst_start_row, 0);
        assert!(block_hash.is_none());
    } else {
        panic!("expected BlockMovedRows without hash");
    }

    if let DiffOp::BlockMovedColumns {
        sheet,
        src_start_col,
        col_count,
        dst_start_col,
        block_hash,
    } = &block_cols_with_hash
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*src_start_col, 7);
        assert_eq!(*col_count, 2);
        assert_eq!(*dst_start_col, 3);
        assert_eq!(block_hash.unwrap(), 0xCAFEBABE);
    } else {
        panic!("expected BlockMovedColumns with hash");
    }

    if let DiffOp::BlockMovedColumns {
        sheet,
        src_start_col,
        col_count,
        dst_start_col,
        block_hash,
    } = &block_cols_without_hash
    {
        assert_eq!(sheet, &sid("Sheet2"));
        assert_eq!(*src_start_col, 4);
        assert_eq!(*col_count, 1);
        assert_eq!(*dst_start_col, 9);
        assert!(block_hash.is_none());
    } else {
        panic!("expected BlockMovedColumns without hash");
    }

    assert_ne!(block_rows_with_hash, block_rows_without_hash);
    assert_ne!(block_cols_with_hash, block_cols_without_hash);
}

#[test]
fn pg4_construct_block_rect_diffops() {
    let rect_with_hash = DiffOp::BlockMovedRect {
        sheet: sid("Sheet1"),
        src_start_row: 5,
        src_row_count: 3,
        src_start_col: 2,
        src_col_count: 4,
        dst_start_row: 10,
        dst_start_col: 6,
        block_hash: Some(0xCAFEBABE),
    };
    let rect_without_hash = DiffOp::BlockMovedRect {
        sheet: sid("Sheet1"),
        src_start_row: 0,
        src_row_count: 1,
        src_start_col: 0,
        src_col_count: 1,
        dst_start_row: 20,
        dst_start_col: 10,
        block_hash: None,
    };

    if let DiffOp::BlockMovedRect {
        sheet,
        src_start_row,
        src_row_count,
        src_start_col,
        src_col_count,
        dst_start_row,
        dst_start_col,
        block_hash,
    } = &rect_with_hash
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*src_start_row, 5);
        assert_eq!(*src_row_count, 3);
        assert_eq!(*src_start_col, 2);
        assert_eq!(*src_col_count, 4);
        assert_eq!(*dst_start_row, 10);
        assert_eq!(*dst_start_col, 6);
        assert_eq!(block_hash.unwrap(), 0xCAFEBABE);
    } else {
        panic!("expected BlockMovedRect with hash");
    }

    if let DiffOp::BlockMovedRect {
        sheet,
        src_start_row,
        src_row_count,
        src_start_col,
        src_col_count,
        dst_start_row,
        dst_start_col,
        block_hash,
    } = &rect_without_hash
    {
        assert_eq!(sheet, &sid("Sheet1"));
        assert_eq!(*src_start_row, 0);
        assert_eq!(*src_row_count, 1);
        assert_eq!(*src_start_col, 0);
        assert_eq!(*src_col_count, 1);
        assert_eq!(*dst_start_row, 20);
        assert_eq!(*dst_start_col, 10);
        assert!(block_hash.is_none());
    } else {
        panic!("expected BlockMovedRect without hash");
    }

    assert_ne!(rect_with_hash, rect_without_hash);
}

#[test]
fn pg4_cell_edited_json_shape() {
    let op = sample_cell_edited();
    let json = serde_json::to_value(&op).expect("serialize");
    assert_cell_edited_invariants(&op, "Sheet1", "C3");

    assert_eq!(json["kind"], "CellEdited");
    assert_eq!(json["sheet"], sid_json("Sheet1"));
    assert_eq!(json["addr"], "C3");
    assert_eq!(json["from"]["addr"], "C3");
    assert_eq!(json["to"]["addr"], "C3");
    assert_eq!(json["formula_diff"], "unchanged");

    let obj = json.as_object().expect("object json");
    let keys: BTreeSet<String> = obj.keys().cloned().collect();
    let expected: BTreeSet<String> = ["addr", "from", "kind", "sheet", "to", "formula_diff"]
        .into_iter()
        .map(String::from)
        .collect();
    assert_eq!(keys, expected);
}

#[test]
fn pg4_row_added_json_optional_signature() {
    let op_without_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 10,
        row_signature: None,
    };
    let json_without = serde_json::to_value(&op_without_sig).expect("serialize without sig");
    let obj_without = json_without.as_object().expect("object json");
    assert_eq!(json_without["kind"], "RowAdded");
    assert_eq!(json_without["sheet"], sid_json("Sheet1"));
    assert_eq!(json_without["row_idx"], 10);
    assert!(obj_without.get("row_signature").is_none());

    let op_with_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 10,
        row_signature: Some(RowSignature { hash: 123 }),
    };
    let json_with = serde_json::to_value(&op_with_sig).expect("serialize with sig");
    assert_eq!(
        json_with["row_signature"]["hash"],
        "0000000000000000000000000000007b"
    );
}

#[test]
fn pg4_column_added_json_optional_signature() {
    let added_without_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet1"),
        col_idx: 5,
        col_signature: None,
    };
    let json_added_without = serde_json::to_value(&added_without_sig).expect("serialize no sig");
    let obj_added_without = json_added_without.as_object().expect("object json");
    assert_eq!(json_added_without["kind"], "ColumnAdded");
    assert_eq!(json_added_without["sheet"], sid_json("Sheet1"));
    assert_eq!(json_added_without["col_idx"], 5);
    assert!(obj_added_without.get("col_signature").is_none());

    let added_with_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet1"),
        col_idx: 6,
        col_signature: Some(ColSignature { hash: 321 }),
    };
    let json_added_with = serde_json::to_value(&added_with_sig).expect("serialize with sig");
    assert_eq!(
        json_added_with["col_signature"]["hash"],
        "00000000000000000000000000000141"
    );

    let removed_without_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 2,
        col_signature: None,
    };
    let json_removed_without =
        serde_json::to_value(&removed_without_sig).expect("serialize removed no sig");
    let obj_removed_without = json_removed_without.as_object().expect("object json");
    assert_eq!(json_removed_without["kind"], "ColumnRemoved");
    assert!(obj_removed_without.get("col_signature").is_none());

    let removed_with_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 1,
        col_signature: Some(ColSignature { hash: 654 }),
    };
    let json_removed_with =
        serde_json::to_value(&removed_with_sig).expect("serialize removed with sig");
    assert_eq!(
        json_removed_with["col_signature"]["hash"],
        "0000000000000000000000000000028e"
    );
}

#[test]
fn pg4_block_moved_rows_json_optional_hash() {
    let op_without_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 1,
        row_count: 2,
        dst_start_row: 5,
        block_hash: None,
    };
    let json_without = serde_json::to_value(&op_without_hash).expect("serialize without hash");
    let obj_without = json_without.as_object().expect("object json");
    assert_eq!(json_without["kind"], "BlockMovedRows");
    assert!(obj_without.get("block_hash").is_none());

    let op_with_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 1,
        row_count: 2,
        dst_start_row: 5,
        block_hash: Some(777),
    };
    let json_with = serde_json::to_value(&op_with_hash).expect("serialize with hash");
    assert_eq!(json_with["block_hash"], Value::from(777));
}

#[test]
fn pg4_block_moved_columns_json_optional_hash() {
    let op_without_hash = DiffOp::BlockMovedColumns {
        sheet: sid("SheetX"),
        src_start_col: 2,
        col_count: 3,
        dst_start_col: 9,
        block_hash: None,
    };
    let json_without = serde_json::to_value(&op_without_hash).expect("serialize without hash");
    let obj_without = json_without.as_object().expect("object json");
    assert_eq!(json_without["kind"], "BlockMovedColumns");
    assert!(obj_without.get("block_hash").is_none());

    let op_with_hash = DiffOp::BlockMovedColumns {
        sheet: sid("SheetX"),
        src_start_col: 2,
        col_count: 3,
        dst_start_col: 9,
        block_hash: Some(4242),
    };
    let json_with = serde_json::to_value(&op_with_hash).expect("serialize with hash");
    assert_eq!(json_with["block_hash"], Value::from(4242));
}

#[test]
fn pg4_sheet_added_and_removed_json_shape() {
    let added = DiffOp::SheetAdded {
        sheet: sid("Sheet1"),
    };
    let added_json = serde_json::to_value(&added).expect("serialize sheet added");
    assert_eq!(added_json["kind"], "SheetAdded");
    assert_eq!(added_json["sheet"], sid_json("Sheet1"));
    let added_keys = json_keys(&added_json);
    let expected_keys: BTreeSet<String> = ["kind", "sheet"].into_iter().map(String::from).collect();
    assert_eq!(added_keys, expected_keys);

    let removed = DiffOp::SheetRemoved {
        sheet: sid("SheetX"),
    };
    let removed_json = serde_json::to_value(&removed).expect("serialize sheet removed");
    assert_eq!(removed_json["kind"], "SheetRemoved");
    assert_eq!(removed_json["sheet"], sid_json("SheetX"));
    let removed_keys = json_keys(&removed_json);
    assert_eq!(removed_keys, expected_keys);
}

#[test]
fn pg4_sheet_renamed_json_shape() {
    let renamed = DiffOp::SheetRenamed {
        sheet: sid("SheetNew"),
        from: sid("SheetOld"),
        to: sid("SheetNew"),
    };
    let renamed_json = serde_json::to_value(&renamed).expect("serialize sheet renamed");
    assert_eq!(renamed_json["kind"], "SheetRenamed");
    assert_eq!(renamed_json["sheet"], sid_json("SheetNew"));
    assert_eq!(renamed_json["from"], sid_json("SheetOld"));
    assert_eq!(renamed_json["to"], sid_json("SheetNew"));
    let renamed_keys = json_keys(&renamed_json);
    let expected_keys: BTreeSet<String> =
        ["kind", "sheet", "from", "to"].into_iter().map(String::from).collect();
    assert_eq!(renamed_keys, expected_keys);
}

#[test]
fn pg4_row_and_column_json_shape_keysets() {
    let expected_row_with_sig: BTreeSet<String> = ["kind", "row_idx", "row_signature", "sheet"]
        .into_iter()
        .map(String::from)
        .collect();
    let expected_row_without_sig: BTreeSet<String> = ["kind", "row_idx", "sheet"]
        .into_iter()
        .map(String::from)
        .collect();
    let expected_col_with_sig: BTreeSet<String> = ["col_idx", "col_signature", "kind", "sheet"]
        .into_iter()
        .map(String::from)
        .collect();
    let expected_col_without_sig: BTreeSet<String> = ["col_idx", "kind", "sheet"]
        .into_iter()
        .map(String::from)
        .collect();

    let row_added_with_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 10,
        row_signature: Some(RowSignature { hash: 0xDEADBEEF }),
    };
    let row_added_without_sig = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 11,
        row_signature: None,
    };
    let row_removed_with_sig = DiffOp::RowRemoved {
        sheet: sid("Sheet1"),
        row_idx: 9,
        row_signature: Some(RowSignature { hash: 0x1234 }),
    };
    let row_removed_without_sig = DiffOp::RowRemoved {
        sheet: sid("Sheet1"),
        row_idx: 8,
        row_signature: None,
    };

    let col_added_with_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet2"),
        col_idx: 2,
        col_signature: Some(ColSignature { hash: 0xABCDEF }),
    };
    let col_added_without_sig = DiffOp::ColumnAdded {
        sheet: sid("Sheet2"),
        col_idx: 3,
        col_signature: None,
    };
    let col_removed_with_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 1,
        col_signature: Some(ColSignature { hash: 0x123456 }),
    };
    let col_removed_without_sig = DiffOp::ColumnRemoved {
        sheet: sid("Sheet2"),
        col_idx: 0,
        col_signature: None,
    };

    let cases = vec![
        (
            row_added_with_sig,
            "RowAdded",
            expected_row_with_sig.clone(),
        ),
        (
            row_added_without_sig,
            "RowAdded",
            expected_row_without_sig.clone(),
        ),
        (
            row_removed_with_sig,
            "RowRemoved",
            expected_row_with_sig.clone(),
        ),
        (
            row_removed_without_sig,
            "RowRemoved",
            expected_row_without_sig.clone(),
        ),
        (
            col_added_with_sig,
            "ColumnAdded",
            expected_col_with_sig.clone(),
        ),
        (
            col_added_without_sig,
            "ColumnAdded",
            expected_col_without_sig.clone(),
        ),
        (
            col_removed_with_sig,
            "ColumnRemoved",
            expected_col_with_sig.clone(),
        ),
        (
            col_removed_without_sig,
            "ColumnRemoved",
            expected_col_without_sig.clone(),
        ),
    ];

    for (op, expected_kind, expected_keys) in cases {
        let json = serde_json::to_value(&op).expect("serialize diffop");
        assert_eq!(json["kind"], expected_kind);
        let keys = json_keys(&json);
        assert_eq!(keys, expected_keys);
    }
}

#[test]
fn pg4_block_move_json_shape_keysets() {
    let expected_rows_with_hash: BTreeSet<String> = [
        "block_hash",
        "dst_start_row",
        "kind",
        "row_count",
        "sheet",
        "src_start_row",
    ]
    .into_iter()
    .map(String::from)
    .collect();
    let expected_rows_without_hash: BTreeSet<String> = [
        "dst_start_row",
        "kind",
        "row_count",
        "sheet",
        "src_start_row",
    ]
    .into_iter()
    .map(String::from)
    .collect();
    let expected_cols_with_hash: BTreeSet<String> = [
        "block_hash",
        "col_count",
        "dst_start_col",
        "kind",
        "sheet",
        "src_start_col",
    ]
    .into_iter()
    .map(String::from)
    .collect();
    let expected_cols_without_hash: BTreeSet<String> = [
        "col_count",
        "dst_start_col",
        "kind",
        "sheet",
        "src_start_col",
    ]
    .into_iter()
    .map(String::from)
    .collect();
    let expected_rect_with_hash: BTreeSet<String> = [
        "block_hash",
        "dst_start_col",
        "dst_start_row",
        "kind",
        "sheet",
        "src_col_count",
        "src_row_count",
        "src_start_col",
        "src_start_row",
    ]
    .into_iter()
    .map(String::from)
    .collect();
    let expected_rect_without_hash: BTreeSet<String> = [
        "dst_start_col",
        "dst_start_row",
        "kind",
        "sheet",
        "src_col_count",
        "src_row_count",
        "src_start_col",
        "src_start_row",
    ]
    .into_iter()
    .map(String::from)
    .collect();

    let block_rows_with_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 10,
        row_count: 3,
        dst_start_row: 5,
        block_hash: Some(0x12345678),
    };
    let block_rows_without_hash = DiffOp::BlockMovedRows {
        sheet: sid("Sheet1"),
        src_start_row: 20,
        row_count: 2,
        dst_start_row: 0,
        block_hash: None,
    };
    let block_cols_with_hash = DiffOp::BlockMovedColumns {
        sheet: sid("Sheet2"),
        src_start_col: 7,
        col_count: 2,
        dst_start_col: 3,
        block_hash: Some(0xCAFEBABE),
    };
    let block_cols_without_hash = DiffOp::BlockMovedColumns {
        sheet: sid("Sheet2"),
        src_start_col: 4,
        col_count: 1,
        dst_start_col: 9,
        block_hash: None,
    };
    let block_rect_with_hash = DiffOp::BlockMovedRect {
        sheet: sid("SheetZ"),
        src_start_row: 2,
        src_row_count: 2,
        src_start_col: 3,
        src_col_count: 4,
        dst_start_row: 8,
        dst_start_col: 1,
        block_hash: Some(0xAABBCCDD),
    };
    let block_rect_without_hash = DiffOp::BlockMovedRect {
        sheet: sid("SheetZ"),
        src_start_row: 5,
        src_row_count: 1,
        src_start_col: 0,
        src_col_count: 2,
        dst_start_row: 10,
        dst_start_col: 4,
        block_hash: None,
    };

    let cases = vec![
        (
            block_rows_with_hash,
            "BlockMovedRows",
            expected_rows_with_hash.clone(),
        ),
        (
            block_rows_without_hash,
            "BlockMovedRows",
            expected_rows_without_hash.clone(),
        ),
        (
            block_cols_with_hash,
            "BlockMovedColumns",
            expected_cols_with_hash.clone(),
        ),
        (
            block_cols_without_hash,
            "BlockMovedColumns",
            expected_cols_without_hash.clone(),
        ),
        (
            block_rect_with_hash,
            "BlockMovedRect",
            expected_rect_with_hash.clone(),
        ),
        (
            block_rect_without_hash,
            "BlockMovedRect",
            expected_rect_without_hash.clone(),
        ),
    ];

    for (op, expected_kind, expected_keys) in cases {
        let json = serde_json::to_value(&op).expect("serialize diffop");
        assert_eq!(json["kind"], expected_kind);
        let keys = json_keys(&json);
        assert_eq!(keys, expected_keys);
    }
}

#[test]
fn pg4_block_rect_json_shape_and_roundtrip() {
    let without_hash = DiffOp::BlockMovedRect {
        sheet: sid("Sheet1"),
        src_start_row: 2,
        src_row_count: 3,
        src_start_col: 1,
        src_col_count: 2,
        dst_start_row: 10,
        dst_start_col: 5,
        block_hash: None,
    };
    let with_hash = DiffOp::BlockMovedRect {
        sheet: sid("Sheet1"),
        src_start_row: 4,
        src_row_count: 1,
        src_start_col: 0,
        src_col_count: 1,
        dst_start_row: 20,
        dst_start_col: 7,
        block_hash: Some(0x55AA),
    };

    let report = DiffReport::new(vec![without_hash.clone(), with_hash.clone()]);
    let json = serde_json::to_value(&report).expect("serialize rect report");

    let ops_json = json["ops"]
        .as_array()
        .expect("ops should be array for report");
    assert_eq!(ops_json.len(), 2);
    assert_eq!(ops_json[0]["kind"], "BlockMovedRect");
    assert_eq!(ops_json[0]["sheet"], sid_json("Sheet1"));
    assert_eq!(ops_json[0]["src_start_row"], 2);
    assert_eq!(ops_json[0]["src_row_count"], 3);
    assert_eq!(ops_json[0]["src_start_col"], 1);
    assert_eq!(ops_json[0]["src_col_count"], 2);
    assert_eq!(ops_json[0]["dst_start_row"], 10);
    assert_eq!(ops_json[0]["dst_start_col"], 5);
    assert!(
        ops_json[0].get("block_hash").is_none(),
        "block_hash should be omitted when None"
    );

    assert_eq!(ops_json[1]["kind"], "BlockMovedRect");
    assert_eq!(ops_json[1]["block_hash"], Value::from(0x55AA));

    let roundtrip: DiffReport =
        serde_json::from_value(json).expect("roundtrip deserialize rect report");
    assert_eq!(roundtrip.ops, vec![without_hash, with_hash]);
}

#[test]
fn pg4_diffop_roundtrip_each_variant() {
    let mut ops = vec![
        DiffOp::SheetAdded {
            sheet: sid("SheetA"),
        },
        DiffOp::SheetRemoved {
            sheet: sid("SheetB"),
        },
        DiffOp::SheetRenamed {
            sheet: sid("SheetC"),
            from: sid("SheetB"),
            to: sid("SheetC"),
        },
        DiffOp::RowAdded {
            sheet: sid("Sheet1"),
            row_idx: 1,
            row_signature: Some(RowSignature { hash: 42 }),
        },
        DiffOp::RowRemoved {
            sheet: sid("Sheet1"),
            row_idx: 0,
            row_signature: None,
        },
        DiffOp::DuplicateKeyCluster {
            sheet: sid("Sheet1"),
            key: vec![
                Some(CellValue::Number(5.0)),
                Some(CellValue::Text(sid("SKU-5"))),
            ],
            left_rows: vec![1, 2],
            right_rows: vec![3],
        },
        DiffOp::ColumnAdded {
            sheet: sid("Sheet1"),
            col_idx: 2,
            col_signature: None,
        },
        DiffOp::ColumnRemoved {
            sheet: sid("Sheet1"),
            col_idx: 3,
            col_signature: Some(ColSignature { hash: 99 }),
        },
        DiffOp::BlockMovedRows {
            sheet: sid("Sheet1"),
            src_start_row: 5,
            row_count: 2,
            dst_start_row: 10,
            block_hash: Some(1234),
        },
        DiffOp::BlockMovedRows {
            sheet: sid("Sheet1"),
            src_start_row: 5,
            row_count: 2,
            dst_start_row: 10,
            block_hash: None,
        },
        DiffOp::BlockMovedColumns {
            sheet: sid("Sheet2"),
            src_start_col: 4,
            col_count: 1,
            dst_start_col: 6,
            block_hash: Some(888),
        },
        DiffOp::BlockMovedColumns {
            sheet: sid("Sheet2"),
            src_start_col: 4,
            col_count: 1,
            dst_start_col: 6,
            block_hash: None,
        },
        DiffOp::BlockMovedRect {
            sheet: sid("Sheet3"),
            src_start_row: 1,
            src_row_count: 2,
            src_start_col: 3,
            src_col_count: 4,
            dst_start_row: 10,
            dst_start_col: 20,
            block_hash: Some(0xABCD),
        },
        DiffOp::BlockMovedRect {
            sheet: sid("Sheet3"),
            src_start_row: 1,
            src_row_count: 2,
            src_start_col: 3,
            src_col_count: 4,
            dst_start_row: 10,
            dst_start_col: 20,
            block_hash: None,
        },
        sample_cell_edited(),
        DiffOp::QueryAdded {
            name: sid("Section1/NewQuery"),
        },
        DiffOp::QueryRemoved {
            name: sid("Section1/OldQuery"),
        },
        DiffOp::QueryRenamed {
            from: sid("Section1/Foo"),
            to: sid("Section1/Bar"),
        },
        DiffOp::QueryDefinitionChanged {
            name: sid("Section1/Query1"),
            change_kind: QueryChangeKind::Semantic,
            old_hash: 0x1234567890ABCDEF,
            new_hash: 0xFEDCBA0987654321,
            semantic_detail: None,
        },
        DiffOp::QueryDefinitionChanged {
            name: sid("Section1/Query2"),
            change_kind: QueryChangeKind::FormattingOnly,
            old_hash: 0xAAAABBBBCCCCDDDD,
            new_hash: 0xAAAABBBBCCCCDDDD,
            semantic_detail: None,
        },
        DiffOp::QueryMetadataChanged {
            name: sid("Section1/Query3"),
            field: QueryMetadataField::LoadToSheet,
            old: Some(sid("true")),
            new: Some(sid("false")),
        },
        DiffOp::QueryMetadataChanged {
            name: sid("Section1/Query4"),
            field: QueryMetadataField::LoadToModel,
            old: Some(sid("false")),
            new: Some(sid("true")),
        },
        DiffOp::QueryMetadataChanged {
            name: sid("Section1/Query5"),
            field: QueryMetadataField::GroupPath,
            old: None,
            new: Some(sid("Reports/Sales")),
        },
        DiffOp::QueryMetadataChanged {
            name: sid("Section1/Query6"),
            field: QueryMetadataField::ConnectionOnly,
            old: Some(sid("true")),
            new: Some(sid("false")),
        },
        DiffOp::NamedRangeAdded {
            name: sid("GlobalAdd"),
        },
        DiffOp::NamedRangeRemoved {
            name: sid("GlobalRemove"),
        },
        DiffOp::NamedRangeChanged {
            name: sid("Sheet1!LocalChange"),
            old_ref: sid("Sheet1!$C$1"),
            new_ref: sid("Sheet1!$C$2"),
        },
        DiffOp::ChartAdded {
            sheet: sid("Sheet1"),
            name: sid("Chart 1"),
        },
        DiffOp::ChartRemoved {
            sheet: sid("Sheet1"),
            name: sid("Chart 2"),
        },
        DiffOp::ChartChanged {
            sheet: sid("Sheet1"),
            name: sid("Chart 1"),
        },
        DiffOp::VbaModuleAdded {
            name: sid("Module1"),
        },
        DiffOp::VbaModuleRemoved {
            name: sid("Module2"),
        },
        DiffOp::VbaModuleChanged {
            name: sid("Module1"),
        },
    ];

    #[cfg(feature = "model-diff")]
    {
        ops.extend([
            DiffOp::TableAdded {
                name: sid("Sales"),
            },
            DiffOp::TableRemoved {
                name: sid("Legacy"),
            },
            DiffOp::ModelColumnAdded {
                table: sid("Sales"),
                name: sid("Amount"),
                data_type: Some(sid("decimal")),
            },
            DiffOp::ModelColumnRemoved {
                table: sid("Sales"),
                name: sid("Obsolete"),
            },
            DiffOp::ModelColumnTypeChanged {
                table: sid("Sales"),
                name: sid("Amount"),
                old_type: Some(sid("int")),
                new_type: Some(sid("decimal")),
            },
            DiffOp::ModelColumnPropertyChanged {
                table: sid("Sales"),
                name: sid("Amount"),
                field: ModelColumnProperty::FormatString,
                old: None,
                new: Some(sid("0.00")),
            },
            DiffOp::CalculatedColumnDefinitionChanged {
                table: sid("Sales"),
                name: sid("Calc"),
                change_kind: ExpressionChangeKind::Semantic,
                old_hash: 1,
                new_hash: 2,
            },
            DiffOp::RelationshipAdded {
                from_table: sid("Sales"),
                from_column: sid("CustomerId"),
                to_table: sid("Customers"),
                to_column: sid("Id"),
            },
            DiffOp::RelationshipRemoved {
                from_table: sid("Sales"),
                from_column: sid("ProductId"),
                to_table: sid("Products"),
                to_column: sid("Id"),
            },
            DiffOp::RelationshipPropertyChanged {
                from_table: sid("Sales"),
                from_column: sid("CustomerId"),
                to_table: sid("Customers"),
                to_column: sid("Id"),
                field: RelationshipProperty::IsActive,
                old: Some(sid("false")),
                new: Some(sid("true")),
            },
            DiffOp::MeasureAdded {
                name: sid("Sales/Total"),
            },
            DiffOp::MeasureDefinitionChanged {
                name: sid("Sales/Total"),
                change_kind: ExpressionChangeKind::Semantic,
                old_hash: 3,
                new_hash: 4,
            },
            DiffOp::MeasureRemoved {
                name: sid("Sales/LegacyTotal"),
            },
        ]);
    }

    for original in ops {
        let serialized = serde_json::to_string(&original).expect("serialize");
        let deserialized: DiffOp = serde_json::from_str(&serialized).expect("deserialize");
        assert_eq!(deserialized, original);

        if let DiffOp::CellEdited { .. } = &deserialized {
            assert_cell_edited_invariants(&deserialized, "Sheet1", "C3");
        }
    }
}

#[test]
fn pg4_cell_edited_roundtrip_preserves_snapshot_addrs() {
    let op = sample_cell_edited();
    let json = serde_json::to_string(&op).expect("serialize");
    let round_tripped: DiffOp = serde_json::from_str(&json).expect("deserialize");

    assert_cell_edited_invariants(&round_tripped, "Sheet1", "C3");
}

#[test]
fn pg4_diff_report_roundtrip_preserves_order() {
    let op1 = DiffOp::SheetAdded {
        sheet: sid("Sheet1"),
    };
    let op2 = DiffOp::RowAdded {
        sheet: sid("Sheet1"),
        row_idx: 10,
        row_signature: None,
    };
    let op3 = sample_cell_edited();

    let ops = vec![op1, op2, op3];
    let report = DiffReport::new(ops.clone());
    assert_eq!(report.version, DiffReport::SCHEMA_VERSION);

    let serialized = serde_json::to_string(&report).expect("serialize report");
    let deserialized: DiffReport = serde_json::from_str(&serialized).expect("deserialize report");
    assert_eq!(deserialized.version, "1");
    assert_eq!(deserialized.ops, ops);

    let kinds: Vec<&str> = deserialized.ops.iter().map(op_kind).collect();
    assert_eq!(kinds, vec!["SheetAdded", "RowAdded", "CellEdited"]);
}

#[test]
fn pg4_diff_report_json_shape() {
    let ops = vec![
        DiffOp::SheetRemoved {
            sheet: sid("SheetX"),
        },
        DiffOp::RowRemoved {
            sheet: sid("SheetX"),
            row_idx: 3,
            row_signature: Some(RowSignature { hash: 7 }),
        },
    ];
    let report = DiffReport::new(ops);
    let json = serde_json::to_value(&report).expect("serialize to value");

    let obj = json.as_object().expect("report json object");
    let keys: BTreeSet<String> = obj.keys().cloned().collect();
    let expected: BTreeSet<String> = ["complete", "ops", "strings", "version"]
        .into_iter()
        .map(String::from)
        .collect();
    assert_eq!(keys, expected);
    assert_eq!(obj.get("version").and_then(Value::as_str), Some("1"));
    assert_eq!(obj.get("complete").and_then(Value::as_bool), Some(true));

    let ops_json = obj
        .get("ops")
        .and_then(Value::as_array)
        .expect("ops should be array");
    assert_eq!(ops_json.len(), 2);
    assert_eq!(ops_json[0]["kind"], "SheetRemoved");
    assert_eq!(ops_json[1]["kind"], "RowRemoved");
}

#[test]
fn pg4_diffop_cell_edited_rejects_invalid_top_level_addr() {
    let sheet_id = sid("Sheet1").0;
    let json = format!(
        r#"{{
        "kind": "CellEdited",
        "sheet": {sheet_id},
        "addr": "1A",
        "from": {{ "addr": "C3", "value": null, "formula": null }},
        "to":   {{ "addr": "C3", "value": null, "formula": null }}
    }}"#
    );

    let err = serde_json::from_str::<DiffOp>(&json)
        .expect_err("invalid top-level addr should fail to deserialize");
    let msg = err.to_string();
    assert!(
        msg.contains("invalid cell address") && msg.contains("1A"),
        "error should mention invalid address: {msg}",
    );
}

#[test]
fn pg4_diffop_cell_edited_rejects_invalid_snapshot_addrs() {
    let sheet_id = sid("Sheet1").0;
    let json = format!(
        r#"{{
        "kind": "CellEdited",
        "sheet": {sheet_id},
        "addr": "C3",
        "from": {{ "addr": "A0", "value": null, "formula": null }},
        "to":   {{ "addr": "C3", "value": null, "formula": null }}
    }}"#
    );

    let err = serde_json::from_str::<DiffOp>(&json)
        .expect_err("invalid snapshot addr should fail to deserialize");
    let msg = err.to_string();
    assert!(
        msg.contains("invalid cell address") && msg.contains("A0"),
        "error should mention invalid address: {msg}",
    );
}

#[test]
fn pg4_diff_report_rejects_invalid_nested_addr() {
    let sheet_id = sid("Sheet1").0;
    let json = format!(
        r#"{{
        "version": "1",
        "strings": [],
        "ops": [{{
            "kind": "CellEdited",
            "sheet": {sheet_id},
            "addr": "1A",
            "from": {{ "addr": "C3", "value": null, "formula": null }},
            "to":   {{ "addr": "C3", "value": null, "formula": null }}
        }}]
    }}"#
    );

    let err = serde_json::from_str::<DiffReport>(&json)
        .expect_err("invalid nested addr should fail to deserialize");
    let msg = err.to_string();
    assert!(
        msg.contains("invalid cell address") && msg.contains("1A"),
        "error should surface nested invalid address: {msg}",
    );
}

#[test]
#[should_panic]
fn pg4_cell_edited_invariant_helper_rejects_mismatched_snapshot_addr() {
    let op = DiffOp::CellEdited {
        sheet: sid("Sheet1"),
        addr: addr("C3"),
        from: snapshot("D4", Some(CellValue::Number(1.0)), None),
        to: snapshot("C3", Some(CellValue::Number(2.0)), None),
        formula_diff: FormulaDiffResult::Unchanged,
    };

    assert_cell_edited_invariants(&op, "Sheet1", "C3");
}

#[test]
#[cfg(feature = "perf-metrics")]
fn pg4_diff_report_json_shape_with_metrics() {
    use excel_diff::perf::DiffMetrics;

    let ops = vec![DiffOp::SheetAdded {
        sheet: sid("NewSheet"),
    }];
    let mut report = DiffReport::new(ops);
    let mut metrics = DiffMetrics::default();
    metrics.parse_time_ms = 6;
    metrics.move_detection_time_ms = 10;
    metrics.alignment_time_ms = 20;
    metrics.cell_diff_time_ms = 30;
    metrics.total_time_ms = 60;
    metrics.diff_time_ms = 54;
    metrics.peak_memory_bytes = 4242;
    metrics.rows_processed = 1000;
    metrics.cells_compared = 5000;
    metrics.anchors_found = 50;
    metrics.moves_detected = 2;
    report.metrics = Some(metrics);

    let json = serde_json::to_value(&report).expect("serialize to value");
    let obj = json.as_object().expect("report json object");

    let keys: BTreeSet<String> = obj.keys().cloned().collect();
    let expected: BTreeSet<String> = ["complete", "metrics", "ops", "strings", "version"]
        .into_iter()
        .map(String::from)
        .collect();
    assert_eq!(keys, expected, "report should include metrics key");

    let metrics_obj = obj
        .get("metrics")
        .and_then(Value::as_object)
        .expect("metrics object");

    assert!(metrics_obj.contains_key("parse_time_ms"));
    assert!(metrics_obj.contains_key("move_detection_time_ms"));
    assert!(metrics_obj.contains_key("alignment_time_ms"));
    assert!(metrics_obj.contains_key("cell_diff_time_ms"));
    assert!(metrics_obj.contains_key("total_time_ms"));
    assert!(metrics_obj.contains_key("diff_time_ms"));
    assert!(metrics_obj.contains_key("peak_memory_bytes"));
    assert!(metrics_obj.contains_key("rows_processed"));
    assert!(metrics_obj.contains_key("cells_compared"));
    assert!(metrics_obj.contains_key("anchors_found"));
    assert!(metrics_obj.contains_key("moves_detected"));

    assert_eq!(
        metrics_obj.get("rows_processed").and_then(Value::as_u64),
        Some(1000)
    );
    assert_eq!(
        metrics_obj.get("cells_compared").and_then(Value::as_u64),
        Some(5000)
    );
}

```

---

### File: `core\tests\pg5_grid_diff_tests.rs`

```rust
mod common;

use common::{grid_from_numbers, single_sheet_workbook};
use excel_diff::{CellValue, DiffConfig, DiffOp, DiffReport, Grid, Workbook, WorkbookPackage};
use std::collections::BTreeSet;

fn sheet_name<'a>(report: &'a DiffReport, id: &excel_diff::SheetId) -> &'a str {
    report.strings[id.0 as usize].as_str()
}

fn diff_workbooks(old: &Workbook, new: &Workbook, config: &DiffConfig) -> DiffReport {
    WorkbookPackage::from(old.clone()).diff(&WorkbookPackage::from(new.clone()), config)
}

#[test]
fn pg5_1_grid_diff_1x1_identical_empty_diff() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert!(report.ops.is_empty());
}

#[test]
fn pg5_2_grid_diff_1x1_value_change_single_cell_edited() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[2]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::CellEdited {
            sheet,
            addr,
            from,
            to,
            ..
        } => {
            assert_eq!(sheet_name(&report, sheet), "Sheet1");
            assert_eq!(addr.to_a1(), "A1");
            assert_eq!(from.value, Some(CellValue::Number(1.0)));
            assert_eq!(to.value, Some(CellValue::Number(2.0)));
        }
        other => panic!("expected CellEdited, got {other:?}"),
    }
}

#[test]
fn pg5_3_grid_diff_row_appended_row_added_only() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1], &[2]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::RowAdded {
            sheet,
            row_idx,
            row_signature,
        } => {
            assert_eq!(sheet_name(&report, sheet), "Sheet1");
            assert_eq!(*row_idx, 1);
            assert!(row_signature.is_none());
        }
        other => panic!("expected RowAdded, got {other:?}"),
    }
}

#[test]
fn pg5_4_grid_diff_column_appended_column_added_only() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1], &[2]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 10], &[2, 20]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::ColumnAdded {
            sheet,
            col_idx,
            col_signature,
        } => {
            assert_eq!(sheet_name(&report, sheet), "Sheet1");
            assert_eq!(*col_idx, 1);
            assert!(col_signature.is_none());
        }
        other => panic!("expected ColumnAdded, got {other:?}"),
    }
}

#[test]
fn pg5_5_grid_diff_same_shape_scattered_cell_edits() {
    let old = single_sheet_workbook(
        "Sheet1",
        grid_from_numbers(&[&[1, 2, 3], &[4, 5, 6], &[7, 8, 9]]),
    );
    let new = single_sheet_workbook(
        "Sheet1",
        grid_from_numbers(&[&[10, 2, 3], &[4, 50, 6], &[7, 8, 90]]),
    );

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 3);
    assert!(
        report
            .ops
            .iter()
            .all(|op| matches!(op, DiffOp::CellEdited { .. }))
    );

    let edited_addrs: BTreeSet<String> = report
        .ops
        .iter()
        .filter_map(|op| match op {
            DiffOp::CellEdited { addr, .. } => Some(addr.to_a1()),
            _ => None,
        })
        .collect();
    let expected: BTreeSet<String> = ["A1", "B2", "C3"].into_iter().map(String::from).collect();
    assert_eq!(edited_addrs, expected);
}

#[test]
fn pg5_dense_row_replacement_emits_row_replaced() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 2, 3, 4, 5]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[10, 20, 30, 40, 50]]));

    let config = DiffConfig::builder()
        .dense_row_replace_ratio(0.5)
        .dense_row_replace_min_cols(1)
        .dense_rect_replace_min_rows(0)
        .build()
        .expect("valid config should build");

    let report = diff_workbooks(&old, &new, &config);
    assert_eq!(report.ops.len(), 1);
    assert!(matches!(report.ops[0], DiffOp::RowReplaced { .. }));
}

#[test]
fn pg5_dense_rect_replacement_emits_rect_replaced() {
    let old = single_sheet_workbook(
        "Sheet1",
        grid_from_numbers(&[&[1, 2, 3], &[4, 5, 6]]),
    );
    let new = single_sheet_workbook(
        "Sheet1",
        grid_from_numbers(&[&[10, 20, 30], &[40, 50, 60]]),
    );

    let config = DiffConfig::builder()
        .dense_row_replace_ratio(0.5)
        .dense_row_replace_min_cols(1)
        .dense_rect_replace_min_rows(2)
        .build()
        .expect("valid config should build");

    let report = diff_workbooks(&old, &new, &config);
    assert_eq!(report.ops.len(), 1);
    assert!(matches!(report.ops[0], DiffOp::RectReplaced { .. }));
}

#[test]
fn pg5_6_grid_diff_degenerate_grids() {
    let empty_old = single_sheet_workbook("Sheet1", Grid::new(0, 0));
    let empty_new = single_sheet_workbook("Sheet1", Grid::new(0, 0));

    let empty_report = diff_workbooks(&empty_old, &empty_new, &DiffConfig::default());
    assert!(empty_report.ops.is_empty());

    let old = single_sheet_workbook("Sheet1", Grid::new(0, 0));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 2);

    let mut row_added = 0;
    let mut col_added = 0;
    let mut cell_edits = 0;

    for op in &report.ops {
        match op {
            DiffOp::RowAdded {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(sheet_name(&report, sheet), "Sheet1");
                assert_eq!(*row_idx, 0);
                assert!(row_signature.is_none());
                row_added += 1;
            }
            DiffOp::ColumnAdded {
                sheet,
                col_idx,
                col_signature,
            } => {
                assert_eq!(sheet_name(&report, sheet), "Sheet1");
                assert_eq!(*col_idx, 0);
                assert!(col_signature.is_none());
                col_added += 1;
            }
            DiffOp::CellEdited { .. } => cell_edits += 1,
            other => panic!("unexpected op: {other:?}"),
        }
    }

    assert_eq!(row_added, 1);
    assert_eq!(col_added, 1);
    assert_eq!(cell_edits, 0);
}

#[test]
fn pg5_7_grid_diff_row_truncated_row_removed_only() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1], &[2]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::RowRemoved {
            sheet,
            row_idx,
            row_signature,
        } => {
            assert_eq!(sheet_name(&report, sheet), "Sheet1");
            assert_eq!(*row_idx, 1);
            assert!(row_signature.is_none());
        }
        other => panic!("expected RowRemoved, got {other:?}"),
    }
}

#[test]
fn pg5_8_grid_diff_column_truncated_column_removed_only() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 10], &[2, 20]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1], &[2]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 1);

    match &report.ops[0] {
        DiffOp::ColumnRemoved {
            sheet,
            col_idx,
            col_signature,
        } => {
            assert_eq!(sheet_name(&report, sheet), "Sheet1");
            assert_eq!(*col_idx, 1);
            assert!(col_signature.is_none());
        }
        other => panic!("expected ColumnRemoved, got {other:?}"),
    }
}

#[test]
fn pg5_9_grid_diff_row_and_column_truncated_structure_only() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 2], &[3, 4]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 2);

    let mut rows_removed = 0;
    let mut cols_removed = 0;
    let mut cell_edits = 0;

    for op in &report.ops {
        match op {
            DiffOp::RowRemoved {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(sheet_name(&report, sheet), "Sheet1");
                assert_eq!(*row_idx, 1);
                assert!(row_signature.is_none());
                rows_removed += 1;
            }
            DiffOp::ColumnRemoved {
                sheet,
                col_idx,
                col_signature,
            } => {
                assert_eq!(sheet_name(&report, sheet), "Sheet1");
                assert_eq!(*col_idx, 1);
                assert!(col_signature.is_none());
                cols_removed += 1;
            }
            DiffOp::CellEdited { .. } => cell_edits += 1,
            other => panic!("unexpected op: {other:?}"),
        }
    }

    assert_eq!(rows_removed, 1);
    assert_eq!(cols_removed, 1);
    assert_eq!(cell_edits, 0);
}

#[test]
fn pg5_10_grid_diff_row_appended_with_overlap_cell_edits() {
    let old = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 2], &[3, 4]]));
    let new = single_sheet_workbook("Sheet1", grid_from_numbers(&[&[1, 20], &[30, 4], &[5, 6]]));

    let report = diff_workbooks(&old, &new, &DiffConfig::default());
    assert_eq!(report.ops.len(), 3);

    let mut row_added = 0;
    let mut cell_edits = BTreeSet::new();

    for op in &report.ops {
        match op {
            DiffOp::RowAdded {
                sheet,
                row_idx,
                row_signature,
            } => {
                assert_eq!(sheet_name(&report, sheet), "Sheet1");
                assert_eq!(*row_idx, 2);
                assert!(row_signature.is_none());
                row_added += 1;
            }
            DiffOp::CellEdited { addr, .. } => {
                cell_edits.insert(addr.to_a1());
            }
            other => panic!("unexpected op: {other:?}"),
        }
    }

    assert_eq!(row_added, 1);
    let expected: BTreeSet<String> = ["B1", "A2"].into_iter().map(String::from).collect();
    assert_eq!(cell_edits, expected);
}

```

---

### File: `core\tests\pg6_object_vs_grid_tests.rs`

```rust
mod common;

use common::{open_fixture_workbook, sid};
use excel_diff::{DiffConfig, DiffOp, WorkbookPackage};

#[test]
fn pg6_1_sheet_added_no_grid_ops_on_main() {
    let old = open_fixture_workbook("pg6_sheet_added_a.xlsx");
    let new = open_fixture_workbook("pg6_sheet_added_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());

    let mut sheet_added = 0;
    for op in &report.ops {
        match op {
            DiffOp::SheetAdded { sheet } if *sheet == sid("NewSheet") => sheet_added += 1,
            DiffOp::RowAdded { sheet, .. }
            | DiffOp::RowRemoved { sheet, .. }
            | DiffOp::ColumnAdded { sheet, .. }
            | DiffOp::ColumnRemoved { sheet, .. }
            | DiffOp::CellEdited { sheet, .. }
                if *sheet == sid("Main") =>
            {
                panic!("unexpected grid op on Main: {op:?}");
            }
            DiffOp::SheetAdded { sheet } => {
                panic!("unexpected sheet added: {sheet}");
            }
            DiffOp::SheetRemoved { sheet } => {
                panic!("unexpected sheet removed: {sheet}");
            }
            DiffOp::BlockMovedRows { .. } | DiffOp::BlockMovedColumns { .. } => {
                panic!("block move ops are not expected in PG6.1: {op:?}");
            }
            _ => panic!("unexpected op variant: {op:?}"),
        }
    }

    assert_eq!(sheet_added, 1, "exactly one NewSheet addition expected");
    assert_eq!(report.ops.len(), 1, "no other operations expected");
}

#[test]
fn pg6_2_sheet_removed_no_grid_ops_on_main() {
    let old = open_fixture_workbook("pg6_sheet_removed_a.xlsx");
    let new = open_fixture_workbook("pg6_sheet_removed_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());

    let mut sheet_removed = 0;
    for op in &report.ops {
        match op {
            DiffOp::SheetRemoved { sheet } if *sheet == sid("OldSheet") => sheet_removed += 1,
            DiffOp::RowAdded { sheet, .. }
            | DiffOp::RowRemoved { sheet, .. }
            | DiffOp::ColumnAdded { sheet, .. }
            | DiffOp::ColumnRemoved { sheet, .. }
            | DiffOp::CellEdited { sheet, .. }
                if *sheet == sid("Main") =>
            {
                panic!("unexpected grid op on Main: {op:?}");
            }
            DiffOp::SheetAdded { sheet } => {
                panic!("unexpected sheet added: {sheet}");
            }
            DiffOp::SheetRemoved { sheet } => {
                panic!("unexpected sheet removed: {sheet}");
            }
            DiffOp::BlockMovedRows { .. } | DiffOp::BlockMovedColumns { .. } => {
                panic!("block move ops are not expected in PG6.2: {op:?}");
            }
            _ => panic!("unexpected op variant: {op:?}"),
        }
    }

    assert_eq!(sheet_removed, 1, "exactly one OldSheet removal expected");
    assert_eq!(report.ops.len(), 1, "no other operations expected");
}

#[test]
fn pg6_3_rename_emits_sheet_renamed_no_grid_ops() {
    let old = open_fixture_workbook("pg6_sheet_renamed_a.xlsx");
    let new = open_fixture_workbook("pg6_sheet_renamed_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());

    let mut renamed = 0;

    for op in &report.ops {
        match op {
            DiffOp::SheetRenamed { sheet, from, to } => {
                assert_eq!(sheet, &sid("NewName"));
                assert_eq!(from, &sid("OldName"));
                assert_eq!(to, &sid("NewName"));
                renamed += 1;
            }
            DiffOp::SheetAdded { sheet } => panic!("unexpected sheet added: {sheet}"),
            DiffOp::SheetRemoved { sheet } => panic!("unexpected sheet removed: {sheet}"),
            DiffOp::RowAdded { .. }
            | DiffOp::RowRemoved { .. }
            | DiffOp::ColumnAdded { .. }
            | DiffOp::ColumnRemoved { .. }
            | DiffOp::CellEdited { .. }
            | DiffOp::BlockMovedRows { .. }
            | DiffOp::BlockMovedColumns { .. } => {
                panic!("no grid-level ops expected for rename scenario: {op:?}");
            }
            _ => panic!("unexpected op variant: {op:?}"),
        }
    }

    assert_eq!(report.ops.len(), 1, "expected only a rename op");
    assert_eq!(renamed, 1, "expected one sheet rename");
}

#[test]
fn pg6_4_sheet_and_grid_change_composed_cleanly() {
    let old = open_fixture_workbook("pg6_sheet_and_grid_change_a.xlsx");
    let new = open_fixture_workbook("pg6_sheet_and_grid_change_b.xlsx");

    let report =
        WorkbookPackage::from(old).diff(&WorkbookPackage::from(new), &DiffConfig::default());

    let mut scratch_added = 0;
    let mut main_cell_edits = 0;

    for op in &report.ops {
        match op {
            DiffOp::SheetAdded { sheet } if *sheet == sid("Scratch") => scratch_added += 1,
            DiffOp::CellEdited { sheet, .. } => {
                assert_eq!(sheet, &sid("Main"), "only Main should have cell edits");
                main_cell_edits += 1;
            }
            DiffOp::SheetRemoved { .. } => {
                panic!("no sheets should be removed in PG6.4: {op:?}");
            }
            DiffOp::RowAdded { .. }
            | DiffOp::RowRemoved { .. }
            | DiffOp::ColumnAdded { .. }
            | DiffOp::ColumnRemoved { .. }
            | DiffOp::BlockMovedRows { .. }
            | DiffOp::BlockMovedColumns { .. } => {
                panic!("no structural row/column ops expected in PG6.4: {op:?}");
            }
            _ => panic!("unexpected op variant: {op:?}"),
        }
    }

    assert_eq!(scratch_added, 1, "exactly one Scratch addition expected");
    assert!(
        main_cell_edits > 0,
        "Main should report at least one cell edit"
    );
}

```

---

### File: `core\tests\robustness_regressions.yaml`

```yaml
fixtures:
  - file: "minimal.xlsx"
    type: "xlsx"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "one_query.xlsx"
    type: "xlsx"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "mashup_utf16_le.xlsx"
    type: "xlsx"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "mashup_base64_whitespace.xlsx"
    type: "xlsx"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "pbix_legacy_one_query_a.pbix"
    type: "pbix"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "pbix_legacy_multi_query_a.pbix"
    type: "pbix"
    expectation:
      result: "ok"
    invariants:
      self_diff_empty: true
      deterministic_open: true

  - file: "random_zip.zip"
    type: "xlsx"
    expectation:
      result: "error"
      error_code: "EXDIFF_CTR_004"

  - file: "no_content_types.xlsx"
    type: "xlsx"
    expectation:
      result: "error"
      error_code: "EXDIFF_CTR_004"

  - file: "not_a_zip.txt"
    type: "xlsx"
    expectation:
      result: "error"
      error_code: "EXDIFF_CTR_003"

  - file: "xlsb_stub.xlsb"
    type: "xlsb"
    expectation:
      result: "error"
      error_code: "EXDIFF_PKG_009"

  - file: "corrupt_base64.xlsx"
    type: "xlsx"
    expectation:
      result: "error"
      error_code: "EXDIFF_DM_001"

  - file: "duplicate_datamashup_parts.xlsx"
    type: "xlsx"
    expectation:
      result: "error"
      error_code: "EXDIFF_DM_003"

  - file: "pbix_no_datamashup_no_schema.pbix"
    type: "pbix"
    expectation:
      result: "error"
      error_code: "EXDIFF_PKG_010"

```

---

### File: `core\tests\robustness_regressions_tests.rs`

```rust
mod common;

use common::fixture_path;
use excel_diff::{
    ContainerLimits, DiffConfig, DiffSession, LimitBehavior, PbixPackage, WorkbookPackage,
    build_data_mashup, parse_data_mashup, with_default_session,
};
use serde::Deserialize;
use std::fs;
use std::path::Path;

#[derive(Debug, Deserialize)]
struct Suite {
    fixtures: Vec<FixtureSpec>,
}

#[derive(Debug, Clone, Deserialize)]
struct FixtureSpec {
    file: String,
    #[serde(rename = "type")]
    kind: FixtureKind,
    expectation: Expectation,
    #[serde(default)]
    invariants: Invariants,
}

#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "snake_case")]
enum FixtureKind {
    Xlsx,
    Xlsm,
    Xlsb,
    Pbix,
    Pbit,
    DmBytes,
}

#[derive(Debug, Clone, Deserialize)]
struct Expectation {
    result: ExpectationResult,
    error_code: Option<String>,
}

#[derive(Debug, Clone, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
enum ExpectationResult {
    Ok,
    Error,
}

#[derive(Debug, Clone, Default, Deserialize)]
struct Invariants {
    no_panic: Option<bool>,
    self_diff_empty: Option<bool>,
    deterministic_open: Option<bool>,
}

#[test]
fn robustness_regressions() {
    let suite = load_suite();
    for fixture in suite.fixtures {
        run_fixture(&fixture);
    }
}

fn load_suite() -> Suite {
    let path = Path::new(env!("CARGO_MANIFEST_DIR"))
        .join("tests")
        .join("robustness_regressions.yaml");
    let contents = fs::read_to_string(&path)
        .unwrap_or_else(|e| panic!("failed to read {}: {e}", path.display()));
    serde_yaml::from_str(&contents)
        .unwrap_or_else(|e| panic!("failed to parse {}: {e}", path.display()))
}

fn run_fixture(fixture: &FixtureSpec) {
    let expected_ok = fixture.expectation.result == ExpectationResult::Ok;
    let invariants = normalized_invariants(&fixture.invariants, expected_ok);
    let path = fixture_path(&fixture.file);

    match fixture.kind {
        FixtureKind::Xlsx | FixtureKind::Xlsm | FixtureKind::Xlsb => {
            let limits = container_limits();
            let result = open_workbook(&path, limits);
            assert_expectation(result, fixture);
            if expected_ok {
                run_workbook_invariants(&path, limits, &invariants);
            }
        }
        FixtureKind::Pbix | FixtureKind::Pbit => {
            let limits = container_limits();
            let result = open_pbix(&path, limits);
            assert_expectation(result, fixture);
            if expected_ok {
                run_pbix_invariants(&path, limits, &invariants);
            }
        }
        FixtureKind::DmBytes => {
            let bytes = fs::read(&path)
                .unwrap_or_else(|e| panic!("failed to read {}: {e}", path.display()));
            let result = parse_dm_bytes(&bytes);
            assert_expectation(result, fixture);
            if expected_ok {
                run_dm_invariants(&bytes, &invariants);
            }
        }
    }
}

fn normalized_invariants(invariants: &Invariants, expected_ok: bool) -> Invariants {
    let mut out = invariants.clone();
    if out.no_panic.is_none() {
        out.no_panic = Some(true);
    }
    if expected_ok {
        if out.self_diff_empty.is_none() {
            out.self_diff_empty = Some(true);
        }
        if out.deterministic_open.is_none() {
            out.deterministic_open = Some(true);
        }
    }
    out
}

fn container_limits() -> ContainerLimits {
    ContainerLimits {
        max_entries: 2000,
        max_part_uncompressed_bytes: 10 * 1024 * 1024,
        max_total_uncompressed_bytes: 50 * 1024 * 1024,
    }
}

fn robust_config() -> DiffConfig {
    let mut config = DiffConfig::default();
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;
    config.hardening.max_memory_mb = Some(256);
    config.hardening.timeout_seconds = Some(10);
    config.hardening.max_ops = Some(200_000);
    config
}

fn reset_session() {
    with_default_session(|session| *session = DiffSession::new());
}

fn open_workbook(
    path: &Path,
    limits: ContainerLimits,
) -> Result<WorkbookPackage, excel_diff::PackageError> {
    let file = std::fs::File::open(path)
        .unwrap_or_else(|e| panic!("failed to open {}: {e}", path.display()));
    WorkbookPackage::open_with_limits(file, limits)
}

fn open_pbix(
    path: &Path,
    limits: ContainerLimits,
) -> Result<PbixPackage, excel_diff::PackageError> {
    let file = std::fs::File::open(path)
        .unwrap_or_else(|e| panic!("failed to open {}: {e}", path.display()));
    PbixPackage::open_with_limits(file, limits)
}

fn parse_dm_bytes(bytes: &[u8]) -> Result<excel_diff::DataMashup, excel_diff::DataMashupError> {
    let raw = parse_data_mashup(bytes)?;
    build_data_mashup(&raw)
}

fn assert_expectation<T, E>(result: Result<T, E>, fixture: &FixtureSpec)
where
    E: std::fmt::Debug + ErrorCode,
{
    match (fixture.expectation.result.clone(), result) {
        (ExpectationResult::Ok, Ok(_)) => {}
        (ExpectationResult::Ok, Err(err)) => {
            panic!("{} expected ok, got error {err:?}", fixture.file)
        }
        (ExpectationResult::Error, Ok(_)) => {
            panic!("{} expected error, got ok", fixture.file)
        }
        (ExpectationResult::Error, Err(err)) => {
            let expected = fixture
                .expectation
                .error_code
                .as_ref()
                .unwrap_or_else(|| {
                    panic!("{} missing error_code for error expectation", fixture.file)
                });
            assert_eq!(
                expected,
                err.code(),
                "{} error code mismatch",
                fixture.file
            );
        }
    }
}

fn run_workbook_invariants(path: &Path, limits: ContainerLimits, invariants: &Invariants) {
    let config = robust_config();
    if invariants.self_diff_empty == Some(true) {
        assert_workbook_self_diff_empty(path, limits, &config);
    }
    if invariants.deterministic_open == Some(true) {
        assert_workbook_self_diff_empty(path, limits, &config);
    }
}

fn run_pbix_invariants(path: &Path, limits: ContainerLimits, invariants: &Invariants) {
    let config = robust_config();
    if invariants.self_diff_empty == Some(true) {
        assert_pbix_self_diff_empty(path, limits, &config);
    }
    if invariants.deterministic_open == Some(true) {
        assert_pbix_self_diff_empty(path, limits, &config);
    }
}

fn run_dm_invariants(bytes: &[u8], invariants: &Invariants) {
    if invariants.self_diff_empty == Some(true) || invariants.deterministic_open == Some(true) {
        let dm_a = parse_dm_bytes(bytes).expect("DataMashup bytes should parse");
        let dm_b = parse_dm_bytes(bytes).expect("DataMashup bytes should parse again");
        assert_eq!(dm_a, dm_b, "DataMashup parsing should be deterministic");
    }
}

fn assert_workbook_self_diff_empty(
    path: &Path,
    limits: ContainerLimits,
    config: &DiffConfig,
) {
    reset_session();
    let pkg_a = open_workbook(path, limits).expect("fixture should parse");
    let pkg_b = open_workbook(path, limits).expect("fixture should parse again");
    let report = pkg_a.diff(&pkg_b, config);
    assert!(
        report.ops.is_empty(),
        "self-diff should be empty for {}",
        path.display()
    );
    reset_session();
}

fn assert_pbix_self_diff_empty(path: &Path, limits: ContainerLimits, config: &DiffConfig) {
    reset_session();
    let pkg_a = open_pbix(path, limits).expect("fixture should parse");
    let pkg_b = open_pbix(path, limits).expect("fixture should parse again");
    let report = pkg_a.diff(&pkg_b, config);
    assert!(
        report.ops.is_empty(),
        "self-diff should be empty for {}",
        path.display()
    );
    reset_session();
}

trait ErrorCode {
    fn code(&self) -> &'static str;
}

impl ErrorCode for excel_diff::PackageError {
    fn code(&self) -> &'static str {
        excel_diff::PackageError::code(self)
    }
}

impl ErrorCode for excel_diff::DataMashupError {
    fn code(&self) -> &'static str {
        excel_diff::DataMashupError::code(self)
    }
}

```

---

### File: `core\tests\schema_guard_tests.rs`

```rust
use excel_diff::{diff_workbooks_to_json, DiffConfig};
use serde_json::Value;
use sha2::{Digest, Sha256};

mod common;
use common::fixture_path;

const EXPECTED_JSON_HASH: &str = "54bb0d6e4fd64b6e3d67c605aa6db56dff67d6157ffdc75d750cda80683b6d4d";

fn canonicalize(value: &Value) -> Value {
    match value {
        Value::Object(map) => {
            let mut keys: Vec<_> = map.keys().cloned().collect();
            keys.sort();
            let mut out = serde_json::Map::new();
            for key in keys {
                if let Some(val) = map.get(&key) {
                    out.insert(key, canonicalize(val));
                }
            }
            Value::Object(out)
        }
        Value::Array(items) => {
            let normalized = items.iter().map(canonicalize).collect();
            Value::Array(normalized)
        }
        other => other.clone(),
    }
}

#[test]
fn json_output_schema_hash_guard() {
    let a = fixture_path("json_diff_single_cell_a.xlsx");
    let b = fixture_path("json_diff_single_cell_b.xlsx");

    let json =
        diff_workbooks_to_json(&a, &b, &DiffConfig::default()).expect("json diff should succeed");
    let value: Value = serde_json::from_str(&json).expect("json should parse");
    let canonical = canonicalize(&value);
    let canonical_json =
        serde_json::to_string(&canonical).expect("canonical json should serialize");

    let digest = Sha256::digest(canonical_json.as_bytes());
    let actual = format!("{:x}", digest);

    assert_eq!(
        actual, EXPECTED_JSON_HASH,
        "update EXPECTED_JSON_HASH when the JSON contract intentionally changes"
    );
}

```

---

### File: `core\tests\signature_tests.rs`

```rust
mod common;

use common::sid;
use excel_diff::{CellValue, Grid, GridView, StringId};

#[derive(Clone)]
struct TestCell {
    row: u32,
    col: u32,
    value: Option<CellValue>,
    formula: Option<StringId>,
}

trait GridTestInsert {
    fn insert_test(&mut self, cell: TestCell);
}

impl GridTestInsert for Grid {
    fn insert_test(&mut self, cell: TestCell) {
        self.insert_cell(cell.row, cell.col, cell.value, cell.formula);
    }
}

fn make_cell(row: u32, col: u32, value: Option<CellValue>, formula: Option<&str>) -> TestCell {
    TestCell {
        row,
        col,
        value,
        formula: formula.map(sid),
    }
}

#[test]
fn identical_rows_have_same_signature() {
    let mut grid1 = Grid::new(1, 3);
    let mut grid2 = Grid::new(1, 3);
    for c in 0..3 {
        let cell = make_cell(0, c, Some(CellValue::Number(c as f64)), None);
        grid1.insert_test(cell.clone());
        grid2.insert_test(cell);
    }
    let sig1 = grid1.compute_row_signature(0);
    let sig2 = grid2.compute_row_signature(0);
    assert_eq!(sig1, sig2);
}

#[test]
fn different_rows_have_different_signatures() {
    let mut grid1 = Grid::new(1, 3);
    let mut grid2 = Grid::new(1, 3);
    for c in 0..3 {
        grid1.insert_test(make_cell(0, c, Some(CellValue::Number(c as f64)), None));
        grid2.insert_test(make_cell(
            0,
            c,
            Some(CellValue::Number((c + 1) as f64)),
            None,
        ));
    }
    let sig1 = grid1.compute_row_signature(0);
    let sig2 = grid2.compute_row_signature(0);
    assert_ne!(sig1, sig2);
}

#[test]
fn compute_all_signatures_populates_fields() {
    let mut grid = Grid::new(5, 5);
    grid.insert_test(make_cell(2, 2, Some(CellValue::Text(sid("center"))), None));
    assert!(grid.row_signatures.is_none());
    assert!(grid.col_signatures.is_none());
    grid.compute_all_signatures();
    assert!(grid.row_signatures.is_some());
    assert!(grid.col_signatures.is_some());
    assert_eq!(grid.row_signatures.as_ref().unwrap().len(), 5);
    assert_eq!(grid.col_signatures.as_ref().unwrap().len(), 5);
    assert_ne!(grid.row_signatures.as_ref().unwrap()[2].hash, 0);
    assert_ne!(grid.col_signatures.as_ref().unwrap()[2].hash, 0);
}

#[test]
fn compute_all_signatures_on_empty_grid_produces_empty_vectors() {
    let mut grid = Grid::new(0, 0);

    grid.compute_all_signatures();

    assert!(grid.row_signatures.is_some());
    assert!(grid.col_signatures.is_some());
    assert!(grid.row_signatures.as_ref().unwrap().is_empty());
    assert!(grid.col_signatures.as_ref().unwrap().is_empty());
}

#[test]
fn compute_all_signatures_with_all_empty_rows_and_cols_is_stable() {
    let mut grid = Grid::new(3, 4);

    grid.compute_all_signatures();
    let first_rows = grid.row_signatures.as_ref().unwrap().clone();
    let first_cols = grid.col_signatures.as_ref().unwrap().clone();

    assert_eq!(first_rows.len(), 3);
    assert_eq!(first_cols.len(), 4);
    let empty_row_hash = first_rows[0].hash;
    let empty_col_hash = first_cols[0].hash;
    assert!(first_rows.iter().all(|sig| sig.hash == empty_row_hash));
    assert!(first_cols.iter().all(|sig| sig.hash == empty_col_hash));

    grid.compute_all_signatures();
    let second_rows = grid.row_signatures.as_ref().unwrap();
    let second_cols = grid.col_signatures.as_ref().unwrap();

    assert_eq!(first_rows, *second_rows);
    assert_eq!(first_cols, *second_cols);
}

#[test]
fn row_and_col_signatures_match_bulk_computation() {
    let mut grid = Grid::new(3, 2);
    grid.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(std::f64::consts::PI)),
        Some("=PI()"),
    ));
    grid.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("text"))), None));
    grid.insert_test(make_cell(2, 0, Some(CellValue::Bool(true)), Some("=A1")));

    grid.compute_all_signatures();

    let row_sigs = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should exist");
    for r in 0..3 {
        assert_eq!(
            grid.compute_row_signature(r).hash,
            row_sigs[r as usize].hash
        );
    }

    let col_sigs = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should exist");
    for c in 0..2 {
        assert_eq!(
            grid.compute_col_signature(c).hash,
            col_sigs[c as usize].hash
        );
    }
}

#[test]
fn compute_all_signatures_recomputes_after_mutation() {
    let mut grid = Grid::new(3, 3);
    grid.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("x"))), None));

    grid.compute_all_signatures();
    let first_rows = grid.row_signatures.as_ref().unwrap().clone();
    let first_cols = grid.col_signatures.as_ref().unwrap().clone();

    grid.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("y"))), None));
    grid.insert_test(make_cell(2, 2, Some(CellValue::Bool(true)), None));

    grid.compute_all_signatures();
    let second_rows = grid.row_signatures.as_ref().unwrap();
    let second_cols = grid.col_signatures.as_ref().unwrap();

    assert_ne!(first_rows[1].hash, second_rows[1].hash);
    assert_ne!(first_cols[1].hash, second_cols[1].hash);
}

#[test]
fn row_signatures_distinguish_column_positions() {
    let mut grid1 = Grid::new(1, 2);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid1.insert_test(make_cell(0, 1, Some(CellValue::Number(2.0)), None));

    let mut grid2 = Grid::new(1, 2);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(2.0)), None));
    grid2.insert_test(make_cell(0, 1, Some(CellValue::Number(1.0)), None));

    let sig1 = grid1.compute_row_signature(0);
    let sig2 = grid2.compute_row_signature(0);
    assert_ne!(sig1.hash, sig2.hash);
}

#[test]
fn col_signatures_distinguish_row_positions() {
    let mut grid1 = Grid::new(2, 1);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid1.insert_test(make_cell(1, 0, Some(CellValue::Number(2.0)), None));

    let mut grid2 = Grid::new(2, 1);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(2.0)), None));
    grid2.insert_test(make_cell(1, 0, Some(CellValue::Number(1.0)), None));

    let sig1 = grid1.compute_col_signature(0);
    let sig2 = grid2.compute_col_signature(0);
    assert_ne!(sig1.hash, sig2.hash);
}

#[test]
fn row_signature_independent_of_insertion_order() {
    let mut grid1 = Grid::new(1, 3);
    grid1.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(10.0)),
        Some("=A1*2"),
    ));
    grid1.insert_test(make_cell(0, 1, Some(CellValue::Text(sid("mix"))), None));
    grid1.insert_test(make_cell(0, 2, Some(CellValue::Bool(true)), None));

    let mut grid2 = Grid::new(1, 3);
    grid2.insert_test(make_cell(0, 2, Some(CellValue::Bool(true)), None));
    grid2.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(10.0)),
        Some("=A1*2"),
    ));
    grid2.insert_test(make_cell(0, 1, Some(CellValue::Text(sid("mix"))), None));

    let sig1 = grid1.compute_row_signature(0).hash;
    let sig2 = grid2.compute_row_signature(0).hash;
    assert_eq!(sig1, sig2);

    grid1.compute_all_signatures();
    grid2.compute_all_signatures();

    let bulk_sig1 = grid1.row_signatures.as_ref().unwrap()[0].hash;
    let bulk_sig2 = grid2.row_signatures.as_ref().unwrap()[0].hash;
    assert_eq!(bulk_sig1, bulk_sig2);
}

#[test]
fn col_signature_independent_of_insertion_order() {
    let mut grid1 = Grid::new(3, 1);
    grid1.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(std::f64::consts::E)),
        Some("=EXP(1)"),
    ));
    grid1.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("col"))), None));
    grid1.insert_test(make_cell(2, 0, Some(CellValue::Bool(false)), None));

    let mut grid2 = Grid::new(3, 1);
    grid2.insert_test(make_cell(2, 0, Some(CellValue::Bool(false)), None));
    grid2.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(std::f64::consts::E)),
        Some("=EXP(1)"),
    ));
    grid2.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("col"))), None));

    let sig1 = grid1.compute_col_signature(0).hash;
    let sig2 = grid2.compute_col_signature(0).hash;
    assert_eq!(sig1, sig2);

    grid1.compute_all_signatures();
    grid2.compute_all_signatures();

    let bulk_sig1 = grid1.col_signatures.as_ref().unwrap()[0].hash;
    let bulk_sig2 = grid2.col_signatures.as_ref().unwrap()[0].hash;
    assert_eq!(bulk_sig1, bulk_sig2);
}

#[test]
fn col_signature_distinguishes_numeric_text_bool() {
    let mut grid_num = Grid::new(3, 1);
    grid_num.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));

    let mut grid_text = Grid::new(3, 1);
    grid_text.insert_test(make_cell(0, 0, Some(CellValue::Text(sid("1"))), None));

    let mut grid_bool = Grid::new(3, 1);
    grid_bool.insert_test(make_cell(0, 0, Some(CellValue::Bool(true)), None));

    let num = grid_num.compute_col_signature(0).hash;
    let txt = grid_text.compute_col_signature(0).hash;
    let boo = grid_bool.compute_col_signature(0).hash;

    assert_ne!(num, txt);
    assert_ne!(num, boo);
    assert_ne!(txt, boo);
}

#[test]
fn row_signature_distinguishes_numeric_text_bool() {
    let mut grid_num = Grid::new(1, 1);
    grid_num.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));

    let mut grid_text = Grid::new(1, 1);
    grid_text.insert_test(make_cell(0, 0, Some(CellValue::Text(sid("1"))), None));

    let mut grid_bool = Grid::new(1, 1);
    grid_bool.insert_test(make_cell(0, 0, Some(CellValue::Bool(true)), None));

    let num = grid_num.compute_row_signature(0).hash;
    let txt = grid_text.compute_row_signature(0).hash;
    let boo = grid_bool.compute_row_signature(0).hash;

    assert_ne!(num, txt);
    assert_ne!(num, boo);
    assert_ne!(txt, boo);
}

#[test]
fn row_signature_ignores_empty_trailing_cells() {
    let mut grid1 = Grid::new(1, 3);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(42.0)), None));

    let mut grid2 = Grid::new(1, 10);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(42.0)), None));

    let sig1 = grid1.compute_row_signature(0).hash;
    let sig2 = grid2.compute_row_signature(0).hash;
    assert_eq!(sig1, sig2);
}

#[test]
fn col_signature_ignores_empty_trailing_rows() {
    let mut grid1 = Grid::new(3, 1);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(42.0)), None));

    let mut grid2 = Grid::new(10, 1);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(42.0)), None));

    let sig1 = grid1.compute_col_signature(0).hash;
    let sig2 = grid2.compute_col_signature(0).hash;
    assert_eq!(sig1, sig2);
}

#[test]
fn col_signature_includes_formulas_by_default() {
    let mut with_formula = Grid::new(2, 1);
    with_formula.insert_test(make_cell(0, 0, Some(CellValue::Number(10.0)), Some("=5+5")));

    let mut without_formula = Grid::new(2, 1);
    without_formula.insert_test(make_cell(0, 0, Some(CellValue::Number(10.0)), None));

    let sig_with = with_formula.compute_col_signature(0).hash;
    let sig_without = without_formula.compute_col_signature(0).hash;
    assert_ne!(sig_with, sig_without);
}

#[test]
fn col_signature_includes_formulas_sparse() {
    let mut formula_short = Grid::new(5, 1);
    formula_short.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Text(sid("foo"))),
        Some("=A2"),
    ));

    let mut formula_tall = Grid::new(10, 1);
    formula_tall.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Text(sid("foo"))),
        Some("=A2"),
    ));

    let mut value_only = Grid::new(10, 1);
    value_only.insert_test(make_cell(0, 0, Some(CellValue::Text(sid("foo"))), None));

    let sig_formula_short = formula_short.compute_col_signature(0).hash;
    let sig_formula_tall = formula_tall.compute_col_signature(0).hash;
    let sig_value_only = value_only.compute_col_signature(0).hash;

    assert_eq!(sig_formula_short, sig_formula_tall);
    assert_ne!(sig_formula_short, sig_value_only);
}

#[test]
fn row_signature_includes_formulas_by_default() {
    let mut grid_with_formula = Grid::new(1, 1);
    grid_with_formula.insert_test(make_cell(0, 0, Some(CellValue::Number(10.0)), Some("=5+5")));

    let mut grid_without_formula = Grid::new(1, 1);
    grid_without_formula.insert_test(make_cell(0, 0, Some(CellValue::Number(10.0)), None));

    let sig_with = grid_with_formula.compute_row_signature(0).hash;
    let sig_without = grid_without_formula.compute_row_signature(0).hash;
    assert_ne!(sig_with, sig_without);
}

#[test]
fn row_signature_is_stable_across_computations() {
    let mut grid = Grid::new(1, 3);
    grid.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid.insert_test(make_cell(0, 1, Some(CellValue::Text(sid("x"))), None));
    grid.insert_test(make_cell(0, 2, Some(CellValue::Bool(false)), None));

    let sig1 = grid.compute_row_signature(0);
    let sig2 = grid.compute_row_signature(0);
    assert_eq!(sig1.hash, sig2.hash);
    assert_ne!(sig1.hash, 0);
}

#[test]
fn row_signature_with_formula_is_stable() {
    let mut grid = Grid::new(1, 2);
    grid.insert_test(make_cell(0, 0, Some(CellValue::Number(10.0)), Some("=5+5")));
    grid.insert_test(make_cell(0, 1, Some(CellValue::Text(sid("bar"))), None));

    let sig1 = grid.compute_row_signature(0);
    let sig2 = grid.compute_row_signature(0);
    assert_eq!(sig1.hash, sig2.hash);
    assert_ne!(sig1.hash, 0);
}

#[test]
fn gridview_rowmeta_hash_matches_compute_all_signatures() {
    let mut grid = Grid::new(3, 2);
    grid.insert_test(make_cell(
        0,
        0,
        Some(CellValue::Number(std::f64::consts::PI)),
        Some("=PI()"),
    ));
    grid.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("text"))), None));
    grid.insert_test(make_cell(2, 0, Some(CellValue::Bool(true)), Some("=A1")));

    grid.compute_all_signatures();

    let row_signatures = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should be computed")
        .clone();
    let col_signatures = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should be computed")
        .clone();

    let view = GridView::from_grid(&grid);

    for (idx, meta) in view.row_meta.iter().enumerate() {
        assert_eq!(meta.signature, row_signatures[idx]);
    }

    for (idx, meta) in view.col_meta.iter().enumerate() {
        assert_eq!(meta.hash, col_signatures[idx]);
    }
}

#[test]
fn row_signature_unchanged_after_column_insert_at_position_zero() {
    let mut grid1 = Grid::new(2, 3);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid1.insert_test(make_cell(0, 1, Some(CellValue::Number(2.0)), None));
    grid1.insert_test(make_cell(0, 2, Some(CellValue::Number(3.0)), None));
    grid1.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("a"))), None));
    grid1.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("b"))), None));
    grid1.insert_test(make_cell(1, 2, Some(CellValue::Text(sid("c"))), None));

    let mut grid2 = Grid::new(2, 4);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(99.0)), None));
    grid2.insert_test(make_cell(0, 1, Some(CellValue::Number(1.0)), None));
    grid2.insert_test(make_cell(0, 2, Some(CellValue::Number(2.0)), None));
    grid2.insert_test(make_cell(0, 3, Some(CellValue::Number(3.0)), None));
    grid2.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("z"))), None));
    grid2.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("a"))), None));
    grid2.insert_test(make_cell(1, 2, Some(CellValue::Text(sid("b"))), None));
    grid2.insert_test(make_cell(1, 3, Some(CellValue::Text(sid("c"))), None));

    let view1 = GridView::from_grid(&grid1);
    let view2 = GridView::from_grid(&grid2);

    assert_ne!(view1.row_meta[0].signature, view2.row_meta[0].signature);
    assert_ne!(view1.row_meta[1].signature, view2.row_meta[1].signature);
}

#[test]
fn row_signature_unchanged_after_column_delete_from_middle() {
    let mut grid1 = Grid::new(2, 4);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid1.insert_test(make_cell(0, 1, Some(CellValue::Number(2.0)), None));
    grid1.insert_test(make_cell(0, 2, Some(CellValue::Number(3.0)), None));
    grid1.insert_test(make_cell(0, 3, Some(CellValue::Number(4.0)), None));
    grid1.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("a"))), None));
    grid1.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("b"))), None));
    grid1.insert_test(make_cell(1, 2, Some(CellValue::Text(sid("c"))), None));
    grid1.insert_test(make_cell(1, 3, Some(CellValue::Text(sid("d"))), None));

    let mut grid2 = Grid::new(2, 3);
    grid2.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid2.insert_test(make_cell(0, 1, Some(CellValue::Number(3.0)), None));
    grid2.insert_test(make_cell(0, 2, Some(CellValue::Number(4.0)), None));
    grid2.insert_test(make_cell(1, 0, Some(CellValue::Text(sid("a"))), None));
    grid2.insert_test(make_cell(1, 1, Some(CellValue::Text(sid("c"))), None));
    grid2.insert_test(make_cell(1, 2, Some(CellValue::Text(sid("d"))), None));

    let view1 = GridView::from_grid(&grid1);
    let view2 = GridView::from_grid(&grid2);

    assert_ne!(view1.row_meta[0].signature, view2.row_meta[0].signature);
    assert_ne!(view1.row_meta[1].signature, view2.row_meta[1].signature);
}

#[test]
fn row_signature_consistent_for_same_content_different_column_indices() {
    let mut grid1 = Grid::new(1, 3);
    grid1.insert_test(make_cell(0, 0, Some(CellValue::Number(1.0)), None));
    grid1.insert_test(make_cell(0, 1, Some(CellValue::Number(2.0)), None));
    grid1.insert_test(make_cell(0, 2, Some(CellValue::Number(3.0)), None));

    let mut grid2 = Grid::new(1, 5);
    grid2.insert_test(make_cell(0, 1, Some(CellValue::Number(1.0)), None));
    grid2.insert_test(make_cell(0, 2, Some(CellValue::Number(2.0)), None));
    grid2.insert_test(make_cell(0, 3, Some(CellValue::Number(3.0)), None));

    let view1 = GridView::from_grid(&grid1);
    let view2 = GridView::from_grid(&grid2);

    assert_eq!(view1.row_meta[0].signature, view2.row_meta[0].signature);
}

```

---

### File: `core\tests\sparse_grid_tests.rs`

```rust
use excel_diff::{CellValue, Grid, with_default_session};

#[test]
fn sparse_grid_empty_has_zero_cells() {
    let grid = Grid::new(1000, 1000);
    assert_eq!(grid.cell_count(), 0);
    assert!(grid.is_empty());
    assert_eq!(grid.nrows, 1000);
    assert_eq!(grid.ncols, 1000);
}

#[test]
fn sparse_grid_insert_and_retrieve() {
    let mut grid = Grid::new(100, 100);
    grid.insert_cell(50, 50, Some(CellValue::Number(42.0)), None);
    assert_eq!(grid.cell_count(), 1);
    let retrieved = grid.get(50, 50).expect("cell should exist");
    assert_eq!(retrieved.value, Some(CellValue::Number(42.0)));
    assert!(grid.get(0, 0).is_none());
}

#[test]
fn sparse_grid_iter_cells_only_populated() {
    let mut grid = Grid::new(1000, 1000);
    for i in 0..10 {
        grid.insert_cell(i * 100, i * 100, Some(CellValue::Number(i as f64)), None);
    }
    let cells: Vec<_> = grid.iter_cells().collect();
    assert_eq!(cells.len(), 10);
}

#[test]
fn sparse_grid_memory_efficiency() {
    let grid = Grid::new(10_000, 1_000);
    assert!(std::mem::size_of_val(&grid) < 1024);
}

#[test]
fn rows_iter_covers_all_rows() {
    let grid = Grid::new(3, 5);
    let rows: Vec<u32> = grid.rows_iter().collect();
    assert_eq!(rows, vec![0, 1, 2]);
}

#[test]
fn cols_iter_covers_all_cols() {
    let grid = Grid::new(4, 2);
    let cols: Vec<u32> = grid.cols_iter().collect();
    assert_eq!(cols, vec![0, 1]);
}

#[test]
fn rows_iter_and_get_are_consistent() {
    let mut grid = Grid::new(2, 2);
    grid.insert_cell(1, 1, Some(CellValue::Number(1.0)), None);

    for r in grid.rows_iter() {
        for c in grid.cols_iter() {
            let _ = grid.get(r, c);
        }
    }
}

#[test]
fn sparse_grid_all_empty_rows_have_zero_signatures() {
    let mut grid = Grid::new(2, 3);

    grid.compute_all_signatures();

    let row_sigs = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should exist");
    let col_sigs = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should exist");

    assert_eq!(row_sigs.len(), 2);
    assert_eq!(col_sigs.len(), 3);
    let empty_row_hash = row_sigs[0].hash;
    let empty_col_hash = col_sigs[0].hash;
    assert!(row_sigs.iter().all(|sig| sig.hash == empty_row_hash));
    assert!(col_sigs.iter().all(|sig| sig.hash == empty_col_hash));
}

#[test]
fn compute_signatures_on_sparse_grid_produces_hashes() {
    let mut grid = Grid::new(4, 4);
    with_default_session(|session| {
        let text_id = session.strings.intern("value");
        let formula_id = session.strings.intern("=A1");
        grid.insert_cell(1, 3, Some(CellValue::Text(text_id)), Some(formula_id));
    });

    grid.compute_all_signatures();

    let row_hash = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should exist")[1]
        .hash;
    let col_hash = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should exist")[3]
        .hash;

    assert_ne!(row_hash, 0);
    assert_ne!(col_hash, 0);
}

#[test]
fn compute_all_signatures_matches_direct_computation() {
    let mut grid = Grid::new(3, 3);
    with_default_session(|session| {
        let formula_a = session.strings.intern("=5+5");
        let text_id = session.strings.intern("x");
        let formula_b = session.strings.intern("=A1");
        grid.insert_cell(0, 1, Some(CellValue::Number(10.0)), Some(formula_a));
        grid.insert_cell(1, 0, Some(CellValue::Text(text_id)), None);
        grid.insert_cell(2, 2, Some(CellValue::Bool(false)), Some(formula_b));
    });

    grid.compute_all_signatures();

    let row_sigs = grid
        .row_signatures
        .as_ref()
        .expect("row signatures should exist");
    let col_sigs = grid
        .col_signatures
        .as_ref()
        .expect("col signatures should exist");

    assert_eq!(grid.compute_row_signature(0).hash, row_sigs[0].hash);
    assert_eq!(grid.compute_row_signature(2).hash, row_sigs[2].hash);
    assert_eq!(grid.compute_col_signature(0).hash, col_sigs[0].hash);
    assert_eq!(grid.compute_col_signature(2).hash, col_sigs[2].hash);
}

```

---

### File: `core\tests\streaming_contract_tests.rs`

```rust
mod common;

use common::{collect_string_ids, fixture_path};
use excel_diff::{
    CellValue, DataMashup, DiffConfig, DiffError, DiffOp, DiffSink, Grid, JsonLinesSink,
    LimitBehavior, Metadata, PackageParts, PackageXml, PbixPackage, Permissions, SectionDocument,
    Sheet, SheetKind, StringPool, VbaModule, VbaModuleType, Workbook, WorkbookPackage,
    PermissionBindingsStatus, try_diff_grids_database_mode_streaming, try_diff_grids_streaming,
    try_diff_sheets_streaming, try_diff_workbooks_streaming,
};
use serde::Deserialize;
use std::fs::File;

#[derive(Default)]
struct StrictLifecycleSink {
    begin_seen: bool,
    finish_seen: bool,
    finish_calls: usize,
    emit_calls: usize,
}

impl DiffSink for StrictLifecycleSink {
    fn begin(&mut self, _pool: &StringPool) -> Result<(), DiffError> {
        if self.begin_seen {
            return Err(DiffError::SinkError {
                message: "begin called twice".to_string(),
            });
        }
        self.begin_seen = true;
        Ok(())
    }

    fn emit(&mut self, _op: DiffOp) -> Result<(), DiffError> {
        if !self.begin_seen {
            return Err(DiffError::SinkError {
                message: "emit before begin".to_string(),
            });
        }
        if self.finish_seen {
            return Err(DiffError::SinkError {
                message: "emit after finish".to_string(),
            });
        }
        self.emit_calls += 1;
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        self.finish_calls += 1;
        self.finish_seen = true;
        Ok(())
    }
}

struct FailAfterNSink {
    fail_after: usize,
    emit_calls: usize,
    finish_calls: usize,
    finish_seen: bool,
}

impl FailAfterNSink {
    fn new(fail_after: usize) -> Self {
        Self {
            fail_after,
            emit_calls: 0,
            finish_calls: 0,
            finish_seen: false,
        }
    }
}

impl DiffSink for FailAfterNSink {
    fn emit(&mut self, _op: DiffOp) -> Result<(), DiffError> {
        self.emit_calls += 1;
        if self.emit_calls > self.fail_after {
            return Err(DiffError::SinkError {
                message: "intentional emit failure".to_string(),
            });
        }
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        self.finish_calls += 1;
        self.finish_seen = true;
        Ok(())
    }
}

#[derive(Default)]
struct FrozenPoolSink {
    pool_ptr: Option<*const StringPool>,
    pool_len: Option<usize>,
    finish_calls: usize,
}

impl DiffSink for FrozenPoolSink {
    fn begin(&mut self, pool: &StringPool) -> Result<(), DiffError> {
        self.pool_ptr = Some(pool as *const StringPool);
        self.pool_len = Some(pool.len());
        Ok(())
    }

    fn emit(&mut self, _op: DiffOp) -> Result<(), DiffError> {
        let Some(ptr) = self.pool_ptr else {
            return Err(DiffError::SinkError {
                message: "emit before begin".to_string(),
            });
        };
        let Some(len) = self.pool_len else {
            return Err(DiffError::SinkError {
                message: "missing pool length".to_string(),
            });
        };
        // Safety: the pool pointer is captured from begin and remains valid for the diff.
        let current = unsafe { (&*ptr).len() };
        assert_eq!(
            current, len,
            "string pool length changed after begin ({} -> {})",
            len, current
        );
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        self.finish_calls += 1;
        Ok(())
    }
}

fn make_workbook(pool: &mut StringPool, values: &[f64]) -> Workbook {
    let grid = make_grid(values);

    Workbook {
        sheets: vec![Sheet {
            name: pool.intern("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn make_grid(values: &[f64]) -> Grid {
    let mut grid = Grid::new(values.len() as u32, 1);
    for (idx, val) in values.iter().enumerate() {
        grid.insert_cell(idx as u32, 0, Some(CellValue::Number(*val)), None);
    }
    grid
}

fn make_sheet(pool: &mut StringPool, name: &str, values: &[f64]) -> Sheet {
    Sheet {
        name: pool.intern(name),
        workbook_sheet_id: None,
        kind: SheetKind::Worksheet,
        grid: make_grid(values),
    }
}

fn make_keyed_grid(keys: &[i32], values: &[i32]) -> Grid {
    let rows = keys.len().max(values.len());
    let mut grid = Grid::new(rows as u32, 2);
    for row in 0..rows {
        let key = keys.get(row).copied().unwrap_or_default() as f64;
        let value = values.get(row).copied().unwrap_or_default() as f64;
        grid.insert_cell(row as u32, 0, Some(CellValue::Number(key)), None);
        grid.insert_cell(row as u32, 1, Some(CellValue::Number(value)), None);
    }
    grid
}

fn make_dm(section_source: &str) -> DataMashup {
    DataMashup::new(
        0,
        PackageParts {
            package_xml: PackageXml {
                raw_xml: "<Package/>".to_string(),
            },
            main_section: SectionDocument {
                source: section_source.to_string(),
            },
            embedded_contents: Vec::new(),
        },
        Permissions::default(),
        Metadata { formulas: Vec::new() },
        Vec::new(),
        PermissionBindingsStatus::Missing,
    )
}

fn is_object_op(op: &DiffOp) -> bool {
    matches!(
        op,
        DiffOp::NamedRangeAdded { .. }
            | DiffOp::NamedRangeRemoved { .. }
            | DiffOp::NamedRangeChanged { .. }
            | DiffOp::ChartAdded { .. }
            | DiffOp::ChartRemoved { .. }
            | DiffOp::ChartChanged { .. }
            | DiffOp::VbaModuleAdded { .. }
            | DiffOp::VbaModuleRemoved { .. }
            | DiffOp::VbaModuleChanged { .. }
    )
}

#[test]
fn engine_workbook_streaming_calls_finish_once() {
    let mut pool = StringPool::new();
    let wb_a = make_workbook(&mut pool, &[1.0, 2.0]);
    let wb_b = make_workbook(&mut pool, &[1.0, 3.0]);

    let mut sink = StrictLifecycleSink::default();
    let summary =
        try_diff_workbooks_streaming(&wb_a, &wb_b, &mut pool, &DiffConfig::default(), &mut sink)
            .expect("streaming diff should succeed");

    assert!(sink.begin_seen, "begin should be called");
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
    assert!(sink.finish_seen, "finish should be seen");
    assert!(sink.emit_calls > 0, "expected at least one emit");
    assert_eq!(
        summary.op_count, sink.emit_calls,
        "summary op_count should match emitted ops"
    );
}

#[test]
fn engine_workbook_streaming_finishes_on_emit_error() {
    let mut pool = StringPool::new();
    let wb_a = make_workbook(&mut pool, &[1.0]);
    let wb_b = make_workbook(&mut pool, &[2.0]);

    let mut sink = FailAfterNSink::new(0);
    let result =
        try_diff_workbooks_streaming(&wb_a, &wb_b, &mut pool, &DiffConfig::default(), &mut sink);

    assert!(result.is_err(), "expected sink error");
    assert!(sink.finish_seen, "finish should be called on emit error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_workbook_streaming_finishes_on_limit_error() {
    let mut pool = StringPool::new();
    let wb_a = make_workbook(&mut pool, &[1.0, 2.0]);
    let wb_b = make_workbook(&mut pool, &[1.0, 3.0]);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 1;
    config.alignment.max_align_cols = 1;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let mut sink = StrictLifecycleSink::default();
    let result = try_diff_workbooks_streaming(&wb_a, &wb_b, &mut pool, &config, &mut sink);

    assert!(matches!(result, Err(DiffError::LimitsExceeded { .. })));
    assert!(sink.finish_seen, "finish should be called on error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_grid_streaming_calls_finish_once() {
    let mut pool = StringPool::new();
    let grid_a = make_grid(&[1.0, 2.0]);
    let grid_b = make_grid(&[1.0, 3.0]);

    let mut sink = StrictLifecycleSink::default();
    let summary = try_diff_grids_streaming(
        &grid_a,
        &grid_b,
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
    )
    .expect("grid streaming diff should succeed");

    assert!(sink.begin_seen, "begin should be called");
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
    assert!(sink.finish_seen, "finish should be seen");
    assert!(sink.emit_calls > 0, "expected at least one emit");
    assert_eq!(
        summary.op_count, sink.emit_calls,
        "summary op_count should match emitted ops"
    );
}

#[test]
fn engine_grid_streaming_finishes_on_emit_error() {
    let mut pool = StringPool::new();
    let grid_a = make_grid(&[1.0]);
    let grid_b = make_grid(&[2.0]);

    let mut sink = FailAfterNSink::new(0);
    let result =
        try_diff_grids_streaming(&grid_a, &grid_b, &mut pool, &DiffConfig::default(), &mut sink);

    assert!(result.is_err(), "expected sink error");
    assert!(sink.finish_seen, "finish should be called on emit error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_grid_streaming_finishes_on_limit_error() {
    let mut pool = StringPool::new();
    let grid_a = make_grid(&[1.0, 2.0]);
    let grid_b = make_grid(&[1.0, 3.0]);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 1;
    config.alignment.max_align_cols = 1;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let mut sink = StrictLifecycleSink::default();
    let result = try_diff_grids_streaming(&grid_a, &grid_b, &mut pool, &config, &mut sink);

    assert!(matches!(result, Err(DiffError::LimitsExceeded { .. })));
    assert!(sink.finish_seen, "finish should be called on error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_sheet_streaming_calls_finish_once() {
    let mut pool = StringPool::new();
    let sheet_a = make_sheet(&mut pool, "Sheet1", &[1.0, 2.0]);
    let sheet_b = make_sheet(&mut pool, "Sheet1", &[1.0, 3.0]);

    let mut sink = StrictLifecycleSink::default();
    let summary = try_diff_sheets_streaming(
        &sheet_a,
        &sheet_b,
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
    )
    .expect("sheet streaming diff should succeed");

    assert!(sink.begin_seen, "begin should be called");
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
    assert!(sink.finish_seen, "finish should be seen");
    assert!(sink.emit_calls > 0, "expected at least one emit");
    assert_eq!(
        summary.op_count, sink.emit_calls,
        "summary op_count should match emitted ops"
    );
}

#[test]
fn engine_sheet_streaming_finishes_on_emit_error() {
    let mut pool = StringPool::new();
    let sheet_a = make_sheet(&mut pool, "Sheet1", &[1.0]);
    let sheet_b = make_sheet(&mut pool, "Sheet1", &[2.0]);

    let mut sink = FailAfterNSink::new(0);
    let result = try_diff_sheets_streaming(
        &sheet_a,
        &sheet_b,
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
    );

    assert!(result.is_err(), "expected sink error");
    assert!(sink.finish_seen, "finish should be called on emit error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_sheet_streaming_finishes_on_limit_error() {
    let mut pool = StringPool::new();
    let sheet_a = make_sheet(&mut pool, "Sheet1", &[1.0, 2.0]);
    let sheet_b = make_sheet(&mut pool, "Sheet1", &[1.0, 3.0]);

    let mut config = DiffConfig::default();
    config.alignment.max_align_rows = 1;
    config.alignment.max_align_cols = 1;
    config.hardening.on_limit_exceeded = LimitBehavior::ReturnError;

    let mut sink = StrictLifecycleSink::default();
    let result = try_diff_sheets_streaming(&sheet_a, &sheet_b, &mut pool, &config, &mut sink);

    assert!(matches!(result, Err(DiffError::LimitsExceeded { .. })));
    assert!(sink.finish_seen, "finish should be called on error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn engine_database_streaming_calls_finish_once() {
    let mut pool = StringPool::new();
    let sheet_id = pool.intern("Data");

    let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
    let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);

    let mut sink = StrictLifecycleSink::default();
    let mut op_count = 0usize;
    let summary = try_diff_grids_database_mode_streaming(
        sheet_id,
        &grid_a,
        &grid_b,
        &[0],
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
        &mut op_count,
    )
    .expect("database streaming diff should succeed");

    assert!(sink.begin_seen, "begin should be called");
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
    assert!(summary.op_count > 0, "expected at least one op");
}

#[test]
fn engine_database_streaming_finishes_on_emit_error() {
    let mut pool = StringPool::new();
    let sheet_id = pool.intern("Data");

    let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
    let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);

    let mut sink = FailAfterNSink::new(0);
    let mut op_count = 0usize;
    let result = try_diff_grids_database_mode_streaming(
        sheet_id,
        &grid_a,
        &grid_b,
        &[0],
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
        &mut op_count,
    );

    assert!(result.is_err(), "expected sink error");
    assert!(sink.finish_seen, "finish should be called on emit error");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn grid_leaf_streaming_does_not_intern_after_begin() {
    let mut pool = StringPool::new();
    let grid_a = make_grid(&[1.0]);
    let grid_b = make_grid(&[2.0]);

    let mut sink = FrozenPoolSink::default();
    try_diff_grids_streaming(
        &grid_a,
        &grid_b,
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
    )
    .expect("grid streaming should succeed");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn sheet_leaf_streaming_does_not_intern_after_begin() {
    let mut pool = StringPool::new();
    let sheet_a = make_sheet(&mut pool, "Sheet1", &[1.0]);
    let sheet_b = make_sheet(&mut pool, "Sheet1", &[2.0]);

    let mut sink = FrozenPoolSink::default();
    try_diff_sheets_streaming(
        &sheet_a,
        &sheet_b,
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
    )
    .expect("sheet streaming should succeed");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn pbix_streaming_calls_finish_once() {
    let path_a = fixture_path("pbix_legacy_multi_query_a.pbix");
    let path_b = fixture_path("pbix_legacy_multi_query_b.pbix");
    let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
        .expect("pbix A should parse");
    let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
        .expect("pbix B should parse");

    let mut sink = StrictLifecycleSink::default();
    let summary = pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("pbix streaming should succeed");

    assert!(sink.begin_seen, "begin should be called");
    assert_eq!(sink.finish_calls, 1, "finish should be called exactly once");
    assert!(summary.op_count > 0, "expected at least one op");
}

#[test]
fn pbix_streaming_does_not_intern_after_begin() {
    let path_a = fixture_path("pbix_legacy_multi_query_a.pbix");
    let path_b = fixture_path("pbix_legacy_multi_query_b.pbix");
    let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
        .expect("pbix A should parse");
    let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
        .expect("pbix B should parse");

    let mut sink = FrozenPoolSink::default();
    pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("pbix streaming should succeed");
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn pbix_streaming_jsonl_header_includes_all_string_ids() {
    #[derive(Deserialize)]
    struct Header {
        kind: String,
        strings: Vec<String>,
    }

    let path_a = fixture_path("pbix_legacy_multi_query_a.pbix");
    let path_b = fixture_path("pbix_legacy_multi_query_b.pbix");
    let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
        .expect("pbix A should parse");
    let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
        .expect("pbix B should parse");

    let mut out = Vec::<u8>::new();
    let mut sink = JsonLinesSink::new(&mut out);
    let summary = pkg_a
        .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
        .expect("pbix streaming should succeed");

    let text = std::str::from_utf8(&out).expect("output should be UTF-8");
    let mut lines = text.lines().filter(|l| !l.trim().is_empty());
    let header_line = lines.next().expect("expected a JSON Lines header line");
    let header: Header = serde_json::from_str(header_line).expect("header should parse");
    assert_eq!(header.kind, "Header");

    let mut op_lines = 0usize;
    for line in lines {
        let op: DiffOp = serde_json::from_str(line).expect("op line should parse as DiffOp");
        for id in collect_string_ids(&op) {
            assert!(
                (id.0 as usize) < header.strings.len(),
                "StringId {} out of range for header string table (len={})",
                id.0,
                header.strings.len()
            );
        }
        op_lines += 1;
    }

    assert!(op_lines > 0, "expected at least one op line after header");
    assert_eq!(
        summary.op_count, op_lines,
        "summary op_count should match ops written after the header"
    );
}

#[test]
fn workbook_package_streaming_orders_categories() {
    let mut pool = StringPool::new();

    let wb_a = make_workbook(&mut pool, &[1.0]);
    let wb_b = make_workbook(&mut pool, &[2.0]);

    let dm_a = make_dm("section Section1;\nshared Foo = 1;");
    let dm_b = make_dm("section Section1;\nshared Bar = 1;");

    let vba_name = pool.intern("Module1");
    let vba_modules = Some(vec![VbaModule {
        name: vba_name,
        module_type: VbaModuleType::Standard,
        code: "Sub Foo()\nEnd Sub".to_string(),
    }]);

    let pkg_a = WorkbookPackage {
        workbook: wb_a,
        data_mashup: Some(dm_a),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb_b,
        data_mashup: Some(dm_b),
        vba_modules,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    let mut sink = excel_diff::VecSink::new();
    pkg_a
        .diff_streaming_with_pool(&pkg_b, &mut pool, &DiffConfig::default(), &mut sink)
        .expect("streaming diff should succeed");
    let ops = sink.into_ops();

    assert!(
        ops.iter().any(|op| !op.is_m_op() && !is_object_op(op)),
        "expected at least one grid op"
    );
    assert!(ops.iter().any(is_object_op), "expected at least one object op");
    assert!(ops.iter().any(DiffOp::is_m_op), "expected at least one M op");

    enum Stage {
        Grid,
        Object,
        M,
    }

    let mut stage = Stage::Grid;
    for op in &ops {
        if op.is_m_op() {
            stage = Stage::M;
            continue;
        }

        if is_object_op(op) {
            match stage {
                Stage::M => panic!("object op appeared after M ops"),
                Stage::Grid => stage = Stage::Object,
                Stage::Object => {}
            }
            continue;
        }

        match stage {
            Stage::Grid => {}
            Stage::Object => panic!("grid op appeared after object ops"),
            Stage::M => panic!("grid op appeared after M ops"),
        }
    }
}

#[test]
fn streaming_timeout_sets_complete_false_and_warns() {
    let mut pool = StringPool::new();
    let wb_a = make_workbook(&mut pool, &[1.0, 2.0]);
    let wb_b = make_workbook(&mut pool, &[1.0, 3.0]);

    let mut config = DiffConfig::default();
    config.hardening.timeout_seconds = Some(0);

    let mut sink = StrictLifecycleSink::default();
    let summary =
        try_diff_workbooks_streaming(&wb_a, &wb_b, &mut pool, &config, &mut sink)
            .expect("streaming diff should return summary on timeout");

    assert!(!summary.complete, "summary should be incomplete on timeout");
    assert!(
        summary
            .warnings
            .iter()
            .any(|w| w.to_lowercase().contains("timeout")),
        "expected timeout warning"
    );
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

#[test]
fn database_streaming_no_key_columns_warns_and_finishes() {
    let mut pool = StringPool::new();
    let sheet_id = pool.intern("Data");

    let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
    let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);

    let mut sink = StrictLifecycleSink::default();
    let mut op_count = 0usize;
    let summary = try_diff_grids_database_mode_streaming(
        sheet_id,
        &grid_a,
        &grid_b,
        &[],
        &mut pool,
        &DiffConfig::default(),
        &mut sink,
        &mut op_count,
    )
    .expect("streaming should fall back to spreadsheet mode");

    assert!(!summary.complete, "summary should be incomplete on fallback");
    assert_eq!(
        summary.warnings,
        vec![
            "database-mode: no key columns provided; falling back to spreadsheet mode"
                .to_string()
        ],
        "warning should be deterministic"
    );
    assert_eq!(sink.finish_calls, 1, "finish should be called once");
}

```

---

### File: `core\tests\streaming_determinism_tests.rs`

```rust
mod common;

use common::{
    StructuredOutput, assert_jsonl_determinism_with_fresh_sessions,
    assert_structured_determinism_with_fresh_sessions, fixture_path,
};
use excel_diff::{
    CellValue, DataMashup, DiffConfig, DiffSession, Grid, JsonLinesSink, Metadata, PackageParts,
    PackageXml, PbixPackage, Permissions, SectionDocument, Sheet, SheetKind, StringPool, VbaModule,
    VbaModuleType, Workbook, WorkbookPackage, VecSink, PermissionBindingsStatus,
    try_diff_grids_database_mode_streaming,
};
use std::fs::File;

fn make_workbook(pool: &mut StringPool, value: f64) -> Workbook {
    let mut grid = Grid::new(1, 1);
    grid.insert_cell(0, 0, Some(CellValue::Number(value)), None);

    Workbook {
        sheets: vec![Sheet {
            name: pool.intern("Sheet1"),
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

fn make_keyed_grid(keys: &[i32], values: &[i32]) -> Grid {
    let rows = keys.len().max(values.len());
    let mut grid = Grid::new(rows as u32, 2);
    for row in 0..rows {
        let key = keys.get(row).copied().unwrap_or_default() as f64;
        let value = values.get(row).copied().unwrap_or_default() as f64;
        grid.insert_cell(row as u32, 0, Some(CellValue::Number(key)), None);
        grid.insert_cell(row as u32, 1, Some(CellValue::Number(value)), None);
    }
    grid
}

fn make_dm(section_source: &str) -> DataMashup {
    DataMashup::new(
        0,
        PackageParts {
            package_xml: PackageXml {
                raw_xml: "<Package/>".to_string(),
            },
            main_section: SectionDocument {
                source: section_source.to_string(),
            },
            embedded_contents: Vec::new(),
        },
        Permissions::default(),
        Metadata { formulas: Vec::new() },
        Vec::new(),
        PermissionBindingsStatus::Missing,
    )
}

fn build_packages(pool: &mut StringPool) -> (WorkbookPackage, WorkbookPackage) {
    let wb_a = make_workbook(pool, 1.0);
    let wb_b = make_workbook(pool, 2.0);

    let dm_a = make_dm("section Section1;\nshared Foo = 1;");
    let dm_b = make_dm("section Section1;\nshared Bar = 1;");

    let vba_name = pool.intern("Module1");
    let vba_modules = Some(vec![VbaModule {
        name: vba_name,
        module_type: VbaModuleType::Standard,
        code: "Sub Foo()\nEnd Sub".to_string(),
    }]);

    let pkg_a = WorkbookPackage {
        workbook: wb_a,
        data_mashup: Some(dm_a),
        vba_modules: None,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };
    let pkg_b = WorkbookPackage {
        workbook: wb_b,
        data_mashup: Some(dm_b),
        vba_modules,
        #[cfg(feature = "perf-metrics")]
        parse_time_ms: 0,
    };

    (pkg_a, pkg_b)
}

#[test]
fn workbook_package_streaming_is_deterministic_with_fresh_sessions() {
    assert_structured_determinism_with_fresh_sessions(2, |session| {
        let (pkg_a, pkg_b) = build_packages(&mut session.strings);
        let mut sink = VecSink::new();
        let summary = pkg_a
            .diff_streaming_with_pool(&pkg_b, &mut session.strings, &DiffConfig::default(), &mut sink)
            .expect("streaming diff should succeed");
        StructuredOutput {
            ops: sink.into_ops(),
            summary,
        }
    });
}

#[test]
fn database_mode_streaming_is_deterministic_with_fresh_sessions() {
    assert_structured_determinism_with_fresh_sessions(2, |session| {
        let grid_a = make_keyed_grid(&[1, 2], &[10, 20]);
        let grid_b = make_keyed_grid(&[1, 2], &[10, 25]);
        let sheet_id = session.strings.intern("Data");

        let mut sink = VecSink::new();
        let mut op_count = 0usize;
        let summary = try_diff_grids_database_mode_streaming(
            sheet_id,
            &grid_a,
            &grid_b,
            &[0],
            &mut session.strings,
            &DiffConfig::default(),
            &mut sink,
            &mut op_count,
        )
        .expect("database streaming diff should succeed");

        StructuredOutput {
            ops: sink.into_ops(),
            summary,
        }
    });
}

#[test]
fn pbix_streaming_jsonl_is_deterministic_with_fresh_sessions() {
    let path_a = fixture_path("pbix_legacy_multi_query_a.pbix");
    let path_b = fixture_path("pbix_legacy_multi_query_b.pbix");

    assert_jsonl_determinism_with_fresh_sessions(2, |_session| {
        excel_diff::with_default_session(|session| *session = DiffSession::new());

        let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
            .expect("pbix A should parse");
        let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
            .expect("pbix B should parse");

        let mut out = Vec::<u8>::new();
        let mut sink = JsonLinesSink::new(&mut out);
        pkg_a
            .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
            .expect("pbix streaming should succeed");
        out
    });
}

#[cfg(all(feature = "model-diff", feature = "excel-open-xml"))]
#[test]
fn pbit_streaming_jsonl_is_deterministic_with_fresh_sessions() {
    let path_a = fixture_path("pbit_model_a.pbit");
    let path_b = fixture_path("pbit_model_b.pbit");

    assert_jsonl_determinism_with_fresh_sessions(2, |_session| {
        excel_diff::with_default_session(|session| *session = DiffSession::new());

        let pkg_a = PbixPackage::open(File::open(&path_a).expect("fixture should exist"))
            .expect("pbit A should parse");
        let pkg_b = PbixPackage::open(File::open(&path_b).expect("fixture should exist"))
            .expect("pbit B should parse");

        let mut out = Vec::<u8>::new();
        let mut sink = JsonLinesSink::new(&mut out);
        pkg_a
            .diff_streaming(&pkg_b, &DiffConfig::default(), &mut sink)
            .expect("pbit streaming should succeed");
        out
    });
}

```

---

### File: `core\tests\streaming_sink_tests.rs`

```rust
mod common;

use common::{StructuredOutput, assert_structured_determinism_with_fresh_sessions};
use excel_diff::{
    CallbackSink, CellValue, DiffConfig, DiffOp, DiffSession, Grid, Sheet, SheetKind, VecSink,
    Workbook, try_diff_workbooks_streaming,
};

fn make_test_workbook(session: &mut DiffSession, values: &[f64]) -> Workbook {
    let mut grid = Grid::new(values.len() as u32, 1);
    for (i, &val) in values.iter().enumerate() {
        grid.insert_cell(i as u32, 0, Some(CellValue::Number(val)), None);
    }

    let sheet_name = session.strings.intern("TestSheet");

    Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        ..Default::default()
    }
}

#[test]
fn vec_sink_and_callback_sink_produce_identical_ops() {
    let mut session = DiffSession::new();

    let wb_a = make_test_workbook(&mut session, &[1.0, 2.0, 3.0]);
    let wb_b = make_test_workbook(&mut session, &[1.0, 5.0, 3.0, 4.0]);

    let config = DiffConfig::default();

    let mut vec_sink = VecSink::new();
    let summary_vec =
        try_diff_workbooks_streaming(&wb_a, &wb_b, &mut session.strings, &config, &mut vec_sink)
            .expect("VecSink diff should succeed");
    let vec_ops = vec_sink.into_ops();

    let mut callback_ops: Vec<DiffOp> = Vec::new();
    {
        let mut callback_sink = CallbackSink::new(|op| callback_ops.push(op));
        let summary_callback = try_diff_workbooks_streaming(
            &wb_a,
            &wb_b,
            &mut session.strings,
            &config,
            &mut callback_sink,
        )
        .expect("CallbackSink diff should succeed");

        assert_eq!(
            summary_vec.op_count, summary_callback.op_count,
            "summaries should report same op count"
        );
        assert_eq!(
            summary_vec.complete, summary_callback.complete,
            "summaries should report same complete status"
        );
    }

    assert_eq!(
        vec_ops.len(),
        callback_ops.len(),
        "both sinks should collect same number of ops"
    );

    for (i, (vec_op, cb_op)) in vec_ops.iter().zip(callback_ops.iter()).enumerate() {
        assert_eq!(
            vec_op, cb_op,
            "op at index {} should be identical between VecSink and CallbackSink",
            i
        );
    }

    assert!(
        !vec_ops.is_empty(),
        "expected at least one diff op for the test workbooks"
    );
}

#[test]
fn streaming_produces_ops_in_consistent_order() {
    let config = DiffConfig::default();
    assert_structured_determinism_with_fresh_sessions(2, |session| {
        let wb_a = make_test_workbook(session, &[1.0, 2.0]);
        let wb_b = make_test_workbook(session, &[3.0, 4.0]);

        let mut sink = VecSink::new();
        let summary =
            try_diff_workbooks_streaming(&wb_a, &wb_b, &mut session.strings, &config, &mut sink)
                .expect("streaming should succeed");
        StructuredOutput {
            ops: sink.into_ops(),
            summary,
        }
    });
}

#[test]
fn streaming_summary_matches_collected_ops() {
    let mut session = DiffSession::new();

    let wb_a = make_test_workbook(&mut session, &[1.0]);
    let wb_b = make_test_workbook(&mut session, &[2.0, 3.0]);

    let config = DiffConfig::default();

    let mut op_count = 0usize;
    let summary = {
        let mut sink = CallbackSink::new(|_op| op_count += 1);
        try_diff_workbooks_streaming(&wb_a, &wb_b, &mut session.strings, &config, &mut sink)
            .expect("streaming should succeed")
    };

    assert_eq!(
        summary.op_count, op_count,
        "summary.op_count should match actual ops emitted"
    );
    assert!(summary.complete, "diff should be complete");
}

```

---

### File: `core\tests\string_pool_tests.rs`

```rust
use excel_diff::StringPool;

#[test]
fn intern_50k_identical_strings_returns_same_id() {
    let mut pool = StringPool::new();
    let first_id = pool.intern("repeated_string");

    for _ in 1..50_000 {
        let id = pool.intern("repeated_string");
        assert_eq!(id, first_id, "interning same string must return same id");
    }

    assert!(
        pool.len() >= 2,
        "pool should have at least 2 entries (empty string + our string)"
    );
    assert!(
        pool.len() <= 3,
        "pool should not grow beyond initial strings"
    );

    assert_eq!(pool.resolve(first_id), "repeated_string");
}

#[test]
fn intern_distinct_strings_returns_different_ids() {
    let mut pool = StringPool::new();

    let id_a = pool.intern("alpha");
    let id_b = pool.intern("beta");
    let id_c = pool.intern("gamma");

    assert_ne!(id_a, id_b);
    assert_ne!(id_b, id_c);
    assert_ne!(id_a, id_c);

    assert_eq!(pool.resolve(id_a), "alpha");
    assert_eq!(pool.resolve(id_b), "beta");
    assert_eq!(pool.resolve(id_c), "gamma");
}

#[test]
fn empty_string_is_pre_interned() {
    let pool = StringPool::new();

    assert!(pool.len() >= 1, "pool should have at least empty string");
    assert_eq!(pool.resolve(excel_diff::StringId(0)), "");
}

#[test]
fn resolve_returns_original_string() {
    let mut pool = StringPool::new();

    let test_cases = vec![
        "hello",
        "world",
        "with spaces",
        "with\nnewline",
        "unicode: ",
        "",
    ];

    for s in &test_cases {
        let id = pool.intern(s);
        assert_eq!(pool.resolve(id), *s);
    }
}

#[test]
fn into_strings_returns_all_interned() {
    let mut pool = StringPool::new();

    pool.intern("first");
    pool.intern("second");
    pool.intern("third");

    let strings = pool.into_strings();

    assert!(strings.contains(&"".to_string()));
    assert!(strings.contains(&"first".to_string()));
    assert!(strings.contains(&"second".to_string()));
    assert!(strings.contains(&"third".to_string()));
    assert_eq!(strings.len(), 4);
}

```

---

### File: `desktop\src-tauri\build.rs`

```rust
fn main() {
    tauri_build::build();
}

```

---

### File: `desktop\src-tauri\Cargo.toml`

```toml
[package]
name = "excel_diff_desktop"
version = "0.1.0"
edition = "2021"
description = "Desktop shell for Excel Diff"
license = "MIT"
build = "build.rs"

[dependencies]
tauri = { version = "2.5.3" }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
excel_diff = { path = "../../core", default-features = false, features = ["excel-open-xml", "vba"] }
ui_payload = { path = "../../ui_payload" }
rfd = "0.14"
rusqlite = { version = "0.31", features = ["bundled"] }
uuid = { version = "1.7", features = ["v4", "serde"] }
time = { version = "0.3", features = ["formatting"] }
thiserror = "1.0"
rust_xlsxwriter = "0.71"
lru = "0.12"
walkdir = "2.5"
globset = "0.4"

[features]
default = ["model-diff"]
model-diff = ["excel_diff/model-diff"]
perf-metrics = ["excel_diff/perf-metrics"]

[build-dependencies]
tauri-build = { version = "2.5.3" }

```

---

### File: `desktop\src-tauri\src\batch.rs`

```rust
use std::collections::{BTreeMap, HashMap};
use std::path::{Path, PathBuf};
use std::sync::atomic::AtomicBool;
use std::sync::Arc;

use globset::{Glob, GlobSet, GlobSetBuilder};
use rusqlite::{params, Connection};
use serde::{Deserialize, Serialize};
use tauri::AppHandle;
use uuid::Uuid;
use walkdir::WalkDir;

use crate::diff_runner::{DiffErrorPayload, DiffRequest, DiffRunner};
use ui_payload::DiffOptions;
use crate::store::{OpStore, StoreError};

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct BatchOutcome {
    pub batch_id: String,
    pub status: String,
    pub item_count: usize,
    pub completed_count: usize,
    pub items: Vec<BatchItemResult>,
}

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct BatchItemResult {
    pub item_id: usize,
    pub old_path: Option<String>,
    pub new_path: Option<String>,
    pub status: String,
    pub diff_id: Option<String>,
    pub op_count: Option<u64>,
    pub warnings_count: Option<usize>,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct BatchRequest {
    pub old_root: String,
    pub new_root: String,
    pub strategy: String,
    pub include_globs: Option<Vec<String>>,
    pub exclude_globs: Option<Vec<String>>,
    pub trusted: bool,
}

pub fn run_batch_compare(
    app: AppHandle,
    runner: DiffRunner,
    store_path: &Path,
    request: BatchRequest,
) -> Result<BatchOutcome, DiffErrorPayload> {
    let old_root = PathBuf::from(&request.old_root);
    let new_root = PathBuf::from(&request.new_root);

    let include = build_globset(&request.include_globs)?;
    let exclude = build_globset(&request.exclude_globs)?;
    let include_all = request
        .include_globs
        .as_ref()
        .map(|list| list.is_empty())
        .unwrap_or(true);

    let old_files = collect_files(&old_root, &include, &exclude, include_all)?;
    let new_files = collect_files(&new_root, &include, &exclude, include_all)?;

    let strategy = request.strategy.to_lowercase();
    let batch_id = Uuid::new_v4().to_string();

    let mut pairs = pair_files(&old_root, &new_root, &old_files, &new_files, &strategy);
    pairs.sort_by(|a, b| a.key.cmp(&b.key));

    let store = OpStore::open(store_path).map_err(store_error)?;
    let conn = store.connection();
    insert_batch_run(conn, &batch_id, &request, pairs.len())?;

    let mut items = Vec::new();
    let mut completed = 0;

    for (idx, pair) in pairs.iter().enumerate() {
        let item_id = idx;
        let mut result = BatchItemResult {
            item_id,
            old_path: pair.old.as_ref().map(|p| p.display().to_string()),
            new_path: pair.new.as_ref().map(|p| p.display().to_string()),
            status: pair.status.clone(),
            diff_id: None,
            op_count: None,
            warnings_count: None,
            error: pair.error.clone(),
        };

        insert_batch_item(conn, &batch_id, &result)?;

        if pair.status != "pending" {
            items.push(result);
            continue;
        }

        let cancel = Arc::new(AtomicBool::new(false));
        let diff_request = DiffRequest {
            old_path: pair.old.as_ref().unwrap().display().to_string(),
            new_path: pair.new.as_ref().unwrap().display().to_string(),
            run_id: 0,
            options: DiffOptions {
                trusted: Some(request.trusted),
                ..DiffOptions::default()
            },
            cancel,
            app: app.clone(),
        };

        match runner.diff(diff_request) {
            Ok(outcome) => {
                result.status = "complete".to_string();
                result.diff_id = Some(outcome.diff_id.clone());
                if let Some(summary) = outcome.summary {
                    result.op_count = Some(summary.op_count);
                    result.warnings_count = Some(summary.warnings.len());
                }
                update_batch_item(conn, &batch_id, &result)?;
            }
            Err(err) => {
                result.status = "failed".to_string();
                result.error = Some(err.message);
                update_batch_item(conn, &batch_id, &result)?;
            }
        }

        completed += 1;
        update_batch_progress(conn, &batch_id, completed)?;
        items.push(result);
    }

    let status = "complete".to_string();
    finish_batch_run(conn, &batch_id, &status, completed)?;

    Ok(BatchOutcome {
        batch_id,
        status,
        item_count: pairs.len(),
        completed_count: completed,
        items,
    })
}

pub fn load_batch_summary(store_path: &Path, batch_id: &str) -> Result<BatchOutcome, DiffErrorPayload> {
    let store = OpStore::open(store_path).map_err(store_error)?;
    let conn = store.connection();

    let (status, item_count, completed_count) = conn
        .query_row(
            "SELECT status, item_count, completed_count FROM batch_runs WHERE batch_id = ?1",
            params![batch_id],
            |row| Ok((row.get::<_, String>(0)?, row.get::<_, i64>(1)?, row.get::<_, i64>(2)?)),
        )
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    let mut stmt = conn
        .prepare(
            "SELECT item_id, old_path, new_path, status, diff_id, op_count, warnings_count, error FROM batch_items WHERE batch_id = ?1 ORDER BY item_id",
        )
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    let rows = stmt
        .query_map(params![batch_id], |row| {
            Ok(BatchItemResult {
                item_id: row.get::<_, i64>(0)? as usize,
                old_path: row.get::<_, Option<String>>(1)?,
                new_path: row.get::<_, Option<String>>(2)?,
                status: row.get::<_, String>(3)?,
                diff_id: row.get::<_, Option<String>>(4)?,
                op_count: row.get::<_, Option<i64>>(5)?.map(|v| v as u64),
                warnings_count: row.get::<_, Option<i64>>(6)?.map(|v| v as usize),
                error: row.get::<_, Option<String>>(7)?,
            })
        })
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    let mut items = Vec::new();
    for row in rows {
        items.push(row.map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?);
    }

    Ok(BatchOutcome {
        batch_id: batch_id.to_string(),
        status,
        item_count: item_count as usize,
        completed_count: completed_count as usize,
        items,
    })
}

struct PairCandidate {
    key: String,
    old: Option<PathBuf>,
    new: Option<PathBuf>,
    status: String,
    error: Option<String>,
}

fn collect_files(
    root: &Path,
    include: &GlobSet,
    exclude: &GlobSet,
    include_all: bool,
) -> Result<Vec<PathBuf>, DiffErrorPayload> {
    let mut files = Vec::new();
    for entry in WalkDir::new(root).into_iter().filter_map(Result::ok) {
        if !entry.file_type().is_file() {
            continue;
        }
        let path = entry.path();
        if !is_supported(path) {
            continue;
        }
        if include_all || include.is_match(path) {
            if !exclude.is_match(path) {
                files.push(path.to_path_buf());
            }
        }
    }
    Ok(files)
}

fn build_globset(globs: &Option<Vec<String>>) -> Result<GlobSet, DiffErrorPayload> {
    let mut builder = GlobSetBuilder::new();
    if let Some(globs) = globs {
        for glob in globs {
            let parsed = Glob::new(glob).map_err(|e| DiffErrorPayload::new("glob", e.to_string(), false))?;
            builder.add(parsed);
        }
    }
    builder.build().map_err(|e| DiffErrorPayload::new("glob", e.to_string(), false))
}

fn pair_files(
    old_root: &Path,
    new_root: &Path,
    old_files: &[PathBuf],
    new_files: &[PathBuf],
    strategy: &str,
) -> Vec<PairCandidate> {
    let mut old_map = group_files(old_root, old_files, strategy);
    let mut new_map = group_files(new_root, new_files, strategy);
    let mut keys = BTreeMap::new();

    for key in old_map.keys() {
        keys.insert(key.clone(), ());
    }
    for key in new_map.keys() {
        keys.insert(key.clone(), ());
    }

    let mut pairs = Vec::new();
    for key in keys.keys() {
        let old_list = old_map.remove(key).unwrap_or_default();
        let new_list = new_map.remove(key).unwrap_or_default();

        match (old_list.len(), new_list.len()) {
            (1, 1) => pairs.push(PairCandidate {
                key: key.clone(),
                old: Some(old_list[0].clone()),
                new: Some(new_list[0].clone()),
                status: "pending".to_string(),
                error: None,
            }),
            (0, 1) => pairs.push(PairCandidate {
                key: key.clone(),
                old: None,
                new: Some(new_list[0].clone()),
                status: "missing_old".to_string(),
                error: Some("Missing old file".to_string()),
            }),
            (1, 0) => pairs.push(PairCandidate {
                key: key.clone(),
                old: Some(old_list[0].clone()),
                new: None,
                status: "missing_new".to_string(),
                error: Some("Missing new file".to_string()),
            }),
            _ => pairs.push(PairCandidate {
                key: key.clone(),
                old: old_list.get(0).cloned(),
                new: new_list.get(0).cloned(),
                status: "duplicate".to_string(),
                error: Some("Duplicate match".to_string()),
            }),
        }
    }

    pairs
}

fn group_files(root: &Path, files: &[PathBuf], strategy: &str) -> HashMap<String, Vec<PathBuf>> {
    let mut map: HashMap<String, Vec<PathBuf>> = HashMap::new();
    for path in files {
        let key = match strategy {
            "filename" => path.file_name().and_then(|s| s.to_str()).unwrap_or_default().to_lowercase(),
            _ => path
                .strip_prefix(root)
                .unwrap_or(path)
                .to_string_lossy()
                .replace('\\', "/")
                .to_lowercase(),
        };
        map.entry(key).or_default().push(path.clone());
    }
    map
}

fn is_supported(path: &Path) -> bool {
    let ext = path.extension().and_then(|s| s.to_str()).unwrap_or("").to_lowercase();
    matches!(ext.as_str(), "xlsx" | "xlsm" | "xltx" | "xltm" | "xlsb" | "pbix" | "pbit")
}

fn insert_batch_run(conn: &Connection, batch_id: &str, req: &BatchRequest, item_count: usize) -> Result<(), DiffErrorPayload> {
    conn.execute(
        "INSERT INTO batch_runs (batch_id, old_root, new_root, strategy, started_at, status, item_count, completed_count)\
         VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)",
        params![
            batch_id,
            req.old_root,
            req.new_root,
            req.strategy,
            now_iso(),
            "running",
            item_count as i64,
            0i64,
        ],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    Ok(())
}

fn insert_batch_item(conn: &Connection, batch_id: &str, item: &BatchItemResult) -> Result<(), DiffErrorPayload> {
    conn.execute(
        "INSERT INTO batch_items (batch_id, item_id, old_path, new_path, status, diff_id, op_count, warnings_count, error)\
         VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9)",
        params![
            batch_id,
            item.item_id as i64,
            item.old_path,
            item.new_path,
            item.status,
            item.diff_id,
            item.op_count.map(|v| v as i64),
            item.warnings_count.map(|v| v as i64),
            item.error,
        ],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    Ok(())
}

fn update_batch_item(conn: &Connection, batch_id: &str, item: &BatchItemResult) -> Result<(), DiffErrorPayload> {
    conn.execute(
        "UPDATE batch_items SET status = ?1, diff_id = ?2, op_count = ?3, warnings_count = ?4, error = ?5 WHERE batch_id = ?6 AND item_id = ?7",
        params![
            item.status,
            item.diff_id,
            item.op_count.map(|v| v as i64),
            item.warnings_count.map(|v| v as i64),
            item.error,
            batch_id,
            item.item_id as i64,
        ],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    Ok(())
}

fn update_batch_progress(conn: &Connection, batch_id: &str, completed: usize) -> Result<(), DiffErrorPayload> {
    conn.execute(
        "UPDATE batch_runs SET completed_count = ?1 WHERE batch_id = ?2",
        params![completed as i64, batch_id],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    Ok(())
}

fn finish_batch_run(conn: &Connection, batch_id: &str, status: &str, completed: usize) -> Result<(), DiffErrorPayload> {
    conn.execute(
        "UPDATE batch_runs SET status = ?1, completed_count = ?2, finished_at = ?3 WHERE batch_id = ?4",
        params![status, completed as i64, now_iso(), batch_id],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    Ok(())
}

fn now_iso() -> String {
    time::OffsetDateTime::now_utc()
        .format(&time::format_description::well_known::Rfc3339)
        .unwrap_or_else(|_| "".to_string())
}

fn store_error(err: StoreError) -> DiffErrorPayload {
    DiffErrorPayload::new("store", err.to_string(), false)
}

```

---

### File: `desktop\src-tauri\src\diff_runner.rs`

```rust
use std::collections::hash_map::Entry;
use std::collections::HashMap;
use std::fs::File;
use std::num::NonZeroUsize;
use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::mpsc::{self, Sender};
use std::sync::Arc;
use std::thread;

use excel_diff::{
    should_use_large_mode, ContainerError, ContainerLimits, DiffConfig, DiffError, DiffReport,
    DiffSink, DiffSummary, PbixPackage, ProgressCallback, WorkbookPackage,
};
use lru::LruCache;
use serde::Serialize;
use tauri::{AppHandle, Emitter};

use crate::export::export_audit_xlsx_from_store;
use crate::store::{
    resolve_sheet_stats, DiffMode, DiffRunSummary, OpStore, OpStoreSink, RunStatus, SheetStats,
    StoreError,
};
use ui_payload::{build_payload_from_pbix_report, limits_from_config, DiffOptions, DiffOutcomeConfig, DiffPreset};
const WORKBOOK_CACHE_CAPACITY: usize = 4;
const PBIX_CACHE_CAPACITY: usize = 2;

#[derive(Debug, Clone)]
pub struct DiffRequest {
    pub old_path: String,
    pub new_path: String,
    pub run_id: u64,
    pub options: DiffOptions,
    pub cancel: Arc<AtomicBool>,
    pub app: AppHandle,
}

#[derive(Debug, Clone)]
pub struct SheetPayloadRequest {
    pub diff_id: String,
    pub sheet_name: String,
    pub cancel: Arc<AtomicBool>,
    pub app: AppHandle,
}

#[derive(Serialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffOutcome {
    pub diff_id: String,
    pub mode: DiffMode,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub payload: Option<ui_payload::DiffWithSheets>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub summary: Option<DiffRunSummary>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub config: Option<DiffOutcomeConfig>,
}

#[derive(Debug, Serialize, Clone)]
#[serde(rename_all = "camelCase")]
pub struct DiffErrorPayload {
    pub code: String,
    pub message: String,
    #[serde(default)]
    pub trusted_retry: bool,
}

impl DiffErrorPayload {
    pub(crate) fn new(code: impl Into<String>, message: impl Into<String>, trusted_retry: bool) -> Self {
        Self {
            code: code.into(),
            message: message.into(),
            trusted_retry,
        }
    }
}

#[derive(Debug, Clone)]
struct CacheKey {
    path: String,
    mtime: u64,
    size: u64,
    trusted: bool,
}

impl PartialEq for CacheKey {
    fn eq(&self, other: &Self) -> bool {
        self.path == other.path
            && self.mtime == other.mtime
            && self.size == other.size
            && self.trusted == other.trusted
    }
}

impl Eq for CacheKey {}

impl std::hash::Hash for CacheKey {
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        self.path.hash(state);
        self.mtime.hash(state);
        self.size.hash(state);
        self.trusted.hash(state);
    }
}

#[derive(Clone)]
pub struct DiffRunner {
    tx: Sender<EngineCommand>,
}

impl DiffRunner {
    pub fn new(store_path: PathBuf, app_version: String, engine_version: String) -> Self {
        let (tx, rx) = mpsc::channel();
        let engine = EngineState::new(store_path, app_version, engine_version, rx);
        thread::spawn(move || engine.run());
        Self { tx }
    }

    pub fn diff(&self, request: DiffRequest) -> Result<DiffOutcome, DiffErrorPayload> {
        let (reply_tx, reply_rx) = mpsc::channel();
        self.tx
            .send(EngineCommand::Diff {
                request,
                respond_to: reply_tx,
            })
            .map_err(|e| {
                DiffErrorPayload::new("engine_down", e.to_string(), false)
            })?;
        reply_rx.recv().map_err(|e| {
            DiffErrorPayload::new("engine_down", e.to_string(), false)
        })?
    }

    pub fn load_sheet_payload(
        &self,
        request: SheetPayloadRequest,
    ) -> Result<ui_payload::DiffWithSheets, DiffErrorPayload> {
        let (reply_tx, reply_rx) = mpsc::channel();
        self.tx
            .send(EngineCommand::LoadSheet {
                request,
                respond_to: reply_tx,
            })
            .map_err(|e| DiffErrorPayload::new("engine_down", e.to_string(), false))?;
        reply_rx.recv().map_err(|e| {
            DiffErrorPayload::new("engine_down", e.to_string(), false)
        })?
    }
}

enum EngineCommand {
    Diff {
        request: DiffRequest,
        respond_to: Sender<Result<DiffOutcome, DiffErrorPayload>>,
    },
    LoadSheet {
        request: SheetPayloadRequest,
        respond_to: Sender<Result<ui_payload::DiffWithSheets, DiffErrorPayload>>,
    },
}

struct EngineState {
    store_path: PathBuf,
    app_version: String,
    engine_version: String,
    workbook_cache: LruCache<CacheKey, WorkbookPackage>,
    pbix_cache: LruCache<CacheKey, PbixPackage>,
    rx: mpsc::Receiver<EngineCommand>,
}

impl EngineState {
    fn new(
        store_path: PathBuf,
        app_version: String,
        engine_version: String,
        rx: mpsc::Receiver<EngineCommand>,
    ) -> Self {
        Self {
            store_path,
            app_version,
            engine_version,
            workbook_cache: LruCache::new(NonZeroUsize::new(WORKBOOK_CACHE_CAPACITY).unwrap()),
            pbix_cache: LruCache::new(NonZeroUsize::new(PBIX_CACHE_CAPACITY).unwrap()),
            rx,
        }
    }

    fn run(mut self) {
        while let Ok(cmd) = self.rx.recv() {
            match cmd {
                EngineCommand::Diff { request, respond_to } => {
                    let result = self.handle_diff(request);
                    let _ = respond_to.send(result);
                }
                EngineCommand::LoadSheet { request, respond_to } => {
                    let result = self.handle_load_sheet(request);
                    let _ = respond_to.send(result);
                }
            }
        }
    }

    fn handle_diff(&mut self, request: DiffRequest) -> Result<DiffOutcome, DiffErrorPayload> {
        emit_progress(&request.app, request.run_id, "read", "Reading files...");

        let old_path = PathBuf::from(&request.old_path);
        let new_path = PathBuf::from(&request.new_path);

        let old_kind = ui_payload::host_kind_from_path(&old_path)
            .ok_or_else(|| DiffErrorPayload::new("unsupported", "Unsupported old file extension", false))?;
        let new_kind = ui_payload::host_kind_from_path(&new_path)
            .ok_or_else(|| DiffErrorPayload::new("unsupported", "Unsupported new file extension", false))?;

        if old_kind != new_kind {
            return Err(DiffErrorPayload::new(
                "mismatch",
                "Old/new files must be the same type",
                false,
            ));
        }

        if request.cancel.load(Ordering::Relaxed) {
            return Err(DiffErrorPayload::new("canceled", "Diff canceled.", false));
        }

        let store = OpStore::open(&self.store_path).map_err(map_store_error)?;
        let options = request.options.clone();
        let trusted = options.trusted.unwrap_or(false);
        let config = options
            .effective_config(DiffConfig::balanced())
            .map_err(|e| DiffErrorPayload::new("config", e, false))?;
        let config_json = serde_json::to_string(&config).unwrap_or_else(|_| "{}".to_string());
        let outcome_config = outcome_config_from_options(&options, &config);

        match old_kind {
            ui_payload::HostKind::Workbook => {
                let old_pkg = self.open_workbook_cached(&old_path, trusted)?;
                let new_pkg = self.open_workbook_cached(&new_path, trusted)?;

                let estimated_cells = estimate_diff_cell_volume(&old_pkg.workbook, &new_pkg.workbook);
                let mode = if should_use_large_mode(estimated_cells, &config) {
                    DiffMode::Large
                } else {
                    DiffMode::Payload
                };

                let diff_id = store
                    .start_run(
                        &request.old_path,
                        &request.new_path,
                        &config_json,
                        &self.engine_version,
                        &self.app_version,
                        mode,
                        trusted,
                    )
                    .map_err(map_store_error)?;

                match mode {
                    DiffMode::Payload => {
                        emit_progress(&request.app, request.run_id, "diff", "Diffing workbooks...");
                        let progress = EngineProgress::new(request.app.clone(), request.run_id, request.cancel.clone());
                        let report = match run_diff_with_progress(
                            || old_pkg.diff_with_progress(&new_pkg, &config, &progress),
                            &request.cancel,
                        ) {
                            Ok(report) => report,
                            Err(err) => {
                                let _ = store.fail_run(&diff_id, status_for_error(&err), &err.message);
                                return Err(err);
                            }
                        };

                        emit_progress(&request.app, request.run_id, "snapshot", "Building previews...");
                        let (counts, sheet_stats) = store
                            .insert_ops_from_report(&diff_id, &report)
                            .map_err(map_store_error)?;
                        let resolved = resolve_sheet_stats(&report.strings, &sheet_stats).map_err(map_store_error)?;
                        let summary = report_to_summary(&report);
                        store
                            .finish_run(&diff_id, &summary, &report.strings, &counts, &resolved, RunStatus::Complete)
                            .map_err(map_store_error)?;

                        let payload = ui_payload::build_payload_from_workbook_report(report, &old_pkg, &new_pkg);
                        let summary_record = store.load_summary(&diff_id).map_err(map_store_error)?;
                        Ok(DiffOutcome {
                            diff_id,
                            mode,
                            payload: Some(payload),
                            summary: Some(summary_record),
                            config: Some(outcome_config.clone()),
                        })
                    }
                    DiffMode::Large => {
                        emit_progress(&request.app, request.run_id, "diff", "Streaming diff to disk...");
                        let progress = EngineProgress::new(request.app.clone(), request.run_id, request.cancel.clone());
                        let sink_store = OpStore::open(&self.store_path).map_err(map_store_error)?;
                        let conn = sink_store.into_connection();
                        let mut sink = OpStoreSink::new(conn, diff_id.clone())
                            .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

                        let summary = match run_diff_with_progress(
                            || old_pkg.diff_streaming_with_progress(&new_pkg, &config, &mut sink, &progress),
                            &request.cancel,
                        ) {
                            Ok(result) => result.map_err(diff_error_from_diff),
                            Err(err) => Err(err),
                        };

                        let summary = match summary {
                            Ok(summary) => summary,
                            Err(err) => {
                                let _ = sink.finish();
                                let _ = store.fail_run(&diff_id, status_for_error(&err), &err.message);
                                return Err(err);
                            }
                        };

                        sink.finish().map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
                        let (_, counts, stats, _) = sink.into_parts();
                        let strings = current_strings();
                        let mut stats: Vec<SheetStats> = stats.into_values().collect();
                        stats.sort_by_key(|entry| entry.sheet_id);
                        let resolved = resolve_sheet_stats(&strings, &stats).map_err(map_store_error)?;
                        store
                            .finish_run(&diff_id, &summary, &strings, &counts, &resolved, RunStatus::Complete)
                            .map_err(map_store_error)?;

                        let summary_record = store.load_summary(&diff_id).map_err(map_store_error)?;
                        Ok(DiffOutcome {
                            diff_id,
                            mode,
                            payload: None,
                            summary: Some(summary_record),
                            config: Some(outcome_config.clone()),
                        })
                    }
                }
            }
            ui_payload::HostKind::Pbix => {
                let old_pkg = self.open_pbix_cached(&old_path, trusted)?;
                let new_pkg = self.open_pbix_cached(&new_path, trusted)?;

                let diff_id = store
                    .start_run(
                        &request.old_path,
                        &request.new_path,
                        &config_json,
                        &self.engine_version,
                        &self.app_version,
                        DiffMode::Payload,
                        trusted,
                    )
                    .map_err(map_store_error)?;

                emit_progress(&request.app, request.run_id, "diff", "Streaming PBIX diff to disk...");
                let progress = EngineProgress::new(request.app.clone(), request.run_id, request.cancel.clone());
                let sink_store = OpStore::open(&self.store_path).map_err(map_store_error)?;
                let conn = sink_store.into_connection();
                let mut sink = OpStoreSink::new(conn, diff_id.clone())
                    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

                let summary = match run_diff_with_progress(
                    || old_pkg.diff_streaming_with_progress(&new_pkg, &config, &mut sink, &progress),
                    &request.cancel,
                ) {
                    Ok(result) => result.map_err(diff_error_from_diff),
                    Err(err) => Err(err),
                };

                let summary = match summary {
                    Ok(summary) => summary,
                    Err(err) => {
                        let _ = sink.finish();
                        let _ = store.fail_run(&diff_id, status_for_error(&err), &err.message);
                        return Err(err);
                    }
                };

                sink.finish().map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
                let (_, counts, stats, _) = sink.into_parts();
                let strings = current_strings();
                let mut stats: Vec<SheetStats> = stats.into_values().collect();
                stats.sort_by_key(|entry| entry.sheet_id);
                let resolved = resolve_sheet_stats(&strings, &stats).map_err(map_store_error)?;
                let use_large_mode = should_use_large_mode(summary.op_count as u64, &config);
                if use_large_mode {
                    store.set_mode(&diff_id, DiffMode::Large).map_err(map_store_error)?;
                }
                store
                    .finish_run(&diff_id, &summary, &strings, &counts, &resolved, RunStatus::Complete)
                    .map_err(map_store_error)?;

                let summary_record = store.load_summary(&diff_id).map_err(map_store_error)?;
                if use_large_mode {
                    Ok(DiffOutcome {
                        diff_id,
                        mode: DiffMode::Large,
                        payload: None,
                        summary: Some(summary_record),
                        config: Some(outcome_config.clone()),
                    })
                } else {
                    let report = store.load_report(&diff_id).map_err(map_store_error)?;
                    let payload = build_payload_from_pbix_report(report);
                    Ok(DiffOutcome {
                        diff_id,
                        mode: DiffMode::Payload,
                        payload: Some(payload),
                        summary: Some(summary_record),
                        config: Some(outcome_config.clone()),
                    })
                }
            }
        }
    }

    fn handle_load_sheet(
        &mut self,
        request: SheetPayloadRequest,
    ) -> Result<ui_payload::DiffWithSheets, DiffErrorPayload> {
        let store = OpStore::open(&self.store_path).map_err(map_store_error)?;
        let summary = store.load_summary(&request.diff_id).map_err(map_store_error)?;

        if request.cancel.load(Ordering::Relaxed) {
            return Err(DiffErrorPayload::new("canceled", "Diff canceled.", false));
        }

        let old_path = PathBuf::from(&summary.old_path);
        let new_path = PathBuf::from(&summary.new_path);
        let old_pkg = self.open_workbook_cached(&old_path, summary.trusted)?;
        let new_pkg = self.open_workbook_cached(&new_path, summary.trusted)?;

        let ops = store.load_sheet_ops(&request.diff_id, &request.sheet_name).map_err(map_store_error)?;
        let strings = store.load_strings(&request.diff_id).map_err(map_store_error)?;

        let mut report = DiffReport::new(ops);
        report.strings = strings;
        report.complete = summary.complete;
        report.warnings = summary.warnings.clone();

        emit_progress(&request.app, 0, "snapshot", "Building previews...");
        Ok(ui_payload::build_payload_from_workbook_report(report, &old_pkg, &new_pkg))
    }

    fn open_workbook_cached(&mut self, path: &Path, trusted: bool) -> Result<WorkbookPackage, DiffErrorPayload> {
        let key = cache_key(path, trusted)?;
        if let Some(pkg) = self.workbook_cache.get(&key) {
            return Ok(pkg.clone());
        }
        let file = File::open(path).map_err(|e| DiffErrorPayload::new("io", e.to_string(), false))?;
        let pkg = if trusted {
            WorkbookPackage::open_with_limits(file, trusted_limits())
                .map_err(map_package_error)?
        } else {
            WorkbookPackage::open(file)
                .map_err(map_package_error)?
        };
        self.workbook_cache.put(key, pkg.clone());
        Ok(pkg)
    }

    fn open_pbix_cached(&mut self, path: &Path, trusted: bool) -> Result<PbixPackage, DiffErrorPayload> {
        let key = cache_key(path, trusted)?;
        if let Some(pkg) = self.pbix_cache.get(&key) {
            return Ok(pkg.clone());
        }
        let file = File::open(path).map_err(|e| DiffErrorPayload::new("io", e.to_string(), false))?;
        let pkg = if trusted {
            PbixPackage::open_with_limits(file, trusted_limits())
                .map_err(map_package_error)?
        } else {
            PbixPackage::open(file)
                .map_err(map_package_error)?
        };
        self.pbix_cache.put(key, pkg.clone());
        Ok(pkg)
    }
}

fn report_to_summary(report: &DiffReport) -> DiffSummary {
    DiffSummary {
        complete: report.complete,
        warnings: report.warnings.clone(),
        op_count: report.ops.len(),
        #[cfg(feature = "perf-metrics")]
        metrics: report.metrics.clone(),
    }
}

fn outcome_config_from_options(options: &DiffOptions, cfg: &DiffConfig) -> DiffOutcomeConfig {
    let preset = if options.config_json.as_ref().map(|v| v.trim()).unwrap_or("").is_empty() {
        Some(options.preset.unwrap_or(DiffPreset::Balanced))
    } else {
        None
    };
    DiffOutcomeConfig {
        preset,
        limits: Some(limits_from_config(cfg)),
    }
}

fn cache_key(path: &Path, trusted: bool) -> Result<CacheKey, DiffErrorPayload> {
    let canonical = std::fs::canonicalize(path).unwrap_or_else(|_| path.to_path_buf());
    let normalized = canonical.to_string_lossy().to_lowercase();
    let meta = std::fs::metadata(&canonical).map_err(|e| DiffErrorPayload::new("io", e.to_string(), false))?;
    let size = meta.len();
    let mtime = meta.modified().ok()
        .and_then(|time| time.duration_since(std::time::UNIX_EPOCH).ok())
        .map(|dur| dur.as_secs())
        .unwrap_or(0);
    Ok(CacheKey {
        path: normalized,
        mtime,
        size,
        trusted,
    })
}

fn trusted_limits() -> ContainerLimits {
    let base = ContainerLimits::default();
    ContainerLimits {
        max_entries: base.max_entries.saturating_mul(4),
        max_part_uncompressed_bytes: base.max_part_uncompressed_bytes.saturating_mul(4),
        max_total_uncompressed_bytes: base.max_total_uncompressed_bytes.saturating_mul(4),
    }
}

fn estimate_diff_cell_volume(old: &excel_diff::Workbook, new: &excel_diff::Workbook) -> u64 {
    excel_diff::with_default_session(|session| {
        let mut max_counts: HashMap<(String, excel_diff::SheetKind), u64> = HashMap::new();
        for sheet in old.sheets.iter().chain(new.sheets.iter()) {
            let name_lower = session.strings.resolve(sheet.name).to_lowercase();
            let key = (name_lower, sheet.kind.clone());
            let cell_count = sheet.grid.cell_count() as u64;
            match max_counts.entry(key) {
                Entry::Occupied(mut entry) => {
                    if cell_count > *entry.get() {
                        entry.insert(cell_count);
                    }
                }
                Entry::Vacant(entry) => {
                    entry.insert(cell_count);
                }
            }
        }
        max_counts.values().copied().sum()
    })
}

fn current_strings() -> Vec<String> {
    excel_diff::with_default_session(|session| session.strings.strings().to_vec())
}

fn map_store_error(err: StoreError) -> DiffErrorPayload {
    DiffErrorPayload::new("store", err.to_string(), false)
}

fn map_package_error(err: excel_diff::PackageError) -> DiffErrorPayload {
    let trusted_retry = matches!(
        err,
        excel_diff::PackageError::Container(ContainerError::TooManyEntries { .. })
            | excel_diff::PackageError::Container(ContainerError::PartTooLarge { .. })
            | excel_diff::PackageError::Container(ContainerError::TotalTooLarge { .. })
    );
    DiffErrorPayload::new(err.code(), err.to_string(), trusted_retry)
}

fn run_diff_with_progress<F, T>(f: F, cancel: &AtomicBool) -> Result<T, DiffErrorPayload>
where
    F: FnOnce() -> T,
{
    match std::panic::catch_unwind(std::panic::AssertUnwindSafe(f)) {
        Ok(result) => Ok(result),
        Err(_) => {
            if cancel.load(Ordering::Relaxed) {
                Err(DiffErrorPayload::new("canceled", "Diff canceled.", false))
            } else {
                Err(DiffErrorPayload::new("failed", "Diff failed unexpectedly.", false))
            }
        }
    }
}

struct EngineProgress {
    app: AppHandle,
    run_id: u64,
    cancel: Arc<AtomicBool>,
    last_phase: std::sync::Mutex<Option<String>>,
}

impl EngineProgress {
    fn new(app: AppHandle, run_id: u64, cancel: Arc<AtomicBool>) -> Self {
        Self {
            app,
            run_id,
            cancel,
            last_phase: std::sync::Mutex::new(None),
        }
    }

    fn map_detail(phase: &str) -> &'static str {
        match phase {
            "parse" => "Parsing workbooks...",
            "alignment" => "Aligning rows and columns...",
            "cell_diff" => "Diffing cells...",
            "move_detection" => "Detecting moved blocks...",
            "m_diff" => "Diffing Power Query...",
            _ => "Diffing workbooks...",
        }
    }

    fn should_emit(&self, phase: &str) -> bool {
        let mut last = self.last_phase.lock().unwrap_or_else(|e| e.into_inner());
        if last.as_deref() == Some(phase) {
            return false;
        }
        *last = Some(phase.to_string());
        true
    }
}

impl ProgressCallback for EngineProgress {
    fn on_progress(&self, phase: &str, _percent: f32) {
        if self.cancel.load(Ordering::Relaxed) {
            panic!("diff canceled");
        }
        if self.should_emit(phase) {
            emit_progress(&self.app, self.run_id, "diff", Self::map_detail(phase));
        }
    }
}

fn emit_progress(app: &AppHandle, run_id: u64, stage: &str, detail: &str) {
    let _ = app.emit(
        "diff-progress",
        ProgressEvent {
            run_id,
            stage: stage.to_string(),
            detail: detail.to_string(),
        },
    );
}

#[derive(Serialize, Clone)]
#[serde(rename_all = "camelCase")]
struct ProgressEvent {
    run_id: u64,
    stage: String,
    detail: String,
}

pub fn export_audit_xlsx(diff_id: &str, store_path: &Path, output_path: &Path) -> Result<(), DiffErrorPayload> {
    let store = OpStore::open(store_path).map_err(map_store_error)?;
    export_audit_xlsx_from_store(&store, diff_id, output_path).map_err(|e| {
        DiffErrorPayload::new("export", e.to_string(), false)
    })
}

pub fn diff_error_from_diff(diff_error: DiffError) -> DiffErrorPayload {
    DiffErrorPayload::new(diff_error.code(), diff_error.to_string(), false)
}

fn status_for_error(err: &DiffErrorPayload) -> RunStatus {
    if err.code == "canceled" {
        RunStatus::Canceled
    } else {
        RunStatus::Failed
    }
}

#[cfg(test)]
mod tests {
    use super::estimate_diff_cell_volume;
    use crate::store::DiffMode;
    use excel_diff::{
        CellValue, DiffConfig, Grid, Sheet, SheetKind, Workbook, AUTO_STREAM_CELL_THRESHOLD,
        should_use_large_mode, with_default_session,
    };

    fn create_dense_grid(nrows: u32, ncols: u32) -> Grid {
        let mut grid = Grid::new(nrows, ncols);
        for row in 0..nrows {
            for col in 0..ncols {
                let value = row as f64 * 1000.0 + col as f64;
                grid.insert_cell(row, col, Some(CellValue::Number(value)), None);
            }
        }
        grid
    }

    fn build_workbook(grid: Grid) -> Workbook {
        let name_id = with_default_session(|session| session.strings.intern("Sheet1"));
        Workbook {
            sheets: vec![Sheet {
                name: name_id,
                workbook_sheet_id: None,
                kind: SheetKind::Worksheet,
                grid,
            }],
            named_ranges: Vec::new(),
            charts: Vec::new(),
        }
    }

    #[test]
    fn large_mode_threshold_triggers_in_desktop() {
        let grid = create_dense_grid(1000, 1001);
        let old = build_workbook(grid.clone());
        let new = build_workbook(grid);
        let estimated = estimate_diff_cell_volume(&old, &new);
        assert!(estimated >= AUTO_STREAM_CELL_THRESHOLD);

        let config = DiffConfig::balanced();
        let mode = if should_use_large_mode(estimated, &config) {
            DiffMode::Large
        } else {
            DiffMode::Payload
        };
        assert_eq!(mode, DiffMode::Large);
    }
}

```

---

### File: `desktop\src-tauri\src\export\audit_xlsx.rs`

```rust
use std::path::Path;

use excel_diff::{CellAddress, CellValue, DiffOp, ExpressionChangeKind, QueryChangeKind};
#[cfg(feature = "model-diff")]
use excel_diff::{ModelColumnProperty, RelationshipProperty};
use rust_xlsxwriter::{Format, Workbook, XlsxError};
use thiserror::Error;

use crate::store::{DiffRunSummary, OpStore, StoreError};

#[derive(Debug, Error)]
pub enum ExportError {
    #[error("Store error: {0}")]
    Store(#[from] StoreError),
    #[error("XLSX error: {0}")]
    Xlsx(#[from] XlsxError),
}

pub fn export_audit_xlsx_from_store(
    store: &OpStore,
    diff_id: &str,
    path: &Path,
) -> Result<(), ExportError> {
    let summary = store.load_summary(diff_id)?;
    let strings = store.load_strings(diff_id)?;

    let mut workbook = Workbook::new();
    let header_format = Format::new().set_bold();

    {
        let summary_sheet = workbook.add_worksheet();
        summary_sheet.set_name("Summary")?;
        write_summary_sheet(summary_sheet, &summary, &header_format);
    }

    {
        let warnings_sheet = workbook.add_worksheet();
        warnings_sheet.set_name("Warnings")?;
        write_warnings_sheet(warnings_sheet, &summary, &header_format);
    }

    {
        let cells_sheet = workbook.add_worksheet();
        cells_sheet.set_name("Cells")?;
        write_cells_header(cells_sheet, &header_format);
    }

    {
        let structure_sheet = workbook.add_worksheet();
        structure_sheet.set_name("Structure")?;
        write_structure_header(structure_sheet, &header_format);
    }

    {
        let query_sheet = workbook.add_worksheet();
        query_sheet.set_name("PowerQuery")?;
        write_query_header(query_sheet, &header_format);
    }

    {
        let model_sheet = workbook.add_worksheet();
        model_sheet.set_name("Model")?;
        write_model_header(model_sheet, &header_format);
    }

    {
        let other_sheet = workbook.add_worksheet();
        other_sheet.set_name("OtherOps")?;
        write_other_header(other_sheet, &header_format);
    }

    let mut rows = ExportRows::default();

    store.stream_ops(diff_id, |op| {
        write_op(&op, &strings, &mut workbook, &mut rows)?;
        Ok(())
    })?;

    workbook.save(path)?;
    Ok(())
}

#[derive(Default)]
struct ExportRows {
    cells: u32,
    structure: u32,
    query: u32,
    model: u32,
    other: u32,
}

fn sheet_mut<'a>(
    workbook: &'a mut Workbook,
    name: &str,
) -> Result<&'a mut rust_xlsxwriter::Worksheet, StoreError> {
    workbook
        .worksheet_from_name(name)
        .map_err(|e| StoreError::InvalidData(e.to_string()))
}

fn write_summary_sheet(sheet: &mut rust_xlsxwriter::Worksheet, summary: &DiffRunSummary, header: &Format) {
    let mut row = 0;
    sheet.write_string_with_format(row, 0, "Old path", header).ok();
    sheet.write_string(row, 1, &summary.old_path).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "New path", header).ok();
    sheet.write_string(row, 1, &summary.new_path).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Started", header).ok();
    sheet.write_string(row, 1, &summary.started_at).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Finished", header).ok();
    if let Some(finished) = &summary.finished_at {
        sheet.write_string(row, 1, finished).ok();
    }
    row += 1;
    sheet.write_string_with_format(row, 0, "Mode", header).ok();
    sheet.write_string(row, 1, summary.mode.as_str()).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Status", header).ok();
    sheet.write_string(row, 1, summary.status.as_str()).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Complete", header).ok();
    sheet.write_string(row, 1, if summary.complete { "true" } else { "false" }).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Op count", header).ok();
    sheet.write_number(row, 1, summary.op_count as f64).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Warnings", header).ok();
    sheet.write_number(row, 1, summary.warnings.len() as f64).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "Engine version", header).ok();
    sheet.write_string(row, 1, &summary.engine_version).ok();
    row += 1;
    sheet.write_string_with_format(row, 0, "App version", header).ok();
    sheet.write_string(row, 1, &summary.app_version).ok();
    row += 2;

    sheet.write_string_with_format(row, 0, "Sheet", header).ok();
    sheet.write_string_with_format(row, 1, "Ops", header).ok();
    sheet.write_string_with_format(row, 2, "Added", header).ok();
    sheet.write_string_with_format(row, 3, "Removed", header).ok();
    sheet.write_string_with_format(row, 4, "Modified", header).ok();
    sheet.write_string_with_format(row, 5, "Moved", header).ok();
    row += 1;

    for sheet_summary in &summary.sheets {
        sheet.write_string(row, 0, &sheet_summary.sheet_name).ok();
        sheet.write_number(row, 1, sheet_summary.op_count as f64).ok();
        sheet.write_number(row, 2, sheet_summary.counts.added as f64).ok();
        sheet.write_number(row, 3, sheet_summary.counts.removed as f64).ok();
        sheet.write_number(row, 4, sheet_summary.counts.modified as f64).ok();
        sheet.write_number(row, 5, sheet_summary.counts.moved as f64).ok();
        row += 1;
    }
}

fn write_warnings_sheet(sheet: &mut rust_xlsxwriter::Worksheet, summary: &DiffRunSummary, header: &Format) {
    sheet.write_string_with_format(0, 0, "Warning", header).ok();
    let mut row = 1;
    for warning in &summary.warnings {
        sheet.write_string(row, 0, warning).ok();
        row += 1;
    }
}

fn write_cells_header(sheet: &mut rust_xlsxwriter::Worksheet, header: &Format) {
    let headers = [
        "Sheet",
        "Address",
        "Old value",
        "New value",
        "Old formula",
        "New formula",
        "Classification",
    ];
    for (idx, title) in headers.iter().enumerate() {
        sheet.write_string_with_format(0, idx as u16, *title, header).ok();
    }
}

fn write_structure_header(sheet: &mut rust_xlsxwriter::Worksheet, header: &Format) {
    let headers = ["Kind", "Sheet", "Detail"];
    for (idx, title) in headers.iter().enumerate() {
        sheet.write_string_with_format(0, idx as u16, *title, header).ok();
    }
}

fn write_query_header(sheet: &mut rust_xlsxwriter::Worksheet, header: &Format) {
    let headers = ["Kind", "Name", "Detail"];
    for (idx, title) in headers.iter().enumerate() {
        sheet.write_string_with_format(0, idx as u16, *title, header).ok();
    }
}

fn write_model_header(sheet: &mut rust_xlsxwriter::Worksheet, header: &Format) {
    let headers = ["Kind", "Name", "Detail"];
    for (idx, title) in headers.iter().enumerate() {
        sheet.write_string_with_format(0, idx as u16, *title, header).ok();
    }
}

fn write_other_header(sheet: &mut rust_xlsxwriter::Worksheet, header: &Format) {
    let headers = ["Kind", "Payload"];
    for (idx, title) in headers.iter().enumerate() {
        sheet.write_string_with_format(0, idx as u16, *title, header).ok();
    }
}

fn write_op(
    op: &DiffOp,
    strings: &[String],
    workbook: &mut Workbook,
    rows: &mut ExportRows,
) -> Result<(), StoreError> {
    match op {
        DiffOp::CellEdited { sheet, addr, from, to, .. } => {
            let row = rows.cells + 1;
            rows.cells += 1;
            let sheet_name = resolve_string(strings, *sheet);
            let addr_text = addr.to_a1();
            let old_value = render_cell_value(strings, &from.value);
            let new_value = render_cell_value(strings, &to.value);
            let old_formula = render_formula(strings, from.formula);
            let new_formula = render_formula(strings, to.formula);
            let classification = classify_cell_change(&old_value, &new_value, &old_formula, &new_formula);

            let cells_sheet = sheet_mut(workbook, "Cells")?;
            cells_sheet.write_string(row, 0, sheet_name).ok();
            cells_sheet.write_string(row, 1, &addr_text).ok();
            cells_sheet.write_string(row, 2, &old_value).ok();
            cells_sheet.write_string(row, 3, &new_value).ok();
            cells_sheet.write_string(row, 4, &old_formula).ok();
            cells_sheet.write_string(row, 5, &new_formula).ok();
            cells_sheet.write_string(row, 6, classification).ok();
        }
        DiffOp::RowAdded { sheet, row_idx, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "RowAdded", strings, *sheet, format!("Row {} added", row_idx + 1));
        }
        DiffOp::RowRemoved { sheet, row_idx, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "RowRemoved", strings, *sheet, format!("Row {} removed", row_idx + 1));
        }
        DiffOp::RowReplaced { sheet, row_idx } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "RowReplaced", strings, *sheet, format!("Row {} replaced", row_idx + 1));
        }
        DiffOp::DuplicateKeyCluster { sheet, key, left_rows, right_rows } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            let detail = format!(
                "Duplicate key [{}]: left rows [{}], right rows [{}]",
                format_key_values(strings, key),
                format_row_list(left_rows),
                format_row_list(right_rows)
            );
            write_structure(structure_sheet, rows, "DuplicateKeyCluster", strings, *sheet, detail);
        }
        DiffOp::ColumnAdded { sheet, col_idx, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "ColumnAdded", strings, *sheet, format!("Column {} added", col_idx + 1));
        }
        DiffOp::ColumnRemoved { sheet, col_idx, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "ColumnRemoved", strings, *sheet, format!("Column {} removed", col_idx + 1));
        }
        DiffOp::BlockMovedRows { sheet, src_start_row, row_count, dst_start_row, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "BlockMovedRows", strings, *sheet, format!("Rows {}-{} moved to {}", src_start_row + 1, src_start_row + row_count, dst_start_row + 1));
        }
        DiffOp::BlockMovedColumns { sheet, src_start_col, col_count, dst_start_col, .. } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "BlockMovedColumns", strings, *sheet, format!("Columns {}-{} moved to {}", src_start_col + 1, src_start_col + col_count, dst_start_col + 1));
        }
        DiffOp::BlockMovedRect { sheet, src_start_row, src_row_count, src_start_col, src_col_count, dst_start_row, dst_start_col, .. } => {
            let src_end = CellAddress::from_coords(src_start_row + src_row_count.saturating_sub(1), src_start_col + src_col_count.saturating_sub(1));
            let dst_start = CellAddress::from_coords(*dst_start_row, *dst_start_col);
            let src_start = CellAddress::from_coords(*src_start_row, *src_start_col);
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "BlockMovedRect", strings, *sheet, format!("{}:{} moved to {}", src_start.to_a1(), src_end.to_a1(), dst_start.to_a1()));
        }
        DiffOp::RectReplaced { sheet, start_row, row_count, start_col, col_count } => {
            let start = CellAddress::from_coords(*start_row, *start_col);
            let end = CellAddress::from_coords(start_row + row_count.saturating_sub(1), start_col + col_count.saturating_sub(1));
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "RectReplaced", strings, *sheet, format!("{}:{} replaced", start.to_a1(), end.to_a1()));
        }
        DiffOp::SheetAdded { sheet } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "SheetAdded", strings, *sheet, "Sheet added".to_string());
        }
        DiffOp::SheetRemoved { sheet } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            write_structure(structure_sheet, rows, "SheetRemoved", strings, *sheet, "Sheet removed".to_string());
        }
        DiffOp::SheetRenamed { sheet, from, to } => {
            let structure_sheet = sheet_mut(workbook, "Structure")?;
            let detail = format!(
                "Sheet renamed: {} -> {}",
                resolve_string(strings, *from),
                resolve_string(strings, *to)
            );
            write_structure(structure_sheet, rows, "SheetRenamed", strings, *sheet, detail);
        }
        DiffOp::QueryAdded { name } => {
            let query_sheet = sheet_mut(workbook, "PowerQuery")?;
            write_query(query_sheet, rows, "QueryAdded", resolve_string(strings, *name), "");
        }
        DiffOp::QueryRemoved { name } => {
            let query_sheet = sheet_mut(workbook, "PowerQuery")?;
            write_query(query_sheet, rows, "QueryRemoved", resolve_string(strings, *name), "");
        }
        DiffOp::QueryRenamed { from, to } => {
            let detail = format!("Renamed to {}", resolve_string(strings, *to));
            let query_sheet = sheet_mut(workbook, "PowerQuery")?;
            write_query(query_sheet, rows, "QueryRenamed", resolve_string(strings, *from), &detail);
        }
        DiffOp::QueryDefinitionChanged { name, change_kind, .. } => {
            let detail = match change_kind {
                QueryChangeKind::Semantic => "Semantic change",
                QueryChangeKind::FormattingOnly => "Formatting only",
                QueryChangeKind::Renamed => "Renamed",
            };
            let query_sheet = sheet_mut(workbook, "PowerQuery")?;
            write_query(query_sheet, rows, "QueryDefinitionChanged", resolve_string(strings, *name), detail);
        }
        DiffOp::QueryMetadataChanged { name, field, .. } => {
            let query_sheet = sheet_mut(workbook, "PowerQuery")?;
            write_query(query_sheet, rows, "QueryMetadataChanged", resolve_string(strings, *name), &format!("{field:?}"));
        }
        #[cfg(feature = "model-diff")]
        DiffOp::TableAdded { name } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(model_sheet, rows, "TableAdded", resolve_string(strings, *name), "");
        }
        #[cfg(feature = "model-diff")]
        DiffOp::TableRemoved { name } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(model_sheet, rows, "TableRemoved", resolve_string(strings, *name), "");
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnAdded {
            table,
            name,
            data_type,
        } => {
            let detail = data_type
                .map(|id| format!("type={}", resolve_string(strings, id)))
                .unwrap_or_default();
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "ModelColumnAdded",
                &format_column_ref(strings, *table, *name),
                &detail,
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnRemoved { table, name } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "ModelColumnRemoved",
                &format_column_ref(strings, *table, *name),
                "",
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnTypeChanged {
            table,
            name,
            old_type,
            new_type,
        } => {
            let old_str = old_type
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let new_str = new_type
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let detail = format!("type: {} -> {}", old_str, new_str);
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "ModelColumnTypeChanged",
                &format_column_ref(strings, *table, *name),
                &detail,
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnPropertyChanged {
            table,
            name,
            field,
            old,
            new,
        } => {
            let old_str = old
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let new_str = new
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let detail = format!(
                "{}: {} -> {}",
                column_field_name(*field),
                old_str,
                new_str
            );
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "ModelColumnPropertyChanged",
                &format_column_ref(strings, *table, *name),
                &detail,
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::CalculatedColumnDefinitionChanged {
            table,
            name,
            change_kind,
            ..
        } => {
            let detail = format!("definition changed ({})", expression_change_label(*change_kind));
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "CalculatedColumnDefinitionChanged",
                &format_column_ref(strings, *table, *name),
                &detail,
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipAdded {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "RelationshipAdded",
                &format_relationship_ref(strings, *from_table, *from_column, *to_table, *to_column),
                "",
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipRemoved {
            from_table,
            from_column,
            to_table,
            to_column,
        } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "RelationshipRemoved",
                &format_relationship_ref(strings, *from_table, *from_column, *to_table, *to_column),
                "",
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipPropertyChanged {
            from_table,
            from_column,
            to_table,
            to_column,
            field,
            old,
            new,
        } => {
            let old_str = old
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let new_str = new
                .map(|id| resolve_string(strings, id))
                .unwrap_or("<none>");
            let detail = format!(
                "{}: {} -> {}",
                relationship_field_name(*field),
                old_str,
                new_str
            );
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "RelationshipPropertyChanged",
                &format_relationship_ref(strings, *from_table, *from_column, *to_table, *to_column),
                &detail,
            );
        }
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureAdded { name } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(model_sheet, rows, "MeasureAdded", resolve_string(strings, *name), "");
        }
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureRemoved { name } => {
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(model_sheet, rows, "MeasureRemoved", resolve_string(strings, *name), "");
        }
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureDefinitionChanged { name, change_kind, .. } => {
            let detail = format!("definition changed ({})", expression_change_label(*change_kind));
            let model_sheet = sheet_mut(workbook, "Model")?;
            write_model(
                model_sheet,
                rows,
                "MeasureDefinitionChanged",
                resolve_string(strings, *name),
                &detail,
            );
        }
        _ => {
            let row = rows.other + 1;
            rows.other += 1;
            let payload = serde_json::to_string(op).unwrap_or_else(|_| "<unserializable>".to_string());
            let other_sheet = sheet_mut(workbook, "OtherOps")?;
            other_sheet.write_string(row, 0, op_kind_label(op)).ok();
            other_sheet.write_string(row, 1, &payload).ok();
        }
    }

    Ok(())
}

fn write_structure(
    sheet: &mut rust_xlsxwriter::Worksheet,
    rows: &mut ExportRows,
    kind: &str,
    strings: &[String],
    sheet_id: excel_diff::StringId,
    detail: String,
) {
    let row = rows.structure + 1;
    rows.structure += 1;
    sheet.write_string(row, 0, kind).ok();
    sheet.write_string(row, 1, resolve_string(strings, sheet_id)).ok();
    sheet.write_string(row, 2, &detail).ok();
}

fn write_query(
    sheet: &mut rust_xlsxwriter::Worksheet,
    rows: &mut ExportRows,
    kind: &str,
    name: &str,
    detail: &str,
) {
    let row = rows.query + 1;
    rows.query += 1;
    sheet.write_string(row, 0, kind).ok();
    sheet.write_string(row, 1, name).ok();
    if !detail.is_empty() {
        sheet.write_string(row, 2, detail).ok();
    }
}

fn write_model(
    sheet: &mut rust_xlsxwriter::Worksheet,
    rows: &mut ExportRows,
    kind: &str,
    name: &str,
    detail: &str,
) {
    let row = rows.model + 1;
    rows.model += 1;
    sheet.write_string(row, 0, kind).ok();
    sheet.write_string(row, 1, name).ok();
    if !detail.is_empty() {
        sheet.write_string(row, 2, detail).ok();
    }
}

fn resolve_string(strings: &[String], id: excel_diff::StringId) -> &str {
    strings.get(id.0 as usize).map(String::as_str).unwrap_or("<unknown>")
}

#[cfg(feature = "model-diff")]
fn format_column_ref(
    strings: &[String],
    table: excel_diff::StringId,
    column: excel_diff::StringId,
) -> String {
    format!("{}.{}", resolve_string(strings, table), resolve_string(strings, column))
}

#[cfg(feature = "model-diff")]
fn format_relationship_ref(
    strings: &[String],
    from_table: excel_diff::StringId,
    from_column: excel_diff::StringId,
    to_table: excel_diff::StringId,
    to_column: excel_diff::StringId,
) -> String {
    format!(
        "{}[{}] -> {}[{}]",
        resolve_string(strings, from_table),
        resolve_string(strings, from_column),
        resolve_string(strings, to_table),
        resolve_string(strings, to_column)
    )
}

#[cfg(feature = "model-diff")]
fn column_field_name(field: ModelColumnProperty) -> &'static str {
    match field {
        ModelColumnProperty::Hidden => "hidden",
        ModelColumnProperty::FormatString => "format_string",
        ModelColumnProperty::SortBy => "sort_by",
        ModelColumnProperty::SummarizeBy => "summarize_by",
    }
}

#[cfg(feature = "model-diff")]
fn relationship_field_name(field: RelationshipProperty) -> &'static str {
    match field {
        RelationshipProperty::CrossFilteringBehavior => "cross_filtering_behavior",
        RelationshipProperty::Cardinality => "cardinality",
        RelationshipProperty::IsActive => "is_active",
    }
}

fn expression_change_label(kind: ExpressionChangeKind) -> &'static str {
    match kind {
        ExpressionChangeKind::Semantic => "semantic change",
        ExpressionChangeKind::FormattingOnly => "formatting only",
        ExpressionChangeKind::Unknown => "unknown",
    }
}

fn render_cell_value(strings: &[String], value: &Option<CellValue>) -> String {
    match value {
        None => String::new(),
        Some(CellValue::Blank) => String::new(),
        Some(CellValue::Number(n)) => n.to_string(),
        Some(CellValue::Text(id)) => resolve_string(strings, *id).to_string(),
        Some(CellValue::Bool(b)) => if *b { "TRUE" } else { "FALSE" }.to_string(),
        Some(CellValue::Error(id)) => resolve_string(strings, *id).to_string(),
    }
}

fn render_formula(strings: &[String], formula: Option<excel_diff::StringId>) -> String {
    match formula {
        Some(id) => {
            let raw = resolve_string(strings, id);
            if raw.is_empty() {
                String::new()
            } else if raw.starts_with('=') {
                raw.to_string()
            } else {
                format!("={}", raw)
            }
        }
        None => String::new(),
    }
}

fn classify_cell_change(
    old_value: &str,
    new_value: &str,
    old_formula: &str,
    new_formula: &str,
) -> &'static str {
    let value_changed = old_value != new_value;
    let formula_changed = old_formula != new_formula;

    if value_changed && formula_changed {
        "Value + Formula"
    } else if value_changed {
        if old_value.is_empty() && !new_value.is_empty() {
            "Added value"
        } else if !old_value.is_empty() && new_value.is_empty() {
            "Removed value"
        } else {
            "Value change"
        }
    } else if formula_changed {
        if old_formula.is_empty() && !new_formula.is_empty() {
            "Added formula"
        } else if !old_formula.is_empty() && new_formula.is_empty() {
            "Removed formula"
        } else {
            "Formula change"
        }
    } else {
        "Unchanged"
    }
}

fn op_kind_label(op: &DiffOp) -> &str {
    match op {
        DiffOp::VbaModuleAdded { .. } => "VbaModuleAdded",
        DiffOp::VbaModuleRemoved { .. } => "VbaModuleRemoved",
        DiffOp::VbaModuleChanged { .. } => "VbaModuleChanged",
        DiffOp::NamedRangeAdded { .. } => "NamedRangeAdded",
        DiffOp::NamedRangeRemoved { .. } => "NamedRangeRemoved",
        DiffOp::NamedRangeChanged { .. } => "NamedRangeChanged",
        DiffOp::ChartAdded { .. } => "ChartAdded",
        DiffOp::ChartRemoved { .. } => "ChartRemoved",
        DiffOp::ChartChanged { .. } => "ChartChanged",
        DiffOp::DuplicateKeyCluster { .. } => "DuplicateKeyCluster",
        _ => "Other",
    }
}

fn format_key_values(strings: &[String], key: &[Option<CellValue>]) -> String {
    let parts: Vec<String> = key
        .iter()
        .map(|value| render_cell_value(strings, value))
        .collect();
    parts.join(", ")
}

fn format_row_list(rows: &[u32]) -> String {
    let parts: Vec<String> = rows.iter().map(|row| (row + 1).to_string()).collect();
    parts.join(", ")
}

```

---

### File: `desktop\src-tauri\src\export\mod.rs`

```rust
mod audit_xlsx;

pub use audit_xlsx::export_audit_xlsx_from_store;

```

---

### File: `desktop\src-tauri\src\main.rs`

```rust
#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]

mod diff_runner;
mod export;
mod store;
mod batch;
mod search;

use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, Mutex};

use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Manager, State};
use ui_payload::{DiffOptions, HostCapabilities, HostDefaults};

use crate::diff_runner::{
    DiffErrorPayload, DiffOutcome, DiffRequest, DiffRunner, SheetPayloadRequest,
};
use crate::store::{DiffRunSummary, OpStore, StoreError};
use crate::batch::{BatchOutcome, BatchRequest};
use crate::search::{SearchIndexResult, SearchIndexSummary, SearchResult};

struct ActiveDiff {
    run_id: u64,
    cancel: Arc<AtomicBool>,
}

#[derive(Default)]
struct DiffState {
    current: Mutex<Option<ActiveDiff>>,
}

struct DesktopState {
    runner: DiffRunner,
    store_path: PathBuf,
}

#[derive(Serialize, Deserialize, Clone)]
#[serde(rename_all = "camelCase")]
struct RecentComparison {
    old_path: String,
    new_path: String,
    old_name: String,
    new_name: String,
    last_run_iso: String,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    diff_id: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    mode: Option<String>,
}

fn recents_path(app: &AppHandle) -> Result<PathBuf, String> {
    let dir = app
        .path()
        .app_local_data_dir()
        .map_err(|e| format!("Unable to resolve app data directory: {e}"))?;
    std::fs::create_dir_all(&dir)
        .map_err(|e| format!("Failed to create app data directory: {e}"))?;
    Ok(dir.join("recents.json"))
}

fn store_path(app: &AppHandle) -> Result<PathBuf, String> {
    let dir = app
        .path()
        .app_local_data_dir()
        .map_err(|e| format!("Unable to resolve app data directory: {e}"))?;
    std::fs::create_dir_all(&dir)
        .map_err(|e| format!("Failed to create app data directory: {e}"))?;
    Ok(dir.join("diff_store.sqlite"))
}

fn load_recents_from_disk(path: &Path) -> Vec<RecentComparison> {
    let data = std::fs::read_to_string(path).unwrap_or_default();
    if data.trim().is_empty() {
        return Vec::new();
    }
    serde_json::from_str(&data).unwrap_or_default()
}

fn save_recents_to_disk(path: &Path, entries: &[RecentComparison]) -> Result<(), String> {
    let data = serde_json::to_string_pretty(entries)
        .map_err(|e| format!("Failed to serialize recents: {e}"))?;
    std::fs::write(path, data).map_err(|e| format!("Failed to write recents: {e}"))
}

fn update_recents(mut entries: Vec<RecentComparison>, entry: RecentComparison) -> Vec<RecentComparison> {
    entries.retain(|item| !(item.old_path == entry.old_path && item.new_path == entry.new_path));
    entries.insert(0, entry);
    entries.truncate(20);
    entries
}

#[tauri::command]
fn get_version() -> String {
    env!("CARGO_PKG_VERSION").to_string()
}

#[tauri::command]
fn get_capabilities() -> HostCapabilities {
    HostCapabilities::new(env!("CARGO_PKG_VERSION").to_string()).with_defaults(HostDefaults {
        max_memory_mb: None,
        large_mode_threshold: excel_diff::AUTO_STREAM_CELL_THRESHOLD,
    })
}

#[tauri::command]
fn load_recents(app: AppHandle) -> Result<Vec<RecentComparison>, String> {
    let path = recents_path(&app)?;
    Ok(load_recents_from_disk(&path))
}

#[tauri::command]
fn save_recent(app: AppHandle, entry: RecentComparison) -> Result<Vec<RecentComparison>, String> {
    let path = recents_path(&app)?;
    let current = load_recents_from_disk(&path);
    let updated = update_recents(current, entry);
    save_recents_to_disk(&path, &updated)?;
    Ok(updated)
}

#[tauri::command]
fn pick_file() -> Option<String> {
    let path = rfd::FileDialog::new()
        .add_filter("Excel / PBIX", &["xlsx", "xlsm", "xltx", "xltm", "xlsb", "pbix", "pbit"])
        .pick_file()?;
    Some(path.display().to_string())
}

#[tauri::command]
fn pick_folder() -> Option<String> {
    let path = rfd::FileDialog::new().pick_folder()?;
    Some(path.display().to_string())
}

#[tauri::command]
fn cancel_diff(state: State<'_, DiffState>, run_id: u64) -> bool {
    let current = match state.current.lock() {
        Ok(lock) => lock,
        Err(poisoned) => poisoned.into_inner(),
    };
    if let Some(active) = current.as_ref() {
        if active.run_id == run_id {
            active.cancel.store(true, Ordering::Relaxed);
            return true;
        }
    }
    false
}

#[tauri::command]
async fn diff_paths_with_sheets(
    app: AppHandle,
    state: State<'_, DiffState>,
    desktop: State<'_, DesktopState>,
    old_path: String,
    new_path: String,
    run_id: u64,
    options: Option<DiffOptions>,
) -> Result<DiffOutcome, DiffErrorPayload> {
    let options = options.unwrap_or_default();
    let cancel_flag = {
        let mut current = match state.current.lock() {
            Ok(lock) => lock,
            Err(poisoned) => poisoned.into_inner(),
        };
        if current.is_some() {
            return Err(DiffErrorPayload::new("busy", "Diff already in progress.", false));
        }
        let cancel = Arc::new(AtomicBool::new(false));
        *current = Some(ActiveDiff {
            run_id,
            cancel: cancel.clone(),
        });
        cancel
    };

    let runner = desktop.runner.clone();
    let app_handle = app.clone();
    let task = tauri::async_runtime::spawn_blocking(move || {
        let request = DiffRequest {
            old_path,
            new_path,
            run_id,
            options,
            cancel: cancel_flag,
            app: app_handle,
        };
        runner.diff(request)
    });

    let result = match task.await {
        Ok(result) => result,
        Err(e) => Err(DiffErrorPayload::new("task", format!("Diff task failed: {e}"), false)),
    };

    let mut current = match state.current.lock() {
        Ok(lock) => lock,
        Err(poisoned) => poisoned.into_inner(),
    };
    if let Some(active) = current.as_ref() {
        if active.run_id == run_id {
            *current = None;
        }
    }

    result
}

#[tauri::command]
fn load_diff_summary(
    desktop: State<'_, DesktopState>,
    diff_id: String,
) -> Result<DiffRunSummary, DiffErrorPayload> {
    let store = OpStore::open(&desktop.store_path).map_err(map_store_error)?;
    store.load_summary(&diff_id).map_err(map_store_error)
}

#[tauri::command]
async fn load_sheet_payload(
    app: AppHandle,
    desktop: State<'_, DesktopState>,
    diff_id: String,
    sheet_name: String,
) -> Result<ui_payload::DiffWithSheets, DiffErrorPayload> {
    let runner = desktop.runner.clone();
    let cancel = Arc::new(AtomicBool::new(false));
    let task = tauri::async_runtime::spawn_blocking(move || {
        runner.load_sheet_payload(SheetPayloadRequest {
            diff_id,
            sheet_name,
            cancel,
            app,
        })
    });

    match task.await {
        Ok(result) => result,
        Err(e) => Err(DiffErrorPayload::new("task", format!("Load task failed: {e}"), false)),
    }
}

#[tauri::command]
fn export_audit_xlsx(
    _app: AppHandle,
    desktop: State<'_, DesktopState>,
    diff_id: String,
) -> Result<String, DiffErrorPayload> {
    let store = OpStore::open(&desktop.store_path).map_err(map_store_error)?;
    let summary = store.load_summary(&diff_id).map_err(map_store_error)?;
    let filename = default_export_name(&summary, "audit", "xlsx");

    let path = rfd::FileDialog::new()
        .set_file_name(&filename)
        .add_filter("Excel", &["xlsx"])
        .save_file()
        .ok_or_else(|| DiffErrorPayload::new("canceled", "Export canceled.", false))?;

    diff_runner::export_audit_xlsx(&diff_id, &desktop.store_path, &path)?;
    Ok(path.display().to_string())
}

#[tauri::command]
async fn run_batch_compare(
    app: AppHandle,
    desktop: State<'_, DesktopState>,
    request: BatchRequest,
) -> Result<BatchOutcome, DiffErrorPayload> {
    let runner = desktop.runner.clone();
    let store_path = desktop.store_path.clone();
    let task = tauri::async_runtime::spawn_blocking(move || {
        batch::run_batch_compare(app, runner, &store_path, request)
    });
    match task.await {
        Ok(result) => result,
        Err(e) => Err(DiffErrorPayload::new("task", format!("Batch task failed: {e}"), false)),
    }
}

#[tauri::command]
fn load_batch_summary(
    desktop: State<'_, DesktopState>,
    batch_id: String,
) -> Result<BatchOutcome, DiffErrorPayload> {
    batch::load_batch_summary(&desktop.store_path, &batch_id)
}

#[tauri::command]
fn search_diff_ops(
    desktop: State<'_, DesktopState>,
    diff_id: String,
    query: String,
    limit: Option<usize>,
) -> Result<Vec<SearchResult>, DiffErrorPayload> {
    let limit = limit.unwrap_or(100);
    search::search_diff_ops(&desktop.store_path, &diff_id, &query, limit)
}

#[tauri::command]
fn build_search_index(
    app: AppHandle,
    desktop: State<'_, DesktopState>,
    path: String,
    side: String,
) -> Result<SearchIndexSummary, DiffErrorPayload> {
    let path = PathBuf::from(path);
    search::build_search_index(app, &desktop.store_path, &path, &side)
}

#[tauri::command]
fn search_workbook_index(
    desktop: State<'_, DesktopState>,
    index_id: String,
    query: String,
    limit: Option<usize>,
) -> Result<Vec<SearchIndexResult>, DiffErrorPayload> {
    let limit = limit.unwrap_or(100);
    search::search_workbook_index(&desktop.store_path, &index_id, &query, limit)
}

fn default_export_name(summary: &DiffRunSummary, prefix: &str, ext: &str) -> String {
    let old = base_name(&summary.old_path);
    let new = base_name(&summary.new_path);
    let date = summary
        .finished_at
        .as_deref()
        .unwrap_or(&summary.started_at)
        .get(0..10)
        .unwrap_or("report");
    format!("excel-diff-{prefix}__{old}__{new}__{date}.{ext}")
}

fn base_name(path: &str) -> String {
    let parts: Vec<&str> = path.split(['\\', '/']).collect();
    parts.last().unwrap_or(&path).to_string()
}

fn map_store_error(err: StoreError) -> DiffErrorPayload {
    DiffErrorPayload::new("store", err.to_string(), false)
}

fn main() {
    tauri::Builder::default()
        .manage(DiffState::default())
        .setup(|app| {
            let store_path = store_path(&app.handle()).map_err(|e| e.to_string())?;
            let app_version = env!("CARGO_PKG_VERSION").to_string();
            let engine_version = app_version.clone();
            let runner = DiffRunner::new(store_path.clone(), app_version, engine_version);
            app.manage(DesktopState { runner, store_path });
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            get_version,
            get_capabilities,
            load_recents,
            save_recent,
            pick_file,
            pick_folder,
            cancel_diff,
            diff_paths_with_sheets,
            load_diff_summary,
            load_sheet_payload,
            export_audit_xlsx,
            run_batch_compare,
            load_batch_summary,
            search_diff_ops,
            build_search_index,
            search_workbook_index,
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}

```

---

### File: `desktop\src-tauri\src\search.rs`

```rust
use std::path::Path;

use excel_diff::{CellValue, WorkbookPackage};
use rusqlite::{params, Connection, OptionalExtension};
use serde::Serialize;
use tauri::AppHandle;
use uuid::Uuid;

use crate::diff_runner::DiffErrorPayload;
use crate::store::{OpStore, StoreError};

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct SearchResult {
    pub kind: String,
    pub sheet: Option<String>,
    pub address: Option<String>,
    pub label: String,
    pub detail: Option<String>,
}

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct SearchIndexResult {
    pub sheet: String,
    pub address: String,
    pub kind: String,
    pub text: String,
}

#[derive(Debug, Clone, Serialize)]
#[serde(rename_all = "camelCase")]
pub struct SearchIndexSummary {
    pub index_id: String,
    pub path: String,
    pub side: String,
    pub created_at: String,
}

pub fn search_diff_ops(
    store_path: &Path,
    diff_id: &str,
    query: &str,
    limit: usize,
) -> Result<Vec<SearchResult>, DiffErrorPayload> {
    let store = OpStore::open(store_path).map_err(store_error)?;
    let strings = store.load_strings(diff_id).map_err(store_error)?;
    let conn = store.into_connection();

    let pattern = format!("%{}%", query.to_lowercase());
    let mut stmt = conn
        .prepare(
            "SELECT payload_json FROM diff_ops WHERE diff_id = ?1 AND lower(payload_json) LIKE ?2 LIMIT ?3",
        )
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    let rows = stmt
        .query_map(params![diff_id, pattern, limit as i64], |row| row.get::<_, String>(0))
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    let mut results = Vec::new();
    for row in rows {
        let payload = row.map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
        let op: excel_diff::DiffOp = serde_json::from_str(&payload)
            .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
        if let Some(result) = match_op(&op, &strings, query) {
            results.push(result);
        }
    }

    Ok(results)
}

pub fn build_search_index(
    _app: AppHandle,
    store_path: &Path,
    path: &Path,
    side: &str,
) -> Result<SearchIndexSummary, DiffErrorPayload> {
    let store = OpStore::open(store_path).map_err(store_error)?;
    let conn = store.connection();
    let path_str = path.display().to_string();

    let meta = std::fs::metadata(path).map_err(|e| DiffErrorPayload::new("io", e.to_string(), false))?;
    let size = meta.len() as i64;
    let mtime = meta
        .modified()
        .ok()
        .and_then(|t| t.duration_since(std::time::UNIX_EPOCH).ok())
        .map(|d| d.as_secs() as i64)
        .unwrap_or(0);

    if let Some(existing) = find_existing_index(conn, &path_str, mtime, size, side)? {
        return Ok(existing);
    }

    let index_id = Uuid::new_v4().to_string();
    let created_at = now_iso();

    conn.execute(
        "INSERT INTO workbook_indexes (index_id, path, mtime, size, side, created_at) VALUES (?1, ?2, ?3, ?4, ?5, ?6)",
        params![index_id, path_str, mtime, size, side, created_at],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    conn.execute(
        "DELETE FROM cell_docs WHERE index_id NOT IN (SELECT index_id FROM workbook_indexes)",
        [],
    )
    .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    index_workbook(conn, path, &index_id).map_err(|e| {
        DiffErrorPayload::new("index", e.to_string(), false)
    })?;

    Ok(SearchIndexSummary {
        index_id,
        path: path.display().to_string(),
        side: side.to_string(),
        created_at: now_iso(),
    })
}

pub fn search_workbook_index(
    store_path: &Path,
    index_id: &str,
    query: &str,
    limit: usize,
) -> Result<Vec<SearchIndexResult>, DiffErrorPayload> {
    let store = OpStore::open(store_path).map_err(store_error)?;
    let conn = store.connection();

    let mut stmt = conn
        .prepare(
            "SELECT sheet, addr, kind, text FROM cell_docs WHERE index_id = ?1 AND cell_docs MATCH ?2 LIMIT ?3",
        )
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;
    let query_text = format!("{}*", query);
    let rows = stmt
        .query_map(params![index_id, query_text, limit as i64], |row| {
            Ok(SearchIndexResult {
                sheet: row.get(0)?,
                address: row.get(1)?,
                kind: row.get(2)?,
                text: row.get(3)?,
            })
        })
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    let mut results = Vec::new();
    for row in rows {
        results.push(row.map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?);
    }
    Ok(results)
}

fn match_op(op: &excel_diff::DiffOp, strings: &[String], query: &str) -> Option<SearchResult> {
    let query_lower = query.to_lowercase();
    match op {
        excel_diff::DiffOp::CellEdited { sheet, addr, from, to, .. } => {
            let sheet_name = resolve_string(strings, *sheet).to_string();
            let old_value = render_cell_value(strings, &from.value);
            let new_value = render_cell_value(strings, &to.value);
            let old_formula = render_formula(strings, from.formula);
            let new_formula = render_formula(strings, to.formula);
            let text = format!("{} {} {} {}", old_value, new_value, old_formula, new_formula).to_lowercase();
            if text.contains(&query_lower) {
                return Some(SearchResult {
                    kind: "cell".to_string(),
                    sheet: Some(sheet_name),
                    address: Some(addr.to_a1()),
                    label: "Cell change".to_string(),
                    detail: Some(format!("{} -> {}", old_value, new_value)),
                });
            }
        }
        excel_diff::DiffOp::QueryAdded { name }
        | excel_diff::DiffOp::QueryRemoved { name }
        | excel_diff::DiffOp::QueryRenamed { from: name, .. }
        | excel_diff::DiffOp::QueryDefinitionChanged { name, .. }
        | excel_diff::DiffOp::QueryMetadataChanged { name, .. } => {
            let query_name = resolve_string(strings, *name);
            if query_name.to_lowercase().contains(&query_lower) {
                return Some(SearchResult {
                    kind: "query".to_string(),
                    sheet: None,
                    address: None,
                    label: format!("Query: {query_name}"),
                    detail: Some(op_kind(op).to_string()),
                });
            }
        }
        excel_diff::DiffOp::DuplicateKeyCluster { sheet, key, left_rows, right_rows } => {
            let sheet_name = resolve_string(strings, *sheet).to_string();
            let key_text = key
                .iter()
                .map(|value| render_cell_value(strings, value))
                .collect::<Vec<_>>()
                .join(" ");
            let row_text = format!("left {} right {}", left_rows.len(), right_rows.len());
            let text = format!("{key_text} {row_text}").to_lowercase();
            if text.contains(&query_lower) {
                return Some(SearchResult {
                    kind: "duplicate_key_cluster".to_string(),
                    sheet: Some(sheet_name),
                    address: None,
                    label: "Duplicate key cluster".to_string(),
                    detail: Some(row_text),
                });
            }
        }
        _ => {}
    }

    None
}

fn op_kind(op: &excel_diff::DiffOp) -> &'static str {
    match op {
        excel_diff::DiffOp::CellEdited { .. } => "CellEdited",
        excel_diff::DiffOp::QueryAdded { .. } => "QueryAdded",
        excel_diff::DiffOp::QueryRemoved { .. } => "QueryRemoved",
        excel_diff::DiffOp::QueryRenamed { .. } => "QueryRenamed",
        excel_diff::DiffOp::QueryDefinitionChanged { .. } => "QueryDefinitionChanged",
        excel_diff::DiffOp::QueryMetadataChanged { .. } => "QueryMetadataChanged",
        excel_diff::DiffOp::DuplicateKeyCluster { .. } => "DuplicateKeyCluster",
        _ => "Other",
    }
}

fn resolve_string(strings: &[String], id: excel_diff::StringId) -> &str {
    strings.get(id.0 as usize).map(String::as_str).unwrap_or("<unknown>")
}

fn render_cell_value(strings: &[String], value: &Option<CellValue>) -> String {
    match value {
        None => String::new(),
        Some(CellValue::Blank) => String::new(),
        Some(CellValue::Number(n)) => n.to_string(),
        Some(CellValue::Text(id)) => resolve_string(strings, *id).to_string(),
        Some(CellValue::Bool(b)) => if *b { "TRUE" } else { "FALSE" }.to_string(),
        Some(CellValue::Error(id)) => resolve_string(strings, *id).to_string(),
    }
}

fn render_formula(strings: &[String], formula: Option<excel_diff::StringId>) -> String {
    match formula {
        Some(id) => {
            let raw = resolve_string(strings, id);
            if raw.is_empty() {
                String::new()
            } else if raw.starts_with('=') {
                raw.to_string()
            } else {
                format!("={}", raw)
            }
        }
        None => String::new(),
    }
}

fn find_existing_index(
    conn: &Connection,
    path: &str,
    mtime: i64,
    size: i64,
    side: &str,
) -> Result<Option<SearchIndexSummary>, DiffErrorPayload> {
    let row = conn
        .query_row(
            "SELECT index_id, created_at FROM workbook_indexes WHERE path = ?1 AND mtime = ?2 AND size = ?3 AND side = ?4",
            params![path, mtime, size, side],
            |row| Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?)),
        )
        .optional()
        .map_err(|e| DiffErrorPayload::new("store", e.to_string(), false))?;

    Ok(row.map(|(index_id, created_at)| SearchIndexSummary {
        index_id,
        path: path.to_string(),
        side: side.to_string(),
        created_at,
    }))
}

fn index_workbook(conn: &Connection, path: &Path, index_id: &str) -> Result<(), StoreError> {
    let file = std::fs::File::open(path).map_err(|e| StoreError::InvalidData(e.to_string()))?;
    let pkg = WorkbookPackage::open(file).map_err(|e| StoreError::InvalidData(e.to_string()))?;

    conn.execute_batch("BEGIN IMMEDIATE")?;

    excel_diff::with_default_session(|session| {
        for sheet in &pkg.workbook.sheets {
            let sheet_name = session.strings.resolve(sheet.name).to_string();
            for ((row, col), cell) in sheet.grid.iter_cells() {
                let addr = excel_diff::CellAddress::from_coords(row, col).to_a1();
                if let Some(value) = &cell.value {
                    let text = match value {
                        CellValue::Number(n) => n.to_string(),
                        CellValue::Text(id) => session.strings.resolve(*id).to_string(),
                        CellValue::Bool(b) => if *b { "TRUE" } else { "FALSE" }.to_string(),
                        CellValue::Error(id) => session.strings.resolve(*id).to_string(),
                        CellValue::Blank => String::new(),
                    };
                    if !text.is_empty() {
                        let _ = conn.execute(
                            "INSERT INTO cell_docs (index_id, sheet, addr, kind, text) VALUES (?1, ?2, ?3, ?4, ?5)",
                            params![index_id, sheet_name, addr, "value", text],
                        );
                    }
                }
                if let Some(formula_id) = cell.formula {
                    let formula = session.strings.resolve(formula_id).to_string();
                    if !formula.is_empty() {
                        let _ = conn.execute(
                            "INSERT INTO cell_docs (index_id, sheet, addr, kind, text) VALUES (?1, ?2, ?3, ?4, ?5)",
                            params![index_id, sheet_name, addr, "formula", formula],
                        );
                    }
                }
            }
        }

        if let Some(dm) = &pkg.data_mashup {
            if let Ok(queries) = excel_diff::build_queries(dm) {
                for query in queries {
                    let text = query.expression_m;
                    let name = query.metadata.formula_name;
                    let _ = conn.execute(
                        "INSERT INTO cell_docs (index_id, sheet, addr, kind, text) VALUES (?1, ?2, ?3, ?4, ?5)",
                        params![index_id, name, "", "query", text],
                    );
                }
            }
        }

        if let Some(vba_modules) = &pkg.vba_modules {
            for module in vba_modules {
                let _ = conn.execute(
                    "INSERT INTO cell_docs (index_id, sheet, addr, kind, text) VALUES (?1, ?2, ?3, ?4, ?5)",
                    params![index_id, session.strings.resolve(module.name), "", "vba", module.code],
                );
            }
        }
    });

    conn.execute_batch("COMMIT")?;
    Ok(())
}

fn now_iso() -> String {
    time::OffsetDateTime::now_utc()
        .format(&time::format_description::well_known::Rfc3339)
        .unwrap_or_else(|_| "".to_string())
}

fn store_error(err: StoreError) -> DiffErrorPayload {
    DiffErrorPayload::new("store", err.to_string(), false)
}

```

---

### File: `desktop\src-tauri\src\store\mod.rs`

```rust
mod op_sink;
mod op_store;
mod types;

pub use op_sink::OpStoreSink;
pub use op_store::{
    DiffMode, DiffRunSummary, OpStore, RunStatus, StoreError, resolve_sheet_stats,
};
pub use types::SheetStats;

```

---

### File: `desktop\src-tauri\src\store\op_sink.rs`

```rust
use std::collections::HashMap;

use excel_diff::{DiffError, DiffOp, DiffSink};
use rusqlite::{params, Connection};

use super::types::{ChangeCounts, OpIndexFields, accumulate_sheet_stats, classify_op, op_index_fields, SheetStats};

pub struct OpStoreSink {
    conn: Connection,
    diff_id: String,
    op_idx: u64,
    counts: ChangeCounts,
    sheet_stats: HashMap<u32, SheetStats>,
    committed: bool,
}

impl OpStoreSink {
    pub fn new(conn: Connection, diff_id: String) -> Result<Self, DiffError> {
        conn.execute_batch("BEGIN IMMEDIATE")
            .map_err(|e| DiffError::SinkError { message: e.to_string() })?;
        Ok(Self {
            conn,
            diff_id,
            op_idx: 0,
            counts: ChangeCounts::default(),
            sheet_stats: HashMap::new(),
            committed: false,
        })
    }

    pub fn into_parts(self) -> (Connection, ChangeCounts, HashMap<u32, SheetStats>, u64) {
        (self.conn, self.counts, self.sheet_stats, self.op_idx)
    }

    fn insert_op(&mut self, fields: OpIndexFields, payload_json: &str) -> Result<(), DiffError> {
        self.conn
            .execute(
                "INSERT INTO diff_ops (diff_id, op_idx, kind, sheet_id, row, col, row_end, col_end, move_id, payload_json)\
                 VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10)",
                params![
                    self.diff_id,
                    self.op_idx as i64,
                    fields.kind,
                    fields.sheet_id.map(|v| v as i64),
                    fields.row.map(|v| v as i64),
                    fields.col.map(|v| v as i64),
                    fields.row_end.map(|v| v as i64),
                    fields.col_end.map(|v| v as i64),
                    fields.move_id,
                    payload_json,
                ],
            )
            .map_err(|e| DiffError::SinkError { message: e.to_string() })?;
        Ok(())
    }
}

impl DiffSink for OpStoreSink {
    fn begin(&mut self, _pool: &excel_diff::StringPool) -> Result<(), DiffError> {
        Ok(())
    }

    fn emit(&mut self, op: DiffOp) -> Result<(), DiffError> {
        if let Some(kind) = classify_op(&op) {
            self.counts.apply(kind);
        }
        accumulate_sheet_stats(&mut self.sheet_stats, &op);

        let fields = op_index_fields(&op);
        let payload_json = serde_json::to_string(&op)
            .map_err(|e| DiffError::SinkError { message: e.to_string() })?;
        self.insert_op(fields, &payload_json)?;

        self.op_idx = self.op_idx.saturating_add(1);
        Ok(())
    }

    fn finish(&mut self) -> Result<(), DiffError> {
        if self.committed {
            return Ok(());
        }
        self.conn
            .execute_batch("COMMIT")
            .map_err(|e| DiffError::SinkError { message: e.to_string() })?;
        self.committed = true;
        Ok(())
    }
}

```

---

### File: `desktop\src-tauri\src\store\op_store.rs`

```rust
use std::path::Path;

use excel_diff::{DiffOp, DiffReport, DiffSummary};
use rusqlite::{params, Connection, OptionalExtension};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use uuid::Uuid;

use super::types::{ChangeCounts, OpIndexFields, SheetStats, accumulate_sheet_stats, op_index_fields};

const SCHEMA_VERSION: i64 = 1;

#[derive(Debug, Error)]
pub enum StoreError {
    #[error("SQLite error: {0}")]
    Sqlite(#[from] rusqlite::Error),
    #[error("JSON error: {0}")]
    Json(#[from] serde_json::Error),
    #[error("Missing diff run: {0}")]
    MissingRun(String),
    #[error("Missing sheet: {0}")]
    MissingSheet(String),
    #[error("Invalid store data: {0}")]
    InvalidData(String),
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum RunStatus {
    Running,
    Complete,
    Failed,
    Canceled,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DiffMode {
    Payload,
    Large,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct SheetSummary {
    pub sheet_id: u32,
    pub sheet_name: String,
    pub op_count: u64,
    pub counts: ChangeCounts,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffRunSummary {
    pub diff_id: String,
    pub old_path: String,
    pub new_path: String,
    pub started_at: String,
    pub finished_at: Option<String>,
    pub engine_version: String,
    pub app_version: String,
    pub mode: DiffMode,
    pub status: RunStatus,
    pub trusted: bool,
    pub complete: bool,
    pub op_count: u64,
    pub warnings: Vec<String>,
    pub counts: ChangeCounts,
    pub sheets: Vec<SheetSummary>,
}

pub struct OpStore {
    conn: Connection,
}

impl OpStore {
    pub fn open(path: &Path) -> Result<Self, StoreError> {
        let conn = Connection::open(path)?;
        conn.execute_batch("PRAGMA foreign_keys = ON;")?;
        Self::apply_schema(&conn)?;
        Ok(Self { conn })
    }

    #[allow(dead_code)]
    pub fn open_in_memory() -> Result<Self, StoreError> {
        let conn = Connection::open_in_memory()?;
        conn.execute_batch("PRAGMA foreign_keys = ON;")?;
        Self::apply_schema(&conn)?;
        Ok(Self { conn })
    }

    #[allow(dead_code)]
    pub fn from_connection(conn: Connection) -> Self {
        Self { conn }
    }

    pub fn connection(&self) -> &Connection {
        &self.conn
    }

    pub fn into_connection(self) -> Connection {
        self.conn
    }

    pub fn start_run(
        &self,
        old_path: &str,
        new_path: &str,
        config_json: &str,
        engine_version: &str,
        app_version: &str,
        mode: DiffMode,
        trusted: bool,
    ) -> Result<String, StoreError> {
        let diff_id = Uuid::new_v4().to_string();
        let started_at = now_iso();
        let status = RunStatus::Running;

        self.conn.execute(
            "INSERT INTO diff_runs (diff_id, old_path, new_path, started_at, config_json, engine_version, app_version, schema_version, mode, status, trusted)\
             VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11)",
            params![
                diff_id,
                old_path,
                new_path,
                started_at,
                config_json,
                engine_version,
                app_version,
                SCHEMA_VERSION,
                mode.as_str(),
                status.as_str(),
                if trusted { 1 } else { 0 },
            ],
        )?;

        Ok(diff_id)
    }

    pub fn set_mode(&self, diff_id: &str, mode: DiffMode) -> Result<(), StoreError> {
        self.conn.execute(
            "UPDATE diff_runs SET mode = ?1 WHERE diff_id = ?2",
            params![mode.as_str(), diff_id],
        )?;
        Ok(())
    }

    pub fn finish_run(
        &self,
        diff_id: &str,
        summary: &DiffSummary,
        strings: &[String],
        counts: &ChangeCounts,
        sheet_stats: &[SheetStatsResolved],
        status: RunStatus,
    ) -> Result<(), StoreError> {
        let finished_at = now_iso();
        let strings_json = serde_json::to_string(strings)?;

        self.conn.execute(
            "UPDATE diff_runs SET finished_at = ?1, status = ?2, complete = ?3, op_count = ?4, warnings_count = ?5,\
             added_count = ?6, removed_count = ?7, modified_count = ?8, moved_count = ?9, strings_json = ?10 WHERE diff_id = ?11",
            params![
                finished_at,
                status.as_str(),
                if summary.complete { 1 } else { 0 },
                summary.op_count as i64,
                summary.warnings.len() as i64,
                counts.added as i64,
                counts.removed as i64,
                counts.modified as i64,
                counts.moved as i64,
                strings_json,
                diff_id,
            ],
        )?;

        self.conn.execute("DELETE FROM diff_warnings WHERE diff_id = ?1", params![diff_id])?;
        for (idx, warning) in summary.warnings.iter().enumerate() {
            self.conn.execute(
                "INSERT INTO diff_warnings (diff_id, idx, text) VALUES (?1, ?2, ?3)",
                params![diff_id, idx as i64, warning],
            )?;
        }

        self.conn.execute("DELETE FROM diff_sheets WHERE diff_id = ?1", params![diff_id])?;
        for sheet in sheet_stats {
            self.conn.execute(
                "INSERT INTO diff_sheets (diff_id, sheet_id, sheet_name, op_count, added_count, removed_count, modified_count, moved_count)\
                 VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)",
                params![
                    diff_id,
                    sheet.sheet_id as i64,
                    sheet.sheet_name,
                    sheet.op_count as i64,
                    sheet.counts.added as i64,
                    sheet.counts.removed as i64,
                    sheet.counts.modified as i64,
                    sheet.counts.moved as i64,
                ],
            )?;
        }

        Ok(())
    }

    pub fn fail_run(&self, diff_id: &str, status: RunStatus, message: &str) -> Result<(), StoreError> {
        let finished_at = now_iso();
        self.conn.execute(
            "UPDATE diff_runs SET finished_at = ?1, status = ?2, complete = 0, warnings_count = warnings_count + 1 WHERE diff_id = ?3",
            params![finished_at, status.as_str(), diff_id],
        )?;
        let idx: i64 = self
            .conn
            .query_row(
                "SELECT COALESCE(MAX(idx) + 1, 0) FROM diff_warnings WHERE diff_id = ?1",
                params![diff_id],
                |row| row.get(0),
            )
            .unwrap_or(0);
        self.conn.execute(
            "INSERT INTO diff_warnings (diff_id, idx, text) VALUES (?1, ?2, ?3)",
            params![diff_id, idx, message],
        )?;
        Ok(())
    }

    pub fn insert_ops_from_report(
        &self,
        diff_id: &str,
        report: &DiffReport,
    ) -> Result<(ChangeCounts, Vec<SheetStats>), StoreError> {
        self.conn.execute_batch("BEGIN IMMEDIATE")?;

        let mut counts = ChangeCounts::default();
        let mut stats_map: std::collections::HashMap<u32, SheetStats> = std::collections::HashMap::new();

        for (idx, op) in report.ops.iter().enumerate() {
            counts.add_op(op);
            accumulate_sheet_stats(&mut stats_map, op);

            let fields: OpIndexFields = op_index_fields(op);
            let payload_json = serde_json::to_string(op)?;

            self.conn.execute(
                "INSERT INTO diff_ops (diff_id, op_idx, kind, sheet_id, row, col, row_end, col_end, move_id, payload_json)\
                 VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10)",
                params![
                    diff_id,
                    idx as i64,
                    fields.kind,
                    fields.sheet_id.map(|v| v as i64),
                    fields.row.map(|v| v as i64),
                    fields.col.map(|v| v as i64),
                    fields.row_end.map(|v| v as i64),
                    fields.col_end.map(|v| v as i64),
                    fields.move_id,
                    payload_json,
                ],
            )?;
        }

        self.conn.execute_batch("COMMIT")?;

        let mut stats: Vec<SheetStats> = stats_map.into_values().collect();
        stats.sort_by_key(|s| s.sheet_id);
        Ok((counts, stats))
    }

    pub fn load_summary(&self, diff_id: &str) -> Result<DiffRunSummary, StoreError> {
        let row = self.conn.query_row(
            "SELECT old_path, new_path, started_at, finished_at, engine_version, app_version, mode, status, trusted, complete, op_count,\
                    added_count, removed_count, modified_count, moved_count\
             FROM diff_runs WHERE diff_id = ?1",
            params![diff_id],
            |row| {
                Ok((
                    row.get::<_, String>(0)?,
                    row.get::<_, String>(1)?,
                    row.get::<_, String>(2)?,
                    row.get::<_, Option<String>>(3)?,
                    row.get::<_, String>(4)?,
                    row.get::<_, String>(5)?,
                    row.get::<_, String>(6)?,
                    row.get::<_, String>(7)?,
                    row.get::<_, i64>(8)?,
                    row.get::<_, i64>(9)?,
                    row.get::<_, i64>(10)?,
                    row.get::<_, i64>(11)?,
                    row.get::<_, i64>(12)?,
                    row.get::<_, i64>(13)?,
                    row.get::<_, i64>(14)?,
                ))
            },
        ).optional()?;

        let Some((old_path, new_path, started_at, finished_at, engine_version, app_version, mode, status, trusted, complete, op_count, added, removed, modified, moved)) = row else {
            return Err(StoreError::MissingRun(diff_id.to_string()));
        };

        let warnings = self.load_warnings(diff_id)?;
        let sheets = self.load_sheet_summaries(diff_id)?;

        Ok(DiffRunSummary {
            diff_id: diff_id.to_string(),
            old_path,
            new_path,
            started_at,
            finished_at,
            engine_version,
            app_version,
            mode: DiffMode::from_str(&mode),
            status: RunStatus::from_str(&status),
            trusted: trusted != 0,
            complete: complete != 0,
            op_count: op_count as u64,
            warnings,
            counts: ChangeCounts {
                added: added as u64,
                removed: removed as u64,
                modified: modified as u64,
                moved: moved as u64,
            },
            sheets,
        })
    }

    pub fn load_report(&self, diff_id: &str) -> Result<DiffReport, StoreError> {
        let summary = self.load_summary(diff_id)?;
        let strings = self.load_strings(diff_id)?;
        let ops = self.load_ops(diff_id)?;
        let mut report = DiffReport::new(ops);
        report.strings = strings;
        report.complete = summary.complete;
        report.warnings = summary.warnings;
        Ok(report)
    }

    pub fn load_sheet_ops(&self, diff_id: &str, sheet_name: &str) -> Result<Vec<DiffOp>, StoreError> {
        let sheet_id = self.sheet_id_for_name(diff_id, sheet_name)?;
        let mut stmt = self.conn.prepare(
            "SELECT payload_json FROM diff_ops WHERE diff_id = ?1 AND sheet_id = ?2 ORDER BY op_idx",
        )?;
        let rows = stmt.query_map(params![diff_id, sheet_id as i64], |row| row.get::<_, String>(0))?;
        let mut ops = Vec::new();
        for row in rows {
            let payload = row?;
            let op: DiffOp = serde_json::from_str(&payload)?;
            ops.push(op);
        }
        Ok(ops)
    }

    pub fn load_ops(&self, diff_id: &str) -> Result<Vec<DiffOp>, StoreError> {
        let mut stmt = self.conn.prepare(
            "SELECT payload_json FROM diff_ops WHERE diff_id = ?1 ORDER BY op_idx",
        )?;
        let rows = stmt.query_map(params![diff_id], |row| row.get::<_, String>(0))?;
        let mut ops = Vec::new();
        for row in rows {
            let payload = row?;
            let op: DiffOp = serde_json::from_str(&payload)?;
            ops.push(op);
        }
        Ok(ops)
    }

    pub fn stream_ops<F>(&self, diff_id: &str, mut f: F) -> Result<(), StoreError>
    where
        F: FnMut(DiffOp) -> Result<(), StoreError>,
    {
        let mut stmt = self.conn.prepare(
            "SELECT payload_json FROM diff_ops WHERE diff_id = ?1 ORDER BY op_idx",
        )?;
        let rows = stmt.query_map(params![diff_id], |row| row.get::<_, String>(0))?;
        for row in rows {
            let payload = row?;
            let op: DiffOp = serde_json::from_str(&payload)?;
            f(op)?;
        }
        Ok(())
    }

    pub fn load_strings(&self, diff_id: &str) -> Result<Vec<String>, StoreError> {
        let json = self.conn.query_row(
            "SELECT strings_json FROM diff_runs WHERE diff_id = ?1",
            params![diff_id],
            |row| row.get::<_, Option<String>>(0),
        )?.ok_or_else(|| StoreError::MissingRun(diff_id.to_string()))?;
        let strings: Vec<String> = serde_json::from_str(&json)?;
        Ok(strings)
    }

    pub fn sheet_id_for_name(&self, diff_id: &str, sheet_name: &str) -> Result<u32, StoreError> {
        let id: Option<i64> = self.conn.query_row(
            "SELECT sheet_id FROM diff_sheets WHERE diff_id = ?1 AND sheet_name = ?2 COLLATE NOCASE",
            params![diff_id, sheet_name],
            |row| row.get(0),
        ).optional()?;
        match id {
            Some(value) => Ok(value as u32),
            None => Err(StoreError::MissingSheet(sheet_name.to_string())),
        }
    }

    fn load_warnings(&self, diff_id: &str) -> Result<Vec<String>, StoreError> {
        let mut stmt = self.conn.prepare(
            "SELECT text FROM diff_warnings WHERE diff_id = ?1 ORDER BY idx",
        )?;
        let rows = stmt.query_map(params![diff_id], |row| row.get::<_, String>(0))?;
        let mut warnings = Vec::new();
        for row in rows {
            warnings.push(row?);
        }
        Ok(warnings)
    }

    fn load_sheet_summaries(&self, diff_id: &str) -> Result<Vec<SheetSummary>, StoreError> {
        let mut stmt = self.conn.prepare(
            "SELECT sheet_id, sheet_name, op_count, added_count, removed_count, modified_count, moved_count\
             FROM diff_sheets WHERE diff_id = ?1 ORDER BY sheet_name",
        )?;
        let rows = stmt.query_map(params![diff_id], |row| {
            Ok(SheetSummary {
                sheet_id: row.get::<_, i64>(0)? as u32,
                sheet_name: row.get(1)?,
                op_count: row.get::<_, i64>(2)? as u64,
                counts: ChangeCounts {
                    added: row.get::<_, i64>(3)? as u64,
                    removed: row.get::<_, i64>(4)? as u64,
                    modified: row.get::<_, i64>(5)? as u64,
                    moved: row.get::<_, i64>(6)? as u64,
                },
            })
        })?;

        let mut sheets = Vec::new();
        for row in rows {
            sheets.push(row?);
        }
        Ok(sheets)
    }

    fn apply_schema(conn: &Connection) -> Result<(), StoreError> {
        let schema = include_str!("schema.sql");
        conn.execute_batch(schema)?;
        conn.execute_batch(&format!("PRAGMA user_version = {SCHEMA_VERSION};"))?;
        Ok(())
    }
}

fn now_iso() -> String {
    time::OffsetDateTime::now_utc()
        .format(&time::format_description::well_known::Rfc3339)
        .unwrap_or_else(|_| "".to_string())
}

impl DiffMode {
    pub fn as_str(&self) -> &'static str {
        match self {
            DiffMode::Payload => "payload",
            DiffMode::Large => "large",
        }
    }

    pub fn from_str(value: &str) -> Self {
        match value {
            "large" => DiffMode::Large,
            _ => DiffMode::Payload,
        }
    }
}

impl RunStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            RunStatus::Running => "running",
            RunStatus::Complete => "complete",
            RunStatus::Failed => "failed",
            RunStatus::Canceled => "canceled",
        }
    }

    pub fn from_str(value: &str) -> Self {
        match value {
            "complete" => RunStatus::Complete,
            "failed" => RunStatus::Failed,
            "canceled" => RunStatus::Canceled,
            _ => RunStatus::Running,
        }
    }
}

#[derive(Debug, Clone)]
pub struct SheetStatsResolved {
    pub sheet_id: u32,
    pub sheet_name: String,
    pub counts: ChangeCounts,
    pub op_count: u64,
}

pub fn resolve_sheet_stats(
    strings: &[String],
    stats: &[SheetStats],
) -> Result<Vec<SheetStatsResolved>, StoreError> {
    let mut resolved = Vec::with_capacity(stats.len());
    for stat in stats {
        let name = strings
            .get(stat.sheet_id as usize)
            .cloned()
            .ok_or_else(|| StoreError::InvalidData("sheet id out of range".to_string()))?;
        resolved.push(SheetStatsResolved {
            sheet_id: stat.sheet_id,
            sheet_name: name,
            counts: stat.counts,
            op_count: stat.op_count,
        });
    }
    Ok(resolved)
}

```

---

### File: `desktop\src-tauri\src\store\types.rs`

```rust
use std::collections::HashMap;

use excel_diff::{DiffOp, StringId};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ChangeKind {
    Added,
    Removed,
    Modified,
    Moved,
}

#[derive(Debug, Default, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ChangeCounts {
    pub added: u64,
    pub removed: u64,
    pub modified: u64,
    pub moved: u64,
}

impl ChangeCounts {
    pub fn apply(&mut self, kind: ChangeKind) {
        match kind {
            ChangeKind::Added => self.added = self.added.saturating_add(1),
            ChangeKind::Removed => self.removed = self.removed.saturating_add(1),
            ChangeKind::Modified => self.modified = self.modified.saturating_add(1),
            ChangeKind::Moved => self.moved = self.moved.saturating_add(1),
        }
    }

    pub fn add_op(&mut self, op: &DiffOp) {
        if let Some(kind) = classify_op(op) {
            self.apply(kind);
        }
    }
}

#[derive(Debug, Default, Clone)]
pub struct SheetStats {
    pub sheet_id: u32,
    pub counts: ChangeCounts,
    pub op_count: u64,
}

impl SheetStats {
    pub fn new(sheet_id: u32) -> Self {
        Self {
            sheet_id,
            counts: ChangeCounts::default(),
            op_count: 0,
        }
    }

    pub fn add_op(&mut self, op: &DiffOp) {
        self.op_count = self.op_count.saturating_add(1);
        self.counts.add_op(op);
    }
}

#[derive(Debug, Default, Clone)]
pub struct OpIndexFields {
    pub kind: String,
    pub sheet_id: Option<u32>,
    pub row: Option<u32>,
    pub col: Option<u32>,
    pub row_end: Option<u32>,
    pub col_end: Option<u32>,
    pub move_id: Option<String>,
}

pub fn op_sheet_id(op: &DiffOp) -> Option<StringId> {
    match op {
        DiffOp::SheetAdded { sheet }
        | DiffOp::SheetRemoved { sheet }
        | DiffOp::SheetRenamed { sheet, .. }
        | DiffOp::RowAdded { sheet, .. }
        | DiffOp::RowRemoved { sheet, .. }
        | DiffOp::RowReplaced { sheet, .. }
        | DiffOp::DuplicateKeyCluster { sheet, .. }
        | DiffOp::ColumnAdded { sheet, .. }
        | DiffOp::ColumnRemoved { sheet, .. }
        | DiffOp::BlockMovedRows { sheet, .. }
        | DiffOp::BlockMovedColumns { sheet, .. }
        | DiffOp::BlockMovedRect { sheet, .. }
        | DiffOp::RectReplaced { sheet, .. }
        | DiffOp::CellEdited { sheet, .. } => Some(*sheet),
        _ => None,
    }
}

pub fn diff_op_kind(op: &DiffOp) -> &'static str {
    match op {
        DiffOp::SheetAdded { .. } => "SheetAdded",
        DiffOp::SheetRemoved { .. } => "SheetRemoved",
        DiffOp::SheetRenamed { .. } => "SheetRenamed",
        DiffOp::RowAdded { .. } => "RowAdded",
        DiffOp::RowRemoved { .. } => "RowRemoved",
        DiffOp::RowReplaced { .. } => "RowReplaced",
        DiffOp::DuplicateKeyCluster { .. } => "DuplicateKeyCluster",
        DiffOp::ColumnAdded { .. } => "ColumnAdded",
        DiffOp::ColumnRemoved { .. } => "ColumnRemoved",
        DiffOp::BlockMovedRows { .. } => "BlockMovedRows",
        DiffOp::BlockMovedColumns { .. } => "BlockMovedColumns",
        DiffOp::BlockMovedRect { .. } => "BlockMovedRect",
        DiffOp::RectReplaced { .. } => "RectReplaced",
        DiffOp::CellEdited { .. } => "CellEdited",
        DiffOp::VbaModuleAdded { .. } => "VbaModuleAdded",
        DiffOp::VbaModuleRemoved { .. } => "VbaModuleRemoved",
        DiffOp::VbaModuleChanged { .. } => "VbaModuleChanged",
        DiffOp::NamedRangeAdded { .. } => "NamedRangeAdded",
        DiffOp::NamedRangeRemoved { .. } => "NamedRangeRemoved",
        DiffOp::NamedRangeChanged { .. } => "NamedRangeChanged",
        DiffOp::ChartAdded { .. } => "ChartAdded",
        DiffOp::ChartRemoved { .. } => "ChartRemoved",
        DiffOp::ChartChanged { .. } => "ChartChanged",
        DiffOp::QueryAdded { .. } => "QueryAdded",
        DiffOp::QueryRemoved { .. } => "QueryRemoved",
        DiffOp::QueryRenamed { .. } => "QueryRenamed",
        DiffOp::QueryDefinitionChanged { .. } => "QueryDefinitionChanged",
        DiffOp::QueryMetadataChanged { .. } => "QueryMetadataChanged",
        #[cfg(feature = "model-diff")]
        DiffOp::TableAdded { .. } => "TableAdded",
        #[cfg(feature = "model-diff")]
        DiffOp::TableRemoved { .. } => "TableRemoved",
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnAdded { .. } => "ModelColumnAdded",
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnRemoved { .. } => "ModelColumnRemoved",
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnTypeChanged { .. } => "ModelColumnTypeChanged",
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnPropertyChanged { .. } => "ModelColumnPropertyChanged",
        #[cfg(feature = "model-diff")]
        DiffOp::CalculatedColumnDefinitionChanged { .. } => "CalculatedColumnDefinitionChanged",
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipAdded { .. } => "RelationshipAdded",
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipRemoved { .. } => "RelationshipRemoved",
        #[cfg(feature = "model-diff")]
        DiffOp::RelationshipPropertyChanged { .. } => "RelationshipPropertyChanged",
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureAdded { .. } => "MeasureAdded",
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureRemoved { .. } => "MeasureRemoved",
        #[cfg(feature = "model-diff")]
        DiffOp::MeasureDefinitionChanged { .. } => "MeasureDefinitionChanged",
        _ => "Unknown",
    }
}

pub fn classify_op(op: &DiffOp) -> Option<ChangeKind> {
    match op {
        DiffOp::SheetAdded { .. }
        | DiffOp::RowAdded { .. }
        | DiffOp::ColumnAdded { .. }
        | DiffOp::NamedRangeAdded { .. }
        | DiffOp::ChartAdded { .. }
        | DiffOp::VbaModuleAdded { .. }
        | DiffOp::QueryAdded { .. }
        | DiffOp::QueryMetadataChanged {
            field: excel_diff::QueryMetadataField::LoadToSheet,
            ..
        } => Some(ChangeKind::Added),
        DiffOp::SheetRemoved { .. }
        | DiffOp::RowRemoved { .. }
        | DiffOp::ColumnRemoved { .. }
        | DiffOp::NamedRangeRemoved { .. }
        | DiffOp::ChartRemoved { .. }
        | DiffOp::VbaModuleRemoved { .. }
        | DiffOp::QueryRemoved { .. } => Some(ChangeKind::Removed),
        DiffOp::BlockMovedRows { .. }
        | DiffOp::BlockMovedColumns { .. }
        | DiffOp::BlockMovedRect { .. } => Some(ChangeKind::Moved),
        DiffOp::RowReplaced { .. }
        | DiffOp::DuplicateKeyCluster { .. }
        | DiffOp::RectReplaced { .. }
        | DiffOp::CellEdited { .. }
        | DiffOp::SheetRenamed { .. }
        | DiffOp::NamedRangeChanged { .. }
        | DiffOp::ChartChanged { .. }
        | DiffOp::VbaModuleChanged { .. }
        | DiffOp::QueryRenamed { .. }
        | DiffOp::QueryDefinitionChanged { .. }
        | DiffOp::QueryMetadataChanged { .. } => Some(ChangeKind::Modified),
        #[cfg(feature = "model-diff")]
        DiffOp::TableAdded { .. }
        | DiffOp::ModelColumnAdded { .. }
        | DiffOp::RelationshipAdded { .. }
        | DiffOp::MeasureAdded { .. } => Some(ChangeKind::Added),
        #[cfg(feature = "model-diff")]
        DiffOp::TableRemoved { .. }
        | DiffOp::ModelColumnRemoved { .. }
        | DiffOp::RelationshipRemoved { .. }
        | DiffOp::MeasureRemoved { .. } => Some(ChangeKind::Removed),
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnTypeChanged { .. }
        | DiffOp::ModelColumnPropertyChanged { .. }
        | DiffOp::CalculatedColumnDefinitionChanged { .. }
        | DiffOp::RelationshipPropertyChanged { .. }
        | DiffOp::MeasureDefinitionChanged { .. } => Some(ChangeKind::Modified),
        _ => None,
    }
}

pub fn op_index_fields(op: &DiffOp) -> OpIndexFields {
    let mut fields = OpIndexFields {
        kind: diff_op_kind(op).to_string(),
        ..OpIndexFields::default()
    };

    if let Some(sheet) = op_sheet_id(op) {
        fields.sheet_id = Some(sheet.0);
    }

    match op {
        DiffOp::RowAdded { row_idx, .. }
        | DiffOp::RowRemoved { row_idx, .. }
        | DiffOp::RowReplaced { row_idx, .. } => {
            fields.row = Some(*row_idx);
            fields.row_end = Some(*row_idx);
        }
        DiffOp::DuplicateKeyCluster { left_rows, right_rows, .. } => {
            let mut rows: Vec<u32> = left_rows.iter().chain(right_rows.iter()).copied().collect();
            rows.sort_unstable();
            if let Some(first) = rows.first() {
                fields.row = Some(*first);
                fields.row_end = Some(*rows.last().unwrap_or(first));
            }
        }
        DiffOp::ColumnAdded { col_idx, .. } | DiffOp::ColumnRemoved { col_idx, .. } => {
            fields.col = Some(*col_idx);
            fields.col_end = Some(*col_idx);
        }
        DiffOp::BlockMovedRows {
            src_start_row,
            row_count,
            dst_start_row,
            ..
        } => {
            fields.row = Some(*src_start_row);
            fields.row_end = Some(src_start_row.saturating_add(*row_count).saturating_sub(1));
            fields.move_id = Some(format!("r:{}+{}->{}", src_start_row, row_count, dst_start_row));
        }
        DiffOp::BlockMovedColumns {
            src_start_col,
            col_count,
            dst_start_col,
            ..
        } => {
            fields.col = Some(*src_start_col);
            fields.col_end = Some(src_start_col.saturating_add(*col_count).saturating_sub(1));
            fields.move_id = Some(format!("c:{}+{}->{}", src_start_col, col_count, dst_start_col));
        }
        DiffOp::BlockMovedRect {
            src_start_row,
            src_row_count,
            src_start_col,
            src_col_count,
            dst_start_row,
            dst_start_col,
            ..
        } => {
            fields.row = Some(*src_start_row);
            fields.col = Some(*src_start_col);
            fields.row_end = Some(src_start_row.saturating_add(*src_row_count).saturating_sub(1));
            fields.col_end = Some(src_start_col.saturating_add(*src_col_count).saturating_sub(1));
            fields.move_id = Some(format!(
                "rect:{},{}+{}x{}->{}", 
                src_start_row, src_start_col, src_row_count, src_col_count, dst_start_row
            ));
            if fields.move_id.is_some() {
                if let Some(id) = fields.move_id.as_mut() {
                    id.push_str(&format!(",{}", dst_start_col));
                }
            }
        }
        DiffOp::RectReplaced {
            start_row,
            row_count,
            start_col,
            col_count,
            ..
        } => {
            fields.row = Some(*start_row);
            fields.col = Some(*start_col);
            fields.row_end = Some(start_row.saturating_add(*row_count).saturating_sub(1));
            fields.col_end = Some(start_col.saturating_add(*col_count).saturating_sub(1));
        }
        DiffOp::CellEdited { addr, .. } => {
            fields.row = Some(addr.row);
            fields.col = Some(addr.col);
            fields.row_end = Some(addr.row);
            fields.col_end = Some(addr.col);
        }
        _ => {}
    }

    fields
}

pub fn accumulate_sheet_stats(
    stats: &mut HashMap<u32, SheetStats>,
    op: &DiffOp,
) {
    if let Some(sheet_id) = op_sheet_id(op).map(|id| id.0) {
        let entry = stats.entry(sheet_id).or_insert_with(|| SheetStats::new(sheet_id));
        entry.add_op(op);
    }
}

```

---

### File: `fixtures\manifest.yaml`

```yaml
scenarios:
  # --- Phase 1.1: Basic File Opening ---
  - id: "smoke_minimal"
    generator: "basic_grid"
    args: { rows: 1, cols: 1 }
    output: "minimal.xlsx"

  # --- Phase 1.2: Is this a ZIP? ---
  - id: "container_random_zip"
    generator: "corrupt_container"
    args: { mode: "random_zip" }
    output: "random_zip.zip"
    
  - id: "container_no_content_types"
    generator: "corrupt_container"
    args: { mode: "no_content_types" }
    output: "no_content_types.xlsx"

  - id: "container_not_zip_text"
    generator: "corrupt_container"
    args: { mode: "not_zip_text" }
    output: "not_a_zip.txt"

  - id: "xlsb_stub"
    generator: "xlsb_stub"
    output: "xlsb_stub.xlsb"

  - id: "dpapi_blob_present"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_firewall_off"
      base_file: "templates/base_query.xlsx"
      bindings_override: "01020304"
    output: "dpapi_blob_present.xlsx"

  # --- PG1: Workbook -> Sheet -> Grid IR sanity ---
  - id: "pg1_basic_two_sheets"
    generator: "basic_grid"
    args: { rows: 3, cols: 3, two_sheets: true } # Sheet1 3x3, Sheet2 5x2 (logic in generator)
    output: "pg1_basic_two_sheets.xlsx"

  - id: "pg1_sparse"
    generator: "sparse_grid"
    output: "pg1_sparse_used_range.xlsx"

  - id: "pg1_mixed"
    generator: "edge_case"
    output: "pg1_empty_and_mixed_sheets.xlsx"

  # --- PG2: Addressing and index invariants ---
  - id: "pg2_addressing"
    generator: "address_sanity"
    args:
      targets: ["A1", "B2", "C3", "Z1", "Z10", "AA1", "AA10", "AB7", "AZ5", "BA1", "ZZ10", "AAA1"]
    output: "pg2_addressing_matrix.xlsx"

  # --- PG3: Cell snapshots and comparison semantics ---
  - id: "pg3_types"
    generator: "value_formula"
    output: "pg3_value_and_formula_cells.xlsx"

  # --- Phase 3: Spreadsheet-mode G1/G2 ---
  - id: "g1_equal_sheet"
    generator: "basic_grid"
    args:
      rows: 5
      cols: 5
      sheet: "Sheet1"
    output:
      - "equal_sheet_a.xlsx"
      - "equal_sheet_b.xlsx"

  - id: "g2_single_cell_value"
    generator: "single_cell_diff"
    args:
      rows: 5
      cols: 5
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: 1.0
      value_b: 2.0
    output:
      - "single_cell_value_a.xlsx"
      - "single_cell_value_b.xlsx"

  # --- Phase 3: Spreadsheet-mode G5-G7 ---

  - id: "g5_multi_cell_edits"
    generator: "multi_cell_diff"
    args:
      rows: 20
      cols: 10
      sheet: "Sheet1"
      edits:
        - { addr: "B2", value_a: 1.0, value_b: 42.0 }
        - { addr: "D5", value_a: 2.0, value_b: 99.0 }
        - { addr: "H7", value_a: 3.0, value_b: 3.5 }
        - { addr: "J10", value_a: "x", value_b: "y" }
    output:
      - "multi_cell_edits_a.xlsx"
      - "multi_cell_edits_b.xlsx"

  - id: "g6_row_append_bottom"
    generator: "grid_tail_diff"
    args:
      mode: "row_append_bottom"
      sheet: "Sheet1"
      base_rows: 10
      tail_rows: 2
    output:
      - "row_append_bottom_a.xlsx"
      - "row_append_bottom_b.xlsx"

  - id: "g6_row_delete_bottom"
    generator: "grid_tail_diff"
    args:
      mode: "row_delete_bottom"
      sheet: "Sheet1"
      base_rows: 10
      tail_rows: 2
    output:
      - "row_delete_bottom_a.xlsx"
      - "row_delete_bottom_b.xlsx"

  - id: "g7_col_append_right"
    generator: "grid_tail_diff"
    args:
      mode: "col_append_right"
      sheet: "Sheet1"
      base_cols: 4
      tail_cols: 2
    output:
      - "col_append_right_a.xlsx"
      - "col_append_right_b.xlsx"

  - id: "g7_col_delete_right"
    generator: "grid_tail_diff"
    args:
      mode: "col_delete_right"
      sheet: "Sheet1"
      base_cols: 4
      tail_cols: 2
    output:
      - "col_delete_right_a.xlsx"
      - "col_delete_right_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G8 ---
  - id: "g8_row_insert_middle"
    generator: "row_alignment_g8"
    args:
      mode: "insert"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      insert_at: 6
    output:
      - "row_insert_middle_a.xlsx"
      - "row_insert_middle_b.xlsx"

  - id: "g8_row_delete_middle"
    generator: "row_alignment_g8"
    args:
      mode: "delete"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      delete_row: 6
    output:
      - "row_delete_middle_a.xlsx"
      - "row_delete_middle_b.xlsx"

  - id: "g8_row_insert_with_edit_below"
    generator: "row_alignment_g8"
    args:
      mode: "insert_with_edit"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      insert_at: 6
      edit_row: 8
      edit_col: 3
    output:
      - "row_insert_with_edit_a.xlsx"
      - "row_insert_with_edit_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G9 ---
  - id: "g9_col_insert_middle"
    generator: "column_alignment_g9"
    args:
      mode: "insert"
      sheet: "Data"
      cols: 8
      data_rows: 9
      insert_at: 4
    output:
      - "col_insert_middle_a.xlsx"
      - "col_insert_middle_b.xlsx"

  - id: "g9_col_delete_middle"
    generator: "column_alignment_g9"
    args:
      mode: "delete"
      sheet: "Data"
      cols: 8
      data_rows: 9
      delete_col: 4
    output:
      - "col_delete_middle_a.xlsx"
      - "col_delete_middle_b.xlsx"

  - id: "g9_col_insert_with_edit"
    generator: "column_alignment_g9"
    args:
      mode: "insert_with_edit"
      sheet: "Data"
      cols: 8
      data_rows: 9
      insert_at: 4
      edit_row: 8
      edit_col_after_insert: 7
    output:
      - "col_insert_with_edit_a.xlsx"
      - "col_insert_with_edit_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G10 ---
  - id: "g10_row_block_insert"
    generator: "row_alignment_g10"
    args:
      mode: "block_insert"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      block_rows: 4
      insert_at: 4
    output:
      - "row_block_insert_a.xlsx"
      - "row_block_insert_b.xlsx"

  - id: "g10_row_block_delete"
    generator: "row_alignment_g10"
    args:
      mode: "block_delete"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      block_rows: 4
      delete_start: 4
    output:
      - "row_block_delete_a.xlsx"
      - "row_block_delete_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G11 ---
  - id: "g11_row_block_move"
    generator: "row_block_move_g11"
    args:
      sheet: "Sheet1"
      total_rows: 20
      cols: 5
      block_rows: 4
      src_start: 5    # 1-based in A
      dst_start: 13   # 1-based in B
    output:
      - "row_block_move_a.xlsx"
      - "row_block_move_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G12 (column move only - G12a) ---
  - id: "g12_column_block_move"
    generator: "column_move_g12"
    args:
      sheet: "Data"
      cols: 8
      data_rows: 9
      src_col: 3      # 1-based: C
      dst_col: 6      # 1-based: F
    output:
      - "column_move_a.xlsx"
      - "column_move_b.xlsx"

  - id: "g12_rect_block_move"
    generator: "rect_block_move_g12"
    args:
      sheet: "Data"
      rows: 15
      cols: 15
      src_top: 3      # 1-based row in A (Excel row 3)
      src_left: 2     # 1-based column in A (Excel column B)
      dst_top: 10     # 1-based row in B (Excel row 10)
      dst_left: 7     # 1-based column in B (Excel column G)
      block_rows: 3
      block_cols: 3
    output:
      - "rect_block_move_a.xlsx"
      - "rect_block_move_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G13 ---
  - id: "g13_fuzzy_row_move"
    generator: "row_fuzzy_move_g13"
    args:
      sheet: "Data"
      total_rows: 24
      cols: 6
      block_rows: 4
      src_start: 5      # 1-based in A
      dst_start: 14     # 1-based in B
      edits:
        - { row_offset: 1, col: 3, delta: 1 }
    output:
      - "grid_move_and_edit_a.xlsx"
      - "grid_move_and_edit_b.xlsx"

  # --- JSON diff: simple non-empty change ---
  - id: "json_diff_single_cell"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: "1"
      value_b: "2"
    output:
      - "json_diff_single_cell_a.xlsx"
      - "json_diff_single_cell_b.xlsx"

  - id: "json_diff_single_bool"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: true
      value_b: false
    output:
      - "json_diff_bool_a.xlsx"
      - "json_diff_bool_b.xlsx"

  - id: "json_diff_value_to_empty"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: "1"
      value_b: null
    output:
      - "json_diff_value_to_empty_a.xlsx"
      - "json_diff_value_to_empty_b.xlsx"

  # --- Sheet identity: case-only renames ---
  - id: "sheet_case_only_rename"
    generator: "sheet_case_rename"
    args:
      sheet_a: "Sheet1"
      sheet_b: "sheet1"
      cell: "A1"
      value_a: 1.0
      value_b: 1.0
    output:
      - "sheet_case_only_rename_a.xlsx"
      - "sheet_case_only_rename_b.xlsx"

  - id: "sheet_case_only_rename_cell_edit"
    generator: "sheet_case_rename"
    args:
      sheet_a: "Sheet1"
      sheet_b: "sheet1"
      cell: "A1"
      value_a: 1.0
      value_b: 2.0
    output:
      - "sheet_case_only_rename_edit_a.xlsx"
      - "sheet_case_only_rename_edit_b.xlsx"

  # --- PG6: Object graph vs grid responsibilities ---
  - id: "pg6_sheet_added"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_added"
    output:
      - "pg6_sheet_added_a.xlsx"
      - "pg6_sheet_added_b.xlsx"

  - id: "pg6_sheet_removed"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_removed"
    output:
      - "pg6_sheet_removed_a.xlsx"
      - "pg6_sheet_removed_b.xlsx"

  - id: "pg6_sheet_renamed"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_renamed"
    output:
      - "pg6_sheet_renamed_a.xlsx"
      - "pg6_sheet_renamed_b.xlsx"

  - id: "pg6_sheet_and_grid_change"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_and_grid_change"
    output:
      - "pg6_sheet_and_grid_change_a.xlsx"
      - "pg6_sheet_and_grid_change_b.xlsx"

  # --- Milestone 2.2: Base64 Correctness ---
  - id: "corrupt_base64"
    generator: "mashup_corrupt"
    args: 
      base_file: "templates/base_query.xlsx"
      mode: "byte_flip"
    output: "corrupt_base64.xlsx"

  - id: "duplicate_datamashup_parts"
    generator: "mashup_duplicate"
    args:
      base_file: "templates/base_query.xlsx"
    output: "duplicate_datamashup_parts.xlsx"

  - id: "duplicate_datamashup_elements"
    generator: "mashup_duplicate"
    args:
      base_file: "templates/base_query.xlsx"
      mode: "element"
    output: "duplicate_datamashup_elements.xlsx"

  - id: "mashup_utf16_le"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      encoding: "utf-16-le"
    output: "mashup_utf16_le.xlsx"

  - id: "mashup_utf16_be"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      encoding: "utf-16-be"
    output: "mashup_utf16_be.xlsx"

  - id: "mashup_base64_whitespace"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      whitespace: true
    output: "mashup_base64_whitespace.xlsx"

  # --- Milestone 4.1: PackageParts ---
  - id: "m4_packageparts_one_query"
    generator: "mashup:one_query"
    args:
      base_file: "templates/base_query.xlsx"
    output: "one_query.xlsx"

  - id: "m4_packageparts_multi_embedded"
    generator: "mashup:multi_query_with_embedded"
    args:
      base_file: "templates/base_query.xlsx"
    output: "multi_query_with_embedded.xlsx"

  # --- Milestone 4.2-4.4: Permissions / Metadata ---
  - id: "permissions_defaults"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_defaults"
      base_file: "templates/base_query.xlsx"
    output: "permissions_defaults.xlsx"

  - id: "permissions_firewall_off"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_firewall_off"
      base_file: "templates/base_query.xlsx"
    output: "permissions_firewall_off.xlsx"

  - id: "metadata_simple"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_simple"
      base_file: "templates/base_query.xlsx"
    output: "metadata_simple.xlsx"

  - id: "metadata_query_groups"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_query_groups"
      base_file: "templates/base_query.xlsx"
    output: "metadata_query_groups.xlsx"

  - id: "metadata_hidden_queries"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_hidden_queries"
      base_file: "templates/base_query.xlsx"
    output: "metadata_hidden_queries.xlsx"

  - id: "metadata_missing_entry"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_missing_entry"
      base_file: "templates/base_query.xlsx"
    output: "metadata_missing_entry.xlsx"

  - id: "metadata_url_encoding"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_url_encoding"
      base_file: "templates/base_query.xlsx"
    output: "metadata_url_encoding.xlsx"

  - id: "metadata_orphan_entries"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_orphan_entries"
      base_file: "templates/base_query.xlsx"
    output: "metadata_orphan_entries.xlsx"

  # --- Milestone 6: Basic M Diffs ---
  - id: "m_add_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_add_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_add_query_a.xlsx"

  - id: "m_add_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_add_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_add_query_b.xlsx"

  - id: "m_remove_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_remove_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_remove_query_a.xlsx"

  - id: "m_remove_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_remove_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_remove_query_b.xlsx"

  - id: "m_change_literal_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_change_literal_a"
      base_file: "templates/base_query.xlsx"
    output: "m_change_literal_a.xlsx"

  - id: "m_change_literal_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_change_literal_b"
      base_file: "templates/base_query.xlsx"
    output: "m_change_literal_b.xlsx"

  - id: "m_metadata_only_change_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_metadata_only_change_a"
      base_file: "templates/base_query.xlsx"
    output: "m_metadata_only_change_a.xlsx"

  - id: "m_metadata_only_change_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_metadata_only_change_b"
      base_file: "templates/base_query.xlsx"
    output: "m_metadata_only_change_b.xlsx"

  - id: "m_def_and_metadata_change_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_def_and_metadata_change_a"
      base_file: "templates/base_query.xlsx"
    output: "m_def_and_metadata_change_a.xlsx"

  - id: "m_def_and_metadata_change_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_def_and_metadata_change_b"
      base_file: "templates/base_query.xlsx"
    output: "m_def_and_metadata_change_b.xlsx"

  - id: "m_rename_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_rename_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_rename_query_a.xlsx"

  - id: "m_rename_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_rename_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_rename_query_b.xlsx"

  # --- Milestone 7: M AST canonicalization ---
  - id: "m_formatting_only_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_a"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_a.xlsx"

  - id: "m_formatting_only_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_b"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_b.xlsx"

  - id: "m_formatting_only_b_variant"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_b_variant"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_b_variant.xlsx"

  # --- Milestone 8: M Parser Expansion ---
  - id: "m_record_equiv_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_record_equiv_a"
      base_file: "templates/base_query.xlsx"
    output: "m_record_equiv_a.xlsx"

  - id: "m_record_equiv_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_record_equiv_b"
      base_file: "templates/base_query.xlsx"
    output: "m_record_equiv_b.xlsx"

  - id: "m_list_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_list_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_list_formatting_a.xlsx"

  - id: "m_list_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_list_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_list_formatting_b.xlsx"

  - id: "m_call_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_call_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_call_formatting_a.xlsx"

  - id: "m_call_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_call_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_call_formatting_b.xlsx"

  - id: "m_primitive_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_primitive_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_primitive_formatting_a.xlsx"

  - id: "m_primitive_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_primitive_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_primitive_formatting_b.xlsx"

  # --- P1: Large Dense Grid (Performance Baseline) ---
  - id: "p1_large_dense"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 20
      mode: "dense" # Deterministic "R1C1" style data
    output: "grid_large_dense.xlsx"

  # --- P2: Large Noise Grid (Worst-case Alignment) ---
  - id: "p2_large_noise"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 20
      mode: "noise" # Random float data
      seed: 12345
    output: "grid_large_noise.xlsx"

  # --- D1: Keyed Equality (Database Mode) ---
  # File A: Ordered IDs 1..1000
  - id: "db_equal_ordered_a"
    generator: "db_keyed"
    args: { count: 1000, shuffle: false, seed: 42 }
    output: "db_equal_ordered_a.xlsx"

  # File B: Same data, random order (Tests O(N) alignment)
  - id: "db_equal_ordered_b"
    generator: "db_keyed"
    args: { count: 1000, shuffle: true, seed: 42 }
    output: "db_equal_ordered_b.xlsx"

  # --- D2: Row Added (Database Mode) ---
  - id: "db_row_added_b"
    generator: "db_keyed"
    args: 
      count: 1000 
      seed: 42 
      extra_rows: [{id: 1001, name: "New Row", amount: 999}]
    output: "db_row_added_b.xlsx"

  # --- D3: Row Update (Database Mode) ---
  - id: "db_row_update_b"
    generator: "db_keyed"
    args:
      count: 1000
      seed: 42
      updates:
        - { id: 7, amount: 120 }
    output: "db_row_update_b.xlsx"

  # --- D4: Reorder + Change (Database Mode) ---
  - id: "db_reorder_and_change_b"
    generator: "db_keyed"
    args:
      count: 1000
      seed: 42
      shuffle: true
      updates:
        - { id: 7, amount: 120 }
    output: "db_reorder_and_change_b.xlsx"

  # --- P3: Adversarial Repetitive Grid (RLE stress test) ---
  - id: "p3_adversarial_repetitive"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 50
      mode: "repetitive"
      pattern_length: 100
      seed: 99999
    output: "grid_adversarial_repetitive.xlsx"

  # --- P4: 99% Blank Grid (Sparse stress test) ---
  - id: "p4_99_percent_blank"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 100
      mode: "sparse"
      fill_percent: 1
      seed: 77777
    output: "grid_99_percent_blank.xlsx"

  # --- P5: Identical Grids (Fast-path baseline) ---
  - id: "p5_identical"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 100
      mode: "dense"
    output: "grid_identical.xlsx"

  # --- Branch 4: Workbook Object Graph ---
  - id: "branch4_named_ranges"
    generator: "named_ranges"
    output:
      - "named_ranges_a.xlsx"
      - "named_ranges_b.xlsx"

  - id: "branch4_charts"
    generator: "charts"
    output:
      - "charts_a.xlsx"
      - "charts_b.xlsx"

  - id: "branch4_vba_base"
    generator: "copy_template"
    args:
      template: "templates/vba_base.xlsm"
    output: "vba_base.xlsm"

  - id: "branch4_vba_added"
    generator: "copy_template"
    args:
      template: "templates/vba_added.xlsm"
    output: "vba_added.xlsm"

  - id: "branch4_vba_changed"
    generator: "copy_template"
    args:
      template: "templates/vba_changed.xlsm"
    output: "vba_changed.xlsm"

```

---

### File: `fixtures\manifest_cli_tests.yaml`

```yaml
scenarios:
  # --- Phase 1.1: Basic File Opening ---
  - id: "smoke_minimal"
    generator: "basic_grid"
    args: { rows: 1, cols: 1 }
    output: "minimal.xlsx"

  # --- Phase 1.2: Is this a ZIP? ---
  - id: "container_random_zip"
    generator: "corrupt_container"
    args: { mode: "random_zip" }
    output: "random_zip.zip"
    
  - id: "container_no_content_types"
    generator: "corrupt_container"
    args: { mode: "no_content_types" }
    output: "no_content_types.xlsx"

  - id: "container_not_zip_text"
    generator: "corrupt_container"
    args: { mode: "not_zip_text" }
    output: "not_a_zip.txt"

  - id: "xlsb_stub"
    generator: "xlsb_stub"
    output: "xlsb_stub.xlsb"

  # --- PG1: Workbook -> Sheet -> Grid IR sanity ---
  - id: "pg1_basic_two_sheets"
    generator: "basic_grid"
    args: { rows: 3, cols: 3, two_sheets: true } # Sheet1 3x3, Sheet2 5x2 (logic in generator)
    output: "pg1_basic_two_sheets.xlsx"

  - id: "pg1_sparse"
    generator: "sparse_grid"
    output: "pg1_sparse_used_range.xlsx"

  - id: "pg1_mixed"
    generator: "edge_case"
    output: "pg1_empty_and_mixed_sheets.xlsx"

  # --- PG2: Addressing and index invariants ---
  - id: "pg2_addressing"
    generator: "address_sanity"
    args:
      targets: ["A1", "B2", "C3", "Z1", "Z10", "AA1", "AA10", "AB7", "AZ5", "BA1", "ZZ10", "AAA1"]
    output: "pg2_addressing_matrix.xlsx"

  # --- PG3: Cell snapshots and comparison semantics ---
  - id: "pg3_types"
    generator: "value_formula"
    output: "pg3_value_and_formula_cells.xlsx"

  # --- Phase 3: Spreadsheet-mode G1/G2 ---
  - id: "g1_equal_sheet"
    generator: "basic_grid"
    args:
      rows: 5
      cols: 5
      sheet: "Sheet1"
    output:
      - "equal_sheet_a.xlsx"
      - "equal_sheet_b.xlsx"

  - id: "g2_single_cell_value"
    generator: "single_cell_diff"
    args:
      rows: 5
      cols: 5
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: 1.0
      value_b: 2.0
    output:
      - "single_cell_value_a.xlsx"
      - "single_cell_value_b.xlsx"

  # --- Phase 3: Spreadsheet-mode G5-G7 ---

  - id: "g5_multi_cell_edits"
    generator: "multi_cell_diff"
    args:
      rows: 20
      cols: 10
      sheet: "Sheet1"
      edits:
        - { addr: "B2", value_a: 1.0, value_b: 42.0 }
        - { addr: "D5", value_a: 2.0, value_b: 99.0 }
        - { addr: "H7", value_a: 3.0, value_b: 3.5 }
        - { addr: "J10", value_a: "x", value_b: "y" }
    output:
      - "multi_cell_edits_a.xlsx"
      - "multi_cell_edits_b.xlsx"

  - id: "g6_row_append_bottom"
    generator: "grid_tail_diff"
    args:
      mode: "row_append_bottom"
      sheet: "Sheet1"
      base_rows: 10
      tail_rows: 2
    output:
      - "row_append_bottom_a.xlsx"
      - "row_append_bottom_b.xlsx"

  - id: "g6_row_delete_bottom"
    generator: "grid_tail_diff"
    args:
      mode: "row_delete_bottom"
      sheet: "Sheet1"
      base_rows: 10
      tail_rows: 2
    output:
      - "row_delete_bottom_a.xlsx"
      - "row_delete_bottom_b.xlsx"

  - id: "g7_col_append_right"
    generator: "grid_tail_diff"
    args:
      mode: "col_append_right"
      sheet: "Sheet1"
      base_cols: 4
      tail_cols: 2
    output:
      - "col_append_right_a.xlsx"
      - "col_append_right_b.xlsx"

  - id: "g7_col_delete_right"
    generator: "grid_tail_diff"
    args:
      mode: "col_delete_right"
      sheet: "Sheet1"
      base_cols: 4
      tail_cols: 2
    output:
      - "col_delete_right_a.xlsx"
      - "col_delete_right_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G8 ---
  - id: "g8_row_insert_middle"
    generator: "row_alignment_g8"
    args:
      mode: "insert"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      insert_at: 6
    output:
      - "row_insert_middle_a.xlsx"
      - "row_insert_middle_b.xlsx"

  - id: "g8_row_delete_middle"
    generator: "row_alignment_g8"
    args:
      mode: "delete"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      delete_row: 6
    output:
      - "row_delete_middle_a.xlsx"
      - "row_delete_middle_b.xlsx"

  - id: "g8_row_insert_with_edit_below"
    generator: "row_alignment_g8"
    args:
      mode: "insert_with_edit"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      insert_at: 6
      edit_row: 8
      edit_col: 3
    output:
      - "row_insert_with_edit_a.xlsx"
      - "row_insert_with_edit_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G9 ---
  - id: "g9_col_insert_middle"
    generator: "column_alignment_g9"
    args:
      mode: "insert"
      sheet: "Data"
      cols: 8
      data_rows: 9
      insert_at: 4
    output:
      - "col_insert_middle_a.xlsx"
      - "col_insert_middle_b.xlsx"

  - id: "g9_col_delete_middle"
    generator: "column_alignment_g9"
    args:
      mode: "delete"
      sheet: "Data"
      cols: 8
      data_rows: 9
      delete_col: 4
    output:
      - "col_delete_middle_a.xlsx"
      - "col_delete_middle_b.xlsx"

  - id: "g9_col_insert_with_edit"
    generator: "column_alignment_g9"
    args:
      mode: "insert_with_edit"
      sheet: "Data"
      cols: 8
      data_rows: 9
      insert_at: 4
      edit_row: 8
      edit_col_after_insert: 7
    output:
      - "col_insert_with_edit_a.xlsx"
      - "col_insert_with_edit_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G10 ---
  - id: "g10_row_block_insert"
    generator: "row_alignment_g10"
    args:
      mode: "block_insert"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      block_rows: 4
      insert_at: 4
    output:
      - "row_block_insert_a.xlsx"
      - "row_block_insert_b.xlsx"

  - id: "g10_row_block_delete"
    generator: "row_alignment_g10"
    args:
      mode: "block_delete"
      sheet: "Sheet1"
      base_rows: 10
      cols: 5
      block_rows: 4
      delete_start: 4
    output:
      - "row_block_delete_a.xlsx"
      - "row_block_delete_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G11 ---
  - id: "g11_row_block_move"
    generator: "row_block_move_g11"
    args:
      sheet: "Sheet1"
      total_rows: 20
      cols: 5
      block_rows: 4
      src_start: 5    # 1-based in A
      dst_start: 13   # 1-based in B
    output:
      - "row_block_move_a.xlsx"
      - "row_block_move_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G12 (column move only - G12a) ---
  - id: "g12_column_block_move"
    generator: "column_move_g12"
    args:
      sheet: "Data"
      cols: 8
      data_rows: 9
      src_col: 3      # 1-based: C
      dst_col: 6      # 1-based: F
    output:
      - "column_move_a.xlsx"
      - "column_move_b.xlsx"

  - id: "g12_rect_block_move"
    generator: "rect_block_move_g12"
    args:
      sheet: "Data"
      rows: 15
      cols: 15
      src_top: 3      # 1-based row in A (Excel row 3)
      src_left: 2     # 1-based column in A (Excel column B)
      dst_top: 10     # 1-based row in B (Excel row 10)
      dst_left: 7     # 1-based column in B (Excel column G)
      block_rows: 3
      block_cols: 3
    output:
      - "rect_block_move_a.xlsx"
      - "rect_block_move_b.xlsx"

  # --- Phase 4: Spreadsheet-mode G13 ---
  - id: "g13_fuzzy_row_move"
    generator: "row_fuzzy_move_g13"
    args:
      sheet: "Data"
      total_rows: 24
      cols: 6
      block_rows: 4
      src_start: 5      # 1-based in A
      dst_start: 14     # 1-based in B
      edits:
        - { row_offset: 1, col: 3, delta: 1 }
    output:
      - "grid_move_and_edit_a.xlsx"
      - "grid_move_and_edit_b.xlsx"

  # --- JSON diff: simple non-empty change ---
  - id: "json_diff_single_cell"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: "1"
      value_b: "2"
    output:
      - "json_diff_single_cell_a.xlsx"
      - "json_diff_single_cell_b.xlsx"

  - id: "json_diff_single_bool"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: true
      value_b: false
    output:
      - "json_diff_bool_a.xlsx"
      - "json_diff_bool_b.xlsx"

  - id: "json_diff_value_to_empty"
    generator: "single_cell_diff"
    args:
      rows: 3
      cols: 3
      sheet: "Sheet1"
      target_cell: "C3"
      value_a: "1"
      value_b: null
    output:
      - "json_diff_value_to_empty_a.xlsx"
      - "json_diff_value_to_empty_b.xlsx"

  # --- Sheet identity: case-only renames ---
  - id: "sheet_case_only_rename"
    generator: "sheet_case_rename"
    args:
      sheet_a: "Sheet1"
      sheet_b: "sheet1"
      cell: "A1"
      value_a: 1.0
      value_b: 1.0
    output:
      - "sheet_case_only_rename_a.xlsx"
      - "sheet_case_only_rename_b.xlsx"

  - id: "sheet_case_only_rename_cell_edit"
    generator: "sheet_case_rename"
    args:
      sheet_a: "Sheet1"
      sheet_b: "sheet1"
      cell: "A1"
      value_a: 1.0
      value_b: 2.0
    output:
      - "sheet_case_only_rename_edit_a.xlsx"
      - "sheet_case_only_rename_edit_b.xlsx"

  # --- PG6: Object graph vs grid responsibilities ---
  - id: "pg6_sheet_added"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_added"
    output:
      - "pg6_sheet_added_a.xlsx"
      - "pg6_sheet_added_b.xlsx"

  - id: "pg6_sheet_removed"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_removed"
    output:
      - "pg6_sheet_removed_a.xlsx"
      - "pg6_sheet_removed_b.xlsx"

  - id: "pg6_sheet_renamed"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_renamed"
    output:
      - "pg6_sheet_renamed_a.xlsx"
      - "pg6_sheet_renamed_b.xlsx"

  - id: "pg6_sheet_and_grid_change"
    generator: "pg6_sheet_scenario"
    args:
      mode: "sheet_and_grid_change"
    output:
      - "pg6_sheet_and_grid_change_a.xlsx"
      - "pg6_sheet_and_grid_change_b.xlsx"

  # --- Milestone 2.2: Base64 Correctness ---
  - id: "corrupt_base64"
    generator: "mashup_corrupt"
    args: 
      base_file: "templates/base_query.xlsx"
      mode: "byte_flip"
    output: "corrupt_base64.xlsx"

  - id: "duplicate_datamashup_parts"
    generator: "mashup_duplicate"
    args:
      base_file: "templates/base_query.xlsx"
    output: "duplicate_datamashup_parts.xlsx"

  - id: "duplicate_datamashup_elements"
    generator: "mashup_duplicate"
    args:
      base_file: "templates/base_query.xlsx"
      mode: "element"
    output: "duplicate_datamashup_elements.xlsx"

  - id: "mashup_utf16_le"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      encoding: "utf-16-le"
    output: "mashup_utf16_le.xlsx"

  - id: "mashup_utf16_be"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      encoding: "utf-16-be"
    output: "mashup_utf16_be.xlsx"

  - id: "mashup_base64_whitespace"
    generator: "mashup_encode"
    args:
      base_file: "templates/base_query.xlsx"
      whitespace: true
    output: "mashup_base64_whitespace.xlsx"

  # --- Milestone 4.1: PackageParts ---
  - id: "m4_packageparts_one_query"
    generator: "mashup:one_query"
    args:
      base_file: "templates/base_query.xlsx"
    output: "one_query.xlsx"

  - id: "m4_packageparts_multi_embedded"
    generator: "mashup:multi_query_with_embedded"
    args:
      base_file: "templates/base_query.xlsx"
    output: "multi_query_with_embedded.xlsx"

  # --- Milestone 4.2-4.4: Permissions / Metadata ---
  - id: "permissions_defaults"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_defaults"
      base_file: "templates/base_query.xlsx"
    output: "permissions_defaults.xlsx"

  - id: "permissions_firewall_off"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_firewall_off"
      base_file: "templates/base_query.xlsx"
    output: "permissions_firewall_off.xlsx"

  - id: "dpapi_blob_present"
    generator: "mashup:permissions_metadata"
    args:
      mode: "permissions_firewall_off"
      base_file: "templates/base_query.xlsx"
      bindings_override: "01020304"
    output: "dpapi_blob_present.xlsx"

  - id: "metadata_simple"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_simple"
      base_file: "templates/base_query.xlsx"
    output: "metadata_simple.xlsx"

  - id: "metadata_query_groups"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_query_groups"
      base_file: "templates/base_query.xlsx"
    output: "metadata_query_groups.xlsx"

  - id: "metadata_hidden_queries"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_hidden_queries"
      base_file: "templates/base_query.xlsx"
    output: "metadata_hidden_queries.xlsx"

  - id: "metadata_missing_entry"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_missing_entry"
      base_file: "templates/base_query.xlsx"
    output: "metadata_missing_entry.xlsx"

  - id: "metadata_url_encoding"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_url_encoding"
      base_file: "templates/base_query.xlsx"
    output: "metadata_url_encoding.xlsx"

  - id: "metadata_orphan_entries"
    generator: "mashup:permissions_metadata"
    args:
      mode: "metadata_orphan_entries"
      base_file: "templates/base_query.xlsx"
    output: "metadata_orphan_entries.xlsx"

  # --- Milestone 6: Basic M Diffs ---
  - id: "m_add_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_add_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_add_query_a.xlsx"

  - id: "m_add_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_add_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_add_query_b.xlsx"

  - id: "m_remove_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_remove_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_remove_query_a.xlsx"

  - id: "m_remove_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_remove_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_remove_query_b.xlsx"

  - id: "m_change_literal_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_change_literal_a"
      base_file: "templates/base_query.xlsx"
    output: "m_change_literal_a.xlsx"

  - id: "m_change_literal_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_change_literal_b"
      base_file: "templates/base_query.xlsx"
    output: "m_change_literal_b.xlsx"

  - id: "m_metadata_only_change_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_metadata_only_change_a"
      base_file: "templates/base_query.xlsx"
    output: "m_metadata_only_change_a.xlsx"

  - id: "m_metadata_only_change_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_metadata_only_change_b"
      base_file: "templates/base_query.xlsx"
    output: "m_metadata_only_change_b.xlsx"

  - id: "m_def_and_metadata_change_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_def_and_metadata_change_a"
      base_file: "templates/base_query.xlsx"
    output: "m_def_and_metadata_change_a.xlsx"

  - id: "m_def_and_metadata_change_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_def_and_metadata_change_b"
      base_file: "templates/base_query.xlsx"
    output: "m_def_and_metadata_change_b.xlsx"

  - id: "m_rename_query_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_rename_query_a"
      base_file: "templates/base_query.xlsx"
    output: "m_rename_query_a.xlsx"

  - id: "m_rename_query_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_rename_query_b"
      base_file: "templates/base_query.xlsx"
    output: "m_rename_query_b.xlsx"

  # --- Milestone 7: M AST canonicalization ---
  - id: "m_formatting_only_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_a"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_a.xlsx"

  - id: "m_formatting_only_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_b"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_b.xlsx"

  - id: "m_formatting_only_b_variant"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_formatting_only_b_variant"
      base_file: "templates/base_query.xlsx"
    output: "m_formatting_only_b_variant.xlsx"

  # --- Milestone 8: M Parser Expansion ---
  - id: "m_record_equiv_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_record_equiv_a"
      base_file: "templates/base_query.xlsx"
    output: "m_record_equiv_a.xlsx"

  - id: "m_record_equiv_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_record_equiv_b"
      base_file: "templates/base_query.xlsx"
    output: "m_record_equiv_b.xlsx"

  - id: "m_list_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_list_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_list_formatting_a.xlsx"

  - id: "m_list_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_list_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_list_formatting_b.xlsx"

  - id: "m_call_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_call_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_call_formatting_a.xlsx"

  - id: "m_call_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_call_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_call_formatting_b.xlsx"

  - id: "m_primitive_formatting_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_primitive_formatting_a"
      base_file: "templates/base_query.xlsx"
    output: "m_primitive_formatting_a.xlsx"

  - id: "m_primitive_formatting_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_primitive_formatting_b"
      base_file: "templates/base_query.xlsx"
    output: "m_primitive_formatting_b.xlsx"

  # --- P1: Large Dense Grid (Performance Baseline) ---
  - id: "p1_large_dense"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 20
      mode: "dense" # Deterministic "R1C1" style data
    output: "grid_large_dense.xlsx"

  # --- P2: Large Noise Grid (Worst-case Alignment) ---
  - id: "p2_large_noise"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 20
      mode: "noise" # Random float data
      seed: 12345
    output: "grid_large_noise.xlsx"

  # --- D1: Keyed Equality (Database Mode) ---
  # File A: Ordered IDs 1..1000
  - id: "db_equal_ordered_a"
    generator: "db_keyed"
    args: { count: 1000, shuffle: false, seed: 42 }
    output: "db_equal_ordered_a.xlsx"

  # File B: Same data, random order (Tests O(N) alignment)
  - id: "db_equal_ordered_b"
    generator: "db_keyed"
    args: { count: 1000, shuffle: true, seed: 42 }
    output: "db_equal_ordered_b.xlsx"

  # --- D2: Row Added (Database Mode) ---
  - id: "db_row_added_b"
    generator: "db_keyed"
    args: 
      count: 1000 
      seed: 42 
      extra_rows: [{id: 1001, name: "New Row", amount: 999}]
    output: "db_row_added_b.xlsx"

  # --- D3: Row Update (Database Mode) ---
  - id: "db_row_update_b"
    generator: "db_keyed"
    args:
      count: 1000
      seed: 42
      updates:
        - { id: 7, amount: 120 }
    output: "db_row_update_b.xlsx"

  # --- D4: Reorder + Change (Database Mode) ---
  - id: "db_reorder_and_change_b"
    generator: "db_keyed"
    args:
      count: 1000
      seed: 42
      shuffle: true
      updates:
        - { id: 7, amount: 120 }
    output: "db_reorder_and_change_b.xlsx"

  # --- P3: Adversarial Repetitive Grid (RLE stress test) ---
  - id: "p3_adversarial_repetitive"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 50
      mode: "repetitive"
      pattern_length: 100
      seed: 99999
    output: "grid_adversarial_repetitive.xlsx"

  # --- P4: 99% Blank Grid (Sparse stress test) ---
  - id: "p4_99_percent_blank"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 100
      mode: "sparse"
      fill_percent: 1
      seed: 77777
    output: "grid_99_percent_blank.xlsx"

  # --- P5: Identical Grids (Fast-path baseline) ---
  - id: "p5_identical"
    generator: "perf_large"
    args: 
      rows: 50000 
      cols: 100
      mode: "dense"
    output: "grid_identical.xlsx"

  # --- Branch 4: Workbook Object Graph ---
  - id: "branch4_named_ranges"
    generator: "named_ranges"
    output:
      - "named_ranges_a.xlsx"
      - "named_ranges_b.xlsx"

  - id: "branch4_charts"
    generator: "charts"
    output:
      - "charts_a.xlsx"
      - "charts_b.xlsx"

  - id: "branch4_vba_base"
    generator: "copy_template"
    args:
      template: "templates/vba_base.xlsm"
    output: "vba_base.xlsm"

  - id: "branch4_vba_added"
    generator: "copy_template"
    args:
      template: "templates/vba_added.xlsm"
    output: "vba_added.xlsm"

  - id: "branch4_vba_changed"
    generator: "copy_template"
    args:
      template: "templates/vba_changed.xlsm"
    output: "vba_changed.xlsm"

  # --- Branch 1: PBIX/PBIT + embedded query fixtures ---
  - id: "m_embedded_change_a"
    generator: "mashup:multi_query_with_embedded"
    args:
      base_file: "templates/base_query.xlsx"
      embedded_guid: "efgh"
      embedded_section: |
        section Section1;
        shared Inner = let
          Source = 1
        in
          Source;
    output: "m_embedded_change_a.xlsx"

  - id: "m_embedded_change_b"
    generator: "mashup:multi_query_with_embedded"
    args:
      base_file: "templates/base_query.xlsx"
      embedded_guid: "efgh"
      embedded_section: |
        section Section1;
        shared Inner = let
          Source = 2
        in
          Source;
    output: "m_embedded_change_b.xlsx"

  - id: "branch1_pbix_legacy_one_query_a"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/m_change_literal_a.xlsx"
    output: "pbix_legacy_one_query_a.pbix"

  - id: "branch1_pbix_legacy_one_query_b"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/m_change_literal_b.xlsx"
    output: "pbix_legacy_one_query_b.pbix"

  - id: "branch1_pbix_legacy_multi_query_a"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/m_add_query_a.xlsx"
    output: "pbix_legacy_multi_query_a.pbix"

  - id: "branch1_pbix_legacy_multi_query_b"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/m_add_query_b.xlsx"
    output: "pbix_legacy_multi_query_b.pbix"

  - id: "branch1_pbix_no_datamashup"
    generator: "pbix"
    args:
      mode: "no_datamashup"
    output: "pbix_no_datamashup.pbix"

  - id: "branch1_pbix_no_datamashup_no_schema"
    generator: "pbix"
    args:
      mode: "no_schema"
    output: "pbix_no_datamashup_no_schema.pbix"

  - id: "branch1_pbit_model_a"
    generator: "pbix"
    args:
      mode: "no_datamashup"
      model_schema: |
        {
          "model": {
            "tables": [
              {
                "name": "Sales",
                "columns": [
                  { "name": "Amount", "dataType": "int64", "summarizeBy": "sum" },
                  { "name": "Net", "dataType": "decimal", "formatString": "0.00", "sortByColumn": "Amount" },
                  { "name": "CustomerId", "dataType": "int64", "isHidden": true },
                  { "name": "Obsolete", "dataType": "string" },
                  { "name": "CalcFmt", "dataType": "int64", "expression": "1 + 2" },
                  { "name": "CalcSemantic", "dataType": "int64", "expression": "Sales[Amount] + 1" }
                ],
                "measures": [
                  { "name": "Total Sales", "expression": "SUM(Sales[Amount])" }
                ]
              },
              {
                "name": "Customers",
                "columns": [
                  { "name": "Id", "dataType": "int64" }
                ]
              },
              {
                "name": "Legacy",
                "columns": [
                  { "name": "OldCol", "dataType": "string" }
                ]
              }
            ],
            "relationships": [
              {
                "fromTable": "Sales",
                "fromColumn": "CustomerId",
                "toTable": "Customers",
                "toColumn": "Id",
                "crossFilteringBehavior": "oneDirection",
                "cardinality": "ManyToOne",
                "isActive": false
              }
            ]
          }
        }
    output: "pbit_model_a.pbit"

  - id: "branch1_pbit_model_b"
    generator: "pbix"
    args:
      mode: "no_datamashup"
      model_schema: |
        {
          "model": {
            "tables": [
              {
                "name": "Sales",
                "columns": [
                  { "name": "Amount", "dataType": "decimal", "summarizeBy": "average" },
                  { "name": "Net", "dataType": "decimal", "formatString": "$0.00", "sortByColumn": "Discount" },
                  { "name": "CustomerId", "dataType": "int64", "isHidden": false },
                  { "name": "Discount", "dataType": "decimal" },
                  { "name": "RegionId", "dataType": "int64" },
                  { "name": "CalcFmt", "dataType": "int64", "expression": "1+2" },
                  { "name": "CalcSemantic", "dataType": "int64", "expression": "Sales[Amount] + 2" }
                ],
                "measures": [
                  { "name": "Total Sales", "expression": "SUMX(Sales, Sales[Amount])" },
                  { "name": "Net Sales", "expression": "SUM(Sales[Net])" }
                ]
              },
              {
                "name": "Customers",
                "columns": [
                  { "name": "Id", "dataType": "int64" }
                ]
              },
              {
                "name": "Regions",
                "columns": [
                  { "name": "Id", "dataType": "int64" }
                ]
              }
            ],
            "relationships": [
              {
                "fromTable": "Sales",
                "fromColumn": "CustomerId",
                "toTable": "Customers",
                "toColumn": "Id",
                "crossFilteringBehavior": "bothDirections",
                "cardinality": "ManyToOne",
                "isActive": true
              },
              {
                "fromTable": "Sales",
                "fromColumn": "RegionId",
                "toTable": "Regions",
                "toColumn": "Id",
                "crossFilteringBehavior": "oneDirection",
                "cardinality": "ManyToOne",
                "isActive": true
              }
            ]
          }
        }
    output: "pbit_model_b.pbit"

  - id: "branch2_pbix_embedded_queries"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/m_embedded_change_a.xlsx"
    output: "pbix_embedded_queries.pbix"

  # --- Composed fixtures: grid + mashup + embedded queries ---
  - id: "m_composed_mashup_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "composed_mashup_a"
      base_file: "generated/m_embedded_change_a.xlsx"
      embedded_guid: "efgh"
    output: "composed_mashup_a.xlsx"

  - id: "m_composed_mashup_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "composed_mashup_b"
      base_file: "generated/m_embedded_change_a.xlsx"
      embedded_guid: "efgh"
    output: "composed_mashup_b.xlsx"

  - id: "m_adversarial_steps_a"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_adversarial_steps_a"
      base_file: "templates/base_query.xlsx"
    output: "m_adversarial_steps_a.xlsx"

  - id: "m_adversarial_steps_b"
    generator: "mashup:permissions_metadata"
    args:
      mode: "m_adversarial_steps_b"
      base_file: "templates/base_query.xlsx"
    output: "m_adversarial_steps_b.xlsx"

  - id: "composed_grid_mashup_a"
    generator: "mashup_attach"
    args:
      base_file: "generated/row_insert_with_edit_a.xlsx"
      mashup_file: "generated/composed_mashup_a.xlsx"
    output: "composed_grid_mashup_a.xlsx"

  - id: "composed_grid_mashup_b"
    generator: "mashup_attach"
    args:
      base_file: "generated/row_insert_with_edit_b.xlsx"
      mashup_file: "generated/composed_mashup_b.xlsx"
    output: "composed_grid_mashup_b.xlsx"

  - id: "pbix_composed_a"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/composed_mashup_a.xlsx"
    output: "pbix_composed_a.pbix"

  - id: "pbix_composed_b"
    generator: "pbix"
    args:
      mode: "from_xlsx"
      base_file: "generated/composed_mashup_b.xlsx"
    output: "pbix_composed_b.pbix"

```

---

### File: `fixtures\manifest_perf_e2e.yaml`

```yaml
scenarios:
  - id: "e2e_p1_dense_a"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 20
      mode: "dense"
    output: "e2e_p1_dense_a.xlsx"

  - id: "e2e_p1_dense_b"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 20
      mode: "dense"
      edit_row: 25000
      edit_col: 10
      edit_value: "E2E_EDIT"
    output: "e2e_p1_dense_b.xlsx"

  - id: "e2e_p2_noise_a"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 20
      mode: "noise"
      seed: 12345
    output: "e2e_p2_noise_a.xlsx"

  - id: "e2e_p2_noise_b"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 20
      mode: "noise"
      seed: 12345
      edit_row: 25000
      edit_col: 10
      edit_value: 2.0
    output: "e2e_p2_noise_b.xlsx"

  - id: "e2e_p3_repetitive_a"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 50
      mode: "repetitive"
      pattern_length: 100
    output: "e2e_p3_repetitive_a.xlsx"

  - id: "e2e_p3_repetitive_b"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 50
      mode: "repetitive"
      pattern_length: 100
      edit_row: 25000
      edit_col: 25
      edit_value: "E2E_EDIT"
    output: "e2e_p3_repetitive_b.xlsx"

  - id: "e2e_p4_sparse_a"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 100
      mode: "sparse"
      fill_percent: 1
      seed: 77777
    output: "e2e_p4_sparse_a.xlsx"

  - id: "e2e_p4_sparse_b"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 100
      mode: "sparse"
      fill_percent: 1
      seed: 77777
      edit_row: 25000
      edit_col: 50
      edit_value: "E2E_EDIT"
    output: "e2e_p4_sparse_b.xlsx"

  - id: "e2e_p5_identical_a"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 100
      mode: "dense"
    output: "e2e_p5_identical_a.xlsx"

  - id: "e2e_p5_identical_b"
    generator: "perf_large"
    args:
      rows: 50000
      cols: 100
      mode: "dense"
    output: "e2e_p5_identical_b.xlsx"

```

---

### File: `fixtures\manifest_release_smoke.yaml`

```yaml
scenarios:
  - id: "smoke_minimal"
    generator: "basic_grid"
    args: { rows: 1, cols: 1 }
    output: "minimal.xlsx"

  - id: "g7_col_append_right"
    generator: "grid_tail_diff"
    args:
      mode: "col_append_right"
      sheet: "Sheet1"
      base_cols: 4
      tail_cols: 2
    output:
      - "col_append_right_a.xlsx"
      - "col_append_right_b.xlsx"

```

---

### File: `fixtures\pyproject.toml`

```toml
[project]
name = "excel-fixtures"
version = "0.1.0"
description = "Deterministic artifact generator for Excel Diff testing"
readme = "README.md"
requires-python = ">=3.9"
dependencies = [
    "openpyxl>=3.1.0",
    "lxml>=4.9.0",
    "jinja2>=3.1.0",
    "pyyaml>=6.0",
]

[project.scripts]
generate-fixtures = "src.generate:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]


```

---

### File: `fixtures\src\__init__.py`

```python


```

---

### File: `fixtures\src\generate.py`

```python
import argparse
import hashlib
import json
import shutil
import sys
import zipfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from xml.etree import ElementTree as ET

import yaml

# Import generators
try:
    from .generators.corrupt import ContainerCorruptGenerator
    from .generators.database import KeyedTableGenerator
    from .generators.grid import (
        AddressSanityGenerator,
        BasicGridGenerator,
        ColumnAlignmentG9Generator,
        ColumnMoveG12Generator,
        EdgeCaseGenerator,
        GridTailDiffGenerator,
        MultiCellDiffGenerator,
        Pg6SheetScenarioGenerator,
        RectBlockMoveG12Generator,
        RowAlignmentG10Generator,
        RowAlignmentG8Generator,
        RowBlockMoveG11Generator,
        RowFuzzyMoveG13Generator,
        SheetCaseRenameGenerator,
        SingleCellDiffGenerator,
        SparseGridGenerator,
        ValueFormulaGenerator,
    )
    from .generators.mashup import (
        MashupAttachGenerator,
        MashupCorruptGenerator,
        MashupDuplicateGenerator,
        MashupEncodeGenerator,
        MashupInjectGenerator,
        MashupMultiEmbeddedGenerator,
        MashupOneQueryGenerator,
        MashupPermissionsMetadataGenerator,
    )
    from .generators.pbix import PbixGenerator
    from .generators.objects import ChartsGenerator, CopyTemplateGenerator, NamedRangesGenerator
    from .generators.perf import LargeGridGenerator
    from .generators.xlsb import XlsbStubGenerator
except ImportError:
    from generators.corrupt import ContainerCorruptGenerator
    from generators.database import KeyedTableGenerator
    from generators.grid import (
        AddressSanityGenerator,
        BasicGridGenerator,
        ColumnAlignmentG9Generator,
        ColumnMoveG12Generator,
        EdgeCaseGenerator,
        GridTailDiffGenerator,
        MultiCellDiffGenerator,
        Pg6SheetScenarioGenerator,
        RectBlockMoveG12Generator,
        RowAlignmentG10Generator,
        RowAlignmentG8Generator,
        RowBlockMoveG11Generator,
        RowFuzzyMoveG13Generator,
        SheetCaseRenameGenerator,
        SingleCellDiffGenerator,
        SparseGridGenerator,
        ValueFormulaGenerator,
    )
    from generators.mashup import (
        MashupAttachGenerator,
        MashupCorruptGenerator,
        MashupDuplicateGenerator,
        MashupEncodeGenerator,
        MashupInjectGenerator,
        MashupMultiEmbeddedGenerator,
        MashupOneQueryGenerator,
        MashupPermissionsMetadataGenerator,
    )
    from generators.pbix import PbixGenerator
    from generators.objects import ChartsGenerator, CopyTemplateGenerator, NamedRangesGenerator
    from generators.perf import LargeGridGenerator
    from generators.xlsb import XlsbStubGenerator

# Registry of generators
GENERATORS: Dict[str, Any] = {
    "basic_grid": BasicGridGenerator,
    "sparse_grid": SparseGridGenerator,
    "edge_case": EdgeCaseGenerator,
    "address_sanity": AddressSanityGenerator,
    "value_formula": ValueFormulaGenerator,
    "single_cell_diff": SingleCellDiffGenerator,
    "multi_cell_diff": MultiCellDiffGenerator,
    "grid_tail_diff": GridTailDiffGenerator,
    "row_alignment_g8": RowAlignmentG8Generator,
    "row_alignment_g10": RowAlignmentG10Generator,
    "row_block_move_g11": RowBlockMoveG11Generator,
    "row_fuzzy_move_g13": RowFuzzyMoveG13Generator,
    "column_move_g12": ColumnMoveG12Generator,
    "rect_block_move_g12": RectBlockMoveG12Generator,
    "column_alignment_g9": ColumnAlignmentG9Generator,
    "sheet_case_rename": SheetCaseRenameGenerator,
    "pg6_sheet_scenario": Pg6SheetScenarioGenerator,
    "corrupt_container": ContainerCorruptGenerator,
    "mashup_corrupt": MashupCorruptGenerator,
    "mashup_attach": MashupAttachGenerator,
    "mashup_duplicate": MashupDuplicateGenerator,
    "mashup_inject": MashupInjectGenerator,
    "mashup_encode": MashupEncodeGenerator,
    "mashup:one_query": MashupOneQueryGenerator,
    "mashup:multi_query_with_embedded": MashupMultiEmbeddedGenerator,
    "mashup:permissions_metadata": MashupPermissionsMetadataGenerator,
    "pbix": PbixGenerator,
    "perf_large": LargeGridGenerator,
    "db_keyed": KeyedTableGenerator,
    "named_ranges": NamedRangesGenerator,
    "charts": ChartsGenerator,
    "copy_template": CopyTemplateGenerator,
    "xlsb_stub": XlsbStubGenerator,
}

FILE_ARG_KEYS = ("template", "base_file", "model_schema_file", "mashup_file")
ZIP_EXTENSIONS = {".xlsx", ".xlsm", ".xlsb", ".pbix", ".pbit", ".zip"}


def load_manifest(manifest_path: Path) -> Dict[str, Any]:
    if not manifest_path.exists():
        print(f"Error: Manifest file not found at {manifest_path}", file=sys.stderr)
        sys.exit(1)

    with open(manifest_path, "r", encoding="utf-8") as f:
        try:
            return yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(f"Error parsing manifest: {e}", file=sys.stderr)
            sys.exit(1)


def ensure_output_dir(output_dir: Path):
    output_dir.mkdir(parents=True, exist_ok=True)


def clean_output_dir(output_dir: Path):
    if not output_dir.exists():
        return
    resolved = output_dir.resolve()
    if resolved == Path(resolved.anchor):
        raise RuntimeError(f"Refusing to clean root directory: {resolved}")
    for child in output_dir.iterdir():
        if child.is_dir():
            shutil.rmtree(child)
        else:
            child.unlink()


def list_output_names(outputs: Any) -> List[str]:
    if isinstance(outputs, list):
        return [str(name) for name in outputs]
    if isinstance(outputs, str):
        return [outputs]
    return []


def scenario_label(scenario: Dict[str, Any], idx: int) -> str:
    return scenario.get("id") or f"index {idx}"


def resolve_fixture_path(path_value: str, fixtures_root: Path) -> Optional[Path]:
    candidate = Path(path_value)
    if candidate.exists():
        return candidate
    fallback = fixtures_root / path_value
    if fallback.exists():
        return fallback
    return None


def generated_dependency(path_value: str) -> Optional[Path]:
    parts = Path(path_value).parts
    if "generated" not in parts:
        return None
    idx = parts.index("generated")
    if idx + 1 >= len(parts):
        return None
    return Path(*parts[idx + 1 :])


def preflight_manifest(
    manifest: Dict[str, Any],
    output_dir: Path,
    fixtures_root: Path,
) -> List[str]:
    errors: List[str] = []
    scenarios = manifest.get("scenarios", [])
    output_to_info: Dict[str, str] = {}
    output_to_index: Dict[str, int] = {}

    for idx, scenario in enumerate(scenarios):
        label = scenario_label(scenario, idx)
        generator_name = scenario.get("generator")
        outputs = list_output_names(scenario.get("output"))

        if not scenario.get("id") or not generator_name or not outputs:
            errors.append(f"Scenario {label} is missing id, generator, or output.")
            continue

        if generator_name not in GENERATORS:
            errors.append(f"Scenario {label} uses unknown generator '{generator_name}'.")

        for name in outputs:
            if name in output_to_info:
                prev = output_to_info[name]
                errors.append(
                    f"Output '{name}' is duplicated in scenarios {prev} and {label}."
                )
            else:
                output_to_info[name] = label
                output_to_index[name] = idx

    for idx, scenario in enumerate(scenarios):
        label = scenario_label(scenario, idx)
        args = scenario.get("args", {}) or {}

        for key in FILE_ARG_KEYS:
            value = args.get(key)
            if not value:
                continue
            if not isinstance(value, str):
                errors.append(f"Scenario {label} arg '{key}' must be a string.")
                continue

            if key in ("base_file", "mashup_file"):
                dep = generated_dependency(value)
                if dep is not None:
                    dep_name = dep.as_posix()
                    if dep_name in output_to_index:
                        if output_to_index[dep_name] >= idx:
                            errors.append(
                                f"Scenario {label} depends on generated/{dep_name} "
                                "but it is not produced earlier in the manifest."
                            )
                    elif not (output_dir / dep).exists():
                        errors.append(
                            f"Scenario {label} depends on generated/{dep_name} "
                            "but it is not produced by this manifest or present in the output dir."
                        )
                else:
                    if resolve_fixture_path(value, fixtures_root) is None:
                        errors.append(
                            f"Scenario {label} arg '{key}' file '{value}' not found."
                        )
            else:
                if resolve_fixture_path(value, fixtures_root) is None:
                    errors.append(
                        f"Scenario {label} arg '{key}' file '{value}' not found."
                    )

    return errors


def normalize_core_xml(data: bytes) -> bytes:
    try:
        root = ET.fromstring(data)
    except ET.ParseError:
        return data

    for elem in root.iter():
        tag = elem.tag
        if tag.endswith("created") or tag.endswith("modified"):
            elem.text = "1970-01-01T00:00:00Z"

    return ET.tostring(root, encoding="utf-8", xml_declaration=True)


def hash_zip_contents(path: Path) -> str:
    hasher = hashlib.sha256()
    with zipfile.ZipFile(path, "r") as zin:
        entries = [info for info in zin.infolist() if not info.is_dir()]
        entries.sort(key=lambda info: info.filename)
        for info in entries:
            data = zin.read(info.filename)
            if info.filename == "docProps/core.xml":
                data = normalize_core_xml(data)
            entry_hash = hashlib.sha256(data).hexdigest()
            hasher.update(info.filename.encode("utf-8"))
            hasher.update(b"\0")
            hasher.update(entry_hash.encode("utf-8"))
            hasher.update(b"\n")
    return hasher.hexdigest()


def compute_checksum(path: Path) -> Tuple[str, str]:
    ext = path.suffix.lower()
    if ext in ZIP_EXTENSIONS:
        digest = hash_zip_contents(path)
        return digest, "zip-entries-v1"
    digest = hashlib.sha256(path.read_bytes()).hexdigest()
    return digest, "raw"


def collect_outputs_with_meta(
    manifest: Dict[str, Any],
) -> Dict[str, Dict[str, Any]]:
    output_map: Dict[str, Dict[str, Any]] = {}
    for idx, scenario in enumerate(manifest.get("scenarios", [])):
        outputs = list_output_names(scenario.get("output"))
        for name in outputs:
            output_map[name] = {
                "id": scenario_label(scenario, idx),
                "generator": scenario.get("generator"),
                "args": scenario.get("args", {}) or {},
            }
    return output_map


def verify_outputs(
    manifest: Dict[str, Any],
    output_dir: Path,
) -> List[str]:
    errors: List[str] = []
    output_map = collect_outputs_with_meta(manifest)

    for name, meta in output_map.items():
        path = output_dir / name
        if not path.exists():
            errors.append(f"Missing output: {name}")
            continue

        ext = path.suffix.lower()
        if ext in ZIP_EXTENSIONS:
            try:
                with zipfile.ZipFile(path, "r") as zin:
                    if ext in (".xlsx", ".xlsm"):
                        requires_content_types = True
                        if (
                            meta.get("generator") == "corrupt_container"
                            and meta.get("args", {}).get("mode") == "no_content_types"
                        ):
                            requires_content_types = False
                        if requires_content_types and "[Content_Types].xml" not in zin.namelist():
                            errors.append(
                                f"{name} is missing [Content_Types].xml"
                            )
            except zipfile.BadZipFile:
                errors.append(f"{name} is not a valid ZIP container")

    return errors


def write_lock_file(
    manifest_path: Path,
    manifest: Dict[str, Any],
    output_dir: Path,
    lock_path: Path,
) -> List[str]:
    errors: List[str] = []
    output_map = collect_outputs_with_meta(manifest)
    checksums: Dict[str, Dict[str, str]] = {}

    for name in sorted(output_map.keys()):
        path = output_dir / name
        if not path.exists():
            errors.append(f"Missing output: {name}")
            continue
        try:
            digest, mode = compute_checksum(path)
        except zipfile.BadZipFile:
            errors.append(f"{name} is not a valid ZIP container")
            continue
        checksums[name] = {
            "hash": f"sha256:{digest}",
            "mode": mode,
        }

    if errors:
        return errors

    payload = {
        "version": 1,
        "manifest": str(manifest_path).replace("\\", "/"),
        "output_dir": str(output_dir).replace("\\", "/"),
        "algorithm": "sha256",
        "files": checksums,
    }

    lock_path.parent.mkdir(parents=True, exist_ok=True)
    lock_path.write_text(json.dumps(payload, indent=2, sort_keys=True) + "\n", encoding="utf-8")
    return errors


def verify_lock_file(
    manifest_path: Path,
    manifest: Dict[str, Any],
    output_dir: Path,
    lock_path: Path,
) -> List[str]:
    errors: List[str] = []
    if not lock_path.exists():
        return [f"Lock file not found: {lock_path}"]

    try:
        lock = json.loads(lock_path.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        return [f"Failed to parse lock file {lock_path}: {e}"]

    expected_outputs = set(collect_outputs_with_meta(manifest).keys())
    lock_files = lock.get("files", {})
    if not isinstance(lock_files, dict):
        return [f"Lock file {lock_path} has invalid 'files' section"]

    if lock.get("manifest") and lock.get("manifest") != str(manifest_path).replace("\\", "/"):
        errors.append(
            f"Lock file manifest mismatch: {lock.get('manifest')} != {manifest_path}"
        )

    missing_in_lock = expected_outputs - set(lock_files.keys())
    extra_in_lock = set(lock_files.keys()) - expected_outputs

    if missing_in_lock:
        errors.append(
            "Lock file is missing entries for: " + ", ".join(sorted(missing_in_lock))
        )
    if extra_in_lock:
        errors.append(
            "Lock file has extra entries not in manifest: "
            + ", ".join(sorted(extra_in_lock))
        )

    for name in sorted(expected_outputs):
        path = output_dir / name
        if not path.exists():
            errors.append(f"Missing output: {name}")
            continue
        try:
            digest, mode = compute_checksum(path)
        except zipfile.BadZipFile:
            errors.append(f"{name} is not a valid ZIP container")
            continue

        expected = lock_files.get(name, {})
        expected_hash = expected.get("hash")
        expected_mode = expected.get("mode")
        actual_hash = f"sha256:{digest}"

        if expected_hash != actual_hash:
            errors.append(
                f"Checksum mismatch for {name}: expected {expected_hash}, got {actual_hash}"
            )
        if expected_mode and expected_mode != mode:
            errors.append(
                f"Checksum mode mismatch for {name}: expected {expected_mode}, got {mode}"
            )

    return errors


def generate_fixtures(
    manifest: Dict[str, Any],
    output_dir: Path,
    force: bool,
) -> List[str]:
    errors: List[str] = []
    scenarios = manifest.get("scenarios", [])
    print(f"Found {len(scenarios)} scenarios in manifest.")

    for idx, scenario in enumerate(scenarios):
        label = scenario_label(scenario, idx)
        scenario_id = scenario.get("id")
        generator_name = scenario.get("generator")
        generator_args = scenario.get("args", {})
        outputs = scenario.get("output")
        output_names = list_output_names(outputs)

        if not scenario_id or not generator_name or not output_names:
            errors.append(f"Scenario {label} is missing id, generator, or output.")
            continue

        print(f"Processing scenario: {scenario_id} (Generator: {generator_name})")

        if generator_name not in GENERATORS:
            errors.append(f"Scenario {scenario_id}: unknown generator '{generator_name}'.")
            continue

        output_paths = [output_dir / name for name in output_names]
        existing = [path for path in output_paths if path.exists()]
        if existing and not force:
            names = ", ".join(path.name for path in existing)
            errors.append(
                f"Scenario {scenario_id} would overwrite existing outputs: {names}"
            )
            continue
        if force:
            for path in existing:
                if path.is_dir():
                    shutil.rmtree(path)
                else:
                    path.unlink()

        try:
            generator_class = GENERATORS[generator_name]
            generator = generator_class(generator_args)
            generator.generate(output_dir, outputs)
            print(f"  Success: Generated {outputs}")
        except Exception as e:
            errors.append(f"Scenario {scenario_id} failed: {e}")
            import traceback
            traceback.print_exc()

    return errors


def report_errors(errors: List[str]):
    for error in errors:
        print(f"Error: {error}", file=sys.stderr)


def main():
    script_dir = Path(__file__).parent.resolve()
    fixtures_root = script_dir.parent

    default_manifest = fixtures_root / "manifest.yaml"
    default_output = fixtures_root / "generated"

    parser = argparse.ArgumentParser(description="Generate Excel fixtures based on a manifest.")
    parser.add_argument(
        "--manifest",
        type=Path,
        default=default_manifest,
        help="Path to the manifest YAML file.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=default_output,
        help="Directory to output generated files.",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Overwrite existing outputs.",
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Delete existing outputs in the output directory before generating.",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Verify expected outputs exist and are structurally valid.",
    )
    parser.add_argument(
        "--write-lock",
        type=Path,
        help="Write a checksum lock file after generation.",
    )
    parser.add_argument(
        "--verify-lock",
        type=Path,
        help="Verify outputs against a checksum lock file.",
    )
    parser.add_argument(
        "--continue-on-error",
        action="store_true",
        help="Continue and exit 0 even if some scenarios fail.",
    )

    args = parser.parse_args()

    manifest = load_manifest(args.manifest)

    if args.clean:
        clean_output_dir(args.output_dir)

    preflight_errors = preflight_manifest(manifest, args.output_dir, fixtures_root)
    if preflight_errors:
        report_errors(preflight_errors)
        sys.exit(1)

    generate = True
    if args.verify or args.verify_lock:
        generate = False
    if args.write_lock:
        generate = True

    errors: List[str] = []

    if generate:
        ensure_output_dir(args.output_dir)
        errors.extend(generate_fixtures(manifest, args.output_dir, args.force))
        if errors and not args.continue_on_error:
            report_errors(errors)
            sys.exit(1)

    if args.verify:
        errors.extend(verify_outputs(manifest, args.output_dir))

    if args.verify_lock:
        errors.extend(
            verify_lock_file(
                args.manifest, manifest, args.output_dir, args.verify_lock
            )
        )

    if args.write_lock:
        errors.extend(
            write_lock_file(
                args.manifest, manifest, args.output_dir, args.write_lock
            )
        )

    if errors:
        report_errors(errors)
        if not args.continue_on_error:
            sys.exit(1)


if __name__ == "__main__":
    main()

```

---

### File: `fixtures\src\generators\__init__.py`

```python
# Generators package


```

---

### File: `fixtures\src\generators\base.py`

```python
"""Base classes for fixture generators."""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, Union, List


class BaseGenerator(ABC):
    """Abstract base class for all fixture generators."""

    def __init__(self, args: Dict[str, Any]):
        self.args = args

    @abstractmethod
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        """Generate the fixture file(s).

        Args:
            output_dir: The directory to save the file(s) in.
            output_names: The name(s) of the output file(s) as specified in the manifest.
        """
        pass

```

---

### File: `fixtures\src\generators\corrupt.py`

```python
import zipfile
import io
import random
from pathlib import Path
from typing import Union, List
from .base import BaseGenerator

class ContainerCorruptGenerator(BaseGenerator):
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        mode = self.args.get('mode', 'no_content_types')
        
        for name in output_names:
            # Create a dummy zip
            out_path = output_dir / name
            
            if mode == 'random_zip':
                # Just a zip with a text file
                with zipfile.ZipFile(out_path, 'w') as z:
                    z.writestr("hello.txt", "This is not excel")
                    
            elif mode == 'no_content_types':
                # Create a valid excel in memory, then strip [Content_Types].xml
                buffer = io.BytesIO()
                import openpyxl
                wb = openpyxl.Workbook()
                # Add some content just so it's not totally empty
                wb.active['A1'] = 1
                wb.save(buffer)
                buffer.seek(0)
                
                with zipfile.ZipFile(buffer, 'r') as zin:
                    with zipfile.ZipFile(out_path, 'w') as zout:
                        for item in zin.infolist():
                            if item.filename != "[Content_Types].xml":
                                zout.writestr(item, zin.read(item.filename))
            elif mode == 'not_zip_text':
                out_path.write_text("This is not a zip container", encoding="utf-8")
            else:
                raise ValueError(f"Unsupported corrupt_container mode: {mode}")


```

---

### File: `fixtures\src\generators\database.py`

```python
import openpyxl
import random
from pathlib import Path
from typing import Union, List, Dict, Any
from .base import BaseGenerator

class KeyedTableGenerator(BaseGenerator):
    """
    Generates datasets with Primary Keys (ID columns).
    Capable of shuffling rows to test O(N) alignment (Database Mode).
    
    Supports:
    - extra_rows: Add new rows with specified id/name/amount/category
    - updates: Modify existing rows by id (e.g., [{ id: 7, amount: 120 }])
    - shuffle: Randomize row order
    """
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        count = self.args.get('count', 100)
        shuffle = self.args.get('shuffle', False)
        seed = self.args.get('seed', 42)
        extra_rows = self.args.get('extra_rows', [])
        updates = self.args.get('updates', [])

        rng = random.Random(seed)

        for name in output_names:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = "Data"

            data_rows = []
            for i in range(1, count + 1):
                data_rows.append({
                    'id': i,
                    'name': f"Customer_{i}",
                    'amount': i * 10.5,
                    'category': rng.choice(['A', 'B', 'C'])
                })

            for row in extra_rows:
                data_rows.append(row)

            updates_by_id = {u['id']: u for u in updates}
            for row in data_rows:
                if row['id'] in updates_by_id:
                    upd = updates_by_id[row['id']]
                    for key in ['name', 'amount', 'category']:
                        if key in upd:
                            row[key] = upd[key]

            if shuffle:
                rng.shuffle(data_rows)

            headers = ['ID', 'Name', 'Amount', 'Category']
            ws.append(headers)

            for row in data_rows:
                ws.append([
                    row.get('id'),
                    row.get('name'),
                    row.get('amount'),
                    row.get('category')
                ])

            wb.save(output_dir / name)


```

---

### File: `fixtures\src\generators\grid.py`

```python
import openpyxl
import zipfile
import xml.etree.ElementTree as ET
from openpyxl.utils import get_column_letter
from pathlib import Path
from typing import Union, List, Dict, Any
from .base import BaseGenerator

class BasicGridGenerator(BaseGenerator):
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        rows = self.args.get('rows', 5)
        cols = self.args.get('cols', 5)
        two_sheets = self.args.get('two_sheets', False)
        
        for name in output_names:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = "Sheet1"
            
            # Fill grid
            for r in range(1, rows + 1):
                for c in range(1, cols + 1):
                    ws.cell(row=r, column=c, value=f"R{r}C{c}")
            
            # Check if we need a second sheet
            if two_sheets:
                ws2 = wb.create_sheet(title="Sheet2")
                # Different dimensions for Sheet2 (PG1 requirement: 5x2)
                # If args are customized we might need more logic, but for PG1 this is sufficient or we use defaults
                s2_rows = 5
                s2_cols = 2
                for r in range(1, s2_rows + 1):
                    for c in range(1, s2_cols + 1):
                         ws2.cell(row=r, column=c, value=f"S2_R{r}C{c}")

            wb.save(output_dir / name)

class SparseGridGenerator(BaseGenerator):
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        for name in output_names:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = "Sparse"
            
            # Specifics for pg1_sparse_used_range
            ws['A1'] = "A1"
            ws['B2'] = "B2"
            ws['G10'] = "G10" # Forces extent
            # Row 5 and Col D are empty implicitly by not writing to them
            
            wb.save(output_dir / name)

class EdgeCaseGenerator(BaseGenerator):
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
        
        for name in output_names:
            wb = openpyxl.Workbook()
            # Remove default sheet
            default_ws = wb.active
            wb.remove(default_ws)
            
            # Empty Sheet
            wb.create_sheet("Empty")
            
            # Values Only
            ws_val = wb.create_sheet("ValuesOnly")
            for r in range(1, 11):
                for c in range(1, 11):
                    ws_val.cell(row=r, column=c, value=r*c)
            
            # Formulas Only
            ws_form = wb.create_sheet("FormulasOnly")
            for r in range(1, 11):
                for c in range(1, 11):
                    # Reference ValuesOnly sheet
                    col_letter = get_column_letter(c)
                    ws_form.cell(row=r, column=c, value=f"=ValuesOnly!{col_letter}{r}")
            
            wb.save(output_dir / name)

class AddressSanityGenerator(BaseGenerator):
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        targets = self.args.get('targets', ["A1", "B2", "Z10"])
        
        for name in output_names:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = "Addresses"
            
            for addr in targets:
                ws[addr] = addr
                
            wb.save(output_dir / name)

class ValueFormulaGenerator(BaseGenerator):
    """PG3: Types, formulas, values"""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        for name in output_names:
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = "Types"
            
            ws['A1'] = 42
            ws['A2'] = "hello"
            ws['A3'] = True
            # A4 empty
            
            ws['B1'] = "=A1+1"
            ws['B2'] = '="hello" & " world"'
            ws['B3'] = "=A1>0"
            
            output_path = output_dir / name
            wb.save(output_path)
            self._inject_formula_caches(output_path)

    def _inject_formula_caches(self, path: Path):
        ns = "http://schemas.openxmlformats.org/spreadsheetml/2006/main"
        with zipfile.ZipFile(path, "r") as zf:
            sheet_xml = zf.read("xl/worksheets/sheet1.xml")
            other_files = {
                info.filename: zf.read(info.filename)
                for info in zf.infolist()
                if info.filename != "xl/worksheets/sheet1.xml"
            }

        root = ET.fromstring(sheet_xml)

        def update_cell(ref: str, value: str, cell_type: str | None = None):
            cell = root.find(f".//{{{ns}}}c[@r='{ref}']")
            if cell is None:
                return
            if cell_type:
                cell.set("t", cell_type)
            v = cell.find(f"{{{ns}}}v")
            if v is None:
                v = ET.SubElement(cell, f"{{{ns}}}v")
            v.text = value

        update_cell("B1", "43")
        update_cell("B2", "hello world", "str")
        update_cell("B3", "1", "b")

        ET.register_namespace("", ns)
        updated_sheet = ET.tostring(root, encoding="utf-8", xml_declaration=False)
        with zipfile.ZipFile(path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr("xl/worksheets/sheet1.xml", updated_sheet)
            for name, data in other_files.items():
                zf.writestr(name, data)

class SingleCellDiffGenerator(BaseGenerator):
    """Generates a tiny pair of workbooks with a single differing cell."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("single_cell_diff generator expects exactly two output filenames")

        rows = self.args.get('rows', 3)
        cols = self.args.get('cols', 3)
        sheet = self.args.get('sheet', "Sheet1")
        target_cell = self.args.get('target_cell', "C3")
        value_a = self.args.get('value_a', "1")
        value_b = self.args.get('value_b', "2")

        def create_workbook(value, name: str):
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = sheet

            for r in range(1, rows + 1):
                for c in range(1, cols + 1):
                    ws.cell(row=r, column=c, value=f"R{r}C{c}")

            ws[target_cell] = value
            wb.save(output_dir / name)

        create_workbook(value_a, output_names[0])
        create_workbook(value_b, output_names[1])

class MultiCellDiffGenerator(BaseGenerator):
    """Generates workbook pairs that differ in multiple scattered cells."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("multi_cell_diff generator expects exactly two output filenames")

        rows = self.args.get("rows", 20)
        cols = self.args.get("cols", 10)
        sheet = self.args.get("sheet", "Sheet1")
        edits: List[Dict[str, Any]] = self.args.get("edits", [])

        self._create_workbook(output_dir / output_names[0], sheet, rows, cols, edits, "a")
        self._create_workbook(output_dir / output_names[1], sheet, rows, cols, edits, "b")

    def _create_workbook(
        self,
        path: Path,
        sheet: str,
        rows: int,
        cols: int,
        edits: List[Dict[str, Any]],
        value_key: str,
    ):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        self._fill_base_grid(ws, rows, cols)
        self._apply_edits(ws, edits, value_key)

        wb.save(path)

    def _fill_base_grid(self, ws, rows: int, cols: int):
        for r in range(1, rows + 1):
            for c in range(1, cols + 1):
                ws.cell(row=r, column=c, value=f"R{r}C{c}")

    def _apply_edits(self, ws, edits: List[Dict[str, Any]], value_key: str):
        value_field = f"value_{value_key}"

        for edit in edits:
            addr = edit.get("addr")
            if not addr:
                raise ValueError("multi_cell_diff edits require 'addr'")
            if value_field not in edit:
                raise ValueError(f"multi_cell_diff edits require '{value_field}'")
            ws[addr] = edit[value_field]

class GridTailDiffGenerator(BaseGenerator):
    """Generates workbook pairs for simple row/column tail append/delete scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("grid_tail_diff generator expects exactly two output filenames")

        mode = self.args.get("mode")
        sheet = self.args.get("sheet", "Sheet1")

        if mode == "row_append_bottom":
            self._row_append_bottom(output_dir, output_names, sheet)
        elif mode == "row_delete_bottom":
            self._row_delete_bottom(output_dir, output_names, sheet)
        elif mode == "col_append_right":
            self._col_append_right(output_dir, output_names, sheet)
        elif mode == "col_delete_right":
            self._col_delete_right(output_dir, output_names, sheet)
        else:
            raise ValueError(f"Unsupported grid_tail_diff mode: {mode}")

    def _row_append_bottom(self, output_dir: Path, output_names: List[str], sheet: str):
        base_rows = self.args.get("base_rows", 10)
        tail_rows = self.args.get("tail_rows", 2)
        cols = self.args.get("cols", 3)

        self._write_rows(output_dir / output_names[0], sheet, base_rows, cols, 1)
        self._write_rows(
            output_dir / output_names[1],
            sheet,
            base_rows + tail_rows,
            cols,
            1,
        )

    def _row_delete_bottom(self, output_dir: Path, output_names: List[str], sheet: str):
        base_rows = self.args.get("base_rows", 10)
        tail_rows = self.args.get("tail_rows", 2)
        cols = self.args.get("cols", 3)

        self._write_rows(
            output_dir / output_names[0],
            sheet,
            base_rows + tail_rows,
            cols,
            1,
        )
        self._write_rows(output_dir / output_names[1], sheet, base_rows, cols, 1)

    def _col_append_right(self, output_dir: Path, output_names: List[str], sheet: str):
        base_cols = self.args.get("base_cols", 4)
        tail_cols = self.args.get("tail_cols", 2)
        rows = self.args.get("rows", 5)

        self._write_cols(output_dir / output_names[0], sheet, rows, base_cols)
        self._write_cols(
            output_dir / output_names[1],
            sheet,
            rows,
            base_cols + tail_cols,
        )

    def _col_delete_right(self, output_dir: Path, output_names: List[str], sheet: str):
        base_cols = self.args.get("base_cols", 4)
        tail_cols = self.args.get("tail_cols", 2)
        rows = self.args.get("rows", 5)

        self._write_cols(
            output_dir / output_names[0],
            sheet,
            rows,
            base_cols + tail_cols,
        )
        self._write_cols(output_dir / output_names[1], sheet, rows, base_cols)

    def _write_rows(self, path: Path, sheet: str, rows: int, cols: int, start_value: int):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r in range(1, rows + 1):
            ws.cell(row=r, column=1, value=start_value + r - 1)
            for c in range(2, cols + 1):
                ws.cell(row=r, column=c, value=f"R{r}C{c}")

        wb.save(path)

    def _write_cols(self, path: Path, sheet: str, rows: int, cols: int):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r in range(1, rows + 1):
            for c in range(1, cols + 1):
                ws.cell(row=r, column=c, value=f"R{r}C{c}")

        wb.save(path)

class RowAlignmentG8Generator(BaseGenerator):
    """Generates workbook pairs for G8-style middle row insert/delete scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("row_alignment_g8 generator expects exactly two output filenames")

        mode = self.args.get("mode")
        sheet = self.args.get("sheet", "Sheet1")
        base_rows = self.args.get("base_rows", 10)
        cols = self.args.get("cols", 5)
        insert_at = self.args.get("insert_at", 6)  # 1-based position in B
        delete_row = self.args.get("delete_row", 6)  # 1-based position in A
        edit_row = self.args.get("edit_row")  # Optional extra edit row (1-based in B after insert)
        edit_col = self.args.get("edit_col", 2)  # 1-based column for extra edit

        base_data = [self._base_row_values(idx, cols) for idx in range(1, base_rows + 1)]

        if mode == "insert":
            data_a = base_data
            data_b = self._with_insert(base_data, insert_at, cols)
        elif mode == "delete":
            data_a = base_data
            data_b = self._with_delete(base_data, delete_row)
        elif mode == "insert_with_edit":
            data_a = base_data
            data_b = self._with_insert(base_data, insert_at, cols)
            target_row = edit_row or (insert_at + 2)
            if 1 <= target_row <= len(data_b):
                row_values = list(data_b[target_row - 1])
                col_index = max(1, min(edit_col, cols)) - 1
                row_values[col_index] = "EditedAfterInsert"
                data_b[target_row - 1] = row_values
        else:
            raise ValueError(f"Unsupported row_alignment_g8 mode: {mode}")

        self._write_workbook(output_dir / output_names[0], sheet, data_a)
        self._write_workbook(output_dir / output_names[1], sheet, data_b)

    def _base_row_values(self, row_number: int, cols: int) -> List[str]:
        return [f"Row{row_number}_Col{c}" for c in range(1, cols + 1)]

    def _insert_row_values(self, cols: int) -> List[str]:
        return [f"Inserted_Row_Col{c}" for c in range(1, cols + 1)]

    def _with_insert(self, base_data: List[List[str]], insert_at: int, cols: int) -> List[List[str]]:
        insert_idx = max(1, min(insert_at, len(base_data) + 1))
        insert_row = self._insert_row_values(cols)
        return base_data[: insert_idx - 1] + [insert_row] + base_data[insert_idx - 1 :]

    def _with_delete(self, base_data: List[List[str]], delete_row: int) -> List[List[str]]:
        if not (1 <= delete_row <= len(base_data)):
            raise ValueError(f"delete_row must be within 1..{len(base_data)}")
        return base_data[: delete_row - 1] + base_data[delete_row:]

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[str]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class RowAlignmentG10Generator(BaseGenerator):
    """Generates workbook pairs for G10 contiguous row block insert/delete scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("row_alignment_g10 generator expects exactly two output filenames")

        mode = self.args.get("mode")
        sheet = self.args.get("sheet", "Sheet1")
        base_rows = self.args.get("base_rows", 10)
        cols = self.args.get("cols", 5)
        block_rows = self.args.get("block_rows", 4)
        insert_at = self.args.get("insert_at", 4)  # 1-based position of first inserted row in B
        delete_start = self.args.get("delete_start", 4)  # 1-based starting row in A to delete

        base_data = [self._row_values(idx, cols, 0) for idx in range(1, base_rows + 1)]

        if mode == "block_insert":
            data_a = base_data
            data_b = self._with_block_insert(base_data, insert_at, block_rows, cols)
        elif mode == "block_delete":
            data_a = base_data
            data_b = self._with_block_delete(base_data, delete_start, block_rows)
        else:
            raise ValueError(f"Unsupported row_alignment_g10 mode: {mode}")

        self._write_workbook(output_dir / output_names[0], sheet, data_a)
        self._write_workbook(output_dir / output_names[1], sheet, data_b)

    def _row_values(self, row_number: int, cols: int, offset: int) -> List[int]:
        row_id = row_number + offset
        values = [row_id]
        for c in range(1, cols):
            values.append(row_id * 10 + c)
        return values

    def _block_rows(self, count: int, cols: int) -> List[List[int]]:
        return [self._row_values(1000 + idx, cols, 0) for idx in range(1, count + 1)]

    def _with_block_insert(
        self, base_data: List[List[int]], insert_at: int, block_rows: int, cols: int
    ) -> List[List[int]]:
        insert_idx = max(1, min(insert_at, len(base_data) + 1)) - 1
        block = self._block_rows(block_rows, cols)
        return base_data[:insert_idx] + block + base_data[insert_idx:]

    def _with_block_delete(
        self, base_data: List[List[int]], delete_start: int, block_rows: int
    ) -> List[List[int]]:
        if not (1 <= delete_start <= len(base_data)):
            raise ValueError(f"delete_start must be within 1..{len(base_data)}")
        if delete_start - 1 + block_rows > len(base_data):
            raise ValueError("delete block exceeds base data length")

        delete_idx = delete_start - 1
        return base_data[:delete_idx] + base_data[delete_idx + block_rows :]

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[int]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class RowBlockMoveG11Generator(BaseGenerator):
    """Generates workbook pairs for G11 exact row block move scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("row_block_move_g11 generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Sheet1")
        total_rows = self.args.get("total_rows", 20)
        cols = self.args.get("cols", 5)
        block_rows = self.args.get("block_rows", 4)
        src_start = self.args.get("src_start", 5)
        dst_start = self.args.get("dst_start", 13)

        if block_rows <= 0:
            raise ValueError("block_rows must be positive")
        if src_start < 1 or src_start + block_rows - 1 > total_rows:
            raise ValueError("source block must fit within total_rows")
        if dst_start < 1 or dst_start + block_rows - 1 > total_rows:
            raise ValueError("destination block must fit within total_rows")

        src_end = src_start + block_rows - 1
        dst_end = dst_start + block_rows - 1
        if not (src_end < dst_start or dst_end < src_start):
            raise ValueError("source and destination blocks must not overlap")

        rows_a = self._build_rows(total_rows, cols, src_start, block_rows)
        rows_b = self._move_block(rows_a, src_start, block_rows, dst_start)

        self._write_workbook(output_dir / output_names[0], sheet, rows_a)
        self._write_workbook(output_dir / output_names[1], sheet, rows_b)

    def _build_rows(self, total_rows: int, cols: int, src_start: int, block_rows: int) -> List[List[str]]:
        block_end = src_start + block_rows - 1
        rows: List[List[str]] = []
        for r in range(1, total_rows + 1):
            if src_start <= r <= block_end:
                rows.append([f"BLOCK_r{r}_c{c}" for c in range(1, cols + 1)])
            else:
                rows.append([f"R{r}_C{c}" for c in range(1, cols + 1)])
        return rows

    def _move_block(
        self, rows: List[List[str]], src_start: int, block_rows: int, dst_start: int
    ) -> List[List[str]]:
        rows_b = [list(r) for r in rows]
        src_idx = src_start - 1
        src_end = src_idx + block_rows
        block = rows_b[src_idx:src_end]
        del rows_b[src_idx:src_end]

        dst_idx = min(dst_start - 1, len(rows_b))

        rows_b[dst_idx:dst_idx] = block
        return rows_b

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[str]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class RowFuzzyMoveG13Generator(BaseGenerator):
    """Generates workbook pairs for G13 fuzzy row block move scenarios with internal edits."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("row_fuzzy_move_g13 generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Data")
        total_rows = self.args.get("total_rows", 24)
        cols = self.args.get("cols", 6)
        block_rows = self.args.get("block_rows", 4)
        src_start = self.args.get("src_start", 5)
        dst_start = self.args.get("dst_start", 14)
        edits = self.args.get(
            "edits",
            [
                {"row_offset": 1, "col": 3, "delta": 1},
            ],
        )

        if block_rows <= 0:
            raise ValueError("block_rows must be positive")
        if src_start < 1 or src_start + block_rows - 1 > total_rows:
            raise ValueError("source block must fit within total_rows")
        if dst_start < 1 or dst_start + block_rows - 1 > total_rows:
            raise ValueError("destination block must fit within total_rows")

        src_end = src_start + block_rows - 1
        dst_end = dst_start + block_rows - 1
        if not (src_end < dst_start or dst_end < src_start):
            raise ValueError("source and destination blocks must not overlap")

        rows_a = self._build_rows(total_rows, cols, src_start, block_rows)
        rows_b = self._move_block(rows_a, src_start, block_rows, dst_start)
        self._apply_edits(rows_b, dst_start, block_rows, cols, edits)

        self._write_workbook(output_dir / output_names[0], sheet, rows_a)
        self._write_workbook(output_dir / output_names[1], sheet, rows_b)

    def _build_rows(self, total_rows: int, cols: int, block_start: int, block_rows: int) -> List[List[int]]:
        block_end = block_start + block_rows - 1
        rows: List[List[int]] = []
        for r in range(1, total_rows + 1):
            if block_start <= r <= block_end:
                row_id = 1_000 + (r - block_start)
            else:
                row_id = r
            row_values = [row_id]
            for c in range(1, cols):
                row_values.append(row_id * 10 + c)
            rows.append(row_values)
        return rows

    def _move_block(
        self, rows: List[List[int]], src_start: int, block_rows: int, dst_start: int
    ) -> List[List[int]]:
        rows_b = [list(r) for r in rows]
        src_idx = src_start - 1
        src_end = src_idx + block_rows
        block = rows_b[src_idx:src_end]
        del rows_b[src_idx:src_end]

        dst_idx = min(dst_start - 1, len(rows_b))
        rows_b[dst_idx:dst_idx] = block
        return rows_b

    def _apply_edits(
        self,
        rows: List[List[int]],
        dst_start: int,
        block_rows: int,
        cols: int,
        edits: List[Dict[str, Any]],
    ):
        dst_idx = dst_start - 1
        if dst_idx + block_rows > len(rows):
            return

        for edit in edits:
            row_offset = int(edit.get("row_offset", 0))
            col = int(edit.get("col", 1))
            delta = int(edit.get("delta", 1))

            if row_offset < 0 or row_offset >= block_rows:
                continue

            col_idx = max(1, min(col, cols)) - 1
            target_row = dst_idx + row_offset
            if col_idx >= len(rows[target_row]):
                continue
            rows[target_row][col_idx] += delta

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[int]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class ColumnMoveG12Generator(BaseGenerator):
    """Generates workbook pairs for G12 exact column move scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("column_move_g12 generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Data")
        cols = self.args.get("cols", 8)
        data_rows = self.args.get("data_rows", 9)
        src_col = self.args.get("src_col", 3)
        dst_col = self.args.get("dst_col", 6)

        if not (1 <= src_col <= cols):
            raise ValueError("src_col must be within 1..cols")
        if not (1 <= dst_col <= cols):
            raise ValueError("dst_col must be within 1..cols")
        if src_col == dst_col:
            raise ValueError("src_col and dst_col must differ for a move")

        base_rows = self._build_rows(cols, data_rows, src_col)
        moved_rows = self._move_column(base_rows, src_col, dst_col)

        self._write_workbook(output_dir / output_names[0], sheet, base_rows)
        self._write_workbook(output_dir / output_names[1], sheet, moved_rows)

    def _build_rows(self, cols: int, data_rows: int, key_col: int) -> List[List[Any]]:
        header: List[Any] = []
        for c in range(1, cols + 1):
            if c == key_col:
                header.append("C_key")
            else:
                header.append(f"Col{c}")

        rows: List[List[Any]] = [header]
        for r in range(1, data_rows + 1):
            row: List[Any] = []
            for c in range(1, cols + 1):
                if c == key_col:
                    row.append(100 * r)
                else:
                    row.append(r * 10 + c)
            rows.append(row)

        return rows

    def _move_column(
        self, rows: List[List[Any]], src_col: int, dst_col: int
    ) -> List[List[Any]]:
        src_idx = src_col - 1
        dst_idx = dst_col - 1
        moved_rows: List[List[Any]] = []

        for row in rows:
            new_row = list(row)
            value = new_row.pop(src_idx)
            insert_at = max(0, min(dst_idx, len(new_row)))
            new_row.insert(insert_at, value)
            moved_rows.append(new_row)

        return moved_rows

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[Any]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class RectBlockMoveG12Generator(BaseGenerator):
    """Generates workbook pairs for G12 exact rectangular block move scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("rect_block_move_g12 generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Data")
        rows = self.args.get("rows", 15)
        cols = self.args.get("cols", 15)
        src_top = self.args.get("src_top", 3)  # 1-based
        src_left = self.args.get("src_left", 2)  # 1-based (column B)
        dst_top = self.args.get("dst_top", 10)  # 1-based
        dst_left = self.args.get("dst_left", 7)  # 1-based (column G)
        block_rows = self.args.get("block_rows", 3)
        block_cols = self.args.get("block_cols", 3)

        self._write_workbook(
            output_dir / output_names[0],
            sheet,
            rows,
            cols,
            src_top,
            src_left,
            block_rows,
            block_cols,
        )
        self._write_workbook(
            output_dir / output_names[1],
            sheet,
            rows,
            cols,
            dst_top,
            dst_left,
            block_rows,
            block_cols,
        )

    def _write_workbook(
        self,
        path: Path,
        sheet: str,
        rows: int,
        cols: int,
        block_top: int,
        block_left: int,
        block_rows: int,
        block_cols: int,
    ):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        self._fill_background(ws, rows, cols)
        self._write_block(ws, block_top, block_left, block_rows, block_cols)

        wb.save(path)

    def _fill_background(self, ws, rows: int, cols: int):
        for r in range(1, rows + 1):
            for c in range(1, cols + 1):
                ws.cell(row=r, column=c, value=self._background_value(r, c))

    def _background_value(self, row: int, col: int) -> int:
        return 1000 * row + col

    def _write_block(self, ws, top: int, left: int, block_rows: int, block_cols: int):
        for r_offset in range(block_rows):
            for c_offset in range(block_cols):
                value = 9000 + r_offset * 10 + c_offset
                ws.cell(row=top + r_offset, column=left + c_offset, value=value)

class ColumnAlignmentG9Generator(BaseGenerator):
    """Generates workbook pairs for G9-style middle column insert/delete scenarios."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("column_alignment_g9 generator expects exactly two output filenames")

        mode = self.args.get("mode")
        sheet = self.args.get("sheet", "Data")
        base_cols = self.args.get("cols", 8)
        data_rows = self.args.get("data_rows", 9)  # excludes header
        insert_at = self.args.get("insert_at", 4)  # 1-based position in B after insert
        delete_col = self.args.get("delete_col", 4)
        edit_row = self.args.get("edit_row", 8)
        edit_col_after_insert = self.args.get("edit_col_after_insert", 7)

        base_table = self._base_table(base_cols, data_rows)

        if mode == "insert":
            data_a = self._clone_rows(base_table)
            data_b = self._with_insert(base_table, insert_at)
        elif mode == "delete":
            data_a = self._clone_rows(base_table)
            data_b = self._with_delete(base_table, delete_col)
        elif mode == "insert_with_edit":
            data_a = self._clone_rows(base_table)
            data_b = self._with_insert(base_table, insert_at)
            row_idx = max(2, min(edit_row, len(data_b))) - 1  # stay below header
            col_idx = max(1, min(edit_col_after_insert, len(data_b[row_idx]))) - 1
            data_b[row_idx][col_idx] = "EditedAfterInsert"
        else:
            raise ValueError(f"Unsupported column_alignment_g9 mode: {mode}")

        self._write_workbook(output_dir / output_names[0], sheet, data_a)
        self._write_workbook(output_dir / output_names[1], sheet, data_b)

    def _base_table(self, cols: int, data_rows: int) -> List[List[str]]:
        header = [f"Col{c}" for c in range(1, cols + 1)]
        rows = [header]
        for r in range(1, data_rows + 1):
            rows.append([f"R{r}_C{c}" for c in range(1, cols + 1)])
        return rows

    def _with_insert(self, base_data: List[List[str]], insert_at: int) -> List[List[str]]:
        insert_idx = max(1, min(insert_at, len(base_data[0]) + 1))
        result: List[List[str]] = []
        for row_idx, row in enumerate(base_data):
            new_row = list(row)
            value = "Inserted" if row_idx == 0 else f"Inserted_{row_idx}"
            new_row.insert(insert_idx - 1, value)
            result.append(new_row)
        return result

    def _with_delete(self, base_data: List[List[str]], delete_col: int) -> List[List[str]]:
        if not base_data:
            return []
        if not (1 <= delete_col <= len(base_data[0])):
            raise ValueError(f"delete_col must be within 1..{len(base_data[0])}")
        result: List[List[str]] = []
        for row in base_data:
            new_row = list(row)
            del new_row[delete_col - 1]
            result.append(new_row)
        return result

    def _clone_rows(self, rows: List[List[str]]) -> List[List[str]]:
        return [list(r) for r in rows]

    def _write_workbook(self, path: Path, sheet: str, rows: List[List[str]]):
        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = sheet

        for r_idx, row_values in enumerate(rows, start=1):
            for c_idx, value in enumerate(row_values, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        wb.save(path)

class SheetCaseRenameGenerator(BaseGenerator):
    """Generates a pair of workbooks that differ only by sheet name casing, with optional cell edit."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("sheet_case_rename generator expects exactly two output filenames")

        sheet_a = self.args.get("sheet_a", "Sheet1")
        sheet_b = self.args.get("sheet_b", "sheet1")
        cell = self.args.get("cell", "A1")
        value_a = self.args.get("value_a", 1.0)
        value_b = self.args.get("value_b", value_a)

        def create_workbook(sheet_name: str, value, output_name: str):
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = sheet_name
            ws[cell] = value
            wb.save(output_dir / output_name)

        create_workbook(sheet_a, value_a, output_names[0])
        create_workbook(sheet_b, value_b, output_names[1])

class Pg6SheetScenarioGenerator(BaseGenerator):
    """Generates workbook pairs for PG6 sheet add/remove/rename vs grid responsibilities."""
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("pg6_sheet_scenario generator expects exactly two output filenames")

        mode = self.args.get("mode")
        a_path = output_dir / output_names[0]
        b_path = output_dir / output_names[1]

        if mode == "sheet_added":
            self._gen_sheet_added(a_path, b_path)
        elif mode == "sheet_removed":
            self._gen_sheet_removed(a_path, b_path)
        elif mode == "sheet_renamed":
            self._gen_sheet_renamed(a_path, b_path)
        elif mode == "sheet_and_grid_change":
            self._gen_sheet_and_grid_change(a_path, b_path)
        else:
            raise ValueError(f"Unsupported PG6 mode: {mode}")

    def _fill_grid(self, worksheet, rows: int, cols: int, prefix: str = "R"):
        for r in range(1, rows + 1):
            for c in range(1, cols + 1):
                worksheet.cell(row=r, column=c, value=f"{prefix}{r}C{c}")

    def _gen_sheet_added(self, a_path: Path, b_path: Path):
        wb_a = openpyxl.Workbook()
        ws_main_a = wb_a.active
        ws_main_a.title = "Main"
        self._fill_grid(ws_main_a, 5, 5)
        wb_a.save(a_path)

        wb_b = openpyxl.Workbook()
        ws_main_b = wb_b.active
        ws_main_b.title = "Main"
        self._fill_grid(ws_main_b, 5, 5)
        ws_new = wb_b.create_sheet("NewSheet")
        self._fill_grid(ws_new, 3, 3, prefix="N")
        wb_b.save(b_path)

    def _gen_sheet_removed(self, a_path: Path, b_path: Path):
        wb_a = openpyxl.Workbook()
        ws_main_a = wb_a.active
        ws_main_a.title = "Main"
        self._fill_grid(ws_main_a, 5, 5)
        ws_old = wb_a.create_sheet("OldSheet")
        self._fill_grid(ws_old, 3, 3, prefix="O")
        wb_a.save(a_path)

        wb_b = openpyxl.Workbook()
        ws_main_b = wb_b.active
        ws_main_b.title = "Main"
        self._fill_grid(ws_main_b, 5, 5)
        wb_b.save(b_path)

    def _gen_sheet_renamed(self, a_path: Path, b_path: Path):
        wb_a = openpyxl.Workbook()
        ws_old = wb_a.active
        ws_old.title = "OldName"
        self._fill_grid(ws_old, 3, 3)
        wb_a.save(a_path)

        wb_b = openpyxl.Workbook()
        ws_new = wb_b.active
        ws_new.title = "NewName"
        self._fill_grid(ws_new, 3, 3)
        wb_b.save(b_path)

    def _gen_sheet_and_grid_change(self, a_path: Path, b_path: Path):
        base_rows = 5
        base_cols = 5

        wb_a = openpyxl.Workbook()
        ws_main_a = wb_a.active
        ws_main_a.title = "Main"
        self._fill_grid(ws_main_a, base_rows, base_cols)
        ws_aux_a = wb_a.create_sheet("Aux")
        self._fill_grid(ws_aux_a, 3, 3, prefix="A")
        wb_a.save(a_path)

        wb_b = openpyxl.Workbook()
        ws_main_b = wb_b.active
        ws_main_b.title = "Main"
        self._fill_grid(ws_main_b, base_rows, base_cols)
        ws_main_b["A1"] = "Main changed 1"
        ws_main_b["B2"] = "Main changed 2"
        ws_main_b["C3"] = "Main changed 3"

        ws_aux_b = wb_b.create_sheet("Aux")
        self._fill_grid(ws_aux_b, 3, 3, prefix="A")

        ws_scratch = wb_b.create_sheet("Scratch")
        self._fill_grid(ws_scratch, 2, 2, prefix="S")
        wb_b.save(b_path)

```

---

### File: `fixtures\src\generators\mashup.py`

```python
import base64
import copy
import io
import random
import re
import struct
import zipfile
from pathlib import Path
from typing import Callable, List, Optional, Union
from xml.etree import ElementTree as ET
from lxml import etree
from .base import BaseGenerator

# XML Namespaces
NS = {'dm': 'http://schemas.microsoft.com/DataMashup'}
FIXED_ZIP_DATE = (1980, 1, 1, 0, 0, 0)


def _zipinfo_fixed(name: str) -> zipfile.ZipInfo:
    info = zipfile.ZipInfo(name)
    info.date_time = FIXED_ZIP_DATE
    info.compress_type = zipfile.ZIP_DEFLATED
    return info


def _writestr_fixed(zout: zipfile.ZipFile, name: str, data):
    zout.writestr(_zipinfo_fixed(name), data)

class MashupBaseGenerator(BaseGenerator):
    """Base class for handling the outer Excel container and finding DataMashup."""
    
    def _get_mashup_element(self, tree):
        if tree.tag.endswith("DataMashup"):
            return tree
        return tree.find('.//dm:DataMashup', namespaces=NS)

    def _process_excel_container(
        self,
        base_path,
        output_path,
        callback,
        text_mutator: Optional[Callable[[str], str]] = None,
    ):
        """
        Generic wrapper to open xlsx, find customXml, apply a callback to the 
        DataMashup bytes, and save the result.
        """
        # Copy base file structure to output
        with zipfile.ZipFile(base_path, 'r') as zin:
            with zipfile.ZipFile(output_path, 'w') as zout:
                for item in zin.infolist():
                    buffer = zin.read(item.filename)
                    
                    # We only care about the item containing DataMashup
                    # Usually customXml/item1.xml, but we check content to be safe
                    has_marker = b"DataMashup" in buffer or b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p" in buffer
                    if item.filename.startswith("customXml/item") and has_marker:
                        # Parse XML
                        root = etree.fromstring(buffer)
                        dm_node = self._get_mashup_element(root)
                        
                        if dm_node is not None:
                            # 1. Decode
                            # The text content might have whitespace/newlines, strip them
                            b64_text = dm_node.text.strip() if dm_node.text else ""
                            if b64_text:
                                raw_bytes = base64.b64decode(b64_text)
                                
                                # 2. Apply modification (The Callback)
                                new_bytes = callback(raw_bytes)
                                
                                # 3. Encode back
                                new_text = base64.b64encode(new_bytes).decode('utf-8')
                                if text_mutator is not None:
                                    new_text = text_mutator(new_text)
                                dm_node.text = new_text
                                buffer = etree.tostring(root, encoding='utf-8', xml_declaration=True)
                    
                    zout.writestr(item, buffer)

class MashupCorruptGenerator(MashupBaseGenerator):
    """Fuzzes the DataMashup bytes to test error handling."""
    
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        base_file_arg = self.args.get('base_file')
        if not base_file_arg:
            raise ValueError("MashupCorruptGenerator requires 'base_file' argument")

        # Resolve base file relative to current working directory or fixtures/templates
        base = Path(base_file_arg)
        if not base.exists():
             # Try looking in fixtures/templates if a relative path was given
             candidate = Path("fixtures") / base_file_arg
             if candidate.exists():
                 base = candidate
             else:
                raise FileNotFoundError(f"Template {base} not found.")

        mode = self.args.get('mode', 'byte_flip')

        def corruptor(data):
            mutable = bytearray(data)
            if len(mutable) == 0:
                return bytes(mutable)

            if mode == 'byte_flip':
                # Flip a byte in the middle
                idx = len(mutable) // 2
                mutable[idx] = mutable[idx] ^ 0xFF
            elif mode == 'truncate':
                return mutable[:len(mutable)//2]
            return bytes(mutable)

        for name in output_names:
            # Convert Path objects to strings for resolve() to work correctly if there's a mix
            # Actually output_dir is a Path. name is str.
            # .resolve() resolves symlinks and relative paths to absolute
            target_path = (output_dir / name).resolve()
            text_mutator = self._garble_base64_text if mode == 'byte_flip' else None
            self._process_excel_container(
                base.resolve(),
                target_path,
                corruptor,
                text_mutator=text_mutator,
            )

    def _garble_base64_text(self, encoded: str) -> str:
        if not encoded:
            return "!!"
        chars = list(encoded)
        chars[0] = "!"
        return "".join(chars)


class MashupInjectGenerator(MashupBaseGenerator):
    """
    Peels the onion:
    1. Parses MS-QDEFF binary header.
    2. Unzips PackageParts.
    3. Injects new M-Code into Section1.m.
    4. Re-zips and fixes header lengths.
    """
    
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]
            
        base_file_arg = self.args.get('base_file')
        new_m_code = self.args.get('m_code')

        if not base_file_arg:
             raise ValueError("MashupInjectGenerator requires 'base_file' argument")
        if new_m_code is None:
             raise ValueError("MashupInjectGenerator requires 'm_code' argument")

        base = Path(base_file_arg)
        if not base.exists():
             candidate = Path("fixtures") / base_file_arg
             if candidate.exists():
                 base = candidate
             else:
                raise FileNotFoundError(f"Template {base} not found.")

        def injector(raw_bytes):
            return self._inject_m_code(raw_bytes, new_m_code)

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._process_excel_container(base.resolve(), target_path, injector)

    def _inject_m_code(self, raw_bytes, m_code):
        # --- 1. Parse MS-QDEFF Header ---
        # Format: Version(4) + LenPP(4) + PackageParts(...) + LenPerm(4) + ...
        # We assume Version is 0 (first 4 bytes)
        
        if len(raw_bytes) < 8:
            return raw_bytes # Too short to handle

        offset = 4
        # Read PackageParts Length
        pp_len = struct.unpack('<I', raw_bytes[offset:offset+4])[0]
        offset += 4
        
        # Extract existing components
        pp_bytes = raw_bytes[offset : offset + pp_len]
        
        # Keep the rest of the stream (Permissions, Metadata, Bindings) intact
        # We just append it later
        remainder_bytes = raw_bytes[offset + pp_len :]

        # --- 2. Modify PackageParts (Inner ZIP) ---
        new_pp_bytes = self._replace_in_zip(pp_bytes, 'Formulas/Section1.m', m_code)

        # --- 3. Rebuild Stream ---
        # New Length for PackageParts
        new_pp_len = len(new_pp_bytes)
        
        # Reconstruct: Version(0) + NewLen + NewPP + Remainder
        header = raw_bytes[:4] # Version
        len_pack = struct.pack('<I', new_pp_len)
        
        return header + len_pack + new_pp_bytes + remainder_bytes

    def _replace_in_zip(self, zip_bytes, filename, new_content):
        """Opens a ZIP byte stream, replaces a file, returns new ZIP byte stream."""
        in_buffer = io.BytesIO(zip_bytes)
        out_buffer = io.BytesIO()
        
        try:
            with zipfile.ZipFile(in_buffer, 'r') as zin:
                with zipfile.ZipFile(out_buffer, 'w', compression=zipfile.ZIP_DEFLATED) as zout:
                    for item in zin.infolist():
                        if item.filename == filename:
                            # Write the new M code
                            zout.writestr(item, new_content.encode('utf-8'))
                        else:
                            # Copy others
                            zout.writestr(item, zin.read(item.filename))
        except zipfile.BadZipFile:
            # Fallback if inner stream isn't a valid zip (shouldn't happen on valid QDEFF)
            return zip_bytes
            
        return out_buffer.getvalue()


class MashupPackagePartsGenerator(MashupBaseGenerator):
    """
    Generates PackageParts-focused fixtures starting from a base workbook.
    """

    variant: str = "one_query"

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        base_file_arg = self.args.get("base_file", "templates/base_query.xlsx")
        base = Path(base_file_arg)
        if not base.exists():
            candidate = Path("fixtures") / base_file_arg
            if candidate.exists():
                base = candidate
            else:
                raise FileNotFoundError(f"Template {base} not found.")

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._process_excel_container(base.resolve(), target_path, self._rewrite_datamashup)

    def _rewrite_datamashup(self, raw_bytes: bytes) -> bytes:
        if self.variant == "one_query":
            return raw_bytes

        version, package_parts, permissions, metadata, bindings = self._split_sections(raw_bytes)
        package_xml, main_section_text, content_types = self._extract_package_parts(package_parts)

        embedded_guid = self.args.get(
            "embedded_guid", "{11111111-2222-3333-4444-555555555555}"
        )
        embedded_section_text = self.args.get(
            "embedded_section",
            self._default_embedded_section(),
        )
        updated_main_section = self._extend_main_section(main_section_text, embedded_guid)
        embedded_bytes = self._build_embedded_package(embedded_section_text, content_types)
        updated_package_parts = self._build_package_parts(
            package_xml,
            updated_main_section,
            content_types,
            embedded_guid,
            embedded_bytes,
        )

        return self._assemble_sections(
            version,
            updated_package_parts,
            permissions,
            metadata,
            bindings,
        )

    def _split_sections(self, raw_bytes: bytes):
        min_size = 4 + 4 * 4
        if len(raw_bytes) < min_size:
            raise ValueError("DataMashup stream too short")

        offset = 0
        version = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4

        package_parts_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        package_parts_end = offset + package_parts_len
        if package_parts_end > len(raw_bytes):
            raise ValueError("invalid PackageParts length")
        package_parts = raw_bytes[offset:package_parts_end]
        offset = package_parts_end

        permissions_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        permissions_end = offset + permissions_len
        if permissions_end > len(raw_bytes):
            raise ValueError("invalid permissions length")
        permissions = raw_bytes[offset:permissions_end]
        offset = permissions_end

        metadata_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        metadata_end = offset + metadata_len
        if metadata_end > len(raw_bytes):
            raise ValueError("invalid metadata length")
        metadata = raw_bytes[offset:metadata_end]
        offset = metadata_end

        bindings_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        bindings_end = offset + bindings_len
        if bindings_end > len(raw_bytes):
            raise ValueError("invalid bindings length")
        bindings = raw_bytes[offset:bindings_end]
        offset = bindings_end

        if offset != len(raw_bytes):
            raise ValueError("DataMashup trailing bytes mismatch")

        return version, package_parts, permissions, metadata, bindings

    def _assemble_sections(
        self,
        version: int,
        package_parts: bytes,
        permissions: bytes,
        metadata: bytes,
        bindings: bytes,
    ) -> bytes:
        return b"".join(
            [
                struct.pack("<I", version),
                struct.pack("<I", len(package_parts)),
                package_parts,
                struct.pack("<I", len(permissions)),
                permissions,
                struct.pack("<I", len(metadata)),
                metadata,
                struct.pack("<I", len(bindings)),
                bindings,
            ]
        )

    def _extract_package_parts(self, package_parts: bytes):
        with zipfile.ZipFile(io.BytesIO(package_parts), "r") as z:
            package_xml = z.read("Config/Package.xml")
            content_types = z.read("[Content_Types].xml")
            main_section = z.read("Formulas/Section1.m")
        return package_xml, main_section.decode("utf-8", errors="ignore"), content_types

    def _extend_main_section(self, base_section: str, embedded_guid: str) -> str:
        stripped = base_section.rstrip()
        lines = [
            stripped,
            "",
            "shared EmbeddedQuery = let",
            f'    Source = Embedded.Value("Content/{embedded_guid}.package")',
            "in",
            "    Source;",
        ]
        return "\n".join(lines)

    def _build_embedded_package(self, section_text: str, content_types_template: bytes) -> bytes:
        content_types = self._augment_content_types(content_types_template)
        buffer = io.BytesIO()
        with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as z:
            _writestr_fixed(z, "[Content_Types].xml", content_types)
            _writestr_fixed(z, "Formulas/Section1.m", section_text)
        return buffer.getvalue()

    def _build_package_parts(
        self,
        package_xml: bytes,
        main_section: str,
        content_types_template: bytes,
        embedded_guid: str,
        embedded_package: bytes,
    ) -> bytes:
        content_types = self._augment_content_types(content_types_template)
        buffer = io.BytesIO()
        with zipfile.ZipFile(buffer, "w", compression=zipfile.ZIP_DEFLATED) as z:
            _writestr_fixed(z, "[Content_Types].xml", content_types)
            _writestr_fixed(z, "Config/Package.xml", package_xml)
            _writestr_fixed(z, "Formulas/Section1.m", main_section)
            _writestr_fixed(z, f"Content/{embedded_guid}.package", embedded_package)
        return buffer.getvalue()

    def _augment_content_types(self, content_types_bytes: bytes) -> str:
        text = content_types_bytes.decode("utf-8", errors="ignore")
        if "Extension=\"package\"" not in text and "Extension='package'" not in text:
            text = text.replace(
                "</Types>",
                '<Default Extension="package" ContentType="application/octet-stream" /></Types>',
                1,
            )
        return text

    def _default_embedded_section(self) -> str:
        return "\n".join(
            [
                "section Section1;",
                "",
                "shared Inner = let",
                "    Source = 1",
                "in",
                "    Source;",
            ]
        )


class MashupOneQueryGenerator(MashupPackagePartsGenerator):
    variant = "one_query"


class MashupMultiEmbeddedGenerator(MashupPackagePartsGenerator):
    variant = "multi_query_with_embedded"


class MashupDuplicateGenerator(MashupBaseGenerator):
    """
    Duplicates the customXml part that contains DataMashup to produce two
    DataMashup occurrences in a single workbook.
    """

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        base_file_arg = self.args.get('base_file')
        mode = self.args.get('mode', 'part')
        if not base_file_arg:
            raise ValueError("MashupDuplicateGenerator requires 'base_file' argument")

        base = Path(base_file_arg)
        if not base.exists():
            candidate = Path("fixtures") / base_file_arg
            if candidate.exists():
                base = candidate
            else:
                raise FileNotFoundError(f"Template {base} not found.")

        for name in output_names:
            target_path = (output_dir / name).resolve()
            if mode == 'part':
                self._duplicate_datamashup_part(base.resolve(), target_path)
            elif mode == 'element':
                self._duplicate_datamashup_element(base.resolve(), target_path)
            else:
                raise ValueError(f"Unsupported duplicate mode: {mode}")

    def _duplicate_datamashup_part(self, base_path: Path, output_path: Path):
        with zipfile.ZipFile(base_path, 'r') as zin:
            try:
                item1_xml = zin.read("customXml/item1.xml")
                item_props1 = zin.read("customXml/itemProps1.xml")
                item1_rels = zin.read("customXml/_rels/item1.xml.rels")
                content_types = zin.read("[Content_Types].xml")
                workbook_rels = zin.read("xl/_rels/workbook.xml.rels")
            except KeyError as e:
                raise FileNotFoundError(f"Required DataMashup part missing: {e}") from e

            updated_content_types = self._add_itemprops_override(content_types)
            updated_workbook_rels = self._add_workbook_relationship(workbook_rels)
            item2_rels = item1_rels.replace(b"itemProps1.xml", b"itemProps2.xml")
            item_props2 = item_props1.replace(
                b"{37E9CB8A-1D60-4852-BCC8-3140E13993BE}",
                b"{37E9CB8A-1D60-4852-BCC8-3140E13993BF}",
            )

            with zipfile.ZipFile(output_path, 'w') as zout:
                for info in zin.infolist():
                    data = zin.read(info.filename)
                    if info.filename == "[Content_Types].xml":
                        data = updated_content_types
                    elif info.filename == "xl/_rels/workbook.xml.rels":
                        data = updated_workbook_rels
                    zout.writestr(info, data)

                zout.writestr("customXml/item2.xml", item1_xml)
                zout.writestr("customXml/itemProps2.xml", item_props2)
                zout.writestr("customXml/_rels/item2.xml.rels", item2_rels)

    def _add_itemprops_override(self, content_types_bytes: bytes) -> bytes:
        ns = "http://schemas.openxmlformats.org/package/2006/content-types"
        root = ET.fromstring(content_types_bytes)
        override_tag = f"{{{ns}}}Override"
        if not any(
            elem.get("PartName") == "/customXml/itemProps2.xml"
            for elem in root.findall(override_tag)
        ):
            new_override = ET.SubElement(root, override_tag)
            new_override.set("PartName", "/customXml/itemProps2.xml")
            new_override.set(
                "ContentType",
                "application/vnd.openxmlformats-officedocument.customXmlProperties+xml",
            )
        return ET.tostring(root, xml_declaration=True, encoding="utf-8")

    def _add_workbook_relationship(self, rels_bytes: bytes) -> bytes:
        ns = "http://schemas.openxmlformats.org/package/2006/relationships"
        root = ET.fromstring(rels_bytes)
        rel_tag = f"{{{ns}}}Relationship"
        existing_ids = {elem.get("Id") for elem in root.findall(rel_tag)}
        next_id = 1
        while f"rId{next_id}" in existing_ids:
            next_id += 1
        new_rel = ET.SubElement(root, rel_tag)
        new_rel.set("Id", f"rId{next_id}")
        new_rel.set(
            "Type",
            "http://schemas.openxmlformats.org/officeDocument/2006/relationships/customXml",
        )
        new_rel.set("Target", "../customXml/item2.xml")
        return ET.tostring(root, xml_declaration=True, encoding="utf-8")

    def _duplicate_datamashup_element(self, base_path: Path, output_path: Path):
        with zipfile.ZipFile(base_path, 'r') as zin:
            with zipfile.ZipFile(output_path, 'w') as zout:
                for info in zin.infolist():
                    data = zin.read(info.filename)
                    if info.filename.startswith("customXml/item") and (
                        b"DataMashup" in data
                        or b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p" in data
                    ):
                        try:
                            root = etree.fromstring(data)
                            dm_node = self._get_mashup_element(root)
                            if dm_node is not None:
                                duplicate = copy.deepcopy(dm_node)
                                parent = dm_node.getparent()
                                if parent is not None:
                                    parent.append(duplicate)
                                    target_root = root
                                else:
                                    container = etree.Element("root", nsmap=root.nsmap)
                                    container.append(dm_node)
                                    container.append(duplicate)
                                    target_root = container
                                data = etree.tostring(
                                    target_root, encoding="utf-8", xml_declaration=True
                                )
                        except etree.XMLSyntaxError:
                            pass
                    zout.writestr(info, data)


class MashupEncodeGenerator(MashupBaseGenerator):
    """
    Re-encodes the DataMashup customXml stream to a target encoding and optionally
    inserts whitespace into the base64 payload.
    """

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        base_file_arg = self.args.get('base_file')
        encoding = self.args.get('encoding', 'utf-8')
        whitespace = bool(self.args.get('whitespace', False))
        if not base_file_arg:
            raise ValueError("MashupEncodeGenerator requires 'base_file' argument")

        base = Path(base_file_arg)
        if not base.exists():
            candidate = Path("fixtures") / base_file_arg
            if candidate.exists():
                base = candidate
            else:
                raise FileNotFoundError(f"Template {base} not found.")

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._rewrite_datamashup_xml(base.resolve(), target_path, encoding, whitespace)

    def _rewrite_datamashup_xml(
        self,
        base_path: Path,
        output_path: Path,
        encoding: str,
        whitespace: bool,
    ):
        with zipfile.ZipFile(base_path, 'r') as zin:
            with zipfile.ZipFile(output_path, 'w') as zout:
                for info in zin.infolist():
                    data = zin.read(info.filename)
                    if info.filename.startswith("customXml/item") and (
                        b"DataMashup" in data
                        or b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p" in data
                    ):
                        try:
                            data = self._process_datamashup_stream(data, encoding, whitespace)
                        except etree.XMLSyntaxError:
                            pass
                    zout.writestr(info, data)

    def _process_datamashup_stream(
        self,
        xml_bytes: bytes,
        encoding: str,
        whitespace: bool,
    ) -> bytes:
        root = etree.fromstring(xml_bytes)
        dm_node = self._get_mashup_element(root)
        if dm_node is None:
            return xml_bytes

        if dm_node.text and whitespace:
            dm_node.text = self._with_whitespace(dm_node.text)

        xml_bytes = etree.tostring(root, encoding="utf-8", xml_declaration=True)
        return self._encode_bytes(xml_bytes, encoding)

    def _with_whitespace(self, text: str) -> str:
        cleaned = text.strip()
        if not cleaned:
            return text
        midpoint = max(1, len(cleaned) // 2)
        return f"\n  {cleaned[:midpoint]}\n  {cleaned[midpoint:]}\n"

    def _encode_bytes(self, xml_bytes: bytes, encoding: str) -> bytes:
        enc = encoding.lower()
        if enc == "utf-8":
            return xml_bytes
        if enc == "utf-16-le":
            return self._to_utf16(xml_bytes, little_endian=True)
        if enc == "utf-16-be":
            return self._to_utf16(xml_bytes, little_endian=False)
        raise ValueError(f"Unsupported encoding: {encoding}")

    def _to_utf16(self, xml_bytes: bytes, little_endian: bool) -> bytes:
        text = xml_bytes.decode("utf-8")
        text = self._rewrite_declaration(text)
        encoded = text.encode("utf-16-le" if little_endian else "utf-16-be")
        bom = b"\xff\xfe" if little_endian else b"\xfe\xff"
        return bom + encoded

    def _rewrite_declaration(self, text: str) -> str:
        pattern = r'encoding=["\'][^"\']+["\']'
        if re.search(pattern, text):
            return re.sub(pattern, 'encoding="UTF-16"', text, count=1)
        prefix = "<?xml version='1.0'?>"
        if text.startswith(prefix):
            return text.replace(prefix, "<?xml version='1.0' encoding='UTF-16'?>", 1)
        return text


class MashupPermissionsMetadataGenerator(MashupBaseGenerator):
    """
    Builds fixtures that exercise Permissions and Metadata parsing by rewriting
    the PackageParts Section1.m, Permissions XML, and Metadata XML inside
    the DataMashup stream.
    """

    def __init__(self, args):
        super().__init__(args)
        self.mode = args.get("mode")

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if not self.mode:
            raise ValueError("MashupPermissionsMetadataGenerator requires 'mode' argument")

        if isinstance(output_names, str):
            output_names = [output_names]

        base_file_arg = self.args.get("base_file", "templates/base_query.xlsx")
        base = Path(base_file_arg)
        if not base.exists():
            candidate = Path("fixtures") / base_file_arg
            if candidate.exists():
                base = candidate
            else:
                raise FileNotFoundError(f"Template {base} not found.")

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._process_excel_container(base.resolve(), target_path, self._rewrite_datamashup)

    def _rewrite_datamashup(self, raw_bytes: bytes) -> bytes:
        if self.args.get("inspect_bindings"):
            self._log_permission_bindings(raw_bytes)

        version, package_parts, _, _, bindings = self._split_sections(raw_bytes)
        bindings_override = self._bindings_override()
        if bindings_override is not None:
            bindings = bindings_override
        scenario = self._scenario_definition()

        updated_package_parts = self._replace_section(
            package_parts,
            scenario["section_text"],
        )
        permissions_bytes = self._permissions_bytes(**scenario["permissions"])
        metadata_bytes = self._metadata_bytes(scenario["metadata_entries"])

        return self._assemble_sections(
            version,
            updated_package_parts,
            permissions_bytes,
            metadata_bytes,
            bindings,
        )

    def _log_permission_bindings(self, raw_bytes: bytes):
        _, _, _, _, bindings = self._split_sections(raw_bytes)
        head = bindings[:16]
        print(
            f"Permission bindings length={len(bindings)} head={head!r}"
        )

    def _bindings_override(self) -> Optional[bytes]:
        override = self.args.get("bindings_override")
        if not override:
            return None
        if not isinstance(override, str):
            raise ValueError("bindings_override must be a hex string")
        cleaned = override.strip().replace(" ", "")
        if len(cleaned) % 2 != 0:
            raise ValueError("bindings_override hex string must have even length")
        try:
            return bytes.fromhex(cleaned)
        except ValueError as exc:
            raise ValueError("bindings_override must be valid hex") from exc

    def _scenario_definition(self):
        shared_section_simple = "\n".join(
            [
                "section Section1;",
                "",
                "shared LoadToSheet = 1;",
                "shared LoadToModel = 2;",
            ]
        )

        def default_permissions():
            return {
                "can_eval": False,
                "firewall_enabled": True,
                "group_type": "Organizational",
            }

        def build_section_text(query_specs):
            lines = ["section Section1;", ""]
            for spec in query_specs:
                lines.append(f"shared {spec['name']} = {spec['body']};")
            return "\n".join(lines)

        def build_metadata_entries(query_specs):
            entries = []
            for spec in query_specs:
                stable_entries = []
                if spec.get("load_to_sheet"):
                    stable_entries.append(("FillEnabled", True))
                if spec.get("load_to_model"):
                    stable_entries.append(("FillToDataModelEnabled", True))
                entries.append(
                    {
                        "path": f"Section1/{spec['name']}",
                        "entries": stable_entries,
                    }
                )
            return entries

        def m_diff_scenario(query_specs):
            return {
                "section_text": build_section_text(query_specs),
                "permissions": default_permissions(),
                "metadata_entries": build_metadata_entries(query_specs),
            }

        if self.mode in ("permissions_defaults", "permissions_firewall_off", "metadata_simple"):
            return {
                "section_text": shared_section_simple,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": self.mode != "permissions_firewall_off",
                    "group_type": "Organizational",
                },
                "metadata_entries": [
                    {
                        "path": "Section1/LoadToSheet",
                        "entries": [
                            ("FillEnabled", True),
                            ("FillToDataModelEnabled", False),
                        ],
                    },
                    {
                        "path": "Section1/LoadToModel",
                        "entries": [
                            ("FillEnabled", False),
                            ("FillToDataModelEnabled", True),
                        ],
                    },
                ],
            }

        if self.mode == "m_add_query_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_add_query_b":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                    {"name": "Bar", "body": "2", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_remove_query_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                    {"name": "Bar", "body": "2", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_remove_query_b":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_change_literal_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_change_literal_b":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "2", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_metadata_only_change_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_metadata_only_change_b":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": False, "load_to_model": True},
                ]
            )

        if self.mode == "m_def_and_metadata_change_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_def_and_metadata_change_b":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "2", "load_to_sheet": False, "load_to_model": True},
                ]
            )

        if self.mode == "m_rename_query_a":
            return m_diff_scenario(
                [
                    {"name": "Foo", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode == "m_rename_query_b":
            return m_diff_scenario(
                [
                    {"name": "Bar", "body": "1", "load_to_sheet": True, "load_to_model": False},
                ]
            )

        if self.mode in ("composed_mashup_a", "composed_mashup_b"):
            embedded_guid = self.args.get("embedded_guid", "efgh")
            sales_body = "\n".join(
                [
                    "let",
                    '    Source = #table({"Id","Name","RegionId","Amount"}, {{1, "Alice", 1, 10}, {2, "Bob", 2, 20}}),',
                    "    KeptRows = Table.SelectRows(Source, each [Amount] > 0),",
                    '    Renamed = Table.RenameColumns(KeptRows, {{"Name", "Customer"}})',
                    "in",
                    "    Renamed",
                ]
            )
            regions_body = "\n".join(
                [
                    "let",
                    '    Source = #table({"Id","RegionName"}, {{1, "North"}, {2, "South"}})',
                    "in",
                    "    Source",
                ]
            )
            join_step = "Joined" if self.mode == "composed_mashup_a" else "Merged"
            left_key = "RegionId" if self.mode == "composed_mashup_a" else "Id"
            sales_with_regions_body = "\n".join(
                [
                    "let",
                    "    Source = Sales,",
                    f'    {join_step} = Table.NestedJoin(Source, {{"{left_key}"}}, Regions, {{"Id"}}, "Region", JoinKind.LeftOuter),',
                    f'    Expanded = Table.ExpandTableColumn({join_step}, "Region", {{"RegionName"}}, {{"RegionName"}})',
                    "in",
                    "    Expanded",
                ]
            )
            query_specs = [
                {
                    "name": "Sales",
                    "body": sales_body,
                    "load_to_sheet": True,
                    "load_to_model": False,
                },
                {
                    "name": "Regions",
                    "body": regions_body,
                    "load_to_sheet": False,
                    "load_to_model": True,
                },
                {
                    "name": "SalesWithRegions",
                    "body": sales_with_regions_body,
                    "load_to_sheet": True,
                    "load_to_model": self.mode == "composed_mashup_b",
                },
                {
                    "name": "EmbeddedQuery",
                    "body": f'Embedded.Value("Content/{embedded_guid}.package")',
                    "load_to_sheet": False,
                    "load_to_model": False,
                },
            ]
            return m_diff_scenario(query_specs)

        if self.mode in ("m_adversarial_steps_a", "m_adversarial_steps_b"):
            join_key_step2 = "Id" if self.mode == "m_adversarial_steps_a" else "Group"
            lookup_body = "\n".join(
                [
                    "let",
                    '    Source = #table({"Id","Group","Value"}, {{1, "A", "X"}, {2, "B", "Y"}})',
                    "in",
                    "    Source",
                ]
            )
            adversarial_body = "\n".join(
                [
                    "let",
                    '    Source = #table({"Id","Group"}, {{1, "A"}, {2, "B"}}),',
                    '    Step1 = Table.NestedJoin(Source, {"Id"}, Lookup, {"Id"}, "Join1", JoinKind.LeftOuter),',
                    f'    Step2 = Table.NestedJoin(Step1, {{"{join_key_step2}"}}, Lookup, {{"{join_key_step2}"}}, "Join2", JoinKind.LeftOuter),',
                    '    Step3 = Table.NestedJoin(Step2, {"Id"}, Lookup, {"Id"}, "Join3", JoinKind.LeftOuter)',
                    "in",
                    "    Step3",
                ]
            )
            return m_diff_scenario(
                [
                    {
                        "name": "Lookup",
                        "body": lookup_body,
                        "load_to_sheet": False,
                        "load_to_model": False,
                    },
                    {
                        "name": "Adversarial",
                        "body": adversarial_body,
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_formatting_only_a":
            body = "let Source = 1, Foo = 2 in Source"
            return m_diff_scenario(
                [
                    {
                        "name": "FormatTest",
                        "body": body,
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_formatting_only_b":
            body = "\n".join(
                [
                    "let",
                    "    // Same semantics as m_formatting_only_a with different formatting",
                    "    Source = 1,",
                    "    Foo = 2",
                    "in",
                    "    Source",
                ]
            )
            return m_diff_scenario(
                [
                    {
                        "name": "FormatTest",
                        "body": body,
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_formatting_only_b_variant":
            body = "\n".join(
                [
                    "let",
                    "    Source = 1,",
                    "    Foo = 3",
                    "in",
                    "    Source",
                ]
            )
            return m_diff_scenario(
                [
                    {
                        "name": "FormatTest",
                        "body": body,
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_record_equiv_a":
            return m_diff_scenario(
                [
                    {
                        "name": "RecordRoot",
                        "body": "[B=2, A=1]",
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_record_equiv_b":
            return m_diff_scenario(
                [
                    {
                        "name": "RecordRoot",
                        "body": "[A=1, B=2]",
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_list_formatting_a":
            return m_diff_scenario(
                [
                    {
                        "name": "ListRoot",
                        "body": "{1,2,3}",
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_list_formatting_b":
            return m_diff_scenario(
                [
                    {
                        "name": "ListRoot",
                        "body": "{ 1, /*c*/ 2, 3 }",
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_call_formatting_a":
            return m_diff_scenario(
                [
                    {
                        "name": "CallRoot",
                        "body": 'Table.FromRows({{1,2},{3,4}}, {"A","B"})',
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_call_formatting_b":
            body = "\n".join(
                [
                    "Table.FromRows(",
                    "    {{1,2},{3,4}},",
                    "    {\"A\", \"B\"}",
                    ")",
                ]
            )
            return m_diff_scenario(
                [
                    {
                        "name": "CallRoot",
                        "body": body,
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_primitive_formatting_a":
            return m_diff_scenario(
                [
                    {
                        "name": "PrimRoot",
                        "body": '"hello"',
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "m_primitive_formatting_b":
            return m_diff_scenario(
                [
                    {
                        "name": "PrimRoot",
                        "body": '"hello" // formatting-only whitespace and comment',
                        "load_to_sheet": True,
                        "load_to_model": False,
                    },
                ]
            )

        if self.mode == "metadata_query_groups":
            section_text = "\n".join(
                [
                    "section Section1;",
                    "",
                    "shared RootQuery = 1;",
                    "shared GroupedFoo = 2;",
                    "shared NestedBar = 3;",
                ]
            )
            return {
                "section_text": section_text,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": True,
                    "group_type": "Organizational",
                },
                "metadata_entries": [
                    {
                        "path": "Section1/RootQuery",
                        "entries": [("FillEnabled", True)],
                    },
                    {
                        "path": "Section1/GroupedFoo",
                        "entries": [
                            ("FillEnabled", True),
                            ("QueryGroupPath", "Inputs/DimTables"),
                        ],
                    },
                    {
                        "path": "Section1/NestedBar",
                        "entries": [
                            ("FillToDataModelEnabled", True),
                            ("QueryGroupPath", "Inputs/DimTables"),
                        ],
                    },
                ],
            }

        if self.mode == "metadata_hidden_queries":
            section_text = "\n".join(
                [
                    "section Section1;",
                    "",
                    "shared ConnectionOnly = 1;",
                    "shared VisibleLoad = 2;",
                ]
            )
            return {
                "section_text": section_text,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": True,
                    "group_type": "Organizational",
                },
                "metadata_entries": [
                    {
                        "path": "Section1/ConnectionOnly",
                        "entries": [
                            ("FillEnabled", False),
                            ("FillToDataModelEnabled", False),
                        ],
                    },
                    {
                        "path": "Section1/VisibleLoad",
                        "entries": [
                            ("FillEnabled", True),
                            ("FillToDataModelEnabled", False),
                        ],
                    },
                ],
            }

        if self.mode == "metadata_missing_entry":
            section_text = "\n".join(
                [
                    "section Section1;",
                    "",
                    "shared MissingMetadata = 1;",
                ]
            )
            return {
                "section_text": section_text,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": True,
                    "group_type": "Organizational",
                },
                "metadata_entries": [],
            }

        if self.mode == "metadata_url_encoding":
            section_text = "\n".join(
                [
                    "section Section1;",
                    "",
                    'shared #"Query with space & #" = 1;',
                ]
            )
            return {
                "section_text": section_text,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": True,
                    "group_type": "Organizational",
                },
                "metadata_entries": [
                    {
                        "path": "Section1/Query%20with%20space%20%26%20%23",
                        "entries": [
                            ("FillEnabled", True),
                            ("FillToDataModelEnabled", False),
                        ],
                    },
                ],
            }

        if self.mode == "metadata_orphan_entries":
            section_text = "\n".join(
                [
                    "section Section1;",
                    "",
                    "shared Foo = 1;",
                ]
            )
            return {
                "section_text": section_text,
                "permissions": {
                    "can_eval": False,
                    "firewall_enabled": True,
                    "group_type": "Organizational",
                },
                "metadata_entries": [
                    {
                        "path": "Section1/Foo",
                        "entries": [("FillEnabled", True)],
                    },
                    {
                        "path": "Section1/Nonexistent",
                        "entries": [("FillEnabled", False)],
                    },
                ],
            }

        raise ValueError(f"Unsupported mode: {self.mode}")

    def _split_sections(self, raw_bytes: bytes):
        min_size = 4 + 4 * 4
        if len(raw_bytes) < min_size:
            raise ValueError("DataMashup stream too short")

        offset = 0
        version = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4

        package_parts_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        package_parts = raw_bytes[offset : offset + package_parts_len]
        offset += package_parts_len

        permissions_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        permissions = raw_bytes[offset : offset + permissions_len]
        offset += permissions_len

        metadata_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        metadata = raw_bytes[offset : offset + metadata_len]
        offset += metadata_len

        bindings_len = struct.unpack_from("<I", raw_bytes, offset)[0]
        offset += 4
        bindings = raw_bytes[offset : offset + bindings_len]

        return version, package_parts, permissions, metadata, bindings

    def _assemble_sections(
        self,
        version: int,
        package_parts: bytes,
        permissions: bytes,
        metadata: bytes,
        bindings: bytes,
    ) -> bytes:
        return b"".join(
            [
                struct.pack("<I", version),
                struct.pack("<I", len(package_parts)),
                package_parts,
                struct.pack("<I", len(permissions)),
                permissions,
                struct.pack("<I", len(metadata)),
                metadata,
                struct.pack("<I", len(bindings)),
                bindings,
            ]
        )

    def _replace_section(self, package_parts: bytes, section_text: str) -> bytes:
        return self._replace_in_zip(package_parts, "Formulas/Section1.m", section_text)

    def _replace_in_zip(self, zip_bytes: bytes, filename: str, new_content: str) -> bytes:
        in_buffer = io.BytesIO(zip_bytes)
        out_buffer = io.BytesIO()

        with zipfile.ZipFile(in_buffer, "r") as zin:
            with zipfile.ZipFile(out_buffer, "w", compression=zipfile.ZIP_DEFLATED) as zout:
                for item in zin.infolist():
                    if item.filename == filename:
                        zout.writestr(item, new_content.encode("utf-8"))
                    else:
                        zout.writestr(item, zin.read(item.filename))
        return out_buffer.getvalue()

    def _permissions_bytes(self, can_eval: bool, firewall_enabled: bool, group_type: str) -> bytes:
        xml = (
            '<?xml version="1.0" encoding="utf-8"?>'
            "<PermissionList xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" "
            "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">"
            f"<CanEvaluateFuturePackages>{str(can_eval).lower()}</CanEvaluateFuturePackages>"
            f"<FirewallEnabled>{str(firewall_enabled).lower()}</FirewallEnabled>"
            f"<WorkbookGroupType>{group_type}</WorkbookGroupType>"
            "</PermissionList>"
        )
        return ("\ufeff" + xml).encode("utf-8")

    def _metadata_bytes(self, items: List[dict]) -> bytes:
        xml = self._metadata_xml(items)
        xml_bytes = ("\ufeff" + xml).encode("utf-8")
        header = struct.pack("<I", 0) + struct.pack("<I", len(xml_bytes))
        return header + xml_bytes

    def _metadata_xml(self, items: List[dict]) -> str:
        parts = [
            '<?xml version="1.0" encoding="utf-8"?>',
            '<LocalPackageMetadataFile xmlns:xsd="http://www.w3.org/2001/XMLSchema" '
            'xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">',
            "<Items>",
            "<Item><ItemLocation><ItemType>AllFormulas</ItemType><ItemPath /></ItemLocation><StableEntries /></Item>",
        ]

        for item in items:
            parts.append("<Item>")
            parts.append("<ItemLocation>")
            parts.append("<ItemType>Formula</ItemType>")
            parts.append(f"<ItemPath>{item['path']}</ItemPath>")
            parts.append("</ItemLocation>")
            parts.append("<StableEntries>")
            for entry_name, entry_value in item.get("entries", []):
                value = self._format_entry_value(entry_value)
                parts.append(f'<Entry Type="{entry_name}" Value="{value}" />')
            parts.append("</StableEntries>")
            parts.append("</Item>")

        parts.append("</Items></LocalPackageMetadataFile>")
        return "".join(parts)

    def _format_entry_value(self, value):
        if isinstance(value, bool):
            return f"l{'1' if value else '0'}"
        return f"s{value}"


class MashupAttachGenerator(BaseGenerator):
    """
    Attach a DataMashup customXml payload from one workbook to another.
    """

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        base_file_arg = self.args.get("base_file")
        mashup_file_arg = self.args.get("mashup_file")
        if not base_file_arg or not mashup_file_arg:
            raise ValueError("MashupAttachGenerator requires base_file and mashup_file")

        base = self._resolve_fixture_path(base_file_arg)
        mashup = self._resolve_fixture_path(mashup_file_arg)
        parts = self._read_mashup_parts(mashup)

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._attach_parts(base, target_path, parts)

    def _resolve_fixture_path(self, value: str) -> Path:
        candidate = Path(value)
        if candidate.exists():
            return candidate
        fallback = Path("fixtures") / value
        if fallback.exists():
            return fallback
        raise FileNotFoundError(f"Fixture file not found: {value}")

    def _read_mashup_parts(self, path: Path) -> dict:
        with zipfile.ZipFile(path, "r") as zin:
            item_name = None
            item_data = None
            for info in zin.infolist():
                name = info.filename
                if not (name.startswith("customXml/item") and name.endswith(".xml")):
                    continue
                if "itemProps" in name:
                    continue
                data = zin.read(name)
                if (
                    b"DataMashup" in data
                    or b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p" in data
                ):
                    item_name = name
                    item_data = data
                    break

            if item_name is None or item_data is None:
                raise ValueError("DataMashup customXml part not found")

            item_stem = Path(item_name).stem  # item1
            suffix = item_stem.replace("item", "", 1)
            item_props_name = f"customXml/itemProps{suffix}.xml"
            item_rels_name = f"customXml/_rels/{Path(item_name).name}.rels"

            item_props_data = zin.read(item_props_name)
            item_rels_data = zin.read(item_rels_name)

        return {
            "item_name": item_name,
            "item_data": item_data,
            "item_props_name": item_props_name,
            "item_props_data": item_props_data,
            "item_rels_name": item_rels_name,
            "item_rels_data": item_rels_data,
        }

    def _attach_parts(self, base_path: Path, output_path: Path, parts: dict):
        ct_name = "[Content_Types].xml"
        rels_name = "xl/_rels/workbook.xml.rels"
        item_name = parts["item_name"]
        item_props_name = parts["item_props_name"]
        item_rels_name = parts["item_rels_name"]

        with zipfile.ZipFile(base_path, "r") as zin:
            with zipfile.ZipFile(output_path, "w") as zout:
                for info in zin.infolist():
                    name = info.filename
                    if name in (item_name, item_props_name, item_rels_name):
                        continue
                    data = zin.read(name)
                    if name == ct_name:
                        data = self._ensure_itemprops_override(data, item_props_name)
                    elif name == rels_name:
                        data = self._ensure_customxml_rel(data, Path(item_name).name)
                    zout.writestr(info, data)

                zout.writestr(item_name, parts["item_data"])
                zout.writestr(item_props_name, parts["item_props_data"])
                zout.writestr(item_rels_name, parts["item_rels_data"])

    def _ensure_itemprops_override(self, content_types_bytes: bytes, item_props_name: str) -> bytes:
        ns = "http://schemas.openxmlformats.org/package/2006/content-types"
        ET.register_namespace("", ns)
        root = ET.fromstring(content_types_bytes)
        override_tag = f"{{{ns}}}Override"
        item_props_path = "/" + item_props_name.replace("\\", "/")
        if not any(
            elem.get("PartName") == item_props_path for elem in root.findall(override_tag)
        ):
            new_override = ET.SubElement(root, override_tag)
            new_override.set("PartName", item_props_path)
            new_override.set(
                "ContentType",
                "application/vnd.openxmlformats-officedocument.customXmlProperties+xml",
            )
        return ET.tostring(root, xml_declaration=True, encoding="utf-8")

    def _ensure_customxml_rel(self, rels_bytes: bytes, item_filename: str) -> bytes:
        ns = "http://schemas.openxmlformats.org/package/2006/relationships"
        ET.register_namespace("", ns)
        root = ET.fromstring(rels_bytes)
        rel_tag = f"{{{ns}}}Relationship"
        target = f"../customXml/{item_filename}"

        for rel in root.findall(rel_tag):
            if (
                rel.get("Type")
                == "http://schemas.openxmlformats.org/officeDocument/2006/relationships/customXml"
                and rel.get("Target") == target
            ):
                return ET.tostring(root, xml_declaration=True, encoding="utf-8")

        max_id = 0
        for rel in root.findall(rel_tag):
            rid = rel.get("Id", "")
            if rid.startswith("rId") and rid[3:].isdigit():
                max_id = max(max_id, int(rid[3:]))

        new_rel = ET.SubElement(root, rel_tag)
        new_rel.set("Id", f"rId{max_id + 1}")
        new_rel.set(
            "Type",
            "http://schemas.openxmlformats.org/officeDocument/2006/relationships/customXml",
        )
        new_rel.set("Target", target)
        return ET.tostring(root, xml_declaration=True, encoding="utf-8")

```

---

### File: `fixtures\src\generators\objects.py`

```python
import shutil
from pathlib import Path
from typing import Any, Dict, List, Union

import openpyxl
from openpyxl.chart import BarChart, LineChart, Reference
from openpyxl.workbook.defined_name import DefinedName

from .base import BaseGenerator


class NamedRangesGenerator(BaseGenerator):
    """Generates a workbook pair that exercises workbook- and sheet-scoped defined names."""

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("named_ranges generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Sheet1")

        def create_workbook(global_names: List[DefinedName], local_name: DefinedName, path: Path):
            wb = openpyxl.Workbook()
            ws1 = wb.active
            ws1.title = sheet
            wb.create_sheet("Sheet2")

            ws1["A1"] = 1
            ws1["A2"] = 2
            ws1["A3"] = 3
            ws1["B1"] = 4
            ws1["C1"] = 5
            ws1["C2"] = 6
            ws1["D1"] = 7
            ws1["D2"] = 8

            for name in global_names:
                wb.defined_names.add(name)
            wb.defined_names.add(local_name)

            wb.save(path)

        global_keep = DefinedName("GlobalKeep", attr_text=f"{sheet}!$A$1:$A$3")
        global_remove = DefinedName("GlobalRemove", attr_text=f"{sheet}!$B$1")
        local_change_a = DefinedName("LocalChange", attr_text=f"{sheet}!$C$1", localSheetId=0)

        global_add = DefinedName("GlobalAdd", attr_text=f"{sheet}!$D$1:$D$2")
        local_change_b = DefinedName("LocalChange", attr_text=f"{sheet}!$C$2", localSheetId=0)

        create_workbook(
            global_names=[global_keep, global_remove],
            local_name=local_change_a,
            path=output_dir / output_names[0],
        )
        create_workbook(
            global_names=[global_keep, global_add],
            local_name=local_change_b,
            path=output_dir / output_names[1],
        )


class ChartsGenerator(BaseGenerator):
    """Generates a workbook pair that exercises chart add/remove/change detection."""

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        if len(output_names) != 2:
            raise ValueError("charts generator expects exactly two output filenames")

        sheet = self.args.get("sheet", "Sheet1")

        def create_workbook(path: Path, chart1: Any, chart2: Any = None):
            wb = openpyxl.Workbook()
            ws = wb.active
            ws.title = sheet

            ws.append(["X", "Y"])
            for idx in range(1, 6):
                ws.append([idx, idx * 2])

            data = Reference(ws, min_col=2, min_row=1, max_row=6)
            cats = Reference(ws, min_col=1, min_row=2, max_row=6)

            chart1.add_data(data, titles_from_data=True)
            chart1.set_categories(cats)
            ws.add_chart(chart1, "E2")

            if chart2 is not None:
                chart2.add_data(data, titles_from_data=True)
                chart2.set_categories(cats)
                ws.add_chart(chart2, "E18")

            wb.save(path)

        create_workbook(output_dir / output_names[0], chart1=LineChart())
        create_workbook(output_dir / output_names[1], chart1=BarChart(), chart2=LineChart())


class CopyTemplateGenerator(BaseGenerator):
    """Copies a binary template file into the output directory."""

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        template_arg = self.args.get("template")
        if not template_arg:
            raise ValueError("copy_template generator requires 'template' argument")

        template = Path(template_arg)
        if not template.exists():
            candidate = Path("fixtures") / template_arg
            if candidate.exists():
                template = candidate
            else:
                raise FileNotFoundError(f"Template {template} not found.")

        for name in output_names:
            shutil.copyfile(template, output_dir / name)

```

---

### File: `fixtures\src\generators\pbix.py`

```python
import base64
import zipfile
from pathlib import Path

from lxml import etree

from .base import BaseGenerator


_NS = {"dm": "http://schemas.microsoft.com/DataMashup"}


def _find_datamashup_element(root):
    if root is None:
        return None
    if root.tag.endswith("DataMashup"):
        return root
    return root.find(".//dm:DataMashup", namespaces=_NS)


def _extract_datamashup_bytes_from_xlsx(path: Path) -> bytes:
    with zipfile.ZipFile(path, "r") as zin:
        for info in zin.infolist():
            name = info.filename
            if not (name.startswith("customXml/item") and name.endswith(".xml")):
                continue

            buf = zin.read(name)
            if (
                b"DataMashup" not in buf
                and b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p" not in buf
            ):
                continue

            root = etree.fromstring(buf)
            node = _find_datamashup_element(root)
            if node is None or node.text is None:
                continue

            text = node.text.strip()
            if not text:
                continue

            return base64.b64decode(text)

    raise ValueError("DataMashup not found in xlsx")


class PbixGenerator(BaseGenerator):
    def generate(self, out_dir: Path, outputs):
        if isinstance(outputs, str):
            outputs = [outputs]

        if len(outputs) != 1:
            raise ValueError("pbix generator expects exactly one output filename")

        out_path = out_dir / outputs[0]

        mode = self.args.get("mode", "from_xlsx")
        base_file = self.args.get("base_file")

        if mode not in ("from_xlsx", "no_datamashup", "no_schema"):
            raise ValueError(f"Unsupported pbix generator mode: {mode}")

        include_datamashup = mode == "from_xlsx"
        include_markers = True
        include_schema = mode != "no_schema"

        schema_bytes = None
        model_schema = self.args.get("model_schema")
        model_schema_file = self.args.get("model_schema_file")
        if model_schema_file:
            model_schema_path = Path(model_schema_file)
            if not model_schema_path.exists():
                model_schema_path = Path("fixtures") / model_schema_file
            schema_bytes = model_schema_path.read_bytes()
        elif model_schema is not None:
            schema_bytes = str(model_schema).encode("utf-8")

        dm_bytes = b""
        if include_datamashup:
            if not base_file:
                raise ValueError("base_file is required for mode=from_xlsx")
            base_path = Path(base_file)
            if not base_path.exists():
                base_path = Path("fixtures") / base_file
            dm_bytes = _extract_datamashup_bytes_from_xlsx(base_path)

        with zipfile.ZipFile(out_path, "w", compression=zipfile.ZIP_DEFLATED) as zout:
            if include_datamashup:
                zout.writestr("DataMashup", dm_bytes)
            if include_markers:
                zout.writestr("Report/Layout", b"{}")
                zout.writestr("Report/Version", b"1")
                if include_schema:
                    zout.writestr("DataModelSchema", schema_bytes or b"{}")

```

---

### File: `fixtures\src\generators\perf.py`

```python
import openpyxl
import random
from pathlib import Path
from typing import Union, List
from .base import BaseGenerator

class LargeGridGenerator(BaseGenerator):
    """
    Generates massive grids using WriteOnly mode to save memory.
    Targeting P1/P2/P3/P4/P5 milestones.
    """
    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        rows = self.args.get('rows', 1000)
        cols = self.args.get('cols', 10)
        mode = self.args.get('mode', 'dense')
        seed = self.args.get('seed', 0)
        pattern_length = self.args.get('pattern_length', 100)
        fill_percent = self.args.get('fill_percent', 100)
        edit_row = self.args.get('edit_row')
        edit_col = self.args.get('edit_col')
        edit_value = self.args.get('edit_value')
        if edit_row is not None:
            edit_row = int(edit_row)
        if edit_col is not None:
            edit_col = int(edit_col)

        rng = random.Random(seed)

        for name in output_names:
            wb = openpyxl.Workbook(write_only=True)
            ws = wb.create_sheet()
            ws.title = "Performance"

            header = [f"Col_{c}" for c in range(1, cols + 1)]
            ws.append(header)

            for r in range(1, rows + 1):
                row_data = []
                if mode == 'dense':
                    row_data = [f"R{r}C{c}" for c in range(1, cols + 1)]
                
                elif mode == 'noise':
                    row_data = [rng.random() for _ in range(cols)]
                
                elif mode == 'repetitive':
                    pattern_idx = (r - 1) % pattern_length
                    row_data = [f"P{pattern_idx}C{c}" for c in range(1, cols + 1)]
                
                elif mode == 'sparse':
                    row_data = []
                    for c in range(1, cols + 1):
                        if rng.randint(1, 100) <= fill_percent:
                            row_data.append(f"R{r}C{c}")
                        else:
                            row_data.append(None)
                if edit_row is not None and edit_col is not None and r == edit_row:
                    idx = edit_col - 1
                    if 0 <= idx < cols:
                        row_data[idx] = edit_value

                ws.append(row_data)

            wb.save(output_dir / name)


```

---

### File: `fixtures\src\generators\xlsb.py`

```python
import zipfile
from pathlib import Path
from typing import List, Union

from .base import BaseGenerator


class XlsbStubGenerator(BaseGenerator):
    """
    Create a minimal OPC container with xl/workbook.bin to exercise XLSB detection.
    """

    def generate(self, output_dir: Path, output_names: Union[str, List[str]]):
        if isinstance(output_names, str):
            output_names = [output_names]

        for name in output_names:
            target_path = (output_dir / name).resolve()
            self._write_stub(target_path)

    def _write_stub(self, path: Path):
        content_types = (
            '<?xml version="1.0" encoding="UTF-8"?>'
            '<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">'
            '<Default Extension="bin" '
            'ContentType="application/vnd.ms-excel.sheet.binary.macroEnabled.main" />'
            "</Types>"
        )
        rels = (
            '<?xml version="1.0" encoding="UTF-8"?>'
            '<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">'
            "</Relationships>"
        )

        with zipfile.ZipFile(path, "w", compression=zipfile.ZIP_DEFLATED) as zout:
            zout.writestr("[Content_Types].xml", content_types)
            zout.writestr("_rels/.rels", rels)
            zout.writestr("xl/workbook.bin", b"XLSB-STUB")

```

---

### File: `scripts\add_regression_fixture.py`

```python
import argparse
import hashlib
import re
import shutil
from pathlib import Path


def repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def slugify(text: str) -> str:
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9]+", "_", text)
    return text.strip("_") or "regression"


def sha256_file(path: Path) -> str:
    hasher = hashlib.sha256()
    with path.open("rb") as handle:
        for chunk in iter(lambda: handle.read(1024 * 1024), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def append_block(path: Path, block: str) -> None:
    content = ""
    if path.exists():
        content = path.read_text(encoding="utf-8")
    if content and not content.endswith("\n"):
        content += "\n"
    content += block
    if not content.endswith("\n"):
        content += "\n"
    path.write_text(content, encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser(description="Add a regression fixture from a fuzz artifact.")
    parser.add_argument("--artifact", required=True, help="Path to minimized artifact")
    parser.add_argument(
        "--type",
        required=True,
        choices=["xlsx", "xlsm", "pbix", "pbit", "dm_bytes"],
        help="Fixture classification",
    )
    parser.add_argument("--area", required=True, help="Area name (e.g. workbook, pbix, datamashup)")
    parser.add_argument("--description", required=True, help="Short description")
    parser.add_argument(
        "--expectation",
        choices=["ok", "error"],
        help="Optional expectation to append to robustness_regressions.yaml",
    )
    parser.add_argument(
        "--error-code",
        help="Required when --expectation=error (e.g. EXDIFF_DM_003)",
    )
    args = parser.parse_args()

    root = repo_root()
    artifact = Path(args.artifact).resolve()
    if not artifact.exists():
        raise FileNotFoundError(f"Artifact not found: {artifact}")

    digest = sha256_file(artifact)[:8]
    slug = slugify(args.description)
    ext = args.type if args.type != "dm_bytes" else "bin"

    filename = f"reg_{args.area}_{slug}_{digest}.{ext}"
    template_dir = root / "fixtures" / "templates" / "regressions" / args.area
    template_dir.mkdir(parents=True, exist_ok=True)
    template_path = template_dir / filename
    shutil.copy2(artifact, template_path)

    manifest_path = root / "fixtures" / "manifest_cli_tests.yaml"
    scenario_id = f"regression_{args.area}_{slug}_{digest}"
    manifest_block = (
        f'  - id: "{scenario_id}"\n'
        f'    generator: "copy_template"\n'
        f'    args:\n'
        f'      template: "templates/regressions/{args.area}/{filename}"\n'
        f'    output: "{filename}"\n'
    )
    append_block(manifest_path, manifest_block)

    if args.expectation:
        if args.expectation == "error" and not args.error_code:
            raise ValueError("--error-code is required when --expectation=error")
        expectations_path = root / "core" / "tests" / "robustness_regressions.yaml"
        if not expectations_path.exists():
            expectations_path.write_text("fixtures:\n", encoding="utf-8")
        error_line = ""
        if args.expectation == "error":
            error_line = f'      error_code: "{args.error_code}"\n'
        expectations_block = (
            f'  - file: "{filename}"\n'
            f'    type: "{args.type}"\n'
            f'    expectation:\n'
            f'      result: "{args.expectation}"\n'
            f"{error_line}"
            f'    invariants:\n'
            f'      self_diff_empty: {str(args.expectation == "ok").lower()}\n'
            f'      deterministic_open: {str(args.expectation == "ok").lower()}\n'
        )
        append_block(expectations_path, expectations_block)

    print(f"Added template: {template_path}")
    print(f"Appended manifest scenario: {scenario_id}")
    print("Remember to regenerate fixtures and update the lock file.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\arch_guard.py`

```python
from __future__ import annotations

from pathlib import Path
import sys


ROOT = Path(__file__).resolve().parents[1]

PARSE_FILES = [
    "core/src/excel_open_xml.rs",
    "core/src/grid_parser.rs",
    "core/src/datamashup_framing.rs",
]

DIFF_FILES = [
    "core/src/diff.rs",
    "core/src/object_diff.rs",
    "core/src/m_diff.rs",
    "core/src/formula_diff.rs",
]
DIFF_GLOBS = [
    "core/src/engine/*.rs",
    "core/src/m_ast_diff/*.rs",
]

PARSE_FORBIDDEN = [
    "crate::diff",
    "crate::engine",
    "crate::package",
]

DIFF_FORBIDDEN = [
    "crate::excel_open_xml",
    "crate::grid_parser",
    "crate::container",
    "crate::datamashup_framing",
]


def expand_globs(globs: list[str]) -> list[str]:
    files: list[str] = []
    for pattern in globs:
        files.extend(str(p.relative_to(ROOT)) for p in ROOT.glob(pattern))
    return sorted(set(files))


def scan_files(files: list[str], forbidden: list[str], label: str) -> list[str]:
    violations: list[str] = []
    for rel in files:
        path = ROOT / rel
        if not path.exists():
            violations.append(f"{label}: missing {rel}")
            continue
        text = path.read_text(encoding="utf-8")
        for token in forbidden:
            if token in text:
                violations.append(f"{label}: {rel} contains {token}")
    return violations


def main() -> int:
    diff_files = DIFF_FILES + expand_globs(DIFF_GLOBS)
    violations = []
    violations.extend(scan_files(PARSE_FILES, PARSE_FORBIDDEN, "parse"))
    violations.extend(scan_files(diff_files, DIFF_FORBIDDEN, "diff"))

    if violations:
        print("Architecture guard violations:")
        for entry in violations:
            print(f"- {entry}")
        return 1

    print("Architecture guard: OK")
    return 0


if __name__ == "__main__":
    sys.exit(main())

```

---

### File: `scripts\check_fixture_references.py`

```python
import re
import sys
from pathlib import Path

import yaml

RE_FIXTURE_NAME = re.compile(r'"([A-Za-z0-9._-]+\.(?:xlsx|xlsm|pbix|pbit|zip|txt|bin))"')
RE_WORKFLOW_REF = re.compile(r"fixtures/generated/([A-Za-z0-9._-]+\.(?:xlsx|xlsm|pbix|pbit|zip|txt|bin))")

IGNORED_FIXTURE_NAMES = {
    "definitely_missing.xlsx",
    "missing_mashup.xlsx",
    "nonexistent_a.xlsx",
    "nonexistent_b.xlsx",
    "book.xlsx",
    "excel_diff_not_zip.txt",
    "Foo.txt",
    "Bar.txt",
    "Baz.txt",
}


def load_manifest_outputs(path: Path) -> set[str]:
    if not path.exists():
        raise FileNotFoundError(f"Manifest not found: {path}")
    data = yaml.safe_load(path.read_text(encoding="utf-8"))
    outputs: set[str] = set()
    for scenario in data.get("scenarios", []):
        out = scenario.get("output")
        if isinstance(out, list):
            outputs.update(str(name) for name in out)
        elif out:
            outputs.add(str(out))
    return outputs


def scan_test_fixtures(paths: list[Path]) -> set[str]:
    fixtures: set[str] = set()
    for path in paths:
        text = path.read_text(encoding="utf-8")
        if '#![cfg(feature = "perf-metrics")]' in text:
            continue
        for name in RE_FIXTURE_NAME.findall(text):
            if name in IGNORED_FIXTURE_NAMES:
                continue
            fixtures.add(name)
    return fixtures


def scan_workflow_fixtures(paths: list[Path]) -> set[str]:
    fixtures: set[str] = set()
    for path in paths:
        text = path.read_text(encoding="utf-8")
        for name in RE_WORKFLOW_REF.findall(text):
            fixtures.add(name)
    return fixtures


def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    core_tests = repo_root / "core" / "tests"
    cli_tests = repo_root / "cli" / "tests"
    workflows = repo_root / ".github" / "workflows"

    test_files = list(core_tests.rglob("*.rs")) + list(cli_tests.rglob("*.rs"))
    workflow_files = list(workflows.rglob("*.yml")) + list(workflows.rglob("*.yaml"))

    manifest_tests = repo_root / "fixtures" / "manifest_cli_tests.yaml"
    manifest_release = repo_root / "fixtures" / "manifest_release_smoke.yaml"

    errors: list[str] = []

    try:
        test_manifest_outputs = load_manifest_outputs(manifest_tests)
    except FileNotFoundError as exc:
        errors.append(str(exc))
        test_manifest_outputs = set()

    try:
        release_manifest_outputs = load_manifest_outputs(manifest_release)
    except FileNotFoundError as exc:
        errors.append(str(exc))
        release_manifest_outputs = set()

    test_refs = scan_test_fixtures(test_files)
    missing_tests = sorted(test_refs - test_manifest_outputs)
    if missing_tests:
        errors.append(
            "Tests reference fixtures not present in fixtures/manifest_cli_tests.yaml: "
            + ", ".join(missing_tests)
        )

    workflow_refs = scan_workflow_fixtures(workflow_files)
    missing_workflows = sorted(workflow_refs - release_manifest_outputs)
    if missing_workflows:
        errors.append(
            "Workflows reference fixtures not present in fixtures/manifest_release_smoke.yaml: "
            + ", ".join(missing_workflows)
        )

    if errors:
        for error in errors:
            print(f"Error: {error}", file=sys.stderr)
        return 1

    print("Fixture reference guard passed.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\check_perf_thresholds.py`

```python
#!/usr/bin/env python3
"""
Performance threshold checker for excel_diff.

This script runs perf tests and enforces:
  - Absolute time caps for selected tests
  - Baseline regression checks for total time and peak memory

Usage:
  python scripts/check_perf_thresholds.py [--suite quick|gate|full-scale] [--export-json PATH] [--export-csv PATH]
  python scripts/check_perf_thresholds.py --full-scale [--export-json PATH] [--export-csv PATH]
"""

import argparse
import csv
import json
import math
import os
import re
import subprocess
import sys
import time
from datetime import datetime, timezone
from pathlib import Path

PERF_TEST_TIMEOUT_SECONDS = 120
GATE_TIMEOUT_SECONDS = 300
FULL_SCALE_TIMEOUT_SECONDS = 600

QUICK_THRESHOLDS = {
    "perf_p1_large_dense": {"max_time_s": 5},
    "perf_p2_large_noise": {"max_time_s": 10},
    "perf_p3_adversarial_repetitive": {"max_time_s": 15},
    "perf_p4_99_percent_blank": {"max_time_s": 2},
    "perf_p5_identical": {"max_time_s": 1},
    "perf_preflight_low_similarity": {"max_time_s": 5, "max_peak_memory_bytes": 105_048_755},
}

FULL_SCALE_THRESHOLDS = {
    "perf_50k_dense_single_edit": {"max_time_s": 30},
    "perf_50k_completely_different": {"max_time_s": 60},
    "perf_50k_adversarial_repetitive": {"max_time_s": 120},
    "perf_50k_99_percent_blank": {"max_time_s": 30},
    "perf_50k_identical": {"max_time_s": 15},
}

GATE_THRESHOLDS = {
    "perf_50k_dense_single_edit": {"max_time_s": 30},
}

ENV_VAR_MAP = {
    "perf_p1_large_dense": "EXCEL_DIFF_PERF_P1_MAX_TIME_S",
    "perf_p2_large_noise": "EXCEL_DIFF_PERF_P2_MAX_TIME_S",
    "perf_p3_adversarial_repetitive": "EXCEL_DIFF_PERF_P3_MAX_TIME_S",
    "perf_p4_99_percent_blank": "EXCEL_DIFF_PERF_P4_MAX_TIME_S",
    "perf_p5_identical": "EXCEL_DIFF_PERF_P5_MAX_TIME_S",
}

QUICK_PATTERNS = (
    "perf_p1_",
    "perf_p2_",
    "perf_p3_",
    "perf_p4_",
    "perf_p5_",
    "perf_preflight_low_similarity",
)
FULL_SCALE_PATTERNS = ("perf_50k_", "perf_100k_", "perf_many_sheets")
GATE_TESTS = ("perf_50k_dense_single_edit",)

BASELINE_SLACK_QUICK = 0.10
BASELINE_SLACK_GATE = 0.15
BASELINE_SLACK_FULL = 0.15

CSV_FIELDS = [
    "test_name",
    "total_time_ms",
    "parse_time_ms",
    "diff_time_ms",
    "move_detection_time_ms",
    "alignment_time_ms",
    "cell_diff_time_ms",
    "peak_memory_bytes",
    "rows_processed",
    "cells_compared",
    "anchors_found",
    "moves_detected",
]


def get_git_commit():
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()[:12]
    except Exception:
        pass
    return "unknown"


def get_git_branch():
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "unknown"


def get_effective_thresholds(thresholds, env_var_map=None):
    effective = {}
    slack_factor = float(os.environ.get("EXCEL_DIFF_PERF_SLACK_FACTOR", "1.0"))

    for test_name, config in thresholds.items():
        max_time_s = config["max_time_s"]

        if env_var_map and test_name in env_var_map:
            env_var = env_var_map[test_name]
            if env_var in os.environ:
                try:
                    max_time_s = float(os.environ[env_var])
                    print(
                        f"  Override: {test_name} max_time_s={max_time_s} (from {env_var})"
                    )
                except ValueError:
                    print(f"  WARNING: Invalid value for {env_var}, using default")

        entry = {"max_time_s": max_time_s * slack_factor}
        max_peak = config.get("max_peak_memory_bytes")
        if max_peak is not None:
            entry["max_peak_memory_bytes"] = int(math.ceil(max_peak * slack_factor))
        effective[test_name] = entry

    if slack_factor != 1.0:
        print(f"  Slack factor: {slack_factor}x applied to absolute caps")

    return effective


def parse_perf_metrics(stdout: str) -> dict:
    metrics = {}
    pattern = re.compile(r"PERF_METRIC\s+(\S+)\s+(.*)")

    for line in stdout.split("\n"):
        match = pattern.search(line)
        if not match:
            continue

        test_name = match.group(1)
        rest = match.group(2)
        data = {key: int(val) for key, val in re.findall(r"(\w+)=([0-9]+)", rest)}
        data.setdefault("total_time_ms", 0)
        metrics[test_name] = data

    return metrics


def matches_patterns(name: str, patterns: tuple[str, ...], match_mode: str) -> bool:
    if match_mode == "prefix":
        return any(name.startswith(prefix) for prefix in patterns)
    if match_mode == "exact":
        return name in patterns
    raise ValueError(f"Unknown match mode: {match_mode}")


def collect_passed_tests(stdout: str) -> list[str]:
    tests = []
    pending_test = None
    for line in stdout.split("\n"):
        start_match = re.search(r"test\s+(\S+)\s+\.\.\.", line)
        if start_match:
            if re.search(r"\b(ok|ignored)\b", line):
                tests.append(start_match.group(1))
                pending_test = None
            else:
                pending_test = start_match.group(1)
        elif pending_test and line.strip() in ("ok", "ignored"):
            tests.append(pending_test)
            pending_test = None
    return tests


def export_json(path: Path, metrics: dict, suite_name: str, full_scale: bool):
    timestamp = datetime.now(timezone.utc).isoformat()
    payload = {
        "timestamp": timestamp,
        "git_commit": get_git_commit(),
        "git_branch": get_git_branch(),
        "suite": suite_name,
        "full_scale": full_scale,
        "tests": metrics,
        "summary": {
            "total_tests": len(metrics),
            "total_time_ms": sum(m.get("total_time_ms", 0) for m in metrics.values()),
            "total_rows_processed": sum(m.get("rows_processed", 0) for m in metrics.values()),
            "total_cells_compared": sum(m.get("cells_compared", 0) for m in metrics.values()),
        },
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)


def export_csv(path: Path, metrics: dict):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=CSV_FIELDS)
        writer.writeheader()
        for test_name, data in sorted(metrics.items()):
            row = {"test_name": test_name}
            for field in CSV_FIELDS:
                if field == "test_name":
                    continue
                row[field] = data.get(field, 0)
            writer.writerow(row)


def parse_baseline_timestamp(value: str, fallback: float) -> float:
    if not value:
        return fallback
    try:
        return datetime.fromisoformat(value.replace("Z", "+00:00")).timestamp()
    except ValueError:
        return fallback


def load_baseline_file(path: Path):
    if not path.exists():
        return None, None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None, None
    return data, path


def load_baseline_dir(results_dir: Path, full_scale: bool):
    if not results_dir.exists():
        return None, None

    candidates = []
    for json_file in results_dir.glob("*.json"):
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (json.JSONDecodeError, OSError):
            continue

        is_full_scale = data.get("full_scale")
        if is_full_scale is None:
            is_full_scale = "_fullscale" in json_file.name

        if bool(is_full_scale) != full_scale:
            continue

        ts = parse_baseline_timestamp(
            data.get("timestamp", ""), json_file.stat().st_mtime
        )
        candidates.append((ts, json_file, data))

    if not candidates:
        return None, None

    candidates.sort(key=lambda item: item[0], reverse=True)
    _, path, data = candidates[0]
    return data, path


def main():
    parser = argparse.ArgumentParser(
        description="Run perf tests and enforce performance thresholds"
    )
    parser.add_argument(
        "--suite",
        choices=["quick", "gate", "full-scale"],
        default=None,
        help="Perf suite to run (default: quick)",
    )
    parser.add_argument(
        "--full-scale",
        action="store_true",
        help="Run the ignored full-scale perf tests (deprecated; use --suite full-scale)",
    )
    parser.add_argument(
        "--export-json",
        type=Path,
        default=None,
        help="Write perf results to JSON (export_perf_metrics schema)",
    )
    parser.add_argument(
        "--export-csv",
        type=Path,
        default=None,
        help="Write perf results to CSV",
    )
    parser.add_argument(
        "--baseline",
        type=Path,
        default=None,
        help="Pinned baseline JSON file (overrides baseline-dir and suite lookup)",
    )
    parser.add_argument(
        "--baseline-dir",
        type=Path,
        default=Path(__file__).parent.parent / "benchmarks" / "results",
        help="Directory containing baseline JSON results",
    )
    parser.add_argument(
        "--test-target",
        type=str,
        default=None,
        help="Run only a specific integration test target (e.g., perf_large_grid_tests)",
    )
    args = parser.parse_args()

    if args.suite and args.full_scale:
        parser.error("Use either --suite or --full-scale, not both")

    suite_name = args.suite or ("full-scale" if args.full_scale else "quick")

    suite_configs = {
        "quick": {
            "thresholds": QUICK_THRESHOLDS,
            "patterns": QUICK_PATTERNS,
            "match_mode": "prefix",
            "timeout": PERF_TEST_TIMEOUT_SECONDS,
            "baseline_slack": BASELINE_SLACK_QUICK,
            "env_map": ENV_VAR_MAP,
            "ignored": False,
            "test_filter": "perf_",
            "default_test_target": None,
            "full_scale": False,
        },
        "gate": {
            "thresholds": GATE_THRESHOLDS,
            "patterns": GATE_TESTS,
            "match_mode": "exact",
            "timeout": GATE_TIMEOUT_SECONDS,
            "baseline_slack": BASELINE_SLACK_GATE,
            "env_map": None,
            "ignored": True,
            "test_filter": "perf_50k_dense_single_edit",
            "default_test_target": "perf_large_grid_tests",
            "full_scale": True,
        },
        "full-scale": {
            "thresholds": FULL_SCALE_THRESHOLDS,
            "patterns": FULL_SCALE_PATTERNS,
            "match_mode": "prefix",
            "timeout": FULL_SCALE_TIMEOUT_SECONDS,
            "baseline_slack": BASELINE_SLACK_FULL,
            "env_map": None,
            "ignored": True,
            "test_filter": "perf_",
            "default_test_target": None,
            "full_scale": True,
        },
    }

    config = suite_configs[suite_name]
    thresholds = config["thresholds"]
    patterns = config["patterns"]
    match_mode = config["match_mode"]
    baseline_slack = config["baseline_slack"]
    env_map = config["env_map"]

    print("=" * 60)
    print(f"Performance Threshold Check ({suite_name})")
    print("=" * 60)
    print("\nLoading thresholds...")
    effective_thresholds = get_effective_thresholds(thresholds, env_map)
    print()

    expected_tests = set(effective_thresholds.keys())
    print("Expected tests:")
    for test_name in sorted(expected_tests):
        print(f"  - {test_name}")
    print()

    core_dir = Path(__file__).parent.parent / "core"
    if not core_dir.exists():
        core_dir = Path("core")

    cmd = [
        "cargo",
        "test",
        "--release",
        "--features",
        "perf-metrics",
    ]

    test_target = args.test_target or config["default_test_target"]
    if test_target:
        cmd.extend(["--test", test_target])

    if config["test_filter"]:
        cmd.append(config["test_filter"])

    test_args = ["--nocapture", "--test-threads=1"]
    if config["ignored"]:
        test_args.insert(0, "--ignored")

    cmd.extend(["--"] + test_args)

    timeout = config["timeout"]

    start_time = time.time()
    try:
        result = subprocess.run(
            cmd,
            cwd=core_dir,
            capture_output=True,
            text=True,
            timeout=timeout,
        )
    except subprocess.TimeoutExpired:
        print(f"ERROR: Performance tests exceeded timeout of {timeout}s")
        return 1

    elapsed = time.time() - start_time
    print(f"Total perf suite time: {elapsed:.2f}s")
    print()

    if result.returncode != 0:
        print("ERROR: Performance tests failed!")
        print("STDOUT:", result.stdout)
        print("STDERR:", result.stderr)
        return 1

    passed_tests = collect_passed_tests(result.stdout)
    suite_passed = {t for t in passed_tests if matches_patterns(t, patterns, match_mode)}

    print(f"Passed suite tests: {len(suite_passed)}")
    for test in sorted(suite_passed):
        print(f"  - {test}")
    print()

    missing_tests = expected_tests - suite_passed
    if missing_tests:
        print(f"ERROR: Some expected perf tests did not run: {missing_tests}")
        return 1

    metrics = parse_perf_metrics(result.stdout)
    suite_metrics = {
        k: v for k, v in metrics.items() if matches_patterns(k, patterns, match_mode)
    }

    if not suite_metrics:
        print("ERROR: No PERF_METRIC output captured for suite tests")
        return 1

    if args.export_json:
        export_json(args.export_json, suite_metrics, suite_name, config["full_scale"])
        print(f"Wrote JSON results to {args.export_json}")

    if args.export_csv:
        export_csv(args.export_csv, suite_metrics)
        print(f"Wrote CSV results to {args.export_csv}")

    missing_metrics = expected_tests - set(suite_metrics.keys())
    if missing_metrics:
        print(f"ERROR: Missing PERF_METRIC output for tests: {missing_metrics}")
        return 1

    failures = []
    print("Absolute threshold checks:")
    for test_name, threshold in effective_thresholds.items():
        max_time_s = threshold["max_time_s"]
        actual_time_ms = suite_metrics[test_name]["total_time_ms"]
        actual_time_s = actual_time_ms / 1000.0
        max_peak = threshold.get("max_peak_memory_bytes")
        actual_peak = suite_metrics[test_name].get("peak_memory_bytes", 0)

        if actual_time_s > max_time_s:
            status = "FAIL"
            failures.append((test_name, actual_time_s, max_time_s))
        else:
            status = "PASS"

        line = f"  {test_name}: {actual_time_s:.3f}s / {max_time_s:.1f}s [{status}]"
        if max_peak is not None:
            if actual_peak > max_peak:
                failures.append((test_name, actual_peak, max_peak))
                mem_status = "FAIL"
            else:
                mem_status = "PASS"
            line += f", peak={actual_peak} / {max_peak} bytes [{mem_status}]"
        print(line)

    print()

    baseline_failures = []
    baseline = None
    baseline_path = None
    if args.baseline:
        baseline, baseline_path = load_baseline_file(args.baseline)
    else:
        repo_root = Path(__file__).parent.parent
        pinned = repo_root / "benchmarks" / "baselines" / f"{suite_name}.json"
        if pinned.exists():
            baseline, baseline_path = load_baseline_file(pinned)
        else:
            baseline, baseline_path = load_baseline_dir(
                args.baseline_dir, config["full_scale"]
            )

    if baseline and baseline_path:
        print(f"Baseline: {baseline_path}")
        baseline_tests = baseline.get("tests", {})

        for test_name in expected_tests:
            if test_name not in baseline_tests:
                print(f"  WARNING: No baseline for {test_name}; skipping regression check")
                continue

            base = baseline_tests[test_name]
            current = suite_metrics.get(test_name, {})
            base_time = base.get("total_time_ms")
            curr_time = current.get("total_time_ms")
            if base_time is None or curr_time is None:
                print(f"  WARNING: Missing total_time_ms for {test_name}; skipping")
                continue

            time_cap = base_time * (1.0 + baseline_slack)
            if curr_time > time_cap:
                baseline_failures.append(
                    (
                        test_name,
                        "total_time_ms",
                        curr_time,
                        base_time,
                        baseline_slack,
                    )
                )

            base_peak = base.get("peak_memory_bytes")
            curr_peak = current.get("peak_memory_bytes")
            if base_peak is None or curr_peak is None or base_peak <= 0:
                continue

            peak_cap = base_peak * (1.0 + baseline_slack)
            if curr_peak > peak_cap:
                baseline_failures.append(
                    (
                        test_name,
                        "peak_memory_bytes",
                        curr_peak,
                        base_peak,
                        baseline_slack,
                    )
                )

    else:
        if args.baseline:
            print(f"WARNING: Baseline file not found: {args.baseline}")
        else:
            print(f"WARNING: No baseline results found in {args.baseline_dir}")

    if failures or baseline_failures:
        print("=" * 60)
        print("PERF FAILURES:")
        for test_name, actual, max_cap in failures:
            if isinstance(actual, float):
                print(f"  {test_name}: {actual:.3f}s exceeded max of {max_cap:.1f}s")
            else:
                print(f"  {test_name}: peak_memory_bytes {actual} exceeded max of {max_cap}")
        if baseline_failures:
            print("Baseline regressions:")
            for test_name, metric, curr, base, slack in baseline_failures:
                print(
                    f"  {test_name}: {metric} {curr} > {base} (+{int(slack*100)}%)"
                )
        print("=" * 60)
        return 1

    print("=" * 60)
    print("All performance checks passed")
    print("=" * 60)
    return 0


if __name__ == "__main__":
    sys.exit(main())

```

---

### File: `scripts\combine_results_to_csv.py`

```python
#!/usr/bin/env python3
"""
Combine benchmark JSON results into a single CSV for comparison over time.

Usage:
    python scripts/combine_results_to_csv.py [--output FILE] [--results-dir DIR]

Options:
    --output      Output CSV file path (default: benchmarks/results/combined_results.csv)
    --results-dir Directory containing JSON results (default: benchmarks/results)
"""

import argparse
import csv
import json
import sys
from pathlib import Path


ALL_TEST_FIELDS = [
    "total_time_ms",
    "parse_time_ms",
    "diff_time_ms",
    "move_detection_time_ms",
    "alignment_time_ms",
    "cell_diff_time_ms",
    "peak_memory_bytes",
    "rows_processed",
    "cells_compared",
    "anchors_found",
    "moves_detected",
]


def load_json_results(results_dir: Path) -> list[dict]:
    """Load all JSON result files from the results directory."""
    results = []
    for json_file in sorted(results_dir.glob("*.json")):
        try:
            with open(json_file) as f:
                data = json.load(f)
                data["_source_file"] = json_file.name
                results.append(data)
        except (json.JSONDecodeError, IOError) as e:
            print(f"Warning: Could not load {json_file}: {e}", file=sys.stderr)
    return results


def flatten_results(results: list[dict]) -> list[dict]:
    """Flatten nested test results into individual rows."""
    rows = []
    for result in results:
        timestamp = result.get("timestamp", "")
        git_commit = result.get("git_commit", "")
        git_branch = result.get("git_branch", "")
        full_scale = result.get("full_scale", False)
        source_file = result.get("_source_file", "")

        tests = result.get("tests", {})
        for test_name, test_data in tests.items():
            row = {
                "source_file": source_file,
                "timestamp": timestamp,
                "git_commit": git_commit,
                "git_branch": git_branch,
                "full_scale": full_scale,
                "test_name": test_name,
            }
            for field in ALL_TEST_FIELDS:
                row[field] = test_data.get(field, "")
            rows.append(row)

        summary = result.get("summary", {})
        if summary:
            row = {
                "source_file": source_file,
                "timestamp": timestamp,
                "git_commit": git_commit,
                "git_branch": git_branch,
                "full_scale": full_scale,
                "test_name": "_SUMMARY_",
                "total_time_ms": summary.get("total_time_ms", ""),
                "rows_processed": summary.get("total_rows_processed", ""),
                "cells_compared": summary.get("total_cells_compared", ""),
            }
            for field in ALL_TEST_FIELDS:
                if field not in row:
                    row[field] = ""
            rows.append(row)

    return rows


def write_csv(rows: list[dict], output_path: Path):
    """Write flattened results to CSV."""
    if not rows:
        print("No data to write.", file=sys.stderr)
        return

    fieldnames = [
        "source_file",
        "timestamp",
        "git_commit",
        "git_branch",
        "full_scale",
        "test_name",
    ] + ALL_TEST_FIELDS

    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def main():
    parser = argparse.ArgumentParser(
        description="Combine benchmark JSON results into a single CSV"
    )
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=Path(__file__).parent.parent / "benchmarks" / "results",
        help="Directory containing JSON results",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Output CSV file path",
    )
    args = parser.parse_args()

    if args.output is None:
        args.output = args.results_dir / "combined_results.csv"

    if not args.results_dir.exists():
        print(f"ERROR: Results directory not found: {args.results_dir}", file=sys.stderr)
        return 1

    print(f"Loading results from: {args.results_dir}")
    results = load_json_results(args.results_dir)

    if not results:
        print("ERROR: No JSON result files found.", file=sys.stderr)
        return 1

    print(f"Found {len(results)} result files")

    rows = flatten_results(results)
    print(f"Generated {len(rows)} rows")

    write_csv(rows, args.output)
    print(f"CSV written to: {args.output}")

    return 0


if __name__ == "__main__":
    sys.exit(main())


```

---

### File: `scripts\compare_perf_results.py`

```python
#!/usr/bin/env python3
"""
Compare performance results between two benchmark runs.

Usage:
    python scripts/compare_perf_results.py [baseline.json] [current.json]
    python scripts/compare_perf_results.py --latest  # Compare two most recent results

If no arguments provided, compares the two most recent results in benchmarks/results/.
"""

import argparse
import json
import sys
from pathlib import Path


def load_result(path: Path) -> dict:
    """Load a benchmark result JSON file."""
    with open(path) as f:
        return json.load(f)


def get_latest_results(results_dir: Path, n: int = 2) -> list[Path]:
    """Get the N most recent result files."""
    files = sorted(results_dir.glob("*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[:n]


def format_delta(baseline: float, current: float) -> str:
    """Format a percentage delta with color indicator."""
    if baseline == 0:
        return "N/A"
    delta = ((current - baseline) / baseline) * 100
    if abs(delta) < 1:
        return f"  {delta:+.1f}%"
    elif delta < 0:
        return f"  {delta:+.1f}% (faster)"
    else:
        return f"  {delta:+.1f}% (SLOWER)"


def compare_results(baseline: dict, current: dict):
    """Compare two benchmark results and print a comparison table."""
    print("=" * 90)
    print("Performance Comparison")
    print("=" * 90)
    print(f"Baseline: {baseline.get('git_commit', 'unknown')} ({baseline.get('timestamp', 'unknown')[:19]})")
    print(f"Current:  {current.get('git_commit', 'unknown')} ({current.get('timestamp', 'unknown')[:19]})")
    print()

    baseline_tests = baseline.get("tests", {})
    current_tests = current.get("tests", {})

    all_tests = sorted(set(baseline_tests.keys()) | set(current_tests.keys()))

    if not all_tests:
        print("No tests found in either result file.")
        return

    print(f"{'Test':<35} {'Baseline':>10} {'Current':>10} {'Delta':>20}")
    print("-" * 90)

    regressions = []
    improvements = []

    for test_name in all_tests:
        base_data = baseline_tests.get(test_name, {})
        curr_data = current_tests.get(test_name, {})

        base_time = base_data.get("total_time_ms", 0)
        curr_time = curr_data.get("total_time_ms", 0)

        if base_time == 0:
            delta_str = "NEW"
        elif curr_time == 0:
            delta_str = "REMOVED"
        else:
            delta_pct = ((curr_time - base_time) / base_time) * 100
            delta_str = format_delta(base_time, curr_time)

            if delta_pct > 10:
                regressions.append((test_name, delta_pct))
            elif delta_pct < -10:
                improvements.append((test_name, delta_pct))

        base_str = f"{base_time:,}ms" if base_time else ""
        curr_str = f"{curr_time:,}ms" if curr_time else ""

        print(f"{test_name:<35} {base_str:>10} {curr_str:>10} {delta_str:>20}")

    print("-" * 90)

    base_total = baseline.get("summary", {}).get("total_time_ms", 0)
    curr_total = current.get("summary", {}).get("total_time_ms", 0)
    print(f"{'TOTAL':<35} {base_total:>10,}ms {curr_total:>10,}ms {format_delta(base_total, curr_total):>20}")
    print("=" * 90)

    if regressions:
        print("\n  REGRESSIONS (>10% slower):")
        for name, delta in sorted(regressions, key=lambda x: -x[1]):
            print(f"   {name}: +{delta:.1f}%")

    if improvements:
        print("\n IMPROVEMENTS (>10% faster):")
        for name, delta in sorted(improvements, key=lambda x: x[1]):
            print(f"   {name}: {delta:.1f}%")

    if not regressions and not improvements:
        print("\n No significant changes detected (within 10%)")


def main():
    parser = argparse.ArgumentParser(description="Compare performance benchmark results")
    parser.add_argument("baseline", nargs="?", type=Path, help="Baseline result JSON file")
    parser.add_argument("current", nargs="?", type=Path, help="Current result JSON file")
    parser.add_argument("--latest", action="store_true", help="Compare two most recent results")
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=Path(__file__).parent.parent / "benchmarks" / "results",
        help="Results directory",
    )
    args = parser.parse_args()

    if args.latest or (args.baseline is None and args.current is None):
        files = get_latest_results(args.results_dir, 2)
        if len(files) < 2:
            print(f"ERROR: Need at least 2 result files in {args.results_dir}")
            print(f"Found: {len(files)} files")
            return 1
        baseline_path = files[1]
        current_path = files[0]
    else:
        if not args.baseline or not args.current:
            parser.error("Must provide both baseline and current files, or use --latest")
        baseline_path = args.baseline
        current_path = args.current

    if not baseline_path.exists():
        print(f"ERROR: Baseline file not found: {baseline_path}")
        return 1
    if not current_path.exists():
        print(f"ERROR: Current file not found: {current_path}")
        return 1

    baseline = load_result(baseline_path)
    current = load_result(current_path)

    compare_results(baseline, current)
    return 0


if __name__ == "__main__":
    sys.exit(main())


```

---

### File: `scripts\dev_test.py`

```python
import shlex
import subprocess
import sys


def run(cmd: list[str]) -> None:
    print(f"+ {' '.join(shlex.quote(part) for part in cmd)}", flush=True)
    subprocess.run(cmd, check=True)


def main() -> int:
    python = sys.executable
    try:
        run([python, "scripts/check_fixture_references.py"])
        run(
            [
                "generate-fixtures",
                "--manifest",
                "fixtures/manifest_cli_tests.yaml",
                "--force",
                "--clean",
            ]
        )
        run(
            [
                "generate-fixtures",
                "--manifest",
                "fixtures/manifest_cli_tests.yaml",
                "--verify-lock",
                "fixtures/manifest_cli_tests.lock.json",
            ]
        )
        run(["cargo", "test", "--workspace"])
    except subprocess.CalledProcessError as exc:
        return exc.returncode
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\export_e2e_metrics.py`

```python
#!/usr/bin/env python3
"""
Export end-to-end workbook open + diff metrics to versioned JSON.

This script generates the e2e fixtures, runs the ignored e2e perf tests, captures
PERF_METRIC output, and writes timestamped results plus a latest JSON (and CSV).

Usage:
    python scripts/export_e2e_metrics.py [--output-dir DIR] [--export-csv PATH] [--skip-fixtures]
"""

from __future__ import annotations

import argparse
import csv
import json
import math
import os
import re
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path


BASELINE_SLACK_E2E = 0.20

E2E_THRESHOLDS = {
    "e2e_p1_dense": {
        "max_total_time_s": 25,
        "max_parse_time_s": 20,
        "max_diff_time_s": 10,
        "max_peak_memory_bytes": 199_327_436,
    },
    "e2e_p2_noise": {
        "max_total_time_s": 20,
        "max_parse_time_s": 15,
        "max_diff_time_s": 10,
        "max_peak_memory_bytes": 66_108_831,
    },
    "e2e_p3_repetitive": {
        "max_total_time_s": 30,
        "max_parse_time_s": 25,
        "max_diff_time_s": 10,
        "max_peak_memory_bytes": 149_252_565,
    },
    "e2e_p4_sparse": {
        "max_total_time_s": 5,
        "max_parse_time_s": 3,
        "max_diff_time_s": 2,
        "max_peak_memory_bytes": 19_226_538,
    },
    "e2e_p5_identical": {
        "max_total_time_s": 80,
        "max_parse_time_s": 75,
        "max_diff_time_s": 10,
        "max_peak_memory_bytes": 966_180_894,
    },
}


CSV_FIELDS = [
    "test_name",
    "total_time_ms",
    "parse_time_ms",
    "diff_time_ms",
    "signature_build_time_ms",
    "move_detection_time_ms",
    "alignment_time_ms",
    "cell_diff_time_ms",
    "op_emit_time_ms",
    "report_serialize_time_ms",
    "peak_memory_bytes",
    "grid_storage_bytes",
    "string_pool_bytes",
    "op_buffer_bytes",
    "alignment_buffer_bytes",
    "rows_processed",
    "cells_compared",
    "anchors_found",
    "moves_detected",
    "hash_lookups_est",
    "allocations_est",
    "old_bytes",
    "new_bytes",
    "total_input_bytes",
]


def get_git_commit() -> str:
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()[:12]
    except Exception:
        pass
    return "unknown"


def get_git_branch() -> str:
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "unknown"


def run_command(
    cmd: list[str], cwd: Path, *, capture_output: bool = False, timeout: int | None = None
) -> subprocess.CompletedProcess[str]:
    print(f"Running: {' '.join(cmd)}")
    return subprocess.run(
        cmd,
        cwd=cwd,
        capture_output=capture_output,
        text=True,
        timeout=timeout,
    )


def install_fixture_generator(repo_root: Path) -> None:
    python = sys.executable
    requirements = repo_root / "fixtures" / "requirements.txt"
    result = run_command(
        [python, "-m", "pip", "install", "-r", str(requirements)],
        cwd=repo_root,
    )
    if result.returncode != 0:
        raise RuntimeError("Failed to install fixture generator requirements.")

    result = run_command(
        [python, "-m", "pip", "install", "-e", "fixtures", "--no-deps"],
        cwd=repo_root,
    )
    if result.returncode != 0:
        raise RuntimeError("Failed to install fixture generator package.")


def generate_fixtures(repo_root: Path, manifest: Path) -> None:
    result = run_command(
        ["generate-fixtures", "--manifest", str(manifest), "--force"],
        cwd=repo_root,
    )
    if result.returncode != 0:
        raise RuntimeError("Failed to generate e2e fixtures.")


def parse_perf_metrics(stdout: str) -> dict:
    metrics: dict[str, dict[str, int]] = {}
    pattern = re.compile(r"PERF_METRIC\s+(\S+)\s+(.*)")

    for line in stdout.split("\n"):
        match = pattern.search(line)
        if not match:
            continue

        test_name = match.group(1)
        rest = match.group(2)
        data = {key: int(val) for key, val in re.findall(r"(\w+)=([0-9]+)", rest)}

        data.setdefault("total_time_ms", 0)
        data.setdefault("parse_time_ms", 0)
        data.setdefault("diff_time_ms", 0)
        data.setdefault("peak_memory_bytes", 0)
        data.setdefault("rows_processed", 0)
        data.setdefault("cells_compared", 0)

        metrics[test_name] = data

    return metrics


def run_e2e_tests(core_dir: Path) -> tuple[dict, bool]:
    cmd = [
        "cargo",
        "test",
        "--release",
        "--features",
        "perf-metrics",
        "--test",
        "e2e_perf_workbook_open",
        "e2e_",
        "--",
        "--ignored",
        "--nocapture",
        "--test-threads=1",
    ]

    try:
        result = run_command(cmd, cwd=core_dir, capture_output=True, timeout=1800)
    except subprocess.TimeoutExpired:
        print("ERROR: Tests timed out")
        return {}, False
    print(result.stdout)
    if result.stderr:
        print("STDERR:", result.stderr, file=sys.stderr)

    metrics = parse_perf_metrics(result.stdout)
    success = result.returncode == 0
    return metrics, success


def save_results(metrics: dict, output_dir: Path) -> tuple[Path, dict]:
    timestamp = datetime.now(timezone.utc)
    filename = timestamp.strftime("%Y-%m-%d_%H%M%S") + ".json"

    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / filename

    result = {
        "timestamp": timestamp.isoformat(),
        "git_commit": get_git_commit(),
        "git_branch": get_git_branch(),
        "tests": metrics,
        "summary": {
            "total_tests": len(metrics),
            "total_time_ms": sum(m.get("total_time_ms", 0) for m in metrics.values()),
            "total_rows_processed": sum(
                m.get("rows_processed", 0) for m in metrics.values()
            ),
            "total_cells_compared": sum(
                m.get("cells_compared", 0) for m in metrics.values()
            ),
        },
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2)

    print(f"\nResults saved to: {output_path}")
    return output_path, result


def write_latest(path: Path, payload: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)


def export_csv(path: Path, metrics: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=CSV_FIELDS)
        writer.writeheader()
        for test_name, data in sorted(metrics.items()):
            row = {"test_name": test_name}
            for field in CSV_FIELDS:
                if field == "test_name":
                    continue
                row[field] = data.get(field, 0)
            writer.writerow(row)


def load_baseline_file(path: Path) -> tuple[dict | None, Path | None]:
    if not path.exists():
        return None, None
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None, None
    return data, path


def load_baseline_dir(results_dir: Path) -> tuple[dict | None, Path | None]:
    if not results_dir.exists():
        return None, None

    candidates = sorted(results_dir.glob("*.json"), key=lambda p: p.stat().st_mtime)
    if not candidates:
        return None, None
    path = candidates[-1]
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None, None
    return data, path


def get_effective_thresholds(thresholds: dict) -> dict:
    slack_factor = float(os.environ.get("EXCEL_DIFF_E2E_SLACK_FACTOR", "1.0"))
    effective = {}
    for test_name, caps in thresholds.items():
        scaled = {}
        for key, value in caps.items():
            if key == "max_peak_memory_bytes":
                scaled[key] = int(math.ceil(value * slack_factor))
            else:
                scaled[key] = value * slack_factor
        effective[test_name] = scaled

    if slack_factor != 1.0:
        print(f"  Slack factor: {slack_factor}x applied to absolute caps")

    return effective


def enforce_thresholds(metrics: dict, thresholds: dict) -> list[str]:
    failures = []
    print("Absolute threshold checks:")
    for test_name, caps in thresholds.items():
        data = metrics.get(test_name, {})
        total_time_s = data.get("total_time_ms", 0) / 1000.0
        parse_time_s = data.get("parse_time_ms", 0) / 1000.0
        diff_time_s = data.get("diff_time_ms", 0) / 1000.0
        peak_bytes = data.get("peak_memory_bytes", 0)

        max_total = caps.get("max_total_time_s")
        max_parse = caps.get("max_parse_time_s")
        max_diff = caps.get("max_diff_time_s")
        max_peak = caps.get("max_peak_memory_bytes")

        if max_total is not None and total_time_s > max_total:
            failures.append(
                f"{test_name}: total_time {total_time_s:.3f}s > {max_total:.1f}s"
            )
        if max_parse is not None and parse_time_s > max_parse:
            failures.append(
                f"{test_name}: parse_time {parse_time_s:.3f}s > {max_parse:.1f}s"
            )
        if max_diff is not None and diff_time_s > max_diff:
            failures.append(
                f"{test_name}: diff_time {diff_time_s:.3f}s > {max_diff:.1f}s"
            )
        if max_peak is not None and peak_bytes > max_peak:
            failures.append(
                f"{test_name}: peak_memory_bytes {peak_bytes} > {max_peak}"
            )

        print(
            f"  {test_name}: total={total_time_s:.3f}s (cap {max_total:.1f}s), "
            f"parse={parse_time_s:.3f}s (cap {max_parse:.1f}s), "
            f"diff={diff_time_s:.3f}s (cap {max_diff:.1f}s), "
            f"peak={peak_bytes} bytes (cap {max_peak})"
        )
    print()
    return failures


def enforce_baseline(metrics: dict, baseline: dict, expected_tests: set[str]) -> list[str]:
    failures = []
    baseline_tests = baseline.get("tests", {})

    print("Baseline regression checks:")
    for test_name in sorted(expected_tests):
        base = baseline_tests.get(test_name)
        if not base:
            print(f"  WARNING: No baseline for {test_name}; skipping")
            continue

        current = metrics.get(test_name, {})
        for metric_key in ("total_time_ms", "parse_time_ms", "diff_time_ms"):
            base_val = base.get(metric_key)
            curr_val = current.get(metric_key)
            if base_val is None or curr_val is None:
                print(f"  WARNING: Missing {metric_key} for {test_name}; skipping")
                continue

            cap = base_val * (1.0 + BASELINE_SLACK_E2E)
            if curr_val > cap:
                failures.append(
                    f"{test_name}: {metric_key} {curr_val} > {base_val} (+{int(BASELINE_SLACK_E2E*100)}%)"
                )

        base_peak = base.get("peak_memory_bytes")
        curr_peak = current.get("peak_memory_bytes")
        if base_peak and curr_peak:
            cap = base_peak * (1.0 + BASELINE_SLACK_E2E)
            if curr_peak > cap:
                failures.append(
                    f"{test_name}: peak_memory_bytes {curr_peak} > {base_peak} (+{int(BASELINE_SLACK_E2E*100)}%)"
                )
    print()
    return failures


def main() -> int:
    repo_root = Path(__file__).resolve().parent.parent
    core_dir = repo_root / "core"
    manifest = repo_root / "fixtures" / "manifest_perf_e2e.yaml"

    parser = argparse.ArgumentParser(
        description="Export end-to-end workbook open perf metrics"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=repo_root / "benchmarks" / "results_e2e",
        help="Output directory for timestamped JSON results",
    )
    parser.add_argument(
        "--latest-json",
        type=Path,
        default=repo_root / "benchmarks" / "latest_e2e.json",
        help="Path to write latest JSON results",
    )
    parser.add_argument(
        "--export-csv",
        type=Path,
        default=None,
        help="Optional path to write latest CSV results",
    )
    parser.add_argument(
        "--skip-fixtures",
        action="store_true",
        help="Skip fixture generator installation and generation",
    )
    parser.add_argument(
        "--baseline",
        type=Path,
        default=None,
        help="Pinned baseline JSON file (overrides baseline-dir and suite lookup)",
    )
    parser.add_argument(
        "--baseline-dir",
        type=Path,
        default=repo_root / "benchmarks" / "results_e2e",
        help="Directory containing baseline JSON results",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("Excel Diff E2E Metrics Export")
    print("=" * 70)
    print(f"Output: {args.output_dir}")
    print(f"Latest JSON: {args.latest_json}")
    if args.export_csv:
        print(f"Latest CSV: {args.export_csv}")
    print(f"Git commit: {get_git_commit()}")
    print(f"Git branch: {get_git_branch()}")
    print()

    try:
        if not args.skip_fixtures:
            install_fixture_generator(repo_root)
            generate_fixtures(repo_root, manifest)
    except RuntimeError as exc:
        print(f"ERROR: {exc}")
        return 1

    metrics, success = run_e2e_tests(core_dir)
    if not metrics:
        print("ERROR: No metrics captured from test output")
        return 1

    expected_tests = set(E2E_THRESHOLDS.keys())
    missing_tests = expected_tests - set(metrics.keys())
    if missing_tests:
        print(f"ERROR: Missing metrics for expected tests: {missing_tests}")
        return 1

    _, payload = save_results(metrics, args.output_dir)
    write_latest(args.latest_json, payload)
    print(f"Latest JSON written to: {args.latest_json}")

    if args.export_csv:
        export_csv(args.export_csv, metrics)
        print(f"Latest CSV written to: {args.export_csv}")

    print("\nThreshold configuration:")
    effective_thresholds = get_effective_thresholds(E2E_THRESHOLDS)
    for test_name, caps in effective_thresholds.items():
        print(
            f"  {test_name}: total<={caps['max_total_time_s']}s "
            f"parse<={caps['max_parse_time_s']}s diff<={caps['max_diff_time_s']}s "
            f"peak<={caps['max_peak_memory_bytes']} bytes"
        )
    print()

    failures = enforce_thresholds(metrics, effective_thresholds)

    baseline = None
    baseline_path = None
    if args.baseline:
        baseline, baseline_path = load_baseline_file(args.baseline)
    else:
        pinned = repo_root / "benchmarks" / "baselines" / "e2e.json"
        if pinned.exists():
            baseline, baseline_path = load_baseline_file(pinned)
        else:
            baseline, baseline_path = load_baseline_dir(args.baseline_dir)

    if baseline and baseline_path:
        print(f"Baseline: {baseline_path}")
        failures.extend(enforce_baseline(metrics, baseline, expected_tests))
    else:
        if args.baseline:
            print(f"WARNING: Baseline file not found: {args.baseline}")
        else:
            print(f"WARNING: No baseline results found in {args.baseline_dir}")

    if failures:
        print("=" * 70)
        print("E2E PERF FAILURES:")
        for failure in failures:
            print(f"  - {failure}")
        print("=" * 70)
        return 1

    if not success:
        print("\nWARNING: Some tests may have failed")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())

```

---

### File: `scripts\export_perf_metrics.py`

```python
#!/usr/bin/env python3
"""
Export performance metrics from excel_diff tests to JSON.

This script runs the performance test suite and captures the PERF_METRIC output,
saving timestamped results to benchmarks/results/ for historical tracking.

Usage:
    python scripts/export_perf_metrics.py [--full-scale] [--parallel] [--output-dir DIR]

Options:
    --full-scale    Run the 50K row tests (slower but comprehensive)
    --parallel      Enable parallel feature (uses Rayon for multi-threaded execution)
    --output-dir    Override the output directory (default: benchmarks/results)
"""

import argparse
import json
import re
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path


def get_git_commit():
    """Get the current git commit hash."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()[:12]
    except Exception:
        pass
    return "unknown"


def get_git_branch():
    """Get the current git branch name."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "unknown"


def parse_perf_metrics(stdout: str) -> dict:
    """Parse PERF_METRIC lines from test output."""
    metrics = {}
    pattern = re.compile(r"PERF_METRIC\s+(\S+)\s+(.*)")

    for line in stdout.split("\n"):
        match = pattern.search(line)
        if not match:
            continue

        test_name = match.group(1)
        rest = match.group(2)
        data = {key: int(val) for key, val in re.findall(r"(\w+)=([0-9]+)", rest)}

        # Ensure required keys exist even if the output is partially missing.
        data.setdefault("total_time_ms", 0)
        data.setdefault("rows_processed", 0)
        data.setdefault("cells_compared", 0)

        metrics[test_name] = data

    return metrics


def run_perf_tests(full_scale: bool = False, parallel: bool = False) -> tuple[dict, bool]:
    """Run performance tests and return parsed metrics."""
    core_dir = Path(__file__).parent.parent / "core"
    if not core_dir.exists():
        core_dir = Path("core")

    features = ["perf-metrics"]
    if parallel:
        features.append("parallel")

    cmd = [
        "cargo",
        "test",
        "--release",
        "--features",
        ",".join(features),
        "--test",
        "perf_large_grid_tests",
        "--",
        "--nocapture",
        "--test-threads=1",
    ]

    if full_scale:
        cmd.append("--ignored")

    print(f"Running: {' '.join(cmd)}")
    print(f"Working directory: {core_dir}")
    print()

    try:
        result = subprocess.run(
            cmd,
            cwd=core_dir,
            capture_output=True,
            text=True,
            timeout=600 if full_scale else 120,
        )
    except subprocess.TimeoutExpired:
        print("ERROR: Tests timed out")
        return {}, False

    print(result.stdout)
    if result.stderr:
        print("STDERR:", result.stderr, file=sys.stderr)

    success = result.returncode == 0
    metrics = parse_perf_metrics(result.stdout)

    return metrics, success


def save_results(metrics: dict, output_dir: Path, full_scale: bool, parallel: bool):
    """Save metrics to a timestamped JSON file."""
    timestamp = datetime.now(timezone.utc)
    filename = timestamp.strftime("%Y-%m-%d_%H%M%S")
    if full_scale:
        filename += "_fullscale"
    if parallel:
        filename += "_parallel"
    filename += ".json"

    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / filename

    result = {
        "timestamp": timestamp.isoformat(),
        "git_commit": get_git_commit(),
        "git_branch": get_git_branch(),
        "full_scale": full_scale,
        "parallel": parallel,
        "tests": metrics,
        "summary": {
            "total_tests": len(metrics),
            "total_time_ms": sum(m["total_time_ms"] for m in metrics.values()),
            "total_rows_processed": sum(m["rows_processed"] for m in metrics.values()),
            "total_cells_compared": sum(m["cells_compared"] for m in metrics.values()),
        },
    }

    with open(output_path, "w") as f:
        json.dump(result, f, indent=2)

    print(f"\nResults saved to: {output_path}")
    return output_path


def print_summary(metrics: dict):
    """Print a summary table of metrics."""
    print("\n" + "=" * 70)
    print("Performance Metrics Summary")
    print("=" * 70)
    print(
        f"{'Test':<40} {'Total':>10} {'Move':>10} {'Align':>10} {'Cell':>10} {'Rows':>10} {'Cells':>12}"
    )
    print("-" * 70)

    for test_name, data in sorted(metrics.items()):
        move = data.get("move_detection_time_ms", 0)
        align = data.get("alignment_time_ms", 0)
        cell = data.get("cell_diff_time_ms", 0)
        print(
            f"{test_name:<40} {data['total_time_ms']:>10,} {move:>10,} {align:>10,} {cell:>10,} {data.get('rows_processed', 0):>10,} {data.get('cells_compared', 0):>12,}"
        )

    print("-" * 70)
    total_time = sum(m["total_time_ms"] for m in metrics.values())
    total_move = sum(m.get("move_detection_time_ms", 0) for m in metrics.values())
    total_align = sum(m.get("alignment_time_ms", 0) for m in metrics.values())
    total_cell = sum(m.get("cell_diff_time_ms", 0) for m in metrics.values())
    total_rows = sum(m["rows_processed"] for m in metrics.values())
    total_cells = sum(m["cells_compared"] for m in metrics.values())
    print(
        f"{'TOTAL':<40} {total_time:>10,} {total_move:>10,} {total_align:>10,} {total_cell:>10,} {total_rows:>10,} {total_cells:>12,}"
    )
    print("=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="Export performance metrics from excel_diff tests"
    )
    parser.add_argument(
        "--full-scale",
        action="store_true",
        help="Run the 50K row tests (slower but comprehensive)",
    )
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="Enable parallel feature (uses Rayon for multi-threaded execution)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path(__file__).parent.parent / "benchmarks" / "results",
        help="Output directory for JSON results",
    )
    args = parser.parse_args()

    print("=" * 70)
    print("Excel Diff Performance Metrics Export")
    print("=" * 70)
    print(f"Mode: {'Full-scale (50K rows)' if args.full_scale else 'Quick (1K rows)'}")
    print(f"Parallel: {'Enabled (Rayon)' if args.parallel else 'Disabled'}")
    print(f"Output: {args.output_dir}")
    print(f"Git commit: {get_git_commit()}")
    print(f"Git branch: {get_git_branch()}")
    print()

    metrics, success = run_perf_tests(args.full_scale, args.parallel)

    if not metrics:
        print("ERROR: No metrics captured from test output")
        return 1

    print_summary(metrics)
    save_results(metrics, args.output_dir, args.full_scale, args.parallel)

    if not success:
        print("\nWARNING: Some tests may have failed")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


```

---

### File: `scripts\fuzz_corpus_maint.py`

```python
import argparse
import subprocess
from pathlib import Path


def repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def run_cmin(target: str, fuzz_dir: Path) -> None:
    subprocess.run(
        ["cargo", "fuzz", "cmin", target],
        cwd=fuzz_dir,
        check=True,
    )


def enforce_limits(
    target_dir: Path,
    max_file_bytes: int | None,
    max_files: int | None,
    max_total_bytes: int | None,
    dry_run: bool,
) -> dict:
    files = [p for p in target_dir.iterdir() if p.is_file()]
    files.sort(key=lambda p: (p.stat().st_size, p.name))

    removed = []
    total_bytes = sum(p.stat().st_size for p in files)

    if max_file_bytes is not None:
        for path in list(files):
            if path.stat().st_size > max_file_bytes:
                removed.append(path)
                files.remove(path)
                total_bytes -= path.stat().st_size
                if not dry_run:
                    path.unlink()

    if max_files is not None and len(files) > max_files:
        overflow = files[max_files:]
        for path in overflow:
            removed.append(path)
            total_bytes -= path.stat().st_size
            if not dry_run:
                path.unlink()
        files = files[:max_files]

    if max_total_bytes is not None and total_bytes > max_total_bytes:
        for path in reversed(files):
            if total_bytes <= max_total_bytes:
                break
            removed.append(path)
            total_bytes -= path.stat().st_size
            if not dry_run:
                path.unlink()

    return {
        "kept": len(files),
        "removed": len(removed),
        "total_bytes": total_bytes,
        "removed_files": [str(p) for p in removed],
    }


def main() -> int:
    root = repo_root()
    parser = argparse.ArgumentParser(description="Maintain fuzz corpora.")
    parser.add_argument(
        "--corpus-root",
        type=Path,
        default=root / "core" / "fuzz" / "corpus",
        help="Corpus root directory",
    )
    parser.add_argument(
        "--targets",
        type=str,
        default="",
        help="Comma-separated list of targets to process (default: all)",
    )
    parser.add_argument(
        "--skip-cmin",
        action="store_true",
        help="Skip cargo fuzz cmin",
    )
    parser.add_argument(
        "--max-file-bytes",
        type=int,
        default=2 * 1024 * 1024,
        help="Max size per seed file (default: 2MB)",
    )
    parser.add_argument(
        "--max-files",
        type=int,
        default=200,
        help="Max number of files per corpus (default: 200)",
    )
    parser.add_argument(
        "--max-total-bytes",
        type=int,
        default=100 * 1024 * 1024,
        help="Max total bytes per corpus (default: 100MB)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Report actions without deleting files",
    )
    args = parser.parse_args()

    corpus_root = args.corpus_root
    if not corpus_root.exists():
        raise FileNotFoundError(f"Corpus root not found: {corpus_root}")

    if args.targets:
        targets = [t.strip() for t in args.targets.split(",") if t.strip()]
    else:
        targets = [p.name for p in corpus_root.iterdir() if p.is_dir()]

    fuzz_dir = root / "core" / "fuzz"
    report = []

    for target in targets:
        target_dir = corpus_root / target
        if not target_dir.exists():
            print(f"Skipping missing corpus target: {target}")
            continue

        if not args.skip_cmin:
            run_cmin(target, fuzz_dir)

        stats = enforce_limits(
            target_dir,
            args.max_file_bytes,
            args.max_files,
            args.max_total_bytes,
            args.dry_run,
        )
        stats["target"] = target
        report.append(stats)

    print("Fuzz corpus maintenance report:")
    for entry in report:
        print(
            f"- {entry['target']}: kept={entry['kept']} removed={entry['removed']} "
            f"total_bytes={entry['total_bytes']}"
        )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\fuzz_triage.py`

```python
import argparse
import subprocess
from pathlib import Path


def repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def run(cmd: list[str], cwd: Path) -> None:
    print(" ".join(cmd))
    subprocess.run(cmd, cwd=cwd, check=True)


def main() -> int:
    parser = argparse.ArgumentParser(description="Reproduce and minimize fuzz artifacts.")
    parser.add_argument("--target", required=True, help="Fuzz target name")
    parser.add_argument("--artifact", required=True, help="Path to artifact file")
    parser.add_argument(
        "--cmin",
        action="store_true",
        help="Run corpus minimization after tmin",
    )
    args = parser.parse_args()

    fuzz_dir = repo_root() / "core" / "fuzz"
    artifact = Path(args.artifact).resolve()
    if not artifact.exists():
        raise FileNotFoundError(f"Artifact not found: {artifact}")

    run(["cargo", "fuzz", "run", args.target, str(artifact), "-runs=1"], fuzz_dir)
    run(["cargo", "fuzz", "tmin", args.target, str(artifact)], fuzz_dir)

    if args.cmin:
        run(["cargo", "fuzz", "cmin", args.target], fuzz_dir)

    print("Triage complete. See minimized artifact in core/fuzz/artifacts.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\generate_web_cli_fixtures.py`

```python
#!/usr/bin/env python3
import argparse
import os
import subprocess
import sys
from pathlib import Path


def write_workbook(path: Path, a1_value: str) -> None:
    from openpyxl import Workbook

    wb = Workbook()
    ws = wb.active
    ws.title = "Sheet1"
    ws["A1"] = a1_value
    ws["B1"] = "static"
    ws["A2"] = 1
    ws["B2"] = 2
    wb.save(path)


def run_cli(bin_path: str, old_path: Path, new_path: Path, fmt: str, output_path: Path) -> None:
    cmd = [bin_path, "diff", "--format", fmt, str(old_path), str(new_path)]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode not in (0, 1):
        sys.stderr.write(result.stderr)
        raise RuntimeError(f"excel-diff failed with exit code {result.returncode}")
    output_path.write_text(result.stdout, encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser(description="Generate CLI payload/outcome fixtures for web UI tests.")
    parser.add_argument("--output-dir", type=Path, required=True, help="Directory for JSON fixtures.")
    parser.add_argument(
        "--bin",
        dest="bin_path",
        default=os.environ.get("EXCEL_DIFF_BIN"),
        help="Path to excel-diff binary (or set EXCEL_DIFF_BIN).",
    )
    args = parser.parse_args()

    if not args.bin_path:
        raise SystemExit("Missing --bin (or EXCEL_DIFF_BIN) for excel-diff.")

    output_dir = args.output_dir.resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    old_path = output_dir / "web_fixture_old.xlsx"
    new_path = output_dir / "web_fixture_new.xlsx"
    payload_path = output_dir / "payload.json"
    outcome_path = output_dir / "outcome.json"

    write_workbook(old_path, "before")
    write_workbook(new_path, "after")

    run_cli(args.bin_path, old_path, new_path, "payload", payload_path)
    run_cli(args.bin_path, old_path, new_path, "outcome", outcome_path)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\ingest_private_corpus.py`

```python
import argparse
import hashlib
import json
import shutil
from datetime import datetime, timezone
from pathlib import Path


DEFAULT_EXTS = {".xlsx", ".xlsm", ".pbix", ".pbit"}


def iter_inputs(root: Path, recursive: bool, exts: set[str]) -> list[Path]:
    pattern = "**/*" if recursive else "*"
    files = []
    for path in root.glob(pattern):
        if path.is_file() and path.suffix.lower() in exts:
            files.append(path)
    return files


def sha256_bytes(path: Path) -> str:
    hasher = hashlib.sha256()
    with path.open("rb") as handle:
        for chunk in iter(lambda: handle.read(1024 * 1024), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def load_index(path: Path) -> dict:
    if not path.exists():
        return {"version": 1, "files": []}
    return json.loads(path.read_text(encoding="utf-8"))


def save_index(path: Path, index: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(index, indent=2, sort_keys=True) + "\n", encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser(description="Ingest private corpus files into hashed storage.")
    parser.add_argument("--input-dir", type=Path, required=True, help="Directory to scan")
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("corpus_private"),
        help="Destination directory for hashed files",
    )
    parser.add_argument(
        "--index",
        type=Path,
        default=None,
        help="Optional metadata index path (defaults to <output-dir>/index.json)",
    )
    parser.add_argument(
        "--source-tag",
        type=str,
        default="",
        help="Optional source label (avoid customer identifiers)",
    )
    parser.add_argument(
        "--no-recursive",
        action="store_true",
        help="Disable recursive scanning",
    )
    parser.add_argument(
        "--extensions",
        type=str,
        default="",
        help="Comma-separated extension list to include (e.g. .xlsx,.pbix)",
    )
    args = parser.parse_args()

    input_dir = args.input_dir
    if not input_dir.exists():
        raise FileNotFoundError(f"Input dir not found: {input_dir}")

    exts = DEFAULT_EXTS
    if args.extensions:
        exts = {ext.strip().lower() for ext in args.extensions.split(",") if ext.strip()}

    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    index_path = args.index or (output_dir / "index.json")
    index = load_index(index_path)
    existing = {entry["sha256"] for entry in index.get("files", [])}

    ingested = 0
    skipped = 0
    now = datetime.now(timezone.utc).isoformat()

    for path in iter_inputs(input_dir, not args.no_recursive, exts):
        digest = sha256_bytes(path)
        ext = path.suffix.lower()
        dest_name = f"sha256_{digest}.{ext.lstrip('.')}"
        dest_path = output_dir / dest_name

        if digest in existing:
            skipped += 1
            continue

        shutil.copy2(path, dest_path)
        entry = {
            "sha256": digest,
            "size_bytes": path.stat().st_size,
            "extension": ext,
            "ingested_at": now,
        }
        if args.source_tag:
            entry["source_tag"] = args.source_tag
        index.setdefault("files", []).append(entry)
        existing.add(digest)
        ingested += 1

    save_index(index_path, index)

    print(f"Ingested {ingested} file(s), skipped {skipped}.")
    print(f"Output: {output_dir}")
    print(f"Index: {index_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\seed_fuzz_corpus.py`

```python
import argparse
import base64
import hashlib
import json
import shutil
import zipfile
from pathlib import Path
from typing import Iterable, Optional
from xml.etree import ElementTree as ET

import yaml

DATA_MASHUP_NS = {"dm": "http://schemas.microsoft.com/DataMashup"}
DM_MARKERS = (
    b"DataMashup",
    b"D\x00a\x00t\x00a\x00M\x00a\x00s\x00h\x00u\x00p",
)


def repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def load_config(path: Path) -> list[dict]:
    data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    fixtures = data.get("fixtures", [])
    if not isinstance(fixtures, list):
        raise ValueError("seed_fixtures.yaml must contain a top-level 'fixtures' list")
    return fixtures


def find_datamashup_element(root: ET.Element) -> Optional[ET.Element]:
    if root.tag.endswith("DataMashup"):
        return root
    return root.find(".//dm:DataMashup", namespaces=DATA_MASHUP_NS)


def extract_datamashup_from_xlsx(path: Path) -> bytes:
    with zipfile.ZipFile(path, "r") as zin:
        for name in zin.namelist():
            if not (name.startswith("customXml/item") and name.endswith(".xml")):
                continue
            buf = zin.read(name)
            if not any(marker in buf for marker in DM_MARKERS):
                continue
            try:
                root = ET.fromstring(buf)
            except ET.ParseError:
                continue
            node = find_datamashup_element(root)
            if node is None or node.text is None:
                continue
            text = "".join(node.text.split())
            if not text:
                continue
            return base64.b64decode(text)
    raise ValueError(f"DataMashup not found in {path}")


def extract_datamashup_from_pbix(path: Path) -> bytes:
    with zipfile.ZipFile(path, "r") as zin:
        try:
            return zin.read("DataMashup")
        except KeyError as exc:
            raise ValueError(f"DataMashup not found in {path}") from exc


def extract_datamashup(path: Path) -> bytes:
    suffix = path.suffix.lower()
    if suffix in {".xlsx", ".xlsm"}:
        return extract_datamashup_from_xlsx(path)
    if suffix in {".pbix", ".pbit"}:
        return extract_datamashup_from_pbix(path)
    raise ValueError(f"Unsupported DataMashup source extension: {path}")


def copy_fixture(src: Path, dest_dir: Path, overwrite: bool) -> Path:
    dest_dir.mkdir(parents=True, exist_ok=True)
    dest = dest_dir / src.name
    if dest.exists() and not overwrite:
        return dest
    shutil.copy2(src, dest)
    return dest


def write_datamashup_seed(
    dm_bytes: bytes, dest_dir: Path, stem: str, overwrite: bool
) -> Path:
    dest_dir.mkdir(parents=True, exist_ok=True)
    digest = hashlib.sha256(dm_bytes).hexdigest()
    dest = dest_dir / f"{stem}_{digest[:8]}.bin"
    if dest.exists() and not overwrite:
        return dest
    dest.write_bytes(dm_bytes)
    return dest


def iter_targets(targets: Iterable[str]) -> list[str]:
    out = []
    for target in targets:
        target = str(target).strip()
        if target and target not in out:
            out.append(target)
    return out


def seed_from_fixtures(
    fixtures: list[dict],
    fixtures_dir: Path,
    corpus_dir: Path,
    overwrite: bool,
    clean: bool,
) -> list[dict]:
    if clean:
        for entry in fixtures:
            for target in iter_targets(entry.get("targets", [])):
                target_dir = corpus_dir / target
                if target_dir.exists():
                    for child in target_dir.iterdir():
                        if child.is_file():
                            child.unlink()

    report = []
    for entry in fixtures:
        filename = entry.get("file")
        if not filename:
            raise ValueError("Each fixture entry must include a 'file'")
        targets = iter_targets(entry.get("targets", []))
        if not targets:
            continue

        src = Path(filename)
        if not src.is_absolute():
            src = fixtures_dir / filename
        if not src.exists():
            raise FileNotFoundError(f"Fixture not found: {src}")

        record = {"file": filename, "targets": targets, "outputs": []}
        for target in targets:
            if target == "fuzz_datamashup_parse":
                try:
                    dm_bytes = extract_datamashup(src)
                except ValueError as exc:
                    record["outputs"].append({"target": target, "skipped": str(exc)})
                    continue
                dest = write_datamashup_seed(
                    dm_bytes, corpus_dir / target, src.stem, overwrite
                )
                record["outputs"].append({"target": target, "path": str(dest)})
            else:
                dest = copy_fixture(src, corpus_dir / target, overwrite)
                record["outputs"].append({"target": target, "path": str(dest)})
        report.append(record)
    return report


def main() -> int:
    root = repo_root()
    parser = argparse.ArgumentParser(description="Seed fuzz corpora from fixtures.")
    parser.add_argument(
        "--config",
        type=Path,
        default=root / "core" / "fuzz" / "seed_fixtures.yaml",
        help="Path to seed_fixtures.yaml",
    )
    parser.add_argument(
        "--fixtures-dir",
        type=Path,
        default=root / "fixtures" / "generated",
        help="Directory containing generated fixtures",
    )
    parser.add_argument(
        "--corpus-dir",
        type=Path,
        default=root / "core" / "fuzz" / "corpus",
        help="Destination corpus root",
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Remove existing files in targeted corpus directories before seeding",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing seeds with the same name",
    )
    parser.add_argument(
        "--report",
        type=Path,
        help="Optional path to write a JSON report of seeded files",
    )
    args = parser.parse_args()

    fixtures = load_config(args.config)
    report = seed_from_fixtures(
        fixtures, args.fixtures_dir, args.corpus_dir, args.overwrite, args.clean
    )

    if args.report:
        args.report.write_text(json.dumps(report, indent=2), encoding="utf-8")

    print(f"Seeded {len(report)} fixture entries into {args.corpus_dir}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\update_baselines.py`

```python
import argparse
import shlex
import shutil
import subprocess
import sys
from pathlib import Path


SUITE_SUFFIX = {
    "quick": "quick",
    "gate": "gate",
    "full-scale": "fullscale",
}


def run(cmd: list[str]) -> None:
    print(f"+ {' '.join(shlex.quote(part) for part in cmd)}", flush=True)
    subprocess.run(cmd, check=True)


def main() -> int:
    parser = argparse.ArgumentParser(description="Run perf suite and update pinned baselines.")
    parser.add_argument(
        "--suite",
        required=True,
        choices=sorted(SUITE_SUFFIX.keys()),
        help="Perf suite to run (quick, gate, full-scale).",
    )
    parser.add_argument(
        "--test-target",
        default=None,
        help="Optional test target to pass through (e.g., perf_large_grid_tests).",
    )
    parser.add_argument(
        "--skip-baseline",
        action="store_true",
        help="Skip baseline regression checks (still enforces absolute caps).",
    )
    args = parser.parse_args()

    repo_root = Path(__file__).resolve().parents[1]
    latest_suffix = SUITE_SUFFIX[args.suite]
    latest_json = repo_root / "benchmarks" / f"latest_{latest_suffix}.json"
    latest_csv = repo_root / "benchmarks" / f"latest_{latest_suffix}.csv"
    baseline_json = repo_root / "benchmarks" / "baselines" / f"{args.suite}.json"

    try:
        cmd = [
            sys.executable,
            "scripts/check_perf_thresholds.py",
            "--suite",
            args.suite,
            "--export-json",
            str(latest_json),
            "--export-csv",
            str(latest_csv),
        ]
        if args.test_target:
            cmd.extend(["--test-target", args.test_target])
        if args.skip_baseline:
            cmd.extend(
                [
                    "--baseline",
                    str(repo_root / "benchmarks" / "baselines" / "_skip_baseline.json"),
                ]
            )
        run(cmd)

        if baseline_json.exists():
            run(
                [
                    sys.executable,
                    "scripts/compare_perf_results.py",
                    str(baseline_json),
                    str(latest_json),
                ]
            )
        else:
            print(f"Baseline not found at {baseline_json}; creating a new one.")

        baseline_json.parent.mkdir(parents=True, exist_ok=True)
        shutil.copyfile(latest_json, baseline_json)
        print(f"Updated baseline: {baseline_json}")
    except subprocess.CalledProcessError as exc:
        return exc.returncode

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

```

---

### File: `scripts\verify_release_versions.py`

```python
#!/usr/bin/env python3
"""
Verify that release tags match crate versions.

Branch 5 packaging expects releases to be tagged as `vX.Y.Z`, and for the
workspace crate versions to match `X.Y.Z` (without the leading `v`).

This script is designed to be used in GitHub Actions, but can also be run
locally:

  python scripts/verify_release_versions.py --tag v0.1.0
"""

from __future__ import annotations

import argparse
import os
import re
import sys
from pathlib import Path


TAG_RE = re.compile(
    r"^v(?P<version>[0-9]+\.[0-9]+\.[0-9]+(?:-[0-9A-Za-z.-]+)?(?:\+[0-9A-Za-z.-]+)?)$"
)


def _read_package_version(cargo_toml_path: Path) -> str:
    if not cargo_toml_path.exists():
        raise FileNotFoundError(f"Missing {cargo_toml_path}")

    if sys.version_info >= (3, 11):
        import tomllib  # type: ignore[import-not-found]
    else:
        raise RuntimeError("Python 3.11+ is required (tomllib)")

    data = tomllib.loads(cargo_toml_path.read_text(encoding="utf-8"))
    package = data.get("package")
    if not isinstance(package, dict):
        raise ValueError(f"{cargo_toml_path} has no [package] section")

    version = package.get("version")
    if not isinstance(version, str) or not version.strip():
        raise ValueError(f"{cargo_toml_path} has no valid package.version")

    return version.strip()


def _resolve_tag(tag_arg: str | None) -> str | None:
    if tag_arg:
        return tag_arg.strip()

    ref = os.environ.get("GITHUB_REF", "").strip()
    if ref.startswith("refs/tags/"):
        return ref.removeprefix("refs/tags/")

    return None


def main() -> int:
    parser = argparse.ArgumentParser(description="Verify tag/version consistency.")
    parser.add_argument("--tag", help="Tag like v0.1.0 (defaults to GITHUB_REF)")
    parser.add_argument(
        "--workspace-root",
        default=str(Path(__file__).resolve().parent.parent),
        help="Repository root (default: inferred from script location)",
    )
    parser.add_argument(
        "--crates",
        nargs="*",
        default=["core", "cli", "wasm", "ui_payload", "desktop/src-tauri"],
        help="Crate directories to check (default: core cli wasm ui_payload desktop/src-tauri)",
    )
    args = parser.parse_args()

    tag = _resolve_tag(args.tag)
    if tag is None:
        print("No tag detected (not running on refs/tags/*); skipping version check.")
        return 0

    match = TAG_RE.match(tag)
    if not match:
        print(f"ERROR: tag {tag!r} does not match expected format vX.Y.Z", file=sys.stderr)
        return 2

    expected = match.group("version")
    root = Path(args.workspace_root)

    mismatches: list[tuple[str, str]] = []
    for crate_dir in args.crates:
        cargo_toml = root / crate_dir / "Cargo.toml"
        actual = _read_package_version(cargo_toml)
        if actual != expected:
            mismatches.append((crate_dir, actual))

    if mismatches:
        print(
            f"ERROR: tag {tag} expects crate version {expected}, but found mismatches:",
            file=sys.stderr,
        )
        for crate_dir, actual in mismatches:
            print(f"  - {crate_dir}/Cargo.toml: {actual}", file=sys.stderr)
        return 1

    print(f"OK: tag {tag} matches crate versions ({expected}).")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


```

---

### File: `scripts\visualize_benchmarks.py`

```python
#!/usr/bin/env python3
"""
Visualize benchmark trends from combined_results.csv.

Usage:
    python scripts/visualize_benchmarks.py [--input FILE] [--output-dir DIR] [--show]

Options:
    --input       Input CSV file (default: benchmarks/results/combined_results.csv)
    --output-dir  Directory to save plots (default: benchmarks/results/plots)
    --show        Display plots interactively instead of saving
"""

import argparse
import sys
from datetime import datetime
from pathlib import Path

try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    import pandas as pd
except ImportError as e:
    print(f"Missing required dependency: {e}")
    print("Install with: pip install matplotlib pandas")
    sys.exit(1)


COLORS = [
    "#2ecc71", "#3498db", "#9b59b6", "#e74c3c", "#f39c12",
    "#1abc9c", "#e67e22", "#34495e", "#16a085", "#c0392b",
]


def load_data(csv_path: Path) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = df[df["test_name"] != "_SUMMARY_"]
    df = df.sort_values("timestamp")
    return df


def plot_time_trends(df: pd.DataFrame, output_dir: Path, show: bool = False):
    fig, ax = plt.subplots(figsize=(14, 8))

    quick_df = df[df["full_scale"] == False]
    full_df = df[df["full_scale"] == True]

    for i, (scale_df, scale_name) in enumerate([(quick_df, "Quick"), (full_df, "Full-Scale")]):
        if scale_df.empty:
            continue

        test_names = scale_df["test_name"].unique()
        for j, test_name in enumerate(test_names):
            test_data = scale_df[scale_df["test_name"] == test_name]
            color = COLORS[j % len(COLORS)]
            linestyle = "-" if scale_name == "Quick" else "--"
            marker = "o" if scale_name == "Quick" else "s"
            label = f"{test_name} ({scale_name})"
            ax.plot(
                test_data["timestamp"],
                test_data["total_time_ms"],
                marker=marker,
                linestyle=linestyle,
                color=color,
                label=label,
                markersize=6,
                linewidth=2,
                alpha=0.8,
            )

    ax.set_xlabel("Timestamp", fontsize=12)
    ax.set_ylabel("Total Time (ms)", fontsize=12)
    ax.set_title("Benchmark Performance Over Time", fontsize=14, fontweight="bold")
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%m/%d %H:%M"))
    ax.tick_params(axis="x", rotation=45)
    ax.legend(bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.set_yscale("log")
    fig.tight_layout()

    if show:
        plt.show()
    else:
        fig.savefig(output_dir / "time_trends.png", dpi=150, bbox_inches="tight")
        print(f"Saved: {output_dir / 'time_trends.png'}")
    plt.close(fig)


def plot_speedup_heatmap(df: pd.DataFrame, output_dir: Path, show: bool = False):
    quick_df = df[df["full_scale"] == False].copy()
    if quick_df.empty:
        print("No quick-scale data for speedup heatmap")
        return

    runs = quick_df.groupby("source_file")["timestamp"].first().sort_values()
    if len(runs) < 2:
        print("Need at least 2 runs for speedup comparison")
        return

    pivot = quick_df.pivot_table(
        index="test_name",
        columns="source_file",
        values="total_time_ms",
        aggfunc="first",
    )
    pivot = pivot[runs.index]

    speedup = pd.DataFrame(index=pivot.index)
    run_files = list(pivot.columns)
    for i in range(1, len(run_files)):
        prev_run = run_files[i - 1]
        curr_run = run_files[i]
        col_name = f"{curr_run[:10]}"
        speedup[col_name] = ((pivot[prev_run] - pivot[curr_run]) / pivot[prev_run] * 100).round(1)

    if speedup.empty or speedup.shape[1] == 0:
        print("Not enough data for speedup heatmap")
        return

    fig, ax = plt.subplots(figsize=(max(10, len(speedup.columns) * 1.5), max(6, len(speedup) * 0.6)))

    im = ax.imshow(speedup.values, cmap="RdYlGn", aspect="auto", vmin=-50, vmax=50)

    ax.set_xticks(range(len(speedup.columns)))
    ax.set_xticklabels(speedup.columns, rotation=45, ha="right", fontsize=9)
    ax.set_yticks(range(len(speedup.index)))
    ax.set_yticklabels(speedup.index, fontsize=9)

    for i in range(len(speedup.index)):
        for j in range(len(speedup.columns)):
            val = speedup.iloc[i, j]
            if pd.notna(val):
                color = "white" if abs(val) > 25 else "black"
                text = f"{val:+.0f}%" if val != 0 else "0%"
                ax.text(j, i, text, ha="center", va="center", color=color, fontsize=8)

    cbar = fig.colorbar(im, ax=ax, shrink=0.8)
    cbar.set_label("Speedup % (positive = faster)", fontsize=10)

    ax.set_title("Performance Change Between Runs (Quick Tests)", fontsize=14, fontweight="bold")
    ax.set_xlabel("Run", fontsize=12)
    ax.set_ylabel("Test", fontsize=12)
    fig.tight_layout()

    if show:
        plt.show()
    else:
        fig.savefig(output_dir / "speedup_heatmap.png", dpi=150, bbox_inches="tight")
        print(f"Saved: {output_dir / 'speedup_heatmap.png'}")
    plt.close(fig)


def plot_latest_comparison(df: pd.DataFrame, output_dir: Path, show: bool = False):
    latest_runs = df.groupby("full_scale")["source_file"].apply(lambda x: x.iloc[-1] if len(x) > 0 else None)

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    for idx, (full_scale, ax) in enumerate(zip([False, True], axes)):
        if full_scale not in latest_runs.index or latest_runs[full_scale] is None:
            ax.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax.transAxes)
            ax.set_title(f"{'Full-Scale' if full_scale else 'Quick'} Tests - No Data")
            continue

        latest_file = latest_runs[full_scale]
        latest_data = df[(df["source_file"] == latest_file) & (df["full_scale"] == full_scale)]

        if latest_data.empty:
            continue

        tests = latest_data["test_name"].values
        times = latest_data["total_time_ms"].values

        colors = [COLORS[i % len(COLORS)] for i in range(len(tests))]
        bars = ax.barh(tests, times, color=colors, alpha=0.8)

        for bar, time in zip(bars, times):
            ax.text(
                bar.get_width() + max(times) * 0.01,
                bar.get_y() + bar.get_height() / 2,
                f"{time:,.0f}ms",
                va="center",
                fontsize=9,
            )

        scale_name = "Full-Scale (50K rows)" if full_scale else "Quick (1-2K rows)"
        timestamp = latest_data["timestamp"].iloc[0].strftime("%Y-%m-%d %H:%M")
        ax.set_title(f"{scale_name}\n{latest_file} ({timestamp})", fontsize=11, fontweight="bold")
        ax.set_xlabel("Time (ms)", fontsize=10)
        ax.set_xlim(0, max(times) * 1.15)
        ax.grid(True, axis="x", alpha=0.3)

    fig.suptitle("Latest Benchmark Results", fontsize=14, fontweight="bold", y=1.02)
    fig.tight_layout()

    if show:
        plt.show()
    else:
        fig.savefig(output_dir / "latest_comparison.png", dpi=150, bbox_inches="tight")
        print(f"Saved: {output_dir / 'latest_comparison.png'}")
    plt.close(fig)


def plot_metric_breakdown(df: pd.DataFrame, output_dir: Path, show: bool = False):
    metrics = ["move_detection_time_ms", "alignment_time_ms", "cell_diff_time_ms"]
    available_metrics = [m for m in metrics if m in df.columns and df[m].notna().any()]

    if not available_metrics:
        print("No detailed timing metrics available for breakdown chart")
        return

    latest_quick = df[df["full_scale"] == False].groupby("test_name").last().reset_index()
    latest_full = df[df["full_scale"] == True].groupby("test_name").last().reset_index()

    for scale_name, scale_df in [("Quick", latest_quick), ("Full-Scale", latest_full)]:
        if scale_df.empty:
            continue

        scale_df = scale_df[scale_df[available_metrics].notna().any(axis=1)]
        if scale_df.empty:
            continue

        fig, ax = plt.subplots(figsize=(12, 6))

        tests = scale_df["test_name"].values
        x = range(len(tests))
        width = 0.25

        metric_labels = {
            "move_detection_time_ms": "Fingerprinting + Move Detection",
            "alignment_time_ms": "Alignment (incl. diff)",
            "cell_diff_time_ms": "Cell Diff",
        }

        for i, metric in enumerate(available_metrics):
            values = scale_df[metric].fillna(0).values
            offset = (i - len(available_metrics) / 2 + 0.5) * width
            bars = ax.bar([xi + offset for xi in x], values, width, label=metric_labels.get(metric, metric), color=COLORS[i], alpha=0.8)

        ax.set_xlabel("Test", fontsize=12)
        ax.set_ylabel("Time (ms)", fontsize=12)
        ax.set_title(f"Timing Breakdown by Phase ({scale_name} Tests)", fontsize=14, fontweight="bold")
        ax.set_xticks(x)
        ax.set_xticklabels(tests, rotation=45, ha="right", fontsize=9)
        ax.legend()
        ax.grid(True, axis="y", alpha=0.3)
        fig.tight_layout()

        suffix = "quick" if scale_name == "Quick" else "fullscale"
        if show:
            plt.show()
        else:
            fig.savefig(output_dir / f"metric_breakdown_{suffix}.png", dpi=150, bbox_inches="tight")
            print(f"Saved: {output_dir / f'metric_breakdown_{suffix}.png'}")
        plt.close(fig)


def plot_commit_comparison(df: pd.DataFrame, output_dir: Path, show: bool = False):
    quick_df = df[df["full_scale"] == False].copy()
    if quick_df.empty:
        print("No quick-scale data for commit comparison")
        return

    commit_totals = quick_df.groupby(["git_commit", "source_file"])["total_time_ms"].sum().reset_index()
    commit_totals = commit_totals.sort_values("source_file")

    if len(commit_totals) < 2:
        print("Need at least 2 commits for comparison")
        return

    fig, ax = plt.subplots(figsize=(12, 6))

    commits = commit_totals["git_commit"].values
    totals = commit_totals["total_time_ms"].values
    files = commit_totals["source_file"].values

    colors = [COLORS[i % len(COLORS)] for i in range(len(commits))]
    bars = ax.bar(range(len(commits)), totals, color=colors, alpha=0.8)

    for i, (bar, total, commit, fname) in enumerate(zip(bars, totals, commits, files)):
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + max(totals) * 0.01,
            f"{total:,.0f}ms",
            ha="center",
            va="bottom",
            fontsize=9,
        )

    ax.set_xlabel("Commit", fontsize=12)
    ax.set_ylabel("Total Time (ms)", fontsize=12)
    ax.set_title("Total Test Suite Time by Commit (Quick Tests)", fontsize=14, fontweight="bold")
    ax.set_xticks(range(len(commits)))
    labels = [f"{c[:8]}\n{f[:10]}" for c, f in zip(commits, files)]
    ax.set_xticklabels(labels, rotation=0, fontsize=8)
    ax.grid(True, axis="y", alpha=0.3)

    if len(totals) >= 2:
        first_total = totals[0]
        last_total = totals[-1]
        overall_change = ((last_total - first_total) / first_total) * 100
        direction = "faster" if overall_change < 0 else "slower"
        ax.text(
            0.98, 0.98,
            f"Overall: {abs(overall_change):.1f}% {direction}",
            transform=ax.transAxes,
            ha="right", va="top",
            fontsize=11,
            fontweight="bold",
            color="green" if overall_change < 0 else "red",
            bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
        )

    fig.tight_layout()

    if show:
        plt.show()
    else:
        fig.savefig(output_dir / "commit_comparison.png", dpi=150, bbox_inches="tight")
        print(f"Saved: {output_dir / 'commit_comparison.png'}")
    plt.close(fig)


def generate_summary_report(df: pd.DataFrame, output_dir: Path):
    lines = [
        "# Benchmark Trend Summary",
        "",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## Overview",
        "",
        f"- Total benchmark runs: {df['source_file'].nunique()}",
        f"- Quick-scale runs: {df[df['full_scale'] == False]['source_file'].nunique()}",
        f"- Full-scale runs: {df[df['full_scale'] == True]['source_file'].nunique()}",
        f"- Unique tests: {df['test_name'].nunique()}",
        f"- Date range: {df['timestamp'].min().strftime('%Y-%m-%d')} to {df['timestamp'].max().strftime('%Y-%m-%d')}",
        "",
    ]

    for scale_name, full_scale in [("Quick", False), ("Full-Scale", True)]:
        scale_df = df[df["full_scale"] == full_scale]
        if scale_df.empty:
            continue

        lines.extend([f"## {scale_name} Tests Performance", ""])

        runs = scale_df.groupby("source_file")["timestamp"].first().sort_values()
        if len(runs) >= 2:
            first_run = runs.index[0]
            last_run = runs.index[-1]

            first_total = scale_df[scale_df["source_file"] == first_run]["total_time_ms"].sum()
            last_total = scale_df[scale_df["source_file"] == last_run]["total_time_ms"].sum()
            change = ((last_total - first_total) / first_total) * 100

            lines.extend([
                f"- First run total: {first_total:,.0f}ms ({first_run})",
                f"- Latest run total: {last_total:,.0f}ms ({last_run})",
                f"- Overall change: {change:+.1f}% ({'faster' if change < 0 else 'slower'})",
                "",
            ])

        lines.append("### Per-Test Trends")
        lines.append("")
        lines.append("| Test | First (ms) | Latest (ms) | Change |")
        lines.append("|:-----|----------:|------------:|-------:|")

        for test_name in scale_df["test_name"].unique():
            test_data = scale_df[scale_df["test_name"] == test_name].sort_values("timestamp")
            if len(test_data) >= 2:
                first_time = test_data.iloc[0]["total_time_ms"]
                last_time = test_data.iloc[-1]["total_time_ms"]
                pct_change = ((last_time - first_time) / first_time) * 100
                lines.append(f"| {test_name} | {first_time:,.0f} | {last_time:,.0f} | {pct_change:+.1f}% |")
            elif len(test_data) == 1:
                lines.append(f"| {test_name} | {test_data.iloc[0]['total_time_ms']:,.0f} | - | N/A |")

        lines.extend(["", ""])

    report_path = output_dir / "trend_summary.md"
    report_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"Saved: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="Visualize benchmark trends")
    parser.add_argument(
        "--input",
        type=Path,
        default=Path(__file__).parent.parent / "benchmarks" / "results" / "combined_results.csv",
        help="Input CSV file",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Output directory for plots",
    )
    parser.add_argument(
        "--show",
        action="store_true",
        help="Display plots interactively",
    )
    args = parser.parse_args()

    if not args.input.exists():
        print(f"ERROR: Input file not found: {args.input}")
        print("Run scripts/combine_results_to_csv.py first to generate the combined CSV.")
        return 1

    if args.output_dir is None:
        args.output_dir = args.input.parent / "plots"

    args.output_dir.mkdir(parents=True, exist_ok=True)

    print(f"Loading data from: {args.input}")
    df = load_data(args.input)
    print(f"Loaded {len(df)} data points from {df['source_file'].nunique()} benchmark runs")
    print()

    print("Generating visualizations...")
    plot_time_trends(df, args.output_dir, args.show)
    plot_speedup_heatmap(df, args.output_dir, args.show)
    plot_latest_comparison(df, args.output_dir, args.show)
    plot_metric_breakdown(df, args.output_dir, args.show)
    plot_commit_comparison(df, args.output_dir, args.show)
    generate_summary_report(df, args.output_dir)

    print()
    print(f"All outputs saved to: {args.output_dir}")
    return 0


if __name__ == "__main__":
    sys.exit(main())

```

---

### File: `ui_payload\Cargo.toml`

```toml
[package]
name = "ui_payload"
version = "0.1.0"
edition = "2024"
description = "UI payload builder for excel_diff (snapshots + alignment)"
license = "MIT"
repository = "https://github.com/dvora/excel_diff"
homepage = "https://github.com/dvora/excel_diff"

[dependencies]
excel_diff = { path = "../core", default-features = false, features = ["excel-open-xml"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

[features]
default = ["model-diff"]
model-diff = ["excel_diff/model-diff"]

```

---

### File: `ui_payload\src\alignment.rs`

```rust
use std::collections::{HashMap, HashSet};

use serde::Serialize;

use crate::{SheetPairSnapshot, SheetSnapshot};

#[derive(Serialize)]
#[serde(rename_all = "snake_case")]
pub enum AxisKind {
    Match,
    Insert,
    Delete,
    MoveSrc,
    MoveDst,
}

#[derive(Serialize)]
pub struct AxisEntry {
    old: Option<u32>,
    new: Option<u32>,
    kind: AxisKind,
    #[serde(skip_serializing_if = "Option::is_none")]
    move_id: Option<String>,
}

#[derive(Serialize)]
pub struct MoveGroup {
    id: String,
    axis: String,
    src_start: u32,
    dst_start: u32,
    count: u32,
}

#[derive(Serialize)]
pub struct SheetAlignment {
    sheet: String,
    rows: Vec<AxisEntry>,
    cols: Vec<AxisEntry>,
    moves: Vec<MoveGroup>,
    skipped: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    skip_reason: Option<String>,
}

// Guardrail: keep the HTML grid from exploding on large sheets.
const MAX_VIEW_ROWS: u32 = 10_000;
const MAX_VIEW_COLS: u32 = 200;

pub fn build_alignments(
    report: &excel_diff::DiffReport,
    sheets: &SheetPairSnapshot,
) -> Vec<SheetAlignment> {
    let ops_by_sheet = group_ops_by_sheet(report);
    let rename_map = build_rename_map(report);
    let renamed_old: HashSet<String> = rename_map.values().cloned().collect();
    let old_lookup = build_sheet_lookup(&sheets.old.sheets);
    let new_lookup = build_sheet_lookup(&sheets.new.sheets);

    let mut names = HashSet::new();
    names.extend(ops_by_sheet.keys().cloned());
    names.extend(new_lookup.keys().cloned());
    for name in old_lookup.keys() {
        if renamed_old.contains(name) {
            continue;
        }
        names.insert(name.clone());
    }

    let mut names: Vec<String> = names.into_iter().collect();
    names.sort();

    let empty_ops: Vec<&excel_diff::DiffOp> = Vec::new();
    let mut alignments = Vec::with_capacity(names.len());

    for sheet in names {
        let ops = ops_by_sheet
            .get(&sheet)
            .map(Vec::as_slice)
            .unwrap_or(empty_ops.as_slice());
        let old_sheet = old_lookup
            .get(&sheet)
            .copied()
            .or_else(|| rename_map.get(&sheet).and_then(|old| old_lookup.get(old).copied()));
        let new_sheet = new_lookup.get(&sheet).copied();
        alignments.push(build_sheet_alignment(&sheet, old_sheet, new_sheet, ops));
    }

    alignments
}

fn build_sheet_lookup<'a>(sheets: &'a [SheetSnapshot]) -> HashMap<String, &'a SheetSnapshot> {
    let mut map = HashMap::new();
    for sheet in sheets {
        map.insert(sheet.name.clone(), sheet);
    }
    map
}

fn group_ops_by_sheet<'a>(
    report: &'a excel_diff::DiffReport,
) -> HashMap<String, Vec<&'a excel_diff::DiffOp>> {
    let mut map = HashMap::new();
    for op in &report.ops {
        let sheet = match op {
            excel_diff::DiffOp::SheetAdded { sheet }
            | excel_diff::DiffOp::SheetRemoved { sheet }
            | excel_diff::DiffOp::SheetRenamed { sheet, .. }
            | excel_diff::DiffOp::RowAdded { sheet, .. }
            | excel_diff::DiffOp::RowRemoved { sheet, .. }
            | excel_diff::DiffOp::RowReplaced { sheet, .. }
            | excel_diff::DiffOp::DuplicateKeyCluster { sheet, .. }
            | excel_diff::DiffOp::ColumnAdded { sheet, .. }
            | excel_diff::DiffOp::ColumnRemoved { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRows { sheet, .. }
            | excel_diff::DiffOp::BlockMovedColumns { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRect { sheet, .. }
            | excel_diff::DiffOp::RectReplaced { sheet, .. }
            | excel_diff::DiffOp::CellEdited { sheet, .. } => Some(*sheet),
            _ => None,
        };

        let Some(sheet_id) = sheet else {
            continue;
        };

        let sheet_name = report.resolve(sheet_id).unwrap_or("<unknown>");
        map.entry(sheet_name.to_string())
            .or_insert_with(Vec::new)
            .push(op);
    }
    map
}

fn build_rename_map(report: &excel_diff::DiffReport) -> HashMap<String, String> {
    let mut map = HashMap::new();
    for op in &report.ops {
        let excel_diff::DiffOp::SheetRenamed { sheet, from, .. } = op else {
            continue;
        };
        let new_name = report.resolve(*sheet).unwrap_or("<unknown>");
        let old_name = report.resolve(*from).unwrap_or("<unknown>");
        map.insert(new_name.to_string(), old_name.to_string());
    }
    map
}

fn build_sheet_alignment(
    sheet: &str,
    old_sheet: Option<&SheetSnapshot>,
    new_sheet: Option<&SheetSnapshot>,
    ops: &[&excel_diff::DiffOp],
) -> SheetAlignment {
    let old_nrows = old_sheet.map(|s| s.nrows).unwrap_or(0);
    let new_nrows = new_sheet.map(|s| s.nrows).unwrap_or(0);
    let old_ncols = old_sheet.map(|s| s.ncols).unwrap_or(0);
    let new_ncols = new_sheet.map(|s| s.ncols).unwrap_or(0);

    let mut added_rows = HashSet::new();
    let mut removed_rows = HashSet::new();
    let mut added_cols = HashSet::new();
    let mut removed_cols = HashSet::new();
    let mut move_src_rows = HashMap::new();
    let mut move_dst_rows = HashMap::new();
    let mut move_src_cols = HashMap::new();
    let mut move_dst_cols = HashMap::new();
    let mut moves = Vec::new();

    for op in ops {
        match op {
            excel_diff::DiffOp::RowAdded { row_idx, .. } => {
                added_rows.insert(*row_idx);
            }
            excel_diff::DiffOp::RowRemoved { row_idx, .. } => {
                removed_rows.insert(*row_idx);
            }
            excel_diff::DiffOp::ColumnAdded { col_idx, .. } => {
                added_cols.insert(*col_idx);
            }
            excel_diff::DiffOp::ColumnRemoved { col_idx, .. } => {
                removed_cols.insert(*col_idx);
            }
            excel_diff::DiffOp::BlockMovedRows {
                src_start_row,
                row_count,
                dst_start_row,
                ..
            } => {
                if *row_count == 0 {
                    continue;
                }
                let move_id = format!("r:{}+{}->{}", src_start_row, row_count, dst_start_row);
                moves.push(MoveGroup {
                    id: move_id.clone(),
                    axis: "row".to_string(),
                    src_start: *src_start_row,
                    dst_start: *dst_start_row,
                    count: *row_count,
                });
                let src_end = src_start_row.saturating_add(*row_count);
                for row in *src_start_row..src_end {
                    move_src_rows.insert(row, move_id.clone());
                }
                let dst_end = dst_start_row.saturating_add(*row_count);
                for row in *dst_start_row..dst_end {
                    move_dst_rows.insert(row, move_id.clone());
                }
            }
            excel_diff::DiffOp::BlockMovedColumns {
                src_start_col,
                col_count,
                dst_start_col,
                ..
            } => {
                if *col_count == 0 {
                    continue;
                }
                let move_id = format!("c:{}+{}->{}", src_start_col, col_count, dst_start_col);
                moves.push(MoveGroup {
                    id: move_id.clone(),
                    axis: "col".to_string(),
                    src_start: *src_start_col,
                    dst_start: *dst_start_col,
                    count: *col_count,
                });
                let src_end = src_start_col.saturating_add(*col_count);
                for col in *src_start_col..src_end {
                    move_src_cols.insert(col, move_id.clone());
                }
                let dst_end = dst_start_col.saturating_add(*col_count);
                for col in *dst_start_col..dst_end {
                    move_dst_cols.insert(col, move_id.clone());
                }
            }
            _ => {}
        }
    }

    let row_summary = axis_summary(
        old_nrows,
        new_nrows,
        &added_rows,
        &removed_rows,
        &move_src_rows,
        &move_dst_rows,
    );
    let col_summary = axis_summary(
        old_ncols,
        new_ncols,
        &added_cols,
        &removed_cols,
        &move_src_cols,
        &move_dst_cols,
    );

    let mut skip_reason = None;
    if !row_summary.consistent || !col_summary.consistent {
        skip_reason = Some("Preview disabled: alignment inconsistent.".to_string());
    }
    if let Some(reason) = limit_reason(row_summary.view_len, col_summary.view_len) {
        skip_reason = Some(reason);
    }

    if skip_reason.is_some() {
        return SheetAlignment {
            sheet: sheet.to_string(),
            rows: Vec::new(),
            cols: Vec::new(),
            moves,
            skipped: true,
            skip_reason,
        };
    }

    let (mut rows, rows_consistent) = build_axis_entries(
        old_nrows,
        new_nrows,
        &added_rows,
        &removed_rows,
        &move_src_rows,
        &move_dst_rows,
    );
    let (mut cols, cols_consistent) = build_axis_entries(
        old_ncols,
        new_ncols,
        &added_cols,
        &removed_cols,
        &move_src_cols,
        &move_dst_cols,
    );

    let mut skip_reason = None;
    if !(rows_consistent && cols_consistent) {
        skip_reason = Some("Preview disabled: alignment inconsistent.".to_string());
    }
    if let Some(reason) = limit_reason(rows.len() as u32, cols.len() as u32) {
        skip_reason = Some(reason);
    }

    let skipped = skip_reason.is_some();
    if skipped {
        rows.clear();
        cols.clear();
    }

    SheetAlignment {
        sheet: sheet.to_string(),
        rows,
        cols,
        moves,
        skipped,
        skip_reason,
    }
}

struct AxisSummary {
    view_len: u32,
    consistent: bool,
}

fn axis_summary(
    old_len: u32,
    new_len: u32,
    added: &HashSet<u32>,
    removed: &HashSet<u32>,
    move_src: &HashMap<u32, String>,
    move_dst: &HashMap<u32, String>,
) -> AxisSummary {
    let added_total = union_count(added, move_dst.keys());
    let removed_total = union_count(removed, move_src.keys());

    let view_from_old = old_len.saturating_add(added_total);
    let view_from_new = new_len.saturating_add(removed_total);
    let view_len = view_from_old.max(view_from_new);

    let consistent = old_len >= removed_total
        && new_len >= added_total
        && (old_len - removed_total) == (new_len - added_total);

    AxisSummary { view_len, consistent }
}

fn union_count<'a>(
    base: &HashSet<u32>,
    extra: impl Iterator<Item = &'a u32>,
) -> u32 {
    let mut count = u32::try_from(base.len()).unwrap_or(u32::MAX);
    for value in extra {
        if !base.contains(value) {
            count = count.saturating_add(1);
        }
    }
    count
}

fn build_axis_entries(
    old_len: u32,
    new_len: u32,
    added: &HashSet<u32>,
    removed: &HashSet<u32>,
    move_src: &HashMap<u32, String>,
    move_dst: &HashMap<u32, String>,
) -> (Vec<AxisEntry>, bool) {
    let mut entries = Vec::new();
    let mut i = 0;
    let mut j = 0;

    while i < old_len || j < new_len {
        if j < new_len {
            if let Some(move_id) = move_dst.get(&j) {
                entries.push(AxisEntry {
                    old: None,
                    new: Some(j),
                    kind: AxisKind::MoveDst,
                    move_id: Some(move_id.clone()),
                });
                j += 1;
                continue;
            }
            if added.contains(&j) {
                entries.push(AxisEntry {
                    old: None,
                    new: Some(j),
                    kind: AxisKind::Insert,
                    move_id: None,
                });
                j += 1;
                continue;
            }
        }

        if i < old_len {
            if let Some(move_id) = move_src.get(&i) {
                entries.push(AxisEntry {
                    old: Some(i),
                    new: None,
                    kind: AxisKind::MoveSrc,
                    move_id: Some(move_id.clone()),
                });
                i += 1;
                continue;
            }
            if removed.contains(&i) {
                entries.push(AxisEntry {
                    old: Some(i),
                    new: None,
                    kind: AxisKind::Delete,
                    move_id: None,
                });
                i += 1;
                continue;
            }
        }

        if i < old_len && j < new_len {
            entries.push(AxisEntry {
                old: Some(i),
                new: Some(j),
                kind: AxisKind::Match,
                move_id: None,
            });
            i += 1;
            j += 1;
            continue;
        }

        if i < old_len {
            entries.push(AxisEntry {
                old: Some(i),
                new: None,
                kind: AxisKind::Delete,
                move_id: None,
            });
            i += 1;
        } else if j < new_len {
            entries.push(AxisEntry {
                old: None,
                new: Some(j),
                kind: AxisKind::Insert,
                move_id: None,
            });
            j += 1;
        }
    }

    let summary = axis_summary(old_len, new_len, added, removed, move_src, move_dst);
    (entries, summary.consistent)
}

fn limit_reason(rows: u32, cols: u32) -> Option<String> {
    if rows > MAX_VIEW_ROWS {
        return Some(format!(
            "Preview disabled: sheet has {} rows (cap is {}).",
            rows, MAX_VIEW_ROWS
        ));
    }
    if cols > MAX_VIEW_COLS {
        return Some(format!(
            "Preview disabled: sheet has {} columns (cap is {}).",
            cols, MAX_VIEW_COLS
        ));
    }
    None
}

```

---

### File: `ui_payload\src\capabilities.rs`

```rust
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct HostDefaults {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub max_memory_mb: Option<u32>,
    pub large_mode_threshold: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct HostCapabilities {
    pub engine_version: String,
    pub features: excel_diff::EngineFeatures,
    pub presets: Vec<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub host_defaults: Option<HostDefaults>,
}

impl HostCapabilities {
    pub fn new(engine_version: String) -> Self {
        Self {
            engine_version,
            features: excel_diff::engine_features(),
            presets: vec![
                "fastest".to_string(),
                "balanced".to_string(),
                "most_precise".to_string(),
            ],
            host_defaults: None,
        }
    }

    pub fn with_defaults(mut self, defaults: HostDefaults) -> Self {
        self.host_defaults = Some(defaults);
        self
    }
}

```

---

### File: `ui_payload\src\lib.rs`

```rust
use std::collections::{HashMap, HashSet};
use std::path::Path;

use serde::Serialize;

mod alignment;
mod capabilities;
mod options;
mod outcome;

pub use alignment::SheetAlignment;
pub use capabilities::{HostCapabilities, HostDefaults};
pub use options::{DiffLimits, DiffOptions, DiffPreset, limits_from_config};
pub use outcome::{
    ChangeCounts, DiffOutcome, DiffOutcomeConfig, DiffOutcomeMode, DiffOutcomeSummary, SheetSummary,
    SummaryMeta, SummarySink, summarize_report,
};

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum HostKind {
    Workbook,
    Pbix,
}

#[derive(Serialize)]
pub struct SheetCell {
    row: u32,
    col: u32,
    value: Option<String>,
    formula: Option<String>,
}

#[derive(Serialize)]
pub struct SheetSnapshot {
    pub name: String,
    pub nrows: u32,
    pub ncols: u32,
    pub cells: Vec<SheetCell>,
    pub truncated: bool,
    pub included_cells: u32,
    pub total_non_empty_cells: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub note: Option<String>,
}

#[derive(Serialize)]
pub struct WorkbookSnapshot {
    pub sheets: Vec<SheetSnapshot>,
}

#[derive(Serialize)]
pub struct SheetPairSnapshot {
    pub old: WorkbookSnapshot,
    pub new: WorkbookSnapshot,
}

#[derive(Serialize)]
pub struct DiffWithSheets {
    pub report: excel_diff::DiffReport,
    pub sheets: SheetPairSnapshot,
    pub alignments: Vec<SheetAlignment>,
}

const MAX_SNAPSHOT_CELLS_PER_SHEET: usize = 50_000;
const MAX_SNAPSHOT_CELLS_TOTAL: usize = 200_000;
const STRUCTURAL_PREVIEW_MAX_ROWS: u32 = 200;
const STRUCTURAL_PREVIEW_MAX_COLS: u32 = 80;
const SNAPSHOT_CONTEXT_ROWS: u32 = 1;
const SNAPSHOT_CONTEXT_COLS: u32 = 1;

struct SnapshotCaps {
    per_sheet: usize,
    max_rows: u32,
    max_cols: u32,
    context_rows: u32,
    context_cols: u32,
}

#[derive(Clone, Copy)]
struct Rect {
    row_start: u32,
    row_end: u32,
    col_start: u32,
    col_end: u32,
}

pub fn host_kind_from_name(name: &str) -> Option<HostKind> {
    let lower = name.to_ascii_lowercase();
    let ext = lower.rsplit('.').next().unwrap_or("");
    match ext {
        "xlsx" | "xlsm" | "xltx" | "xltm" | "xlsb" => Some(HostKind::Workbook),
        "pbix" | "pbit" => Some(HostKind::Pbix),
        _ => None,
    }
}

pub fn host_kind_from_path(path: &Path) -> Option<HostKind> {
    let ext = path.extension()?.to_string_lossy().to_ascii_lowercase();
    match ext.as_str() {
        "xlsx" | "xlsm" | "xltx" | "xltm" | "xlsb" => Some(HostKind::Workbook),
        "pbix" | "pbit" => Some(HostKind::Pbix),
        _ => None,
    }
}

pub fn build_payload_from_workbooks(
    old_pkg: &excel_diff::WorkbookPackage,
    new_pkg: &excel_diff::WorkbookPackage,
    cfg: &excel_diff::DiffConfig,
) -> DiffWithSheets {
    let report = old_pkg.diff(new_pkg, cfg);
    build_payload_from_workbook_report(report, old_pkg, new_pkg)
}

pub fn build_payload_from_workbooks_with_progress(
    old_pkg: &excel_diff::WorkbookPackage,
    new_pkg: &excel_diff::WorkbookPackage,
    cfg: &excel_diff::DiffConfig,
    progress: &dyn excel_diff::ProgressCallback,
) -> DiffWithSheets {
    let report = old_pkg.diff_with_progress(new_pkg, cfg, progress);
    build_payload_from_workbook_report(report, old_pkg, new_pkg)
}

pub fn build_payload_from_pbix(
    old_pkg: &excel_diff::PbixPackage,
    new_pkg: &excel_diff::PbixPackage,
    cfg: &excel_diff::DiffConfig,
) -> DiffWithSheets {
    let report = old_pkg.diff(new_pkg, cfg);
    build_payload_from_pbix_report(report)
}

pub fn build_payload_from_pbix_report(report: excel_diff::DiffReport) -> DiffWithSheets {
    let empty = WorkbookSnapshot { sheets: Vec::new() };
    DiffWithSheets {
        report,
        sheets: SheetPairSnapshot {
            old: empty,
            new: WorkbookSnapshot { sheets: Vec::new() },
        },
        alignments: Vec::new(),
    }
}

pub fn build_payload_from_workbook_report(
    report: excel_diff::DiffReport,
    old_pkg: &excel_diff::WorkbookPackage,
    new_pkg: &excel_diff::WorkbookPackage,
) -> DiffWithSheets {
    let sheet_ids = collect_sheet_ids(&report.ops);
    let ops_by_sheet = group_ops_by_sheet(&report);
    let caps = SnapshotCaps {
        per_sheet: MAX_SNAPSHOT_CELLS_PER_SHEET,
        max_rows: STRUCTURAL_PREVIEW_MAX_ROWS,
        max_cols: STRUCTURAL_PREVIEW_MAX_COLS,
        context_rows: SNAPSHOT_CONTEXT_ROWS,
        context_cols: SNAPSHOT_CONTEXT_COLS,
    };

    let sheets = excel_diff::with_default_session(|session| {
        let mut remaining = MAX_SNAPSHOT_CELLS_TOTAL;
        SheetPairSnapshot {
            old: snapshot_workbook(
                &old_pkg.workbook,
                &sheet_ids,
                &session.strings,
                &ops_by_sheet,
                &caps,
                &mut remaining,
            ),
            new: snapshot_workbook(
                &new_pkg.workbook,
                &sheet_ids,
                &session.strings,
                &ops_by_sheet,
                &caps,
                &mut remaining,
            ),
        }
    });

    let alignments = alignment::build_alignments(&report, &sheets);
    DiffWithSheets {
        report,
        sheets,
        alignments,
    }
}

fn collect_sheet_ids(ops: &[excel_diff::DiffOp]) -> HashSet<excel_diff::StringId> {
    let mut sheets = HashSet::new();
    for op in ops {
        if let excel_diff::DiffOp::SheetRenamed { sheet, from, .. } = op {
            sheets.insert(*sheet);
            sheets.insert(*from);
            continue;
        }
        let sheet = match op {
            excel_diff::DiffOp::SheetAdded { sheet }
            | excel_diff::DiffOp::SheetRemoved { sheet }
            | excel_diff::DiffOp::RowAdded { sheet, .. }
            | excel_diff::DiffOp::RowRemoved { sheet, .. }
            | excel_diff::DiffOp::RowReplaced { sheet, .. }
            | excel_diff::DiffOp::DuplicateKeyCluster { sheet, .. }
            | excel_diff::DiffOp::ColumnAdded { sheet, .. }
            | excel_diff::DiffOp::ColumnRemoved { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRows { sheet, .. }
            | excel_diff::DiffOp::BlockMovedColumns { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRect { sheet, .. }
            | excel_diff::DiffOp::RectReplaced { sheet, .. }
            | excel_diff::DiffOp::CellEdited { sheet, .. } => Some(*sheet),
            _ => None,
        };
        if let Some(sheet_id) = sheet {
            sheets.insert(sheet_id);
        }
    }
    sheets
}

fn group_ops_by_sheet(
    report: &excel_diff::DiffReport,
) -> HashMap<String, Vec<&excel_diff::DiffOp>> {
    let mut map: HashMap<String, Vec<&excel_diff::DiffOp>> = HashMap::new();
    for op in &report.ops {
        let sheet = match op {
            excel_diff::DiffOp::SheetAdded { sheet }
            | excel_diff::DiffOp::SheetRemoved { sheet }
            | excel_diff::DiffOp::SheetRenamed { sheet, .. }
            | excel_diff::DiffOp::RowAdded { sheet, .. }
            | excel_diff::DiffOp::RowRemoved { sheet, .. }
            | excel_diff::DiffOp::RowReplaced { sheet, .. }
            | excel_diff::DiffOp::DuplicateKeyCluster { sheet, .. }
            | excel_diff::DiffOp::ColumnAdded { sheet, .. }
            | excel_diff::DiffOp::ColumnRemoved { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRows { sheet, .. }
            | excel_diff::DiffOp::BlockMovedColumns { sheet, .. }
            | excel_diff::DiffOp::BlockMovedRect { sheet, .. }
            | excel_diff::DiffOp::RectReplaced { sheet, .. }
            | excel_diff::DiffOp::CellEdited { sheet, .. } => Some(*sheet),
            _ => None,
        };
        let Some(sheet_id) = sheet else {
            continue;
        };
        let sheet_name = report.resolve(sheet_id).unwrap_or("<unknown>");
        map.entry(sheet_name.to_string())
            .or_insert_with(Vec::new)
            .push(op);
    }
    map
}

fn render_cell_value(
    pool: &excel_diff::StringPool,
    value: &Option<excel_diff::CellValue>,
) -> Option<String> {
    match value {
        None => None,
        Some(excel_diff::CellValue::Blank) => Some(String::new()),
        Some(excel_diff::CellValue::Number(n)) => Some(n.to_string()),
        Some(excel_diff::CellValue::Text(id)) => Some(pool.resolve(*id).to_string()),
        Some(excel_diff::CellValue::Bool(b)) => {
            Some(if *b { "TRUE".to_string() } else { "FALSE".to_string() })
        }
        Some(excel_diff::CellValue::Error(id)) => Some(pool.resolve(*id).to_string()),
    }
}

impl Rect {
    fn area(&self) -> u64 {
        let rows = self.row_end.saturating_sub(self.row_start).saturating_add(1) as u64;
        let cols = self.col_end.saturating_sub(self.col_start).saturating_add(1) as u64;
        rows.saturating_mul(cols)
    }

    fn contains(&self, row: u32, col: u32) -> bool {
        row >= self.row_start
            && row <= self.row_end
            && col >= self.col_start
            && col <= self.col_end
    }
}

fn clamp_range(start: u32, count: u32, max: u32) -> Option<(u32, u32)> {
    if count == 0 || max == 0 {
        return None;
    }
    if start >= max {
        return None;
    }
    let end = start.saturating_add(count).saturating_sub(1);
    let end = end.min(max.saturating_sub(1));
    Some((start, end))
}

fn rect_from_range(
    row_start: u32,
    row_count: u32,
    col_start: u32,
    col_count: u32,
    nrows: u32,
    ncols: u32,
) -> Option<Rect> {
    let (row_start, row_end) = clamp_range(row_start, row_count, nrows)?;
    let (col_start, col_end) = clamp_range(col_start, col_count, ncols)?;
    Some(Rect {
        row_start,
        row_end,
        col_start,
        col_end,
    })
}

fn expand_rect(rect: Rect, context_rows: u32, context_cols: u32, nrows: u32, ncols: u32) -> Rect {
    if nrows == 0 || ncols == 0 {
        return rect;
    }
    let row_start = rect.row_start.saturating_sub(context_rows);
    let col_start = rect.col_start.saturating_sub(context_cols);
    let row_end = rect
        .row_end
        .saturating_add(context_rows)
        .min(nrows.saturating_sub(1));
    let col_end = rect
        .col_end
        .saturating_add(context_cols)
        .min(ncols.saturating_sub(1));
    Rect {
        row_start,
        row_end,
        col_start,
        col_end,
    }
}

fn collect_interest_rects(
    nrows: u32,
    ncols: u32,
    ops: &[&excel_diff::DiffOp],
    caps: &SnapshotCaps,
) -> Vec<Rect> {
    let mut rects = Vec::new();
    if nrows == 0 || ncols == 0 {
        return rects;
    }

    let preview_cols = caps.max_cols.min(ncols);
    let preview_rows = caps.max_rows.min(nrows);

    for op in ops {
        match op {
            excel_diff::DiffOp::CellEdited { addr, .. } => {
                if addr.row < nrows && addr.col < ncols {
                    if let Some(rect) = rect_from_range(addr.row, 1, addr.col, 1, nrows, ncols) {
                        rects.push(rect);
                    }
                }
            }
            excel_diff::DiffOp::RectReplaced {
                start_row,
                row_count,
                start_col,
                col_count,
                ..
            } => {
                if let Some(rect) = rect_from_range(*start_row, *row_count, *start_col, *col_count, nrows, ncols) {
                    rects.push(rect);
                }
            }
            excel_diff::DiffOp::BlockMovedRect {
                src_start_row,
                src_row_count,
                src_start_col,
                src_col_count,
                dst_start_row,
                dst_start_col,
                ..
            } => {
                if let Some(rect) = rect_from_range(
                    *src_start_row,
                    *src_row_count,
                    *src_start_col,
                    *src_col_count,
                    nrows,
                    ncols,
                ) {
                    rects.push(rect);
                }
                if let Some(rect) = rect_from_range(
                    *dst_start_row,
                    *src_row_count,
                    *dst_start_col,
                    *src_col_count,
                    nrows,
                    ncols,
                ) {
                    rects.push(rect);
                }
            }
            excel_diff::DiffOp::RowAdded { row_idx, .. }
            | excel_diff::DiffOp::RowRemoved { row_idx, .. }
            | excel_diff::DiffOp::RowReplaced { row_idx, .. } => {
                if preview_cols == 0 {
                    continue;
                }
                if let Some(rect) = rect_from_range(*row_idx, 1, 0, preview_cols, nrows, ncols) {
                    rects.push(rect);
                }
            }
            excel_diff::DiffOp::DuplicateKeyCluster { left_rows, right_rows, .. } => {
                if preview_cols == 0 {
                    continue;
                }
                for row_idx in left_rows.iter().chain(right_rows.iter()) {
                    if let Some(rect) = rect_from_range(*row_idx, 1, 0, preview_cols, nrows, ncols) {
                        rects.push(rect);
                    }
                }
            }
            excel_diff::DiffOp::BlockMovedRows {
                src_start_row,
                row_count,
                dst_start_row,
                ..
            } => {
                if preview_cols == 0 {
                    continue;
                }
                if let Some(rect) = rect_from_range(*src_start_row, *row_count, 0, preview_cols, nrows, ncols) {
                    rects.push(rect);
                }
                if let Some(rect) = rect_from_range(*dst_start_row, *row_count, 0, preview_cols, nrows, ncols) {
                    rects.push(rect);
                }
            }
            excel_diff::DiffOp::ColumnAdded { col_idx, .. }
            | excel_diff::DiffOp::ColumnRemoved { col_idx, .. } => {
                if preview_rows == 0 {
                    continue;
                }
                if let Some(rect) = rect_from_range(0, preview_rows, *col_idx, 1, nrows, ncols) {
                    rects.push(rect);
                }
            }
            excel_diff::DiffOp::BlockMovedColumns {
                src_start_col,
                col_count,
                dst_start_col,
                ..
            } => {
                if preview_rows == 0 {
                    continue;
                }
                if let Some(rect) = rect_from_range(0, preview_rows, *src_start_col, *col_count, nrows, ncols) {
                    rects.push(rect);
                }
                if let Some(rect) = rect_from_range(0, preview_rows, *dst_start_col, *col_count, nrows, ncols) {
                    rects.push(rect);
                }
            }
            _ => {}
        }
    }

    rects
        .into_iter()
        .map(|rect| expand_rect(rect, caps.context_rows, caps.context_cols, nrows, ncols))
        .collect()
}

fn push_cell(
    cells: &mut Vec<SheetCell>,
    pool: &excel_diff::StringPool,
    row: u32,
    col: u32,
    cell: &excel_diff::Cell,
) {
    let value = render_cell_value(pool, &cell.value);
    let formula = cell.formula.map(|id| format!("={}", pool.resolve(id)));
    cells.push(SheetCell {
        row,
        col,
        value,
        formula,
    });
}

fn snapshot_sheet_limited(
    sheet: &excel_diff::Sheet,
    pool: &excel_diff::StringPool,
    ops: &[&excel_diff::DiffOp],
    caps: &SnapshotCaps,
    budget: &mut usize,
) -> SheetSnapshot {
    let name = pool.resolve(sheet.name).to_string();
    let nrows = sheet.grid.nrows;
    let ncols = sheet.grid.ncols;
    let total_non_empty = sheet.grid.cell_count();
    let total_non_empty_cells = u32::try_from(total_non_empty).unwrap_or(u32::MAX);

    if total_non_empty == 0 {
        return SheetSnapshot {
            name,
            nrows,
            ncols,
            cells: Vec::new(),
            truncated: false,
            included_cells: 0,
            total_non_empty_cells: 0,
            note: None,
        };
    }

    let per_sheet_limit = caps.per_sheet.min(*budget);
    let mut cells = Vec::new();

    if total_non_empty <= per_sheet_limit {
        cells.reserve(total_non_empty);
        for ((row, col), cell) in sheet.grid.iter_cells() {
            push_cell(&mut cells, pool, row, col, cell);
        }
        *budget = budget.saturating_sub(total_non_empty);
        return SheetSnapshot {
            name,
            nrows,
            ncols,
            cells,
            truncated: false,
            included_cells: total_non_empty_cells,
            total_non_empty_cells,
            note: None,
        };
    }

    if per_sheet_limit == 0 {
        return SheetSnapshot {
            name,
            nrows,
            ncols,
            cells: Vec::new(),
            truncated: true,
            included_cells: 0,
            total_non_empty_cells,
            note: Some("Preview limited: snapshot budget reached.".to_string()),
        };
    }

    let rects = collect_interest_rects(nrows, ncols, ops, caps);
    let total_rect_area: u64 = rects.iter().map(Rect::area).sum();
    let mut seen = HashSet::new();
    let mut remaining = per_sheet_limit;

    if rects.is_empty() || total_rect_area > total_non_empty as u64 {
        for ((row, col), cell) in sheet.grid.iter_cells() {
            if remaining == 0 || *budget == 0 {
                break;
            }
            if !rects.is_empty() && !rects.iter().any(|rect| rect.contains(row, col)) {
                continue;
            }
            if seen.insert((row, col)) {
                push_cell(&mut cells, pool, row, col, cell);
                remaining = remaining.saturating_sub(1);
                *budget = budget.saturating_sub(1);
            }
        }
    } else {
        'rects: for rect in &rects {
            for row in rect.row_start..=rect.row_end {
                for col in rect.col_start..=rect.col_end {
                    if remaining == 0 || *budget == 0 {
                        break 'rects;
                    }
                    if !seen.insert((row, col)) {
                        continue;
                    }
                    if let Some(cell) = sheet.grid.get(row, col) {
                        push_cell(&mut cells, pool, row, col, cell);
                        remaining = remaining.saturating_sub(1);
                        *budget = budget.saturating_sub(1);
                    }
                }
            }
        }
    }

    let included_cells = u32::try_from(cells.len()).unwrap_or(u32::MAX);
    let truncated = included_cells < total_non_empty_cells;
    let note = if truncated {
        Some(format!(
            "Preview limited: showing {} of {} non-empty cells.",
            included_cells, total_non_empty_cells
        ))
    } else {
        None
    };

    SheetSnapshot {
        name,
        nrows,
        ncols,
        cells,
        truncated,
        included_cells,
        total_non_empty_cells,
        note,
    }
}

fn snapshot_workbook(
    workbook: &excel_diff::Workbook,
    sheet_ids: &HashSet<excel_diff::StringId>,
    pool: &excel_diff::StringPool,
    ops_by_sheet: &HashMap<String, Vec<&excel_diff::DiffOp>>,
    caps: &SnapshotCaps,
    budget: &mut usize,
) -> WorkbookSnapshot {
    if sheet_ids.is_empty() {
        return WorkbookSnapshot { sheets: Vec::new() };
    }

    let mut sheets = Vec::new();
    for sheet in &workbook.sheets {
        if !sheet_ids.contains(&sheet.name) {
            continue;
        }
        let sheet_name = pool.resolve(sheet.name).to_string();
        let ops = ops_by_sheet
            .get(&sheet_name)
            .map(Vec::as_slice)
            .unwrap_or(&[]);
        sheets.push(snapshot_sheet_limited(sheet, pool, ops, caps, budget));
    }

    WorkbookSnapshot { sheets }
}

```

---

### File: `ui_payload\src\options.rs`

```rust
use excel_diff::{DiffConfig, LimitBehavior};
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DiffPreset {
    Fastest,
    Balanced,
    MostPrecise,
}

impl DiffPreset {
    pub fn as_str(&self) -> &'static str {
        match self {
            DiffPreset::Fastest => "fastest",
            DiffPreset::Balanced => "balanced",
            DiffPreset::MostPrecise => "most_precise",
        }
    }

    pub fn to_config(self) -> DiffConfig {
        match self {
            DiffPreset::Fastest => DiffConfig::fastest(),
            DiffPreset::Balanced => DiffConfig::balanced(),
            DiffPreset::MostPrecise => DiffConfig::most_precise(),
        }
    }
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffLimits {
    pub max_memory_mb: Option<u32>,
    pub timeout_ms: Option<u64>,
    pub max_ops: Option<usize>,
    pub on_limit_exceeded: Option<LimitBehavior>,
}

impl DiffLimits {
    pub fn apply_to(&self, cfg: &mut DiffConfig) {
        if let Some(value) = self.max_memory_mb {
            cfg.hardening.max_memory_mb = Some(value);
        }
        if let Some(value) = self.timeout_ms {
            let seconds = timeout_seconds_from_ms(value);
            cfg.hardening.timeout_seconds = Some(seconds);
        }
        if let Some(value) = self.max_ops {
            cfg.hardening.max_ops = Some(value);
        }
        if let Some(value) = self.on_limit_exceeded {
            cfg.hardening.on_limit_exceeded = value;
        }
    }
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffOptions {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preset: Option<DiffPreset>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<DiffLimits>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub trusted: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config_json: Option<String>,
}

impl DiffOptions {
    pub fn effective_config(&self, default_config: DiffConfig) -> Result<DiffConfig, String> {
        let mut cfg = if let Some(config_json) = self
            .config_json
            .as_ref()
            .map(|v| v.trim())
            .filter(|v| !v.is_empty())
        {
            serde_json::from_str::<DiffConfig>(config_json)
                .map_err(|e| format!("Invalid configJson: {e}"))?
        } else if let Some(preset) = self.preset {
            preset.to_config()
        } else {
            default_config
        };

        if let Some(limits) = &self.limits {
            limits.apply_to(&mut cfg);
        }

        cfg.validate().map_err(|e| e.to_string())?;
        Ok(cfg)
    }
}

pub fn limits_from_config(cfg: &DiffConfig) -> DiffLimits {
    DiffLimits {
        max_memory_mb: cfg.hardening.max_memory_mb,
        timeout_ms: cfg
            .hardening
            .timeout_seconds
            .map(|v| u64::from(v).saturating_mul(1000)),
        max_ops: cfg.hardening.max_ops,
        on_limit_exceeded: Some(cfg.hardening.on_limit_exceeded),
    }
}

fn timeout_seconds_from_ms(value: u64) -> u32 {
    let seconds = value.saturating_add(999) / 1000;
    u32::try_from(seconds).unwrap_or(u32::MAX)
}

```

---

### File: `ui_payload\src\outcome.rs`

```rust
use std::collections::HashMap;

use excel_diff::{DiffOp, DiffReport, DiffSink, DiffSummary, QueryMetadataField, StringId};
use serde::{Deserialize, Serialize};

use crate::DiffWithSheets;
use crate::options::{DiffLimits, DiffPreset};

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DiffOutcomeMode {
    Payload,
    Large,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffOutcomeConfig {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub preset: Option<DiffPreset>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub limits: Option<DiffLimits>,
}

#[derive(Serialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffOutcome {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub diff_id: Option<String>,
    pub mode: DiffOutcomeMode,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub payload: Option<DiffWithSheets>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub summary: Option<DiffOutcomeSummary>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub config: Option<DiffOutcomeConfig>,
}

#[derive(Debug, Clone, Default)]
pub struct SummaryMeta {
    pub old_path: Option<String>,
    pub new_path: Option<String>,
    pub old_name: Option<String>,
    pub new_name: Option<String>,
}

#[derive(Debug, Default, Clone, Copy, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ChangeCounts {
    pub added: u64,
    pub removed: u64,
    pub modified: u64,
    pub moved: u64,
}

impl ChangeCounts {
    fn apply(&mut self, kind: ChangeKind) {
        match kind {
            ChangeKind::Added => self.added = self.added.saturating_add(1),
            ChangeKind::Removed => self.removed = self.removed.saturating_add(1),
            ChangeKind::Modified => self.modified = self.modified.saturating_add(1),
            ChangeKind::Moved => self.moved = self.moved.saturating_add(1),
        }
    }

    pub fn add_op(&mut self, op: &DiffOp) {
        if let Some(kind) = classify_op(op) {
            self.apply(kind);
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct SheetSummary {
    pub sheet_name: String,
    pub op_count: u64,
    pub counts: ChangeCounts,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DiffOutcomeSummary {
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub old_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub new_path: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub old_name: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub new_name: Option<String>,
    pub complete: bool,
    pub op_count: u64,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub warnings: Vec<String>,
    pub counts: ChangeCounts,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub sheets: Vec<SheetSummary>,
}

#[derive(Debug, Clone, Copy)]
enum ChangeKind {
    Added,
    Removed,
    Modified,
    Moved,
}

#[derive(Debug, Default, Clone)]
struct SheetStats {
    op_count: u64,
    counts: ChangeCounts,
}

pub struct SummarySink {
    counts: ChangeCounts,
    sheet_stats: HashMap<u32, SheetStats>,
    strings: Vec<String>,
}

impl SummarySink {
    pub fn new() -> Self {
        Self {
            counts: ChangeCounts::default(),
            sheet_stats: HashMap::new(),
            strings: Vec::new(),
        }
    }

    pub fn into_summary(self, summary: DiffSummary, meta: SummaryMeta) -> DiffOutcomeSummary {
        let mut sheets = Vec::with_capacity(self.sheet_stats.len());
        for (sheet_id, stats) in self.sheet_stats {
            let sheet_name = self
                .strings
                .get(sheet_id as usize)
                .cloned()
                .unwrap_or_else(|| "<unknown>".to_string());
            sheets.push(SheetSummary {
                sheet_name,
                op_count: stats.op_count,
                counts: stats.counts,
            });
        }
        sheets.sort_by(|a, b| a.sheet_name.cmp(&b.sheet_name));

        DiffOutcomeSummary {
            old_path: meta.old_path,
            new_path: meta.new_path,
            old_name: meta.old_name,
            new_name: meta.new_name,
            complete: summary.complete,
            op_count: summary.op_count as u64,
            warnings: summary.warnings,
            counts: self.counts,
            sheets,
        }
    }
}

impl DiffSink for SummarySink {
    fn begin(&mut self, pool: &excel_diff::StringPool) -> Result<(), excel_diff::DiffError> {
        self.strings = pool.strings().to_vec();
        Ok(())
    }

    fn emit(&mut self, op: DiffOp) -> Result<(), excel_diff::DiffError> {
        self.counts.add_op(&op);

        if let Some(sheet_id) = op_sheet_id(&op).map(|id| id.0) {
            let entry = self
                .sheet_stats
                .entry(sheet_id)
                .or_insert_with(SheetStats::default);
            entry.op_count = entry.op_count.saturating_add(1);
            entry.counts.add_op(&op);
        }

        Ok(())
    }
}

pub fn summarize_report(report: &DiffReport, meta: SummaryMeta) -> DiffOutcomeSummary {
    let mut counts = ChangeCounts::default();
    let mut stats: HashMap<u32, SheetStats> = HashMap::new();

    for op in &report.ops {
        counts.add_op(op);
        if let Some(sheet_id) = op_sheet_id(op).map(|id| id.0) {
            let entry = stats.entry(sheet_id).or_insert_with(SheetStats::default);
            entry.op_count = entry.op_count.saturating_add(1);
            entry.counts.add_op(op);
        }
    }

    let mut sheets = Vec::with_capacity(stats.len());
    for (sheet_id, sheet_stats) in stats {
        let sheet_name = report
            .strings
            .get(sheet_id as usize)
            .cloned()
            .unwrap_or_else(|| "<unknown>".to_string());
        sheets.push(SheetSummary {
            sheet_name,
            op_count: sheet_stats.op_count,
            counts: sheet_stats.counts,
        });
    }
    sheets.sort_by(|a, b| a.sheet_name.cmp(&b.sheet_name));

    DiffOutcomeSummary {
        old_path: meta.old_path,
        new_path: meta.new_path,
        old_name: meta.old_name,
        new_name: meta.new_name,
        complete: report.complete,
        op_count: report.ops.len() as u64,
        warnings: report.warnings.clone(),
        counts,
        sheets,
    }
}

fn op_sheet_id(op: &DiffOp) -> Option<StringId> {
    match op {
        DiffOp::SheetAdded { sheet }
        | DiffOp::SheetRemoved { sheet }
        | DiffOp::SheetRenamed { sheet, .. }
        | DiffOp::RowAdded { sheet, .. }
        | DiffOp::RowRemoved { sheet, .. }
        | DiffOp::RowReplaced { sheet, .. }
        | DiffOp::DuplicateKeyCluster { sheet, .. }
        | DiffOp::ColumnAdded { sheet, .. }
        | DiffOp::ColumnRemoved { sheet, .. }
        | DiffOp::BlockMovedRows { sheet, .. }
        | DiffOp::BlockMovedColumns { sheet, .. }
        | DiffOp::BlockMovedRect { sheet, .. }
        | DiffOp::RectReplaced { sheet, .. }
        | DiffOp::CellEdited { sheet, .. } => Some(*sheet),
        _ => None,
    }
}

fn classify_op(op: &DiffOp) -> Option<ChangeKind> {
    match op {
        DiffOp::SheetAdded { .. }
        | DiffOp::RowAdded { .. }
        | DiffOp::ColumnAdded { .. }
        | DiffOp::NamedRangeAdded { .. }
        | DiffOp::ChartAdded { .. }
        | DiffOp::VbaModuleAdded { .. }
        | DiffOp::QueryAdded { .. }
        | DiffOp::QueryMetadataChanged {
            field: QueryMetadataField::LoadToSheet,
            ..
        } => Some(ChangeKind::Added),
        DiffOp::SheetRemoved { .. }
        | DiffOp::RowRemoved { .. }
        | DiffOp::ColumnRemoved { .. }
        | DiffOp::NamedRangeRemoved { .. }
        | DiffOp::ChartRemoved { .. }
        | DiffOp::VbaModuleRemoved { .. }
        | DiffOp::QueryRemoved { .. } => Some(ChangeKind::Removed),
        DiffOp::BlockMovedRows { .. }
        | DiffOp::BlockMovedColumns { .. }
        | DiffOp::BlockMovedRect { .. } => Some(ChangeKind::Moved),
        DiffOp::RowReplaced { .. }
        | DiffOp::DuplicateKeyCluster { .. }
        | DiffOp::RectReplaced { .. }
        | DiffOp::CellEdited { .. }
        | DiffOp::SheetRenamed { .. }
        | DiffOp::NamedRangeChanged { .. }
        | DiffOp::ChartChanged { .. }
        | DiffOp::VbaModuleChanged { .. }
        | DiffOp::QueryRenamed { .. }
        | DiffOp::QueryDefinitionChanged { .. }
        | DiffOp::QueryMetadataChanged { .. } => Some(ChangeKind::Modified),
        #[cfg(feature = "model-diff")]
        DiffOp::TableAdded { .. }
        | DiffOp::ModelColumnAdded { .. }
        | DiffOp::RelationshipAdded { .. }
        | DiffOp::MeasureAdded { .. } => Some(ChangeKind::Added),
        #[cfg(feature = "model-diff")]
        DiffOp::TableRemoved { .. }
        | DiffOp::ModelColumnRemoved { .. }
        | DiffOp::RelationshipRemoved { .. }
        | DiffOp::MeasureRemoved { .. } => Some(ChangeKind::Removed),
        #[cfg(feature = "model-diff")]
        DiffOp::ModelColumnTypeChanged { .. }
        | DiffOp::ModelColumnPropertyChanged { .. }
        | DiffOp::CalculatedColumnDefinitionChanged { .. }
        | DiffOp::RelationshipPropertyChanged { .. }
        | DiffOp::MeasureDefinitionChanged { .. } => Some(ChangeKind::Modified),
        _ => None,
    }
}

```

---

### File: `wasm\Cargo.toml`

```toml
[package]
name = "excel_diff_wasm"
version = "0.1.0"
edition = "2024"
description = "WebAssembly bindings for excel_diff"
license = "MIT"
repository = "https://github.com/dvora/excel_diff"
homepage = "https://github.com/dvora/excel_diff"

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
excel_diff = { path = "../core", default-features = false, features = ["excel-open-xml", "model-diff"] }
wasm-bindgen = "0.2"
js-sys = "0.3"
serde_json = "1.0"
console_error_panic_hook = "0.1"
ui_payload = { path = "../ui_payload" }


```

---

### File: `wasm\src\lib.rs`

```rust
use std::io::{self, Cursor, Write};

use excel_diff::advanced::{CallbackSink, diff_workbooks_streaming};
use excel_diff::{
    CellValue, DiffConfig, DiffSink, Grid, JsonLinesSink, Sheet, SheetKind, StringPool, Workbook,
};
use js_sys::Function;
use ui_payload::{
    DiffOptions, DiffOutcome, DiffOutcomeConfig, DiffOutcomeMode, DiffPreset, HostCapabilities,
    HostDefaults, SummaryMeta, SummarySink, limits_from_config, summarize_report,
};
use wasm_bindgen::prelude::*;

const WASM_DEFAULT_MAX_MEMORY_MB: u32 = 256;
const JSONL_CHUNK_BYTES: usize = 256 * 1024;

struct JsonlChunkWriter {
    buffer: Vec<u8>,
    on_chunk: Function,
}

impl JsonlChunkWriter {
    fn new(on_chunk: Function) -> Self {
        Self {
            buffer: Vec::with_capacity(JSONL_CHUNK_BYTES),
            on_chunk,
        }
    }

    fn flush_buffer(&mut self) -> io::Result<()> {
        if self.buffer.is_empty() {
            return Ok(());
        }
        let text = String::from_utf8_lossy(&self.buffer);
        self.on_chunk
            .call1(&JsValue::NULL, &JsValue::from_str(text.as_ref()))
            .map_err(|e| io::Error::new(io::ErrorKind::Other, format!("chunk callback failed: {e:?}")))?;
        self.buffer.clear();
        Ok(())
    }
}

impl Write for JsonlChunkWriter {
    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {
        self.buffer.extend_from_slice(buf);
        if self.buffer.len() >= JSONL_CHUNK_BYTES {
            self.flush_buffer()?;
        }
        Ok(buf.len())
    }

    fn flush(&mut self) -> io::Result<()> {
        self.flush_buffer()
    }
}

fn wasm_default_config() -> DiffConfig {
    let mut cfg = DiffConfig::default();
    cfg.hardening.max_memory_mb = Some(WASM_DEFAULT_MAX_MEMORY_MB);
    cfg
}

fn parse_options(options_json: &str) -> Result<DiffOptions, JsValue> {
    if options_json.trim().is_empty() {
        return Ok(DiffOptions::default());
    }
    serde_json::from_str::<DiffOptions>(options_json)
        .map_err(|e| JsValue::from_str(&format!("Invalid options JSON: {e}")))
}

fn outcome_config_from_options(options: &DiffOptions, cfg: &DiffConfig) -> DiffOutcomeConfig {
    let preset = if options.config_json.as_ref().map(|v| v.trim()).unwrap_or("").is_empty() {
        Some(options.preset.unwrap_or(DiffPreset::Balanced))
    } else {
        None
    };
    DiffOutcomeConfig {
        preset,
        limits: Some(limits_from_config(cfg)),
    }
}

fn summary_meta_from_names(old_name: &str, new_name: &str) -> SummaryMeta {
    SummaryMeta {
        old_path: None,
        new_path: None,
        old_name: Some(old_name.to_string()),
        new_name: Some(new_name.to_string()),
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
struct SheetKey {
    name_lower: String,
    kind: SheetKind,
}

fn estimate_diff_cell_volume(old: &Workbook, new: &Workbook) -> u64 {
    excel_diff::with_default_session(|session| {
        let mut max_counts: std::collections::HashMap<SheetKey, u64> =
            std::collections::HashMap::new();
        for sheet in old.sheets.iter().chain(new.sheets.iter()) {
            let name_lower = session.strings.resolve(sheet.name).to_lowercase();
            let key = SheetKey {
                name_lower,
                kind: sheet.kind.clone(),
            };
            let cell_count = sheet.grid.cell_count() as u64;
            max_counts
                .entry(key)
                .and_modify(|v| {
                    if cell_count > *v {
                        *v = cell_count;
                    }
                })
                .or_insert(cell_count);
        }

        max_counts.values().copied().sum()
    })
}

#[wasm_bindgen(start)]
pub fn init_panic_hook() {
    console_error_panic_hook::set_once();
}

#[wasm_bindgen]
pub fn diff_files_json(
    old_bytes: Vec<u8>,
    new_bytes: Vec<u8>,
    old_name: &str,
    new_name: &str,
) -> Result<String, JsValue> {
    let kind_old = ui_payload::host_kind_from_name(old_name)
        .ok_or_else(|| JsValue::from_str("Unsupported old file extension"))?;
    let kind_new = ui_payload::host_kind_from_name(new_name)
        .ok_or_else(|| JsValue::from_str("Unsupported new file extension"))?;

    if kind_old != kind_new {
        return Err(JsValue::from_str("Old/new files must be the same type"));
    }

    let old_cursor = Cursor::new(old_bytes);
    let new_cursor = Cursor::new(new_bytes);

    let cfg = wasm_default_config();

    let report = match kind_old {
        ui_payload::HostKind::Workbook => {
            let pkg_old = excel_diff::WorkbookPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old workbook: {}", e)))?;
            let pkg_new = excel_diff::WorkbookPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new workbook: {}", e)))?;
            pkg_old.diff(&pkg_new, &cfg)
        }
        ui_payload::HostKind::Pbix => {
            let pkg_old = excel_diff::PbixPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old PBIX/PBIT: {}", e)))?;
            let pkg_new = excel_diff::PbixPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new PBIX/PBIT: {}", e)))?;
            pkg_old.diff(&pkg_new, &cfg)
        }
    };

    excel_diff::serialize_diff_report(&report)
        .map_err(|e| JsValue::from_str(&format!("Failed to serialize report: {}", e)))
}

#[wasm_bindgen]
pub fn diff_files_with_sheets_json(
    old_bytes: Vec<u8>,
    new_bytes: Vec<u8>,
    old_name: &str,
    new_name: &str,
) -> Result<String, JsValue> {
    let kind_old = ui_payload::host_kind_from_name(old_name)
        .ok_or_else(|| JsValue::from_str("Unsupported old file extension"))?;
    let kind_new = ui_payload::host_kind_from_name(new_name)
        .ok_or_else(|| JsValue::from_str("Unsupported new file extension"))?;

    if kind_old != kind_new {
        return Err(JsValue::from_str("Old/new files must be the same type"));
    }

    let old_cursor = Cursor::new(old_bytes);
    let new_cursor = Cursor::new(new_bytes);
    let cfg = wasm_default_config();

    let payload = match kind_old {
        ui_payload::HostKind::Workbook => {
            let pkg_old = excel_diff::WorkbookPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old workbook: {}", e)))?;
            let pkg_new = excel_diff::WorkbookPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new workbook: {}", e)))?;
            ui_payload::build_payload_from_workbooks(&pkg_old, &pkg_new, &cfg)
        }
        ui_payload::HostKind::Pbix => {
            let pkg_old = excel_diff::PbixPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old PBIX/PBIT: {}", e)))?;
            let pkg_new = excel_diff::PbixPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new PBIX/PBIT: {}", e)))?;
            ui_payload::build_payload_from_pbix(&pkg_old, &pkg_new, &cfg)
        }
    };

    serde_json::to_string(&payload)
        .map_err(|e| JsValue::from_str(&format!("Failed to serialize report: {}", e)))
}

#[wasm_bindgen]
pub fn diff_files_outcome_json(
    old_bytes: Vec<u8>,
    new_bytes: Vec<u8>,
    old_name: &str,
    new_name: &str,
    options_json: String,
) -> Result<String, JsValue> {
    let kind_old = ui_payload::host_kind_from_name(old_name)
        .ok_or_else(|| JsValue::from_str("Unsupported old file extension"))?;
    let kind_new = ui_payload::host_kind_from_name(new_name)
        .ok_or_else(|| JsValue::from_str("Unsupported new file extension"))?;

    if kind_old != kind_new {
        return Err(JsValue::from_str("Old/new files must be the same type"));
    }

    let options = parse_options(&options_json)?;
    let cfg = options
        .effective_config(wasm_default_config())
        .map_err(|e| JsValue::from_str(&e))?;
    let outcome_config = outcome_config_from_options(&options, &cfg);
    let meta = summary_meta_from_names(old_name, new_name);

    let old_cursor = Cursor::new(old_bytes);
    let new_cursor = Cursor::new(new_bytes);

    let outcome = match kind_old {
        ui_payload::HostKind::Workbook => {
            let pkg_old = excel_diff::WorkbookPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old workbook: {}", e)))?;
            let pkg_new = excel_diff::WorkbookPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new workbook: {}", e)))?;

            let estimated_cells = estimate_diff_cell_volume(&pkg_old.workbook, &pkg_new.workbook);
            let use_large_mode = excel_diff::should_use_large_mode(estimated_cells, &cfg);

            if use_large_mode {
                let mut sink = SummarySink::new();
                let summary = pkg_old
                    .diff_streaming(&pkg_new, &cfg, &mut sink)
                    .map_err(|e| JsValue::from_str(&format!("Streaming diff failed: {}", e)))?;
                let summary = sink.into_summary(summary, meta.clone());
                DiffOutcome {
                    diff_id: None,
                    mode: DiffOutcomeMode::Large,
                    payload: None,
                    summary: Some(summary),
                    config: Some(outcome_config),
                }
            } else {
                let payload = ui_payload::build_payload_from_workbooks(&pkg_old, &pkg_new, &cfg);
                let summary = summarize_report(&payload.report, meta.clone());
                DiffOutcome {
                    diff_id: None,
                    mode: DiffOutcomeMode::Payload,
                    payload: Some(payload),
                    summary: Some(summary),
                    config: Some(outcome_config),
                }
            }
        }
        ui_payload::HostKind::Pbix => {
            let pkg_old = excel_diff::PbixPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old PBIX/PBIT: {}", e)))?;
            let pkg_new = excel_diff::PbixPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new PBIX/PBIT: {}", e)))?;
            let payload = ui_payload::build_payload_from_pbix(&pkg_old, &pkg_new, &cfg);
            let summary = summarize_report(&payload.report, meta);
            DiffOutcome {
                diff_id: None,
                mode: DiffOutcomeMode::Payload,
                payload: Some(payload),
                summary: Some(summary),
                config: Some(outcome_config),
            }
        }
    };

    serde_json::to_string(&outcome)
        .map_err(|e| JsValue::from_str(&format!("Failed to serialize outcome: {}", e)))
}

#[wasm_bindgen]
pub fn diff_files_jsonl_stream(
    old_bytes: Vec<u8>,
    new_bytes: Vec<u8>,
    old_name: &str,
    new_name: &str,
    options_json: String,
    on_chunk: Function,
) -> Result<(), JsValue> {
    let kind_old = ui_payload::host_kind_from_name(old_name)
        .ok_or_else(|| JsValue::from_str("Unsupported old file extension"))?;
    let kind_new = ui_payload::host_kind_from_name(new_name)
        .ok_or_else(|| JsValue::from_str("Unsupported new file extension"))?;

    if kind_old != kind_new {
        return Err(JsValue::from_str("Old/new files must be the same type"));
    }

    let options = parse_options(&options_json)?;
    let cfg = options
        .effective_config(wasm_default_config())
        .map_err(|e| JsValue::from_str(&e))?;

    let old_cursor = Cursor::new(old_bytes);
    let new_cursor = Cursor::new(new_bytes);
    let writer = JsonlChunkWriter::new(on_chunk);
    let mut sink = JsonLinesSink::new(writer);

    match kind_old {
        ui_payload::HostKind::Workbook => {
            let pkg_old = excel_diff::WorkbookPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old workbook: {}", e)))?;
            let pkg_new = excel_diff::WorkbookPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new workbook: {}", e)))?;
            pkg_old
                .diff_streaming(&pkg_new, &cfg, &mut sink)
                .map_err(|e| JsValue::from_str(&format!("Streaming diff failed: {}", e)))?;
        }
        ui_payload::HostKind::Pbix => {
            let pkg_old = excel_diff::PbixPackage::open(old_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open old PBIX/PBIT: {}", e)))?;
            let pkg_new = excel_diff::PbixPackage::open(new_cursor)
                .map_err(|e| JsValue::from_str(&format!("Failed to open new PBIX/PBIT: {}", e)))?;
            pkg_old
                .diff_streaming(&pkg_new, &cfg, &mut sink)
                .map_err(|e| JsValue::from_str(&format!("Streaming diff failed: {}", e)))?;
        }
    };

    sink.finish()
        .map_err(|e| JsValue::from_str(&format!("Failed to finalize JSONL: {}", e)))?;

    Ok(())
}

#[wasm_bindgen]
pub fn diff_workbooks_json(old_bytes: Vec<u8>, new_bytes: Vec<u8>) -> Result<String, JsValue> {
    diff_files_json(old_bytes, new_bytes, "old.xlsx", "new.xlsx")
}

#[wasm_bindgen]
pub fn get_version() -> String {
    env!("CARGO_PKG_VERSION").to_string()
}

#[wasm_bindgen]
pub fn get_capabilities() -> Result<String, JsValue> {
    let caps = HostCapabilities::new(get_version()).with_defaults(HostDefaults {
        max_memory_mb: Some(WASM_DEFAULT_MAX_MEMORY_MB),
        large_mode_threshold: excel_diff::AUTO_STREAM_CELL_THRESHOLD,
    });
    serde_json::to_string(&caps)
        .map_err(|e| JsValue::from_str(&format!("Failed to serialize capabilities: {}", e)))
}

#[wasm_bindgen]
pub struct DiffSummary {
    pub op_count: usize,
    pub sheets_old: usize,
    pub sheets_new: usize,
}

#[wasm_bindgen]
pub fn diff_summary(old_bytes: Vec<u8>, new_bytes: Vec<u8>) -> Result<DiffSummary, JsValue> {
    let old_cursor = Cursor::new(old_bytes);
    let new_cursor = Cursor::new(new_bytes);

    let pkg_old = excel_diff::WorkbookPackage::open(old_cursor)
        .map_err(|e| JsValue::from_str(&format!("Failed to open old file: {}", e)))?;
    let pkg_new = excel_diff::WorkbookPackage::open(new_cursor)
        .map_err(|e| JsValue::from_str(&format!("Failed to open new file: {}", e)))?;

    let report = pkg_old.diff(&pkg_new, &wasm_default_config());

    Ok(DiffSummary {
        op_count: report.ops.len(),
        sheets_old: pkg_old.workbook.sheets.len(),
        sheets_new: pkg_new.workbook.sheets.len(),
    })
}

fn create_dense_grid(nrows: u32, ncols: u32, base_value: i64) -> Grid {
    let mut grid = Grid::new(nrows, ncols);
    for row in 0..nrows {
        for col in 0..ncols {
            let value = base_value + row as i64 * 1000 + col as i64;
            grid.insert_cell(row, col, Some(CellValue::Number(value as f64)), None);
        }
    }
    grid
}

fn single_sheet_workbook(pool: &mut StringPool, grid: Grid) -> Workbook {
    let sheet_name = pool.intern("Sheet1");
    Workbook {
        sheets: vec![Sheet {
            name: sheet_name,
            workbook_sheet_id: None,
            kind: SheetKind::Worksheet,
            grid,
        }],
        named_ranges: Vec::new(),
        charts: Vec::new(),
    }
}

#[wasm_bindgen]
pub fn run_memory_benchmark(case_name: &str) -> Result<u32, JsValue> {
    let (old_grid, new_grid) = match case_name {
        "low_similarity" => (
            create_dense_grid(6000, 50, 0),
            create_dense_grid(6000, 50, 100_000_000),
        ),
        "near_identical" => {
            let grid_a = create_dense_grid(6000, 50, 0);
            let mut grid_b = grid_a.clone();
            grid_b.insert_cell(3000, 25, Some(CellValue::Number(999999.0)), None);
            (grid_a, grid_b)
        }
        _ => {
            return Err(JsValue::from_str(
                "Unknown benchmark case (expected 'low_similarity' or 'near_identical')",
            ))
        }
    };

    let mut pool = StringPool::new();
    let old = single_sheet_workbook(&mut pool, old_grid);
    let new = single_sheet_workbook(&mut pool, new_grid);
    let cfg = wasm_default_config();

    let mut sink = CallbackSink::new(|_op| {});
    let summary = diff_workbooks_streaming(&old, &new, &mut pool, &cfg, &mut sink);
    Ok(summary.op_count as u32)
}

#[wasm_bindgen]
pub fn wasm_memory_bytes() -> u32 {
    #[cfg(target_arch = "wasm32")]
    {
        let pages = core::arch::wasm32::memory_size(0) as u32;
        pages.saturating_mul(65536)
    }
    #[cfg(not(target_arch = "wasm32"))]
    {
        0
    }
}

#[cfg(test)]
mod tests {
    use super::{create_dense_grid, estimate_diff_cell_volume, wasm_default_config};
    use excel_diff::{Sheet, SheetKind, Workbook, AUTO_STREAM_CELL_THRESHOLD, should_use_large_mode, with_default_session};
    use ui_payload::DiffOutcomeMode;

    fn build_workbook(grid: excel_diff::Grid) -> Workbook {
        let name_id = with_default_session(|session| session.strings.intern("Sheet1"));
        Workbook {
            sheets: vec![Sheet {
                name: name_id,
                workbook_sheet_id: None,
                kind: SheetKind::Worksheet,
                grid,
            }],
            named_ranges: Vec::new(),
            charts: Vec::new(),
        }
    }

    #[test]
    fn large_mode_threshold_triggers_in_wasm() {
        let grid = create_dense_grid(1000, 1001, 0);
        let old = build_workbook(grid.clone());
        let new = build_workbook(grid);
        let estimated = estimate_diff_cell_volume(&old, &new);
        assert!(estimated >= AUTO_STREAM_CELL_THRESHOLD);

        let cfg = wasm_default_config();
        let mode = if should_use_large_mode(estimated, &cfg) {
            DiffOutcomeMode::Large
        } else {
            DiffOutcomeMode::Payload
        };
        assert_eq!(mode, DiffOutcomeMode::Large);
    }
}

```

---

### File: `web\diff_worker.js`

```javascript
import init, { diff_files_jsonl_stream, diff_files_outcome_json, get_version } from "./wasm/excel_diff_wasm.js";

let initPromise = null;
let cachedVersion = null;

async function ensureInitialized() {
  if (!initPromise) {
    initPromise = (async () => {
      await init();
      cachedVersion = get_version();
      return cachedVersion;
    })();
  }
  return initPromise;
}

function postProgress(requestId, stage, detail) {
  self.postMessage({ type: "progress", requestId, stage, detail });
}

self.addEventListener("message", async (event) => {
  const msg = event.data || {};
  const requestId = msg.requestId;
  if (!msg.type) return;

  try {
    if (msg.type === "init") {
      postProgress(requestId, "init", "Initializing engine");
      const version = await ensureInitialized();
      self.postMessage({ type: "ready", requestId, version });
      return;
    }

    if (msg.type === "diff") {
      await ensureInitialized();
      postProgress(requestId, "diff", "Diffing workbooks");
      let oldBytes = new Uint8Array(msg.oldBuffer);
      let newBytes = new Uint8Array(msg.newBuffer);
      const optionsJson = JSON.stringify(msg.options || {});
      let json = diff_files_outcome_json(
        oldBytes,
        newBytes,
        msg.oldName || "old",
        msg.newName || "new",
        optionsJson
      );
      postProgress(requestId, "parse", "Parsing results");
      let payload = JSON.parse(json);
      self.postMessage({ type: "result", requestId, payload });
      oldBytes = null;
      newBytes = null;
      json = null;
      payload = null;
      return;
    }

    if (msg.type === "jsonl") {
      await ensureInitialized();
      postProgress(requestId, "diff", "Streaming JSONL output");
      let oldBytes = new Uint8Array(msg.oldBuffer);
      let newBytes = new Uint8Array(msg.newBuffer);
      const optionsJson = JSON.stringify(msg.options || {});
      const onChunk = chunk => {
        if (chunk) {
          self.postMessage({ type: "jsonl-chunk", requestId, chunk });
        }
      };
      diff_files_jsonl_stream(
        oldBytes,
        newBytes,
        msg.oldName || "old",
        msg.newName || "new",
        optionsJson,
        onChunk
      );
      self.postMessage({ type: "jsonl-done", requestId });
      oldBytes = null;
      newBytes = null;
      return;
    }
  } catch (err) {
    const message = err && err.message ? err.message : String(err);
    self.postMessage({ type: "error", requestId, message });
  }
});

```

---

### File: `web\diff_worker_client.js`

```javascript
export function createDiffWorkerClient({ onStatus } = {}) {
  let worker = null;
  let current = null;
  let requestCounter = 0;
  let readyPromise = null;
  let cachedVersion = null;
  let disposed = false;

  function nextRequestId() {
    requestCounter += 1;
    return requestCounter;
  }

  function notify(status) {
    if (typeof onStatus === "function") {
      onStatus(status);
    }
  }

  function resetWorker() {
    if (worker) {
      worker.terminate();
      worker = null;
    }
    current = null;
    readyPromise = null;
    cachedVersion = null;
  }

  function handleWorkerError(error) {
    const message = error && error.message ? error.message : String(error);
    if (current && current.reject) {
      current.reject(new Error(message));
      current = null;
    }
    resetWorker();
  }

  function handleMessage(event) {
    const msg = event.data || {};
    if (!current || msg.requestId !== current.id) return;

    if (msg.type === "progress") {
      notify({ stage: msg.stage, detail: msg.detail, source: "worker" });
      return;
    }

    if (msg.type === "ready") {
      cachedVersion = msg.version || "";
      const resolve = current.resolve;
      current = null;
      resolve(cachedVersion);
      return;
    }

    if (msg.type === "result") {
      const resolve = current.resolve;
      current = null;
      resolve(msg.payload);
      return;
    }

    if (msg.type === "jsonl-chunk") {
      if (current && current.kind === "jsonl") {
        current.chunks.push(msg.chunk || "");
      }
      return;
    }

    if (msg.type === "jsonl-done") {
      if (current && current.kind === "jsonl") {
        const resolve = current.resolve;
        const chunks = current.chunks || [];
        current = null;
        resolve(new Blob(chunks, { type: "application/x-ndjson" }));
      }
      return;
    }

    if (msg.type === "error") {
      const reject = current.reject;
      current = null;
      reject(new Error(msg.message || "Worker error"));
    }
  }

  function ensureWorker() {
    if (disposed) {
      throw new Error("Diff worker client disposed.");
    }
    if (!worker) {
      worker = new Worker(new URL("./diff_worker.js", import.meta.url), { type: "module" });
      worker.addEventListener("message", handleMessage);
      worker.addEventListener("error", handleWorkerError);
      worker.addEventListener("messageerror", handleWorkerError);
    }
    return worker;
  }

  async function ready() {
    if (cachedVersion) return cachedVersion;
    if (readyPromise) return readyPromise;
    const w = ensureWorker();
    const id = nextRequestId();
    readyPromise = new Promise((resolve, reject) => {
      current = { id, resolve, reject, kind: "init" };
      w.postMessage({ type: "init", requestId: id });
    });
    return readyPromise;
  }

  async function diff(files, options = {}) {
    if (current) {
      throw new Error("Diff already in progress.");
    }
    await ready();
    const w = ensureWorker();
    const id = nextRequestId();
    return new Promise((resolve, reject) => {
      current = { id, resolve, reject, kind: "diff" };
      w.postMessage(
        {
          type: "diff",
          requestId: id,
          oldName: files.oldName,
          newName: files.newName,
          oldBuffer: files.oldBuffer,
          newBuffer: files.newBuffer,
          options
        },
        [files.oldBuffer, files.newBuffer]
      );
    });
  }

  async function downloadJsonl(files, options = {}) {
    if (current) {
      throw new Error("Diff already in progress.");
    }
    await ready();
    const w = ensureWorker();
    const id = nextRequestId();
    return new Promise((resolve, reject) => {
      current = { id, resolve, reject, kind: "jsonl", chunks: [] };
      w.postMessage(
        {
          type: "jsonl",
          requestId: id,
          oldName: files.oldName,
          newName: files.newName,
          oldBuffer: files.oldBuffer,
          newBuffer: files.newBuffer,
          options
        },
        [files.oldBuffer, files.newBuffer]
      );
    });
  }

  function cancel() {
    if (!worker) return false;
    if (current && current.reject) {
      current.reject(new Error("Diff canceled."));
      current = null;
    }
    resetWorker();
    return true;
  }

  function dispose() {
    disposed = true;
    resetWorker();
  }

  return {
    ready,
    diff,
    downloadJsonl,
    cancel,
    dispose
  };
}

```

---

### File: `web\export.js`

```javascript
function escapeHtml(text) {
  return String(text ?? "")
    .replace(/&/g, "&amp;")
    .replace(/</g, "&lt;")
    .replace(/>/g, "&gt;")
    .replace(/"/g, "&quot;")
    .replace(/'/g, "&#39;");
}

function safeName(value, fallback) {
  const text = String(value || fallback || "file").trim();
  const cleaned = text.replace(/[^a-z0-9._-]+/gi, "_").replace(/^_+|_+$/g, "");
  return cleaned || fallback || "file";
}

function downloadBlob(filename, mime, textOrBytes) {
  const blob = textOrBytes instanceof Blob ? textOrBytes : new Blob([textOrBytes], { type: mime });
  const url = URL.createObjectURL(blob);
  const a = document.createElement("a");
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();
  a.remove();
  setTimeout(() => URL.revokeObjectURL(url), 1000);
}

export function downloadReportJson({ report, meta }) {
  const payload = { meta: meta || {}, report: report || {} };
  const json = JSON.stringify(payload, null, 2);
  const oldName = safeName(meta?.oldName, "old");
  const newName = safeName(meta?.newName, "new");
  const date = (meta?.createdAtIso || new Date().toISOString()).slice(0, 10);
  const filename = `excel-diff-report__${oldName}__${newName}__${date}.json`;
  downloadBlob(filename, "application/json", json);
}

export function downloadHtmlReport({
  title,
  meta,
  renderedResultsHtml,
  cssText,
  reportJsonText,
  gridPreviews
}) {
  const safeTitle = escapeHtml(title || "Excel Diff Report");
  const createdAt = escapeHtml(meta?.createdAtIso || new Date().toISOString());
  const oldName = escapeHtml(meta?.oldName || "Old file");
  const newName = escapeHtml(meta?.newName || "New file");
  const reportPre = escapeHtml(reportJsonText || "");
  const previews = gridPreviews || {};
  const previewHtml = Object.keys(previews).length
    ? `
      <section class="export-previews">
        <h2>Grid previews</h2>
        ${Object.entries(previews)
          .map(
            ([sheet, dataUrl]) => `
          <div class="export-preview">
            <h3>${escapeHtml(sheet)}</h3>
            <img src="${dataUrl}" alt="Grid preview for ${escapeHtml(sheet)}" />
          </div>`
          )
          .join("")}
      </section>
    `
    : "";

  const html = `<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>${safeTitle}</title>
    <style>
${cssText || ""}
      .export-wrap { max-width: 1100px; margin: 32px auto; padding: 0 20px 40px; }
      .export-meta { color: var(--text-secondary); margin-bottom: 24px; }
      .export-previews img { max-width: 100%; border: 1px solid var(--border-primary); border-radius: 8px; background: var(--bg-primary); }
      .export-preview { margin-bottom: 24px; }
      pre { white-space: pre-wrap; word-break: break-word; background: var(--bg-primary); border: 1px solid var(--border-primary); color: var(--text-secondary); padding: 16px; border-radius: 8px; }
    </style>
  </head>
  <body>
    <div class="export-wrap">
      <header>
        <h1>${safeTitle}</h1>
        <div class="export-meta">
          <div><strong>Old:</strong> ${oldName}</div>
          <div><strong>New:</strong> ${newName}</div>
          <div><strong>Generated:</strong> ${createdAt}</div>
        </div>
      </header>
      <main>
        ${renderedResultsHtml || ""}
        ${previewHtml}
        <section class="export-report-json">
          <h2>Report JSON</h2>
          <pre>${reportPre}</pre>
        </section>
      </main>
    </div>
  </body>
</html>`;

  const oldSafe = safeName(meta?.oldName, "old");
  const newSafe = safeName(meta?.newName, "new");
  const date = (meta?.createdAtIso || new Date().toISOString()).slice(0, 10);
  const filename = `excel-diff-report__${oldSafe}__${newSafe}__${date}.html`;
  downloadBlob(filename, "text/html", html);
}

export function downloadJsonl({ blob, meta }) {
  const oldName = safeName(meta?.oldName, "old");
  const newName = safeName(meta?.newName, "new");
  const date = (meta?.createdAtIso || new Date().toISOString()).slice(0, 10);
  const filename = `excel-diff-stream__${oldName}__${newName}__${date}.jsonl`;
  downloadBlob(filename, "application/x-ndjson", blob || "");
}

```

---

### File: `web\grid_metrics.js`

```javascript
export const GRID_METRICS = {
  rowHeight: 36,
  colWidth: 100,
  rowHeaderWidth: 50,
  colHeaderHeight: 36,
  paneGap: 16
};

```

---

### File: `web\grid_painter.js`

```javascript
function colToLetter(col) {
  let result = "";
  let c = col;
  while (c >= 0) {
    result = String.fromCharCode((c % 26) + 65) + result;
    c = Math.floor(c / 26) - 1;
  }
  return result;
}

function safeText(value) {
  if (value === null || value === undefined) return "";
  return String(value);
}

function resolveCellParts(cell, side) {
  if (!cell) return { value: "", formula: "" };
  if (cell.edit) {
    const value = side === "old" ? cell.edit.fromValue : cell.edit.toValue;
    const formula = side === "old" ? cell.edit.fromFormula : cell.edit.toFormula;
    if (value || formula) return { value: safeText(value), formula: safeText(formula) };
  }
  const payload = side === "old" ? cell.old : cell.new;
  if (!payload || !payload.cell) return { value: "", formula: "" };
  return { value: safeText(payload.cell.value || ""), formula: safeText(payload.cell.formula || "") };
}

function resolveCellText(cell, side, contentMode) {
  const parts = resolveCellParts(cell, side);
  if (contentMode === "formulas") {
    return parts.formula || parts.value || "";
  }
  return parts.value || parts.formula || "";
}

function resolveUnifiedText(cell, contentMode) {
  if (!cell) return "";
  if (cell.diffKind === "added") return resolveCellText(cell, "new", contentMode);
  if (cell.diffKind === "removed") return resolveCellText(cell, "old", contentMode);
  if (cell.diffKind === "moved") {
    return cell.moveRole === "src"
      ? resolveCellText(cell, "old", contentMode)
      : resolveCellText(cell, "new", contentMode);
  }
  return resolveCellText(cell, "new", contentMode) || resolveCellText(cell, "old", contentMode);
}

function truncateText(ctx, text, maxWidth) {
  const value = safeText(text);
  if (!value) return "";
  if (ctx.measureText(value).width <= maxWidth) return value;
  let end = value.length;
  while (end > 0) {
    const candidate = value.slice(0, end) + "...";
    if (ctx.measureText(candidate).width <= maxWidth) return candidate;
    end -= 1;
  }
  return "";
}

function drawStrike(ctx, x, y, width, color) {
  if (!width) return;
  ctx.save();
  ctx.strokeStyle = color;
  ctx.lineWidth = 1;
  ctx.beginPath();
  ctx.moveTo(x, y);
  ctx.lineTo(x + width, y);
  ctx.stroke();
  ctx.restore();
}

function axisStyle(entry, axis, theme) {
  if (!entry) {
    return { bg: theme.bgTertiary, text: theme.textSecondary, marker: "" };
  }
  if (entry.kind === "insert") {
    return { bg: theme.diffAddBg, text: theme.accentGreen, marker: "+" };
  }
  if (entry.kind === "delete") {
    return { bg: theme.diffRemoveBg, text: theme.accentRed, marker: "-" };
  }
  if (entry.kind === "move_src") {
    return { bg: theme.diffMoveBg, text: theme.accentPurple, marker: "M" };
  }
  if (entry.kind === "move_dst") {
    return { bg: theme.diffMoveDstBg || theme.diffMoveBg, text: theme.accentPurple, marker: "M" };
  }
  return { bg: theme.bgTertiary, text: theme.textSecondary, marker: "" };
}

function cellStyle(cell, theme) {
  if (!cell) return { bg: theme.bgPrimary, text: theme.textPrimary };
  if (cell.diffKind === "edited") {
    return { bg: theme.diffModifyBg, text: theme.textPrimary };
  }
  if (cell.diffKind === "added") {
    return { bg: theme.diffAddBg, text: theme.textPrimary };
  }
  if (cell.diffKind === "removed") {
    return { bg: theme.diffRemoveBg, text: theme.textPrimary };
  }
  if (cell.diffKind === "moved") {
    const bg = cell.moveRole === "dst" ? theme.diffMoveDstBg : theme.diffMoveBg;
    return { bg, text: theme.textPrimary };
  }
  if (cell.diffKind === "unchanged") {
    return { bg: theme.bgPrimary, text: theme.textPrimary };
  }
  return { bg: theme.bgPrimary, text: theme.textMuted };
}

function drawHeaderCell(ctx, x, y, width, height, label, style, theme) {
  ctx.fillStyle = style.bg;
  ctx.fillRect(x, y, width, height);
  ctx.strokeStyle = theme.borderSecondary;
  ctx.lineWidth = 1;
  ctx.strokeRect(x, y, width, height);
  ctx.fillStyle = style.text;
  ctx.font = "11px 'JetBrains Mono', monospace";
  ctx.textBaseline = "middle";
  ctx.textAlign = "center";
  const text = style.marker ? `${label} ${style.marker}` : label;
  ctx.fillText(text, x + width / 2, y + height / 2);
}

function drawRowHeader(ctx, x, y, width, height, label, style, theme) {
  ctx.fillStyle = style.bg;
  ctx.fillRect(x, y, width, height);
  ctx.strokeStyle = theme.borderSecondary;
  ctx.lineWidth = 1;
  ctx.strokeRect(x, y, width, height);
  ctx.fillStyle = style.text;
  ctx.font = "11px 'JetBrains Mono', monospace";
  ctx.textBaseline = "middle";
  ctx.textAlign = "right";
  const text = style.marker ? `${label} ${style.marker}` : label;
  ctx.fillText(text, x + width - 6, y + height / 2);
}

function drawCellText(ctx, x, y, width, height, text, color) {
  const paddingX = 8;
  const maxWidth = Math.max(0, width - paddingX * 2);
  ctx.fillStyle = color;
  ctx.font = "12px 'JetBrains Mono', monospace";
  ctx.textBaseline = "middle";
  ctx.textAlign = "left";
  const value = truncateText(ctx, text, maxWidth);
  ctx.fillText(value, x + paddingX, y + height / 2);
}

function drawCellTextLines(ctx, x, y, width, height, primary, secondary, primaryColor, secondaryColor) {
  const paddingX = 8;
  const maxWidth = Math.max(0, width - paddingX * 2);
  ctx.textAlign = "left";
  ctx.textBaseline = "top";
  ctx.font = "11px 'JetBrains Mono', monospace";
  ctx.fillStyle = primaryColor;
  const primaryText = truncateText(ctx, primary, maxWidth);
  ctx.fillText(primaryText, x + paddingX, y + 6);
  ctx.font = "10px 'JetBrains Mono', monospace";
  ctx.fillStyle = secondaryColor;
  const secondaryText = truncateText(ctx, secondary, maxWidth);
  ctx.fillText(secondaryText, x + paddingX, y + height / 2);
}

function drawEditedUnified(ctx, x, y, width, height, cell, theme, contentMode) {
  const paddingX = 8;
  const paddingY = 6;
  const maxWidth = Math.max(0, width - paddingX * 2);
  const oldText = resolveCellText(cell, "old", contentMode);
  const newText = resolveCellText(cell, "new", contentMode);

  ctx.font = "10px 'JetBrains Mono', monospace";
  ctx.textBaseline = "top";
  ctx.textAlign = "left";
  ctx.fillStyle = theme.accentRed;
  const oldDisplay = truncateText(ctx, oldText, maxWidth);
  ctx.fillText(oldDisplay, x + paddingX, y + paddingY);
  const oldWidth = ctx.measureText(oldDisplay).width;
  drawStrike(ctx, x + paddingX, y + paddingY + 6, oldWidth, theme.accentRed);

  ctx.font = "12px 'JetBrains Mono', monospace";
  ctx.fillStyle = theme.accentGreen;
  const newDisplay = truncateText(ctx, newText, maxWidth);
  ctx.fillText(newDisplay, x + paddingX, y + paddingY + 14);
}

function drawCell(ctx, x, y, width, height, cell, theme, mode, side, contentMode) {
  const style = cellStyle(cell, theme);
  ctx.fillStyle = style.bg;
  ctx.fillRect(x, y, width, height);
  ctx.strokeStyle = theme.borderSecondary;
  ctx.lineWidth = 1;
  ctx.strokeRect(x, y, width, height);

  if (!cell || cell.diffKind === "empty") return;

  if (mode === "unified" && cell.diffKind === "edited") {
    drawEditedUnified(ctx, x, y, width, height, cell, theme, contentMode);
    return;
  }

  let text = "";
  if (mode === "side_by_side") {
    if (contentMode === "both") {
      const parts = resolveCellParts(cell, side);
      const primary = parts.value || parts.formula;
      const secondary = parts.value && parts.formula && parts.formula !== parts.value ? parts.formula : "";
      if (secondary && height >= 30) {
        const color =
          cell.diffKind === "added"
            ? theme.accentGreen
            : cell.diffKind === "removed"
              ? theme.accentRed
              : cell.diffKind === "moved"
                ? theme.accentPurple
                : style.text;
        drawCellTextLines(ctx, x, y, width, height, primary, secondary, color, theme.textMuted);
        return;
      }
      text = primary;
    } else {
      text = resolveCellText(cell, side, contentMode);
    }
  } else {
    text = resolveUnifiedText(cell, contentMode);
  }

  if (cell.diffKind === "removed" && mode === "unified") {
    ctx.save();
    ctx.fillStyle = theme.accentRed;
    drawCellText(ctx, x, y, width, height, text, theme.accentRed);
    const textWidth = ctx.measureText(truncateText(ctx, text, Math.max(0, width - 16))).width;
    drawStrike(ctx, x + 8, y + height / 2, textWidth, theme.accentRed);
    ctx.restore();
    return;
  }

  const color =
    cell.diffKind === "added"
      ? theme.accentGreen
      : cell.diffKind === "removed"
        ? theme.accentRed
        : cell.diffKind === "moved"
          ? theme.accentPurple
          : style.text;

  drawCellText(ctx, x, y, width, height, text, color);
}

export function paintGrid(ctx, model) {
  const {
    sheetVm,
    mode,
    contentMode,
    rowMap,
    colMap,
    rowLookup,
    colLookup,
    flash,
    viewport,
    metrics,
    theme,
    selection,
    hover,
    hoverMoveId
  } = model;
  const { width, height, scrollTop, scrollLeft } = viewport;
  const rows = sheetVm.axis.rows.entries;
  const cols = sheetVm.axis.cols.entries;
  const rowCount = rowMap ? rowMap.length : rows.length;
  const colCount = colMap ? colMap.length : cols.length;

  ctx.clearRect(0, 0, width, height);
  ctx.fillStyle = theme.bgPrimary;
  ctx.fillRect(0, 0, width, height);

  if (!rowCount || !colCount || width <= 0 || height <= 0) return;

  const rowHeight = metrics.rowHeight;
  const colWidth = metrics.colWidth;
  const rowHeaderWidth = metrics.rowHeaderWidth;
  const colHeaderHeight = metrics.colHeaderHeight;
  const paneGap = metrics.paneGap;
  const paneCount = mode === "side_by_side" ? 2 : 1;
  const paneWidth =
    paneCount === 2
      ? Math.max(0, Math.floor((width - rowHeaderWidth - paneGap) / 2))
      : Math.max(0, width - rowHeaderWidth);
  const cellAreaHeight = Math.max(0, height - colHeaderHeight);

  const firstRow = Math.max(0, Math.floor(scrollTop / rowHeight));
  const visibleRowCount = cellAreaHeight > 0 ? Math.ceil(cellAreaHeight / rowHeight) + 1 : 0;
  const lastRow = Math.min(rowCount - 1, firstRow + visibleRowCount - 1);

  const firstCol = Math.max(0, Math.floor(scrollLeft / colWidth));
  const visibleColCount = paneWidth > 0 ? Math.ceil(paneWidth / colWidth) + 1 : 0;
  const lastCol = Math.min(colCount - 1, firstCol + visibleColCount - 1);

  const paneOffsets = [rowHeaderWidth];
  if (paneCount === 2) {
    paneOffsets.push(rowHeaderWidth + paneWidth + paneGap);
  }

  ctx.fillStyle = theme.bgTertiary;
  ctx.fillRect(0, 0, rowHeaderWidth, colHeaderHeight);

  for (let c = firstCol; c <= lastCol; c++) {
    const viewCol = colMap ? colMap[c] : c;
    const entry = cols[viewCol];
    const style = axisStyle(entry, "col", theme);
    const label = colToLetter(viewCol);
    for (let p = 0; p < paneCount; p++) {
      const x = paneOffsets[p] + (c * colWidth - scrollLeft);
      drawHeaderCell(ctx, x, 0, colWidth, colHeaderHeight, label, style, theme);
    }
  }

  for (let r = firstRow; r <= lastRow; r++) {
    const viewRow = rowMap ? rowMap[r] : r;
    const entry = rows[viewRow];
    const style = axisStyle(entry, "row", theme);
    const y = colHeaderHeight + (r * rowHeight - scrollTop);
    drawRowHeader(ctx, 0, y, rowHeaderWidth, rowHeight, String(viewRow + 1), style, theme);
  }

  for (let r = firstRow; r <= lastRow; r++) {
    const viewRow = rowMap ? rowMap[r] : r;
    const y = colHeaderHeight + (r * rowHeight - scrollTop);
    for (let c = firstCol; c <= lastCol; c++) {
      const viewCol = colMap ? colMap[c] : c;
      const cell = sheetVm.cellAt(viewRow, viewCol);
      for (let p = 0; p < paneCount; p++) {
        const x = paneOffsets[p] + (c * colWidth - scrollLeft);
        const side = p === 0 ? "old" : "new";
        drawCell(ctx, x, y, colWidth, rowHeight, cell, theme, mode, side, contentMode);
      }

      if (hoverMoveId) {
        const rowEntry = rows[viewRow];
        const colEntry = cols[viewCol];
        if ((rowEntry && rowEntry.move_id === hoverMoveId) || (colEntry && colEntry.move_id === hoverMoveId)) {
          ctx.save();
          ctx.strokeStyle = theme.diffMoveBorder || theme.accentPurple;
          ctx.lineWidth = 2;
          for (let p = 0; p < paneCount; p++) {
            const x = paneOffsets[p] + (c * colWidth - scrollLeft);
            ctx.strokeRect(x + 1, y + 1, colWidth - 2, rowHeight - 2);
          }
          ctx.restore();
        }
      }
    }
  }

  if (hover && hover.viewRow !== null && hover.viewCol !== null) {
    const row = rowLookup ? rowLookup[hover.viewRow] : hover.viewRow;
    const col = colLookup ? colLookup[hover.viewCol] : hover.viewCol;
    if (row !== null && row !== undefined && col !== null && col !== undefined &&
        row >= firstRow && row <= lastRow && col >= firstCol && col <= lastCol) {
      const y = colHeaderHeight + (row * rowHeight - scrollTop);
      for (let p = 0; p < paneCount; p++) {
        if (paneCount === 2 && hover.pane !== null && hover.pane !== undefined && hover.pane !== p) {
          continue;
        }
        const x = paneOffsets[p] + (col * colWidth - scrollLeft);
        ctx.save();
        ctx.strokeStyle = theme.accentBlue;
        ctx.lineWidth = 1;
        ctx.strokeRect(x + 1, y + 1, colWidth - 2, rowHeight - 2);
        ctx.restore();
      }
    }
  }

  if (selection && selection.viewRow !== null && selection.viewCol !== null) {
    const row = rowLookup ? rowLookup[selection.viewRow] : selection.viewRow;
    const col = colLookup ? colLookup[selection.viewCol] : selection.viewCol;
    if (row !== null && row !== undefined && col !== null && col !== undefined &&
        row >= firstRow && row <= lastRow && col >= firstCol && col <= lastCol) {
      const y = colHeaderHeight + (row * rowHeight - scrollTop);
      for (let p = 0; p < paneCount; p++) {
        const x = paneOffsets[p] + (col * colWidth - scrollLeft);
        ctx.save();
        ctx.strokeStyle = theme.accentBlue;
        ctx.lineWidth = 2;
        ctx.strokeRect(x + 1, y + 1, colWidth - 2, rowHeight - 2);
        ctx.restore();
      }
    }
  }

  if (flash && flash.alpha > 0) {
    const mapRange = (start, end, lookup) => {
      let min = null;
      let max = null;
      for (let i = start; i <= end; i++) {
        const vis = lookup ? lookup[i] : i;
        if (vis === null || vis === undefined) continue;
        if (min === null || vis < min) min = vis;
        if (max === null || vis > max) max = vis;
      }
      if (min === null || max === null) return null;
      return { start: min, end: max };
    };

    ctx.save();
    ctx.globalAlpha = flash.alpha;
    ctx.strokeStyle = theme.accentBlue;
    ctx.lineWidth = 2;

    if (flash.kind === "region" && flash.bounds) {
      const rowRange = mapRange(flash.bounds.top, flash.bounds.bottom, rowLookup);
      const colRange = mapRange(flash.bounds.left, flash.bounds.right, colLookup);
      if (rowRange && colRange) {
        const startRow = Math.max(firstRow, rowRange.start);
        const endRow = Math.min(lastRow, rowRange.end);
        const startCol = Math.max(firstCol, colRange.start);
        const endCol = Math.min(lastCol, colRange.end);
        if (startRow <= endRow && startCol <= endCol) {
          const y = colHeaderHeight + (startRow * rowHeight - scrollTop);
          const height = (endRow - startRow + 1) * rowHeight;
          for (let p = 0; p < paneCount; p++) {
            const x = paneOffsets[p] + (startCol * colWidth - scrollLeft);
            const width = (endCol - startCol + 1) * colWidth;
            ctx.strokeRect(x + 1, y + 1, width - 2, height - 2);
          }
        }
      }
    } else if (flash.kind === "row") {
      const row = rowLookup ? rowLookup[flash.viewRow] : flash.viewRow;
      if (row !== null && row !== undefined && row >= firstRow && row <= lastRow) {
        const y = colHeaderHeight + (row * rowHeight - scrollTop);
        ctx.strokeRect(0 + 1, y + 1, rowHeaderWidth - 2, rowHeight - 2);
      }
    } else if (flash.kind === "col") {
      const col = colLookup ? colLookup[flash.viewCol] : flash.viewCol;
      if (col !== null && col !== undefined && col >= firstCol && col <= lastCol) {
        for (let p = 0; p < paneCount; p++) {
          const x = paneOffsets[p] + (col * colWidth - scrollLeft);
          ctx.strokeRect(x + 1, 0 + 1, colWidth - 2, colHeaderHeight - 2);
        }
      }
    } else if (flash.kind === "cell") {
      const row = rowLookup ? rowLookup[flash.viewRow] : flash.viewRow;
      const col = colLookup ? colLookup[flash.viewCol] : flash.viewCol;
      if (row !== null && row !== undefined && col !== null && col !== undefined &&
          row >= firstRow && row <= lastRow && col >= firstCol && col <= lastCol) {
        const y = colHeaderHeight + (row * rowHeight - scrollTop);
        for (let p = 0; p < paneCount; p++) {
          const x = paneOffsets[p] + (col * colWidth - scrollLeft);
          ctx.strokeRect(x + 1, y + 1, colWidth - 2, rowHeight - 2);
        }
      }
    }

    ctx.restore();
  }
}

```

---

### File: `web\grid_theme.js`

```javascript
function resolveCssVar(style, name, fallback) {
  const value = style.getPropertyValue(name);
  return value && value.trim() ? value.trim() : fallback;
}

export function readGridTheme(rootEl = document.documentElement) {
  const style = getComputedStyle(rootEl);
  return {
    bgPrimary: resolveCssVar(style, "--bg-primary", "#0d1117"),
    bgSecondary: resolveCssVar(style, "--bg-secondary", "#161b22"),
    bgTertiary: resolveCssVar(style, "--bg-tertiary", "#21262d"),
    borderPrimary: resolveCssVar(style, "--border-primary", "#30363d"),
    borderSecondary: resolveCssVar(style, "--border-secondary", "#21262d"),
    textPrimary: resolveCssVar(style, "--text-primary", "#e6edf3"),
    textSecondary: resolveCssVar(style, "--text-secondary", "#8b949e"),
    textMuted: resolveCssVar(style, "--text-muted", "#6e7681"),
    accentBlue: resolveCssVar(style, "--accent-blue", "#58a6ff"),
    accentGreen: resolveCssVar(style, "--accent-green", "#3fb950"),
    accentRed: resolveCssVar(style, "--accent-red", "#f85149"),
    accentYellow: resolveCssVar(style, "--accent-yellow", "#d29922"),
    accentPurple: resolveCssVar(style, "--accent-purple", "#a371f7"),
    diffAddBg: resolveCssVar(style, "--diff-add-bg", "rgba(46, 160, 67, 0.15)"),
    diffRemoveBg: resolveCssVar(style, "--diff-remove-bg", "rgba(248, 81, 73, 0.15)"),
    diffModifyBg: resolveCssVar(style, "--diff-modify-bg", "rgba(210, 153, 34, 0.15)"),
    diffMoveBg: resolveCssVar(style, "--diff-move-bg", "rgba(163, 113, 247, 0.15)"),
    diffMoveBorder: resolveCssVar(style, "--diff-move-border", "rgba(163, 113, 247, 0.4)"),
    diffMoveDstBg: resolveCssVar(style, "--diff-move-dst-bg", "rgba(163, 113, 247, 0.25)")
  };
}

```

---

### File: `web\grid_viewer.js`

```javascript
import { GRID_METRICS } from "./grid_metrics.js";
import { paintGrid } from "./grid_painter.js";
import { readGridTheme } from "./grid_theme.js";

function colToLetter(col) {
  let result = "";
  let c = col;
  while (c >= 0) {
    result = String.fromCharCode((c % 26) + 65) + result;
    c = Math.floor(c / 26) - 1;
  }
  return result;
}

function formatCellAddress(row, col) {
  return `${colToLetter(col)}${row + 1}`;
}

function clamp(value, min, max) {
  return Math.max(min, Math.min(max, value));
}

function createEl(tag, className, text) {
  const el = document.createElement(tag);
  if (className) el.className = className;
  if (text !== undefined) el.textContent = text;
  return el;
}

function extractSideDetails(cell, side) {
  if (!cell) return { value: "", formula: "" };
  const edit = cell.edit;
  let value = side === "old" ? edit?.fromValue : edit?.toValue;
  let formula = side === "old" ? edit?.fromFormula : edit?.toFormula;
  if (!value && !formula) {
    const payload = side === "old" ? cell.old : cell.new;
    value = payload?.cell?.value ?? "";
    formula = payload?.cell?.formula ?? "";
  }
  return { value: String(value || ""), formula: String(formula || "") };
}

function buildCellSummary(cell, viewRow, viewCol) {
  const summary = {
    viewAddress: formatCellAddress(viewRow, viewCol),
    diffKind: cell?.diffKind || "empty",
    moveId: cell?.moveId || "",
    moveRole: cell?.moveRole || "",
    oldAddress: "",
    newAddress: "",
    old: extractSideDetails(cell, "old"),
    fresh: extractSideDetails(cell, "new")
  };
  if (cell?.old) {
    summary.oldAddress = formatCellAddress(cell.old.row, cell.old.col);
  }
  if (cell?.new) {
    summary.newAddress = formatCellAddress(cell.new.row, cell.new.col);
  }
  return summary;
}

export function mountSheetGridViewer({ mountEl, sheetVm, opts = {} }) {
  const theme = readGridTheme(document.documentElement);
  const metrics = { ...GRID_METRICS };
  const regionLookup = new Map((sheetVm.changes?.regions || []).map(region => [region.id, region]));
  const anchorList = Array.isArray(sheetVm.changes?.anchors) ? sheetVm.changes.anchors : [];
  const gridAnchors = anchorList
    .filter(anchor => anchor?.target?.kind === "grid")
    .map(anchor => ({
      id: anchor.id,
      viewRow: anchor.target.viewRow,
      viewCol: anchor.target.viewCol,
      regionId: anchor.target.regionId,
      moveId: anchor.target.moveId,
      group: anchor.group
    }));
  const anchorIndexById = new Map(gridAnchors.map((anchor, idx) => [anchor.id, idx]));

  let initialAnchorIndex = 0;
  if (typeof opts.initialAnchor === "string") {
    const idx = anchorIndexById.get(opts.initialAnchor);
    if (Number.isFinite(idx)) initialAnchorIndex = idx;
  } else if (Number.isFinite(opts.initialAnchor)) {
    initialAnchorIndex = opts.initialAnchor;
  }
  if (gridAnchors.length === 0) {
    initialAnchorIndex = -1;
  }

  const state = {
    mode: opts.initialMode === "unified" ? "unified" : "side_by_side",
    selected: null,
    hover: null,
    hoverMoveId: null,
    anchorIndex: initialAnchorIndex,
    pinned: false,
    contentMode: opts.displayOptions?.contentMode || "values",
    focusRows: Boolean(opts.displayOptions?.focusRows),
    focusCols: Boolean(opts.displayOptions?.focusCols),
    rowMap: null,
    colMap: null,
    rowLookup: null,
    colLookup: null,
    rowCount: sheetVm.axis.rows.entries.length,
    colCount: sheetVm.axis.cols.entries.length,
    flash: null
  };

  mountEl.innerHTML = "";

  const root = createEl("div", "grid-viewer");
  root.tabIndex = 0;

  const toolbar = createEl("div", "grid-toolbar");
  const toolbarGroup = createEl("div", "grid-toolbar-group");
  const sideBtn = createEl("button", "grid-mode-btn", "Side-by-side");
  sideBtn.type = "button";
  sideBtn.dataset.mode = "side_by_side";
  const unifiedBtn = createEl("button", "grid-mode-btn", "Unified");
  unifiedBtn.type = "button";
  unifiedBtn.dataset.mode = "unified";
  toolbarGroup.append(sideBtn, unifiedBtn);
  const toolbarHint = createEl("div", "grid-toolbar-hint", "Arrow keys: move, N/P: next/prev change");
  toolbar.append(toolbarGroup, toolbarHint);

  const body = createEl("div", "grid-viewer-body");
  const canvasWrap = createEl("div", "grid-canvas-wrap");
  const scroll = createEl("div", "grid-scroll");
  const spacer = createEl("div", "grid-scroll-spacer");
  const canvas = createEl("canvas", "grid-canvas");
  canvas.setAttribute("role", "presentation");
  scroll.append(spacer, canvas);
  const tooltip = createEl("div", "grid-tooltip");
  canvasWrap.append(scroll, tooltip);

  const inspector = createEl("div", "grid-inspector");
  const inspectorTitle = createEl("div", "grid-inspector-title", "Inspector");
  const inspectorEmpty = createEl("div", "grid-inspector-empty", "Select a cell to inspect.");
  const inspectorContent = createEl("div", "grid-inspector-content");
  inspector.append(inspectorTitle, inspectorEmpty, inspectorContent);

  body.append(canvasWrap, inspector);
  root.append(toolbar, body);
  mountEl.append(root);

  const ctx = canvas.getContext("2d");
  let rafId = null;

  let contentWidth = 0;
  let contentHeight = 0;

  function buildIndexLookup(map, total) {
    if (!Array.isArray(map)) return null;
    const lookup = new Array(total).fill(null);
    for (let i = 0; i < map.length; i++) {
      const viewIndex = map[i];
      if (Number.isFinite(viewIndex)) {
        lookup[viewIndex] = i;
      }
    }
    return lookup;
  }

  function computeFocusMap(axis, context) {
    const entries = axis === "row" ? sheetVm.axis.rows.entries : sheetVm.axis.cols.entries;
    const maxIndex = entries.length - 1;
    if (maxIndex < 0) return null;
    const changed = new Set();
    for (let i = 0; i < entries.length; i++) {
      const entry = entries[i];
      if (entry && entry.kind && entry.kind !== "match") {
        changed.add(i);
      }
    }
    for (const item of sheetVm.changes?.items || []) {
      if (axis === "row" && item.group === "rows" && Number.isFinite(item.viewStart) && Number.isFinite(item.viewEnd)) {
        for (let r = item.viewStart; r <= item.viewEnd; r++) {
          changed.add(r);
        }
      }
      if (axis === "col" && item.group === "cols" && Number.isFinite(item.viewStart) && Number.isFinite(item.viewEnd)) {
        for (let c = item.viewStart; c <= item.viewEnd; c++) {
          changed.add(c);
        }
      }
    }
    for (const region of sheetVm.changes?.regions || []) {
      if (axis === "row") {
        for (let r = region.top; r <= region.bottom; r++) {
          changed.add(r);
        }
      } else {
        for (let c = region.left; c <= region.right; c++) {
          changed.add(c);
        }
      }
    }
    if (changed.size === 0) return null;
    const expanded = new Set();
    const ctx = Math.max(0, context || 0);
    for (const idx of changed) {
      for (let offset = -ctx; offset <= ctx; offset++) {
        const next = idx + offset;
        if (next >= 0 && next <= maxIndex) {
          expanded.add(next);
        }
      }
    }
    if (expanded.size === 0) return null;
    return Array.from(expanded).sort((a, b) => a - b);
  }

  function updateContentSize() {
    contentWidth = metrics.rowHeaderWidth + state.colCount * metrics.colWidth;
    contentHeight = metrics.colHeaderHeight + state.rowCount * metrics.rowHeight;
    spacer.style.width = `${contentWidth}px`;
    spacer.style.height = `${contentHeight}px`;
  }

  function updateDisplayMaps() {
    const rows = sheetVm.axis.rows.entries.length;
    const cols = sheetVm.axis.cols.entries.length;
    if (state.focusRows) {
      const map = computeFocusMap("row", sheetVm.renderPlan?.contextRows || 0);
      state.rowMap = map && map.length ? map : null;
    } else {
      state.rowMap = null;
    }
    if (state.focusCols) {
      const map = computeFocusMap("col", sheetVm.renderPlan?.contextCols || 0);
      state.colMap = map && map.length ? map : null;
    } else {
      state.colMap = null;
    }
    state.rowLookup = buildIndexLookup(state.rowMap, rows);
    state.colLookup = buildIndexLookup(state.colMap, cols);
    state.rowCount = state.rowMap ? state.rowMap.length : rows;
    state.colCount = state.colMap ? state.colMap.length : cols;
    updateContentSize();
  }

  function visibleToViewRow(visibleRow) {
    if (state.rowMap) return state.rowMap[visibleRow];
    return visibleRow;
  }

  function visibleToViewCol(visibleCol) {
    if (state.colMap) return state.colMap[visibleCol];
    return visibleCol;
  }

  function viewToVisibleRow(viewRow) {
    if (state.rowLookup) return state.rowLookup[viewRow];
    return viewRow;
  }

  function viewToVisibleCol(viewCol) {
    if (state.colLookup) return state.colLookup[viewCol];
    return viewCol;
  }

  function ensureSelectionVisible() {
    if (!state.selected) return;
    const row = viewToVisibleRow(state.selected.viewRow);
    const col = viewToVisibleCol(state.selected.viewCol);
    if (row === null || row === undefined || col === null || col === undefined) {
      const fallbackRow = state.rowMap ? state.rowMap[0] : 0;
      const fallbackCol = state.colMap ? state.colMap[0] : 0;
      state.selected = { viewRow: fallbackRow, viewCol: fallbackCol };
    }
  }

  updateDisplayMaps();

  function updateModeButtons() {
    const active = state.mode;
    for (const btn of [sideBtn, unifiedBtn]) {
      btn.classList.toggle("active", btn.dataset.mode === active);
    }
  }

  function getLayout() {
    const width = scroll.clientWidth;
    const height = scroll.clientHeight;
    const paneCount = state.mode === "side_by_side" ? 2 : 1;
    const paneWidth =
      paneCount === 2
        ? Math.max(0, Math.floor((width - metrics.rowHeaderWidth - metrics.paneGap) / 2))
        : Math.max(0, width - metrics.rowHeaderWidth);
    return {
      width,
      height,
      paneCount,
      paneWidth,
      rowHeaderWidth: metrics.rowHeaderWidth,
      colHeaderHeight: metrics.colHeaderHeight,
      paneGap: metrics.paneGap
    };
  }

  function resizeCanvas() {
    const { width, height } = getLayout();
    const dpr = window.devicePixelRatio || 1;
    const targetWidth = Math.max(1, Math.floor(width));
    const targetHeight = Math.max(1, Math.floor(height));
    if (canvas.width !== targetWidth * dpr || canvas.height !== targetHeight * dpr) {
      canvas.width = targetWidth * dpr;
      canvas.height = targetHeight * dpr;
      canvas.style.width = `${targetWidth}px`;
      canvas.style.height = `${targetHeight}px`;
      ctx.setTransform(dpr, 0, 0, dpr, 0, 0);
    }
    return { width: targetWidth, height: targetHeight };
  }

  function schedulePaint() {
    if (rafId !== null) return;
    rafId = requestAnimationFrame(() => {
      rafId = null;
      paint();
    });
  }

  function paint() {
    const size = resizeCanvas();
    const now = performance.now();
    let flashModel = null;
    if (state.flash) {
      const elapsed = now - state.flash.start;
      const duration = 900;
      if (elapsed >= duration) {
        state.flash = null;
      } else {
        flashModel = {
          ...state.flash,
          alpha: Math.max(0, 1 - elapsed / duration)
        };
      }
    }
    paintGrid(ctx, {
      sheetVm,
      mode: state.mode,
      contentMode: state.contentMode,
      rowMap: state.rowMap,
      colMap: state.colMap,
      rowLookup: state.rowLookup,
      colLookup: state.colLookup,
      flash: flashModel,
      viewport: {
        width: size.width,
        height: size.height,
        scrollTop: scroll.scrollTop,
        scrollLeft: scroll.scrollLeft
      },
      metrics,
      theme,
      selection: state.selected,
      hover: state.hover,
      hoverMoveId: state.hoverMoveId
    });
    if (state.flash) {
      schedulePaint();
    }
  }

  function resolvePane(x, layout) {
    if (layout.paneWidth <= 0) return null;
    const leftStart = layout.rowHeaderWidth;
    if (layout.paneCount === 1) {
      if (x < leftStart) return null;
      return { index: 0, localX: x - leftStart };
    }
    const rightStart = layout.rowHeaderWidth + layout.paneWidth + layout.paneGap;
    if (x >= leftStart && x < leftStart + layout.paneWidth) {
      return { index: 0, localX: x - leftStart };
    }
    if (x >= rightStart && x < rightStart + layout.paneWidth) {
      return { index: 1, localX: x - rightStart };
    }
    return null;
  }

  function hitTest(clientX, clientY) {
    const rect = canvas.getBoundingClientRect();
    const x = clientX - rect.left;
    const y = clientY - rect.top;
    const layout = getLayout();
    if (x < 0 || y < 0 || x > rect.width || y > rect.height) return null;
    if (x < layout.rowHeaderWidth && y < layout.colHeaderHeight) {
      return { type: "corner" };
    }
    if (y < layout.colHeaderHeight) {
      const pane = resolvePane(x, layout);
      if (!pane) return null;
      const visibleCol = Math.floor((scroll.scrollLeft + pane.localX) / metrics.colWidth);
      if (visibleCol < 0 || visibleCol >= state.colCount) return null;
      const viewCol = visibleToViewCol(visibleCol);
      if (!Number.isFinite(viewCol)) return null;
      return { type: "col-header", viewCol, pane: pane.index };
    }
    if (x < layout.rowHeaderWidth) {
      const visibleRow = Math.floor((scroll.scrollTop + (y - layout.colHeaderHeight)) / metrics.rowHeight);
      if (visibleRow < 0 || visibleRow >= state.rowCount) return null;
      const viewRow = visibleToViewRow(visibleRow);
      if (!Number.isFinite(viewRow)) return null;
      return { type: "row-header", viewRow };
    }
    const pane = resolvePane(x, layout);
    if (!pane) return null;
    const visibleCol = Math.floor((scroll.scrollLeft + pane.localX) / metrics.colWidth);
    const visibleRow = Math.floor((scroll.scrollTop + (y - layout.colHeaderHeight)) / metrics.rowHeight);
    if (visibleRow < 0 || visibleRow >= state.rowCount) return null;
    if (visibleCol < 0 || visibleCol >= state.colCount) return null;
    const viewRow = visibleToViewRow(visibleRow);
    const viewCol = visibleToViewCol(visibleCol);
    if (!Number.isFinite(viewRow) || !Number.isFinite(viewCol)) return null;
    return { type: "cell", viewRow, viewCol, pane: pane.index };
  }

  function updateTooltip(cell, viewRow, viewCol, clientX, clientY) {
    if (!cell) {
      tooltip.classList.remove("visible");
      return;
    }
    tooltip.innerHTML = "";
    const summary = buildCellSummary(cell, viewRow, viewCol);
    const title = createEl("div", "grid-tooltip-title", `View ${summary.viewAddress}`);
    const meta = createEl("div", "grid-tooltip-meta", `Diff: ${summary.diffKind}`);
    tooltip.append(title, meta);

    function appendTooltipSection(label, details) {
      if (!details.value && !details.formula) return;
      const section = createEl("div", "grid-tooltip-section");
      section.append(createEl("div", "grid-tooltip-label", label));
      if (state.contentMode === "formulas") {
        if (details.formula) {
          section.append(createEl("div", "grid-tooltip-formula", details.formula));
        } else if (details.value) {
          section.append(createEl("div", "grid-tooltip-value", details.value));
        }
      } else {
        if (details.value) section.append(createEl("div", "grid-tooltip-value", details.value));
        if (details.formula) section.append(createEl("div", "grid-tooltip-formula", details.formula));
      }
      tooltip.append(section);
    }

    if (summary.oldAddress || summary.old.value || summary.old.formula) {
      appendTooltipSection(summary.oldAddress ? `Old ${summary.oldAddress}` : "Old", summary.old);
    }

    if (summary.newAddress || summary.fresh.value || summary.fresh.formula) {
      appendTooltipSection(summary.newAddress ? `New ${summary.newAddress}` : "New", summary.fresh);
    }

    tooltip.classList.add("visible");

    const wrapRect = canvasWrap.getBoundingClientRect();
    const tooltipRect = tooltip.getBoundingClientRect();
    let left = clientX - wrapRect.left + 12;
    let top = clientY - wrapRect.top + 12;
    if (left + tooltipRect.width > wrapRect.width) {
      left = Math.max(8, wrapRect.width - tooltipRect.width - 8);
    }
    if (top + tooltipRect.height > wrapRect.height) {
      top = Math.max(8, wrapRect.height - tooltipRect.height - 8);
    }
    tooltip.style.transform = `translate(${left}px, ${top}px)`;
  }

  function updateInspector() {
    inspectorContent.innerHTML = "";
    if (!state.selected) {
      inspectorEmpty.style.display = "block";
      return;
    }
    inspectorEmpty.style.display = "none";

    const cell = sheetVm.cellAt(state.selected.viewRow, state.selected.viewCol);
    const summary = buildCellSummary(cell, state.selected.viewRow, state.selected.viewCol);

    const addRow = (label, value) => {
      if (!value) return;
      const row = createEl("div", "grid-inspector-row");
      row.append(createEl("div", "grid-inspector-label", label));
      row.append(createEl("div", "grid-inspector-value", value));
      inspectorContent.append(row);
    };

    addRow("View", summary.viewAddress);
    addRow("Diff", summary.diffKind);
    if (summary.moveId) {
      addRow("Move", summary.moveRole ? `${summary.moveRole} ${summary.moveId}` : summary.moveId);
    }
    addRow("Old", summary.oldAddress);
    if (state.contentMode === "formulas") {
      if (summary.old.formula) addRow("Old Formula", summary.old.formula);
      else if (summary.old.value) addRow("Old Value", summary.old.value);
    } else {
      if (summary.old.value) addRow("Old Value", summary.old.value);
      if (summary.old.formula) addRow("Old Formula", summary.old.formula);
    }
    addRow("New", summary.newAddress);
    if (state.contentMode === "formulas") {
      if (summary.fresh.formula) addRow("New Formula", summary.fresh.formula);
      else if (summary.fresh.value) addRow("New Value", summary.fresh.value);
    } else {
      if (summary.fresh.value) addRow("New Value", summary.fresh.value);
      if (summary.fresh.formula) addRow("New Formula", summary.fresh.formula);
    }

    const jumpTarget = resolveMoveTarget(cell);
    if (jumpTarget) {
      const jumpBtn = createEl("button", "grid-inspector-jump", "Jump to other end");
      jumpBtn.type = "button";
      jumpBtn.addEventListener("click", () => {
        selectCell(jumpTarget.viewRow, jumpTarget.viewCol, { center: true });
      });
      inspectorContent.append(jumpBtn);
    }
  }

  function resolveMoveTarget(cell) {
    if (!cell || !cell.moveId) return null;
    const rowEntry = sheetVm.axis.rows.entries[cell.viewRow];
    if (rowEntry && rowEntry.move_id === cell.moveId) {
      const targetKind = rowEntry.kind === "move_src" ? "move_dst" : "move_src";
      const targetRow = sheetVm.axis.rows.entries.findIndex(entry => entry?.move_id === cell.moveId && entry?.kind === targetKind);
      if (targetRow >= 0) {
        return { viewRow: targetRow, viewCol: cell.viewCol };
      }
    }
    const colEntry = sheetVm.axis.cols.entries[cell.viewCol];
    if (colEntry && colEntry.move_id === cell.moveId) {
      const targetKind = colEntry.kind === "move_src" ? "move_dst" : "move_src";
      const targetCol = sheetVm.axis.cols.entries.findIndex(entry => entry?.move_id === cell.moveId && entry?.kind === targetKind);
      if (targetCol >= 0) {
        return { viewRow: cell.viewRow, viewCol: targetCol };
      }
    }
    return null;
  }

  function setScroll(left, top) {
    const layout = getLayout();
    const maxLeft = Math.max(0, contentWidth - layout.width);
    const maxTop = Math.max(0, contentHeight - layout.height);
    scroll.scrollLeft = clamp(left, 0, maxLeft);
    scroll.scrollTop = clamp(top, 0, maxTop);
  }

  function centerOn(viewRow, viewCol) {
    const visibleRow = viewToVisibleRow(viewRow);
    const visibleCol = viewToVisibleCol(viewCol);
    if (visibleRow === null || visibleRow === undefined || visibleCol === null || visibleCol === undefined) return;
    const layout = getLayout();
    const cellAreaHeight = Math.max(0, layout.height - layout.colHeaderHeight);
    const cellX = visibleCol * metrics.colWidth;
    const cellY = visibleRow * metrics.rowHeight;
    const targetLeft = cellX - (layout.paneWidth - metrics.colWidth) / 2;
    const targetTop = cellY - (cellAreaHeight - metrics.rowHeight) / 2;
    setScroll(targetLeft, targetTop);
  }

  function scrollIntoView(viewRow, viewCol) {
    const visibleRow = viewToVisibleRow(viewRow);
    const visibleCol = viewToVisibleCol(viewCol);
    if (visibleRow === null || visibleRow === undefined || visibleCol === null || visibleCol === undefined) return;
    const layout = getLayout();
    const cellAreaHeight = Math.max(0, layout.height - layout.colHeaderHeight);
    const cellX = visibleCol * metrics.colWidth;
    const cellY = visibleRow * metrics.rowHeight;
    const minLeft = scroll.scrollLeft;
    const maxLeft = scroll.scrollLeft + layout.paneWidth - metrics.colWidth;
    const minTop = scroll.scrollTop;
    const maxTop = scroll.scrollTop + cellAreaHeight - metrics.rowHeight;
    let nextLeft = scroll.scrollLeft;
    let nextTop = scroll.scrollTop;
    if (cellX < minLeft) nextLeft = cellX;
    else if (cellX > maxLeft) nextLeft = cellX - (layout.paneWidth - metrics.colWidth);
    if (cellY < minTop) nextTop = cellY;
    else if (cellY > maxTop) nextTop = cellY - (cellAreaHeight - metrics.rowHeight);
    setScroll(nextLeft, nextTop);
  }

  function selectCell(viewRow, viewCol, { center = false } = {}) {
    state.selected = { viewRow, viewCol };
    ensureSelectionVisible();
    updateInspector();
    const target = state.selected;
    if (center) {
      centerOn(target.viewRow, target.viewCol);
    } else {
      scrollIntoView(target.viewRow, target.viewCol);
    }
    schedulePaint();
  }

  function dispatchViewerEvent(name, detail) {
    mountEl.dispatchEvent(new CustomEvent(name, { detail }));
  }

  function resolveAnchorIndex(anchorIdOrIndex) {
    if (typeof anchorIdOrIndex === "string") {
      const idx = anchorIndexById.get(anchorIdOrIndex);
      return Number.isFinite(idx) ? idx : -1;
    }
    if (Number.isFinite(anchorIdOrIndex)) {
      return anchorIdOrIndex;
    }
    return -1;
  }

  function announceAnchor(anchor) {
    if (!anchor) return;
    dispatchViewerEvent("gridviewer:anchor", { anchorId: anchor.id });
  }

  function jumpToAnchor(anchorIdOrIndex) {
    if (!gridAnchors.length) return false;
    const nextIndex = resolveAnchorIndex(anchorIdOrIndex);
    if (nextIndex < 0 || nextIndex >= gridAnchors.length) return false;
    const anchor = gridAnchors[nextIndex];
    state.anchorIndex = nextIndex;
    selectCell(anchor.viewRow, anchor.viewCol, { center: true });
    announceAnchor(anchor);
    return true;
  }

  function flashAnchor(anchorIdOrIndex) {
    const idx = resolveAnchorIndex(anchorIdOrIndex);
    if (idx < 0 || idx >= gridAnchors.length) return false;
    const anchor = gridAnchors[idx];
    let flash = null;
    if (anchor.regionId) {
      const region = regionLookup.get(anchor.regionId);
      if (region) {
        flash = {
          kind: "region",
          bounds: { top: region.top, bottom: region.bottom, left: region.left, right: region.right },
          start: performance.now()
        };
      }
    }
    if (!flash && anchor.group === "rows") {
      flash = { kind: "row", viewRow: anchor.viewRow, start: performance.now() };
    }
    if (!flash && anchor.group === "cols") {
      flash = { kind: "col", viewCol: anchor.viewCol, start: performance.now() };
    }
    if (!flash && anchor.moveId) {
      if (anchor.moveId.startsWith("r:")) {
        flash = { kind: "row", viewRow: anchor.viewRow, start: performance.now() };
      } else if (anchor.moveId.startsWith("c:")) {
        flash = { kind: "col", viewCol: anchor.viewCol, start: performance.now() };
      }
    }
    if (!flash) {
      flash = { kind: "cell", viewRow: anchor.viewRow, viewCol: anchor.viewCol, start: performance.now() };
    }
    state.flash = flash;
    schedulePaint();
    return true;
  }

  function nextAnchor() {
    if (!gridAnchors.length) return false;
    const nextIndex = state.anchorIndex < 0 ? 0 : state.anchorIndex + 1;
    if (nextIndex >= gridAnchors.length) return false;
    return jumpToAnchor(nextIndex);
  }

  function prevAnchor() {
    if (!gridAnchors.length) return false;
    const nextIndex = state.anchorIndex < 0 ? gridAnchors.length - 1 : state.anchorIndex - 1;
    if (nextIndex < 0) return false;
    return jumpToAnchor(nextIndex);
  }

  function onScroll() {
    state.hover = null;
    state.hoverMoveId = null;
    tooltip.classList.remove("visible");
    schedulePaint();
  }

  function onPointerMove(e) {
    const hit = hitTest(e.clientX, e.clientY);
    if (!hit) {
      state.hover = null;
      state.hoverMoveId = null;
      tooltip.classList.remove("visible");
      schedulePaint();
      return;
    }
    if (hit.type === "cell") {
      state.hover = { viewRow: hit.viewRow, viewCol: hit.viewCol, pane: hit.pane };
      const cell = sheetVm.cellAt(hit.viewRow, hit.viewCol);
      state.hoverMoveId = cell?.moveId || null;
      updateTooltip(cell, hit.viewRow, hit.viewCol, e.clientX, e.clientY);
      schedulePaint();
      return;
    }
    if (hit.type === "row-header") {
      const entry = sheetVm.axis.rows.entries[hit.viewRow];
      state.hover = { viewRow: hit.viewRow, viewCol: null, pane: null };
      state.hoverMoveId = entry?.move_id || null;
    } else if (hit.type === "col-header") {
      const entry = sheetVm.axis.cols.entries[hit.viewCol];
      state.hover = { viewRow: null, viewCol: hit.viewCol, pane: null };
      state.hoverMoveId = entry?.move_id || null;
    } else {
      state.hover = null;
      state.hoverMoveId = null;
    }
    tooltip.classList.remove("visible");
    schedulePaint();
  }

  function onPointerLeave() {
    state.hover = null;
    state.hoverMoveId = null;
    tooltip.classList.remove("visible");
    schedulePaint();
  }

  function onClick(e) {
    const hit = hitTest(e.clientX, e.clientY);
    if (!hit) return;
    if (hit.type === "cell") {
      selectCell(hit.viewRow, hit.viewCol);
    } else if (hit.type === "row-header") {
      const targetCol = state.selected ? state.selected.viewCol : 0;
      selectCell(hit.viewRow, clamp(targetCol, 0, sheetVm.axis.cols.entries.length - 1));
    } else if (hit.type === "col-header") {
      const targetRow = state.selected ? state.selected.viewRow : 0;
      selectCell(clamp(targetRow, 0, sheetVm.axis.rows.entries.length - 1), hit.viewCol);
    }
    root.focus();
  }

  function onKeyDown(e) {
    if (!["ArrowUp", "ArrowDown", "ArrowLeft", "ArrowRight", "Enter", "n", "p", "N", "P"].includes(e.key)) {
      return;
    }
    if (e.key === "Enter") {
      state.pinned = !state.pinned;
      inspector.classList.toggle("pinned", state.pinned);
      e.preventDefault();
      return;
    }
    if (e.key === "n" || e.key === "N") {
      if (nextAnchor()) {
        flashAnchor(state.anchorIndex);
      }
      e.preventDefault();
      return;
    }
    if (e.key === "p" || e.key === "P") {
      if (prevAnchor()) {
        flashAnchor(state.anchorIndex);
      }
      e.preventDefault();
      return;
    }

    const maxVisibleRow = Math.max(0, state.rowCount - 1);
    const maxVisibleCol = Math.max(0, state.colCount - 1);
    let viewRow = state.selected ? state.selected.viewRow : 0;
    let viewCol = state.selected ? state.selected.viewCol : 0;
    let visibleRow = viewToVisibleRow(viewRow);
    let visibleCol = viewToVisibleCol(viewCol);
    if (visibleRow === null || visibleRow === undefined) visibleRow = 0;
    if (visibleCol === null || visibleCol === undefined) visibleCol = 0;
    if (e.key === "ArrowUp") visibleRow = clamp(visibleRow - 1, 0, maxVisibleRow);
    if (e.key === "ArrowDown") visibleRow = clamp(visibleRow + 1, 0, maxVisibleRow);
    if (e.key === "ArrowLeft") visibleCol = clamp(visibleCol - 1, 0, maxVisibleCol);
    if (e.key === "ArrowRight") visibleCol = clamp(visibleCol + 1, 0, maxVisibleCol);
    viewRow = visibleToViewRow(visibleRow);
    viewCol = visibleToViewCol(visibleCol);
    if (Number.isFinite(viewRow) && Number.isFinite(viewCol)) {
      selectCell(viewRow, viewCol);
    }
    e.preventDefault();
  }

  function setDisplayOptions(next = {}) {
    let updated = false;
    const allowedModes = new Set(["values", "formulas", "both"]);
    if (typeof next.contentMode === "string" && allowedModes.has(next.contentMode) && next.contentMode !== state.contentMode) {
      state.contentMode = next.contentMode;
      updated = true;
      updateInspector();
    }
    if (typeof next.focusRows === "boolean" && next.focusRows !== state.focusRows) {
      state.focusRows = next.focusRows;
      updated = true;
    }
    if (typeof next.focusCols === "boolean" && next.focusCols !== state.focusCols) {
      state.focusCols = next.focusCols;
      updated = true;
    }
    if (updated) {
      updateDisplayMaps();
      ensureSelectionVisible();
      setScroll(scroll.scrollLeft, scroll.scrollTop);
      if (state.selected) {
        scrollIntoView(state.selected.viewRow, state.selected.viewCol);
      }
      schedulePaint();
    }
  }

  function onFocus() {
    dispatchViewerEvent("gridviewer:focus", {});
  }

  function onModeClick(e) {
    const btn = e.target.closest(".grid-mode-btn");
    if (!btn) return;
    const nextMode = btn.dataset.mode === "unified" ? "unified" : "side_by_side";
    if (state.mode !== nextMode) {
      state.mode = nextMode;
      updateModeButtons();
      schedulePaint();
    }
  }

  toolbar.addEventListener("click", onModeClick);
  scroll.addEventListener("scroll", onScroll);
  canvas.addEventListener("pointermove", onPointerMove);
  canvas.addEventListener("pointerleave", onPointerLeave);
  canvas.addEventListener("click", onClick);
  root.addEventListener("keydown", onKeyDown);
  root.addEventListener("focus", onFocus);

  const resizeObserver = new ResizeObserver(() => schedulePaint());
  resizeObserver.observe(scroll);

  updateModeButtons();
  schedulePaint();
  if (Number.isFinite(state.anchorIndex) && gridAnchors.length > 0 && state.anchorIndex >= 0) {
    jumpToAnchor(state.anchorIndex);
  }

  return {
    destroy() {
      if (rafId !== null) cancelAnimationFrame(rafId);
      resizeObserver.disconnect();
      toolbar.removeEventListener("click", onModeClick);
      scroll.removeEventListener("scroll", onScroll);
      canvas.removeEventListener("pointermove", onPointerMove);
      canvas.removeEventListener("pointerleave", onPointerLeave);
      canvas.removeEventListener("click", onClick);
      root.removeEventListener("keydown", onKeyDown);
      root.removeEventListener("focus", onFocus);
    },
    focus() {
      root.focus();
    },
    jumpTo(viewRow, viewCol) {
      selectCell(viewRow, viewCol, { center: true });
    },
    jumpToAnchor(anchorIdOrIndex) {
      return jumpToAnchor(anchorIdOrIndex);
    },
    nextAnchor() {
      return nextAnchor();
    },
    prevAnchor() {
      return prevAnchor();
    },
    setDisplayOptions(options) {
      setDisplayOptions(options);
    },
    flashAnchor(anchorIdOrIndex) {
      return flashAnchor(anchorIdOrIndex);
    },
    capturePng() {
      paint();
      try {
        return canvas.toDataURL("image/png");
      } catch (err) {
        return "";
      }
    }
  };
}

```

---

### File: `web\index.html`

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Excel Diff</title>
    <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'wasm-unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:; connect-src 'self'; worker-src 'self'; object-src 'none'; base-uri 'none'; frame-ancestors 'none';" />
    <style>
      :root {
        --font-sans: system-ui, -apple-system, "Segoe UI", "Helvetica Neue", Arial, sans-serif;
        --font-mono: ui-monospace, "SFMono-Regular", "Menlo", "Consolas", "Liberation Mono", monospace;
        --bg-primary: #0d1117;
        --bg-secondary: #161b22;
        --bg-tertiary: #21262d;
        --bg-hover: #30363d;
        --border-primary: #30363d;
        --border-secondary: #21262d;
        --text-primary: #e6edf3;
        --text-secondary: #8b949e;
        --text-muted: #6e7681;
        --accent-blue: #58a6ff;
        --accent-green: #3fb950;
        --accent-red: #f85149;
        --accent-yellow: #d29922;
        --accent-purple: #a371f7;
        --diff-add-bg: rgba(46, 160, 67, 0.15);
        --diff-add-border: rgba(46, 160, 67, 0.4);
        --diff-remove-bg: rgba(248, 81, 73, 0.15);
        --diff-remove-border: rgba(248, 81, 73, 0.4);
        --diff-modify-bg: rgba(210, 153, 34, 0.15);
        --diff-modify-border: rgba(210, 153, 34, 0.4);
        --diff-move-bg: rgba(163, 113, 247, 0.15);
        --diff-move-border: rgba(163, 113, 247, 0.4);
        --diff-move-dst-bg: rgba(163, 113, 247, 0.25);
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family: var(--font-sans);
        background: var(--bg-primary);
        color: var(--text-primary);
        min-height: 100vh;
        line-height: 1.5;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
        padding: 32px 24px;
      }

      header {
        display: flex;
        align-items: center;
        justify-content: space-between;
        margin-bottom: 32px;
        padding-bottom: 24px;
        border-bottom: 1px solid var(--border-primary);
      }

      .logo {
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .logo-icon {
        width: 40px;
        height: 40px;
        background: linear-gradient(135deg, var(--accent-green), var(--accent-blue));
        border-radius: 10px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: 700;
        font-size: 18px;
      }

      .logo h1 {
        font-size: 24px;
        font-weight: 700;
        background: linear-gradient(135deg, var(--text-primary), var(--text-secondary));
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .version {
        font-family: var(--font-mono);
        font-size: 12px;
        color: var(--text-muted);
        background: var(--bg-tertiary);
        padding: 4px 8px;
        border-radius: 6px;
      }

      .upload-section {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        padding: 32px;
        margin-bottom: 32px;
      }

      .upload-grid {
        display: grid;
        grid-template-columns: 1fr 1fr auto;
        gap: 20px;
        align-items: end;
      }

      .upload-box {
        min-width: 0;
      }

      @media (max-width: 800px) {
        .upload-grid {
          grid-template-columns: 1fr;
        }
        .diff-btn {
          width: 100%;
          justify-content: center;
        }
      }

      .action-bar {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 16px;
        flex-wrap: wrap;
        margin-top: 20px;
      }

      .action-left {
        display: flex;
        align-items: center;
        gap: 12px;
        flex-wrap: wrap;
      }

      .export-group {
        display: flex;
        align-items: center;
        gap: 8px;
        flex-wrap: wrap;
      }

      .secondary-btn {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        color: var(--text-secondary);
        padding: 10px 14px;
        border-radius: 10px;
        font-size: 12px;
        cursor: pointer;
        transition: border-color 0.2s ease, color 0.2s ease, background 0.2s ease;
        font-family: inherit;
      }

      .secondary-btn:hover {
        border-color: var(--accent-blue);
        color: var(--text-primary);
      }

      .secondary-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .privacy-note {
        font-size: 12px;
        color: var(--text-muted);
      }

      .recents-section {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        padding: 20px 24px;
        margin-bottom: 32px;
        display: none;
      }

      .recents-section.visible {
        display: block;
      }

      .batch-section,
      .search-section {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        padding: 24px;
        margin-bottom: 32px;
        display: none;
      }

      .batch-section.visible,
      .search-section.visible {
        display: block;
      }

      .section-header {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 16px;
        margin-bottom: 16px;
      }

      .section-header h2 {
        font-size: 18px;
        margin-bottom: 4px;
      }

      .section-header p {
        font-size: 13px;
        color: var(--text-secondary);
      }

      .batch-controls {
        display: grid;
        grid-template-columns: 1fr 1fr auto;
        gap: 16px;
        align-items: end;
        margin-bottom: 16px;
      }

      .batch-picker {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px;
        display: grid;
        gap: 8px;
      }

      .batch-label {
        font-size: 11px;
        text-transform: uppercase;
        letter-spacing: 0.4px;
        color: var(--text-muted);
      }

      .batch-path {
        font-family: var(--font-mono);
        font-size: 12px;
        color: var(--text-secondary);
        word-break: break-all;
        min-height: 16px;
      }

      .batch-run {
        align-self: stretch;
      }

      .batch-results {
        display: grid;
        gap: 12px;
      }

      .batch-summary {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        font-size: 12px;
        color: var(--text-secondary);
      }

      .batch-table-wrap {
        overflow-x: auto;
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        background: var(--bg-primary);
      }

      .batch-table {
        width: 100%;
        border-collapse: collapse;
        font-size: 12px;
      }

      .batch-table th,
      .batch-table td {
        padding: 10px 12px;
        border-bottom: 1px solid var(--border-secondary);
        text-align: left;
      }

      .batch-table th {
        background: var(--bg-tertiary);
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.4px;
        font-size: 11px;
      }

      .batch-table tr:hover td {
        background: var(--bg-hover);
      }

      .batch-table td:last-child {
        white-space: nowrap;
      }

      .search-controls {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        align-items: center;
        margin-bottom: 16px;
      }

      .search-input,
      .search-select {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        border-radius: 8px;
        padding: 8px 12px;
        color: var(--text-primary);
        font-size: 13px;
        font-family: inherit;
      }

      .search-input {
        flex: 1 1 240px;
      }

      .search-input:focus,
      .search-select:focus {
        outline: none;
        border-color: var(--accent-blue);
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.2);
      }

      .search-results {
        display: grid;
        gap: 12px;
      }

      .search-summary {
        font-size: 12px;
        color: var(--text-secondary);
      }

      .search-item {
        background: var(--bg-tertiary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px 14px;
        display: grid;
        gap: 4px;
      }

      .search-title {
        font-weight: 600;
      }

      .search-meta {
        font-size: 11px;
        color: var(--text-muted);
        font-family: var(--font-mono);
      }

      .search-detail {
        font-size: 12px;
        color: var(--text-secondary);
      }

      .empty-state {
        background: var(--bg-primary);
        border: 1px dashed var(--border-primary);
        border-radius: 10px;
        padding: 12px 14px;
        color: var(--text-muted);
        font-size: 12px;
      }

      @media (max-width: 900px) {
        .batch-controls {
          grid-template-columns: 1fr;
        }

        .batch-run {
          width: 100%;
        }
      }

      .recents-header {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
        margin-bottom: 12px;
      }

      .recents-title {
        font-size: 12px;
        font-weight: 600;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
      }

      .recents-empty {
        font-size: 13px;
        color: var(--text-muted);
      }

      .recents-list {
        display: grid;
        gap: 12px;
      }

      .recent-item {
        background: var(--bg-tertiary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px 16px;
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 16px;
        flex-wrap: wrap;
      }

      .recent-main {
        display: flex;
        flex-direction: column;
        gap: 6px;
        min-width: 200px;
      }

      .recent-names {
        display: flex;
        align-items: center;
        gap: 8px;
        font-size: 13px;
      }

      .recent-name {
        font-family: var(--font-mono);
        color: var(--text-primary);
      }

      .recent-arrow {
        color: var(--text-muted);
      }

      .recent-meta {
        font-size: 12px;
        color: var(--text-muted);
      }

      .recent-actions {
        display: flex;
        gap: 8px;
        flex-wrap: wrap;
      }

      @media (max-width: 800px) {
        .action-bar {
          flex-direction: column;
          align-items: flex-start;
        }
      }

      .upload-box label {
        display: block;
        font-size: 13px;
        font-weight: 600;
        color: var(--text-secondary);
        margin-bottom: 8px;
        text-transform: uppercase;
        letter-spacing: 0.5px;
      }

      .file-drop {
        position: relative;
        border: 2px dashed var(--border-primary);
        border-radius: 12px;
        padding: 20px 16px;
        text-align: center;
        transition: all 0.2s ease;
        cursor: pointer;
        background: var(--bg-primary);
        min-height: 100px;
      }

      .file-drop:hover {
        border-color: var(--accent-blue);
        background: rgba(88, 166, 255, 0.05);
      }

      .file-drop.dragover {
        border-color: var(--accent-green);
        background: rgba(63, 185, 80, 0.1);
      }

      .file-drop.has-file {
        border-style: solid;
        border-color: var(--accent-green);
      }

      .file-drop input[type="file"] {
        position: absolute;
        inset: 0;
        opacity: 0;
        cursor: pointer;
      }

      .file-drop-icon {
        font-size: 24px;
        margin-bottom: 6px;
      }

      .file-drop-text {
        color: var(--text-secondary);
        font-size: 14px;
      }

      .file-drop-text strong {
        color: var(--accent-blue);
      }

      .file-name {
        font-family: var(--font-mono);
        font-size: 13px;
        color: var(--accent-green);
        margin-top: 8px;
        word-break: break-all;
      }

      .diff-btn {
        background: linear-gradient(135deg, var(--accent-blue), #1f6feb);
        color: white;
        border: none;
        padding: 16px 32px;
        border-radius: 12px;
        font-size: 15px;
        font-weight: 600;
        cursor: pointer;
        transition: all 0.2s ease;
        display: flex;
        align-items: center;
        gap: 8px;
        font-family: inherit;
      }

      .diff-btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(88, 166, 255, 0.3);
      }

      .diff-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
        box-shadow: none;
      }

      #status {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 8px;
        text-align: center;
        padding: 16px;
        color: var(--text-secondary);
        font-size: 14px;
      }

      #status.loading {
        color: var(--accent-blue);
      }

      #status.loading::before {
        content: "";
        width: 14px;
        height: 14px;
        border-radius: 50%;
        border: 2px solid rgba(88, 166, 255, 0.3);
        border-top-color: var(--accent-blue);
        animation: statusSpin 1s linear infinite;
      }

      #status.error {
        color: var(--accent-red);
      }

      @keyframes statusSpin {
        to { transform: rotate(360deg); }
      }

      .results {
        display: none;
      }

      .results.visible {
        display: block;
      }

      .large-mode-nav {
        display: none;
        align-items: center;
        gap: 12px;
        flex-wrap: wrap;
        padding: 12px 16px;
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        margin-bottom: 16px;
      }

      .large-mode-nav.visible {
        display: flex;
      }

      .large-mode-title {
        font-weight: 600;
        color: var(--text-primary);
      }

      .large-summary {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        padding: 24px;
      }

      .large-summary-header {
        display: flex;
        align-items: flex-start;
        justify-content: space-between;
        gap: 16px;
        margin-bottom: 20px;
        flex-wrap: wrap;
      }

      .large-summary-header h2 {
        font-size: 20px;
        margin-bottom: 6px;
      }

      .large-summary-header p {
        font-size: 13px;
        color: var(--text-secondary);
      }

      .large-summary-meta {
        display: flex;
        gap: 12px;
        font-size: 12px;
        color: var(--text-muted);
      }

      .large-summary-side {
        display: flex;
        flex-direction: column;
        align-items: flex-end;
        gap: 8px;
      }

      .large-sheet-list {
        display: grid;
        gap: 12px;
        margin-top: 16px;
      }

      .large-sheet-item {
        background: var(--bg-tertiary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px 16px;
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
        flex-wrap: wrap;
      }

      .large-sheet-main {
        display: flex;
        flex-direction: column;
        gap: 4px;
        min-width: 200px;
      }

      .large-sheet-name {
        font-weight: 600;
      }

      .large-sheet-meta {
        font-size: 12px;
        color: var(--text-secondary);
      }

      .large-sheet-counts {
        display: flex;
        gap: 6px;
        flex-wrap: wrap;
      }

      .pill {
        font-size: 11px;
        padding: 4px 8px;
        border-radius: 999px;
        font-family: var(--font-mono);
      }

      .pill.added {
        background: var(--diff-add-bg);
        color: var(--accent-green);
        border: 1px solid var(--diff-add-border);
      }

      .pill.removed {
        background: var(--diff-remove-bg);
        color: var(--accent-red);
        border: 1px solid var(--diff-remove-border);
      }

      .pill.modified {
        background: var(--diff-modify-bg);
        color: var(--accent-yellow);
        border: 1px solid var(--diff-modify-border);
      }

      .pill.moved {
        background: var(--diff-move-bg);
        color: var(--accent-purple);
        border: 1px solid var(--diff-move-border);
      }

      .summary-cards {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
        gap: 16px;
        margin-bottom: 32px;
      }

      .summary-card {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 20px;
        text-align: center;
      }

      .summary-card.added {
        border-color: var(--diff-add-border);
        background: var(--diff-add-bg);
      }

      .summary-card.removed {
        border-color: var(--diff-remove-border);
        background: var(--diff-remove-bg);
      }

      .summary-card.modified {
        border-color: var(--diff-modify-border);
        background: var(--diff-modify-bg);
      }

      .summary-card.moved {
        border-color: var(--diff-move-border);
        background: var(--diff-move-bg);
      }

      .summary-card .count {
        font-size: 36px;
        font-weight: 700;
        font-family: var(--font-mono);
      }

      .summary-card.added .count { color: var(--accent-green); }
      .summary-card.removed .count { color: var(--accent-red); }
      .summary-card.modified .count { color: var(--accent-yellow); }
      .summary-card.moved .count { color: var(--accent-purple); }

      .summary-card .label {
        font-size: 13px;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-top: 4px;
      }

      .no-changes {
        text-align: center;
        padding: 64px 32px;
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
      }

      .no-changes-icon {
        font-size: 64px;
        margin-bottom: 16px;
      }

      .no-changes h2 {
        font-size: 24px;
        margin-bottom: 8px;
        color: var(--accent-green);
      }

      .no-changes p {
        color: var(--text-secondary);
      }

      .review-toolbar {
        position: sticky;
        top: 16px;
        z-index: 5;
        display: flex;
        flex-wrap: wrap;
        gap: 16px;
        align-items: center;
        justify-content: space-between;
        padding: 16px 20px;
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 14px;
        margin-bottom: 20px;
      }

      .review-toolbar-left,
      .review-toolbar-right {
        display: flex;
        align-items: center;
        gap: 16px;
        flex-wrap: wrap;
      }

      .toolbar-field {
        display: flex;
        flex-direction: column;
        gap: 6px;
        min-width: 220px;
      }

      .toolbar-label {
        font-size: 12px;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
      }

      .sheet-search {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        border-radius: 8px;
        padding: 8px 12px;
        color: var(--text-primary);
        font-size: 13px;
      }

      .sheet-search:focus {
        outline: none;
        border-color: var(--accent-blue);
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.2);
      }

      .toolbar-actions {
        display: flex;
        gap: 8px;
      }

      .review-nav-btn {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        color: var(--text-primary);
        padding: 8px 12px;
        border-radius: 8px;
        font-size: 12px;
        cursor: pointer;
        transition: border-color 0.2s ease, color 0.2s ease;
      }

      .review-nav-btn:hover {
        border-color: var(--accent-blue);
      }

      .toolbar-toggle {
        display: flex;
        align-items: center;
        gap: 8px;
        font-size: 12px;
        color: var(--text-secondary);
      }

      .toolbar-select {
        display: flex;
        align-items: center;
        gap: 8px;
        font-size: 12px;
        color: var(--text-secondary);
      }

      .toolbar-select select {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        border-radius: 6px;
        padding: 6px 8px;
        color: var(--text-primary);
        font-size: 12px;
      }

      .sheet-index {
        margin-bottom: 24px;
      }

      .sheet-index-title {
        font-size: 12px;
        font-weight: 600;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 10px;
      }

      .sheet-index-list {
        display: grid;
        gap: 10px;
      }

      .sheet-index-item {
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px 16px;
        background: var(--bg-secondary);
        color: inherit;
        cursor: pointer;
        text-align: left;
        display: flex;
        flex-direction: column;
        gap: 8px;
        transition: border-color 0.2s ease, transform 0.2s ease;
      }

      .sheet-index-item:hover {
        border-color: var(--accent-blue);
        transform: translateY(-1px);
      }

      .sheet-index-item.active {
        border-color: var(--accent-blue);
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.2);
      }

      .sheet-index-main {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 8px;
      }

      .sheet-index-name {
        font-weight: 600;
      }

      .sheet-index-badges {
        display: flex;
        gap: 6px;
      }

      .sheet-index-badge {
        font-family: var(--font-mono);
        font-size: 11px;
        padding: 2px 8px;
        border-radius: 10px;
        background: var(--bg-primary);
        color: var(--text-secondary);
      }

      .sheet-index-meta {
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .density-bar {
        flex: 1;
        height: 6px;
        background: var(--bg-primary);
        border-radius: 999px;
        overflow: hidden;
      }

      .density-bar span {
        display: block;
        height: 100%;
        background: var(--accent-blue);
        opacity: 0.7;
      }

      .status-pill {
        border: 1px solid var(--border-primary);
        border-radius: 999px;
        padding: 4px 8px;
        font-size: 11px;
        font-weight: 600;
        letter-spacing: 0.4px;
        text-transform: uppercase;
        background: var(--bg-primary);
        color: var(--text-secondary);
        cursor: pointer;
      }

      .status-pill.ok {
        border-color: var(--accent-green);
        color: var(--accent-green);
      }

      .status-pill.skipped {
        border-color: var(--accent-yellow);
        color: var(--accent-yellow);
      }

      .status-pill.partial {
        border-color: var(--accent-blue);
        color: var(--accent-blue);
      }

      .status-pill.missing {
        border-color: var(--accent-red);
        color: var(--accent-red);
      }

      .status-pill:focus {
        outline: none;
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.2);
      }

      .sheet-index .status-pill {
        cursor: default;
      }

      .anchor-badge {
        background: rgba(88, 166, 255, 0.12);
        border: 1px solid rgba(88, 166, 255, 0.4);
        color: var(--accent-blue);
      }

      .preview-limitations {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 16px 20px;
        margin-bottom: 24px;
        color: var(--text-secondary);
        font-size: 13px;
      }

      .preview-limitations-title {
        font-size: 12px;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 8px;
        color: var(--text-secondary);
      }

      .preview-limitations p + p {
        margin-top: 6px;
      }

      .sheet-section {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        margin-bottom: 24px;
        overflow: hidden;
      }

      .sheet-header {
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 20px 24px;
        background: var(--bg-tertiary);
        cursor: pointer;
        transition: background 0.2s ease;
      }

      .sheet-header:hover {
        background: var(--bg-hover);
      }

      .sheet-title {
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .sheet-icon {
        width: 36px;
        height: 36px;
        background: var(--bg-primary);
        border-radius: 8px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 18px;
      }

      .sheet-name {
        font-size: 18px;
        font-weight: 600;
      }

      .sheet-badge {
        font-family: var(--font-mono);
        font-size: 12px;
        padding: 4px 10px;
        border-radius: 20px;
        background: var(--bg-primary);
        color: var(--text-secondary);
      }

      .sheet-content {
        display: none;
        padding: 24px;
      }

      .sheet-section.expanded .sheet-content {
        display: block;
      }

      .sheet-section.expanded .sheet-header .expand-icon {
        transform: rotate(180deg);
      }

      .expand-icon {
        transition: transform 0.2s ease;
        color: var(--text-muted);
      }

      .change-group {
        margin-bottom: 24px;
      }

      .change-group:last-child {
        margin-bottom: 0;
      }

      .change-group-title {
        font-size: 13px;
        font-weight: 600;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        margin-bottom: 12px;
        display: flex;
        align-items: center;
        gap: 8px;
      }

      .change-list {
        display: flex;
        flex-direction: column;
        gap: 8px;
      }

      .change-item {
        display: flex;
        align-items: center;
        gap: 12px;
        padding: 12px 16px;
        border-radius: 8px;
        font-family: var(--font-mono);
        font-size: 13px;
        position: relative;
      }

      .change-item.added {
        background: var(--diff-add-bg);
        border: 1px solid var(--diff-add-border);
      }

      .change-item.removed {
        background: var(--diff-remove-bg);
        border: 1px solid var(--diff-remove-border);
      }

      .change-item.modified {
        background: var(--diff-modify-bg);
        border: 1px solid var(--diff-modify-border);
      }

      .change-item.moved {
        background: var(--diff-move-bg);
        border: 1px solid var(--diff-move-border);
      }

      .change-icon {
        width: 24px;
        height: 24px;
        border-radius: 6px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: 700;
        flex-shrink: 0;
      }

      .change-item.added .change-icon {
        background: var(--accent-green);
        color: var(--bg-primary);
      }

      .change-item.removed .change-icon {
        background: var(--accent-red);
        color: var(--bg-primary);
      }

      .change-item.modified .change-icon {
        background: var(--accent-yellow);
        color: var(--bg-primary);
      }

      .change-item.moved .change-icon {
        background: var(--accent-purple);
        color: var(--bg-primary);
      }

      .change-location {
        color: var(--text-secondary);
        min-width: 80px;
      }

      .change-detail {
        flex: 1;
        display: flex;
        align-items: center;
        gap: 8px;
        flex-wrap: wrap;
        min-width: 0;
      }

      .change-actions {
        display: flex;
        gap: 6px;
        margin-left: auto;
      }

      .change-jump {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        color: var(--text-secondary);
        padding: 4px 8px;
        border-radius: 6px;
        font-size: 11px;
        cursor: pointer;
        transition: border-color 0.2s ease, color 0.2s ease;
      }

      .change-jump:hover {
        border-color: var(--accent-blue);
        color: var(--text-primary);
      }

      .change-item.flash {
        animation: changeFlash 1.2s ease-out;
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.6);
      }

      @keyframes changeFlash {
        0% { box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.6); }
        100% { box-shadow: 0 0 0 0 rgba(88, 166, 255, 0); }
      }

      .change-value {
        padding: 4px 8px;
        border-radius: 4px;
        background: var(--bg-primary);
      }

      .change-value.old {
        text-decoration: line-through;
        opacity: 0.7;
      }

      .change-arrow {
        color: var(--text-muted);
      }

      .cell-grid {
        display: grid;
        gap: 1px;
        background: var(--border-primary);
        border: 1px solid var(--border-primary);
        border-radius: 8px;
        overflow: hidden;
        font-family: var(--font-mono);
        font-size: 12px;
      }

      .cell-grid-row {
        display: contents;
      }

      .cell {
        padding: 8px 12px;
        background: var(--bg-primary);
        min-width: 80px;
      }

      .cell.header {
        background: var(--bg-tertiary);
        font-weight: 600;
        color: var(--text-secondary);
        text-align: center;
      }

      .cell.row-header {
        background: var(--bg-tertiary);
        font-weight: 600;
        color: var(--text-secondary);
        text-align: right;
        min-width: 50px;
      }

      .cell.added {
        background: var(--diff-add-bg);
        color: var(--accent-green);
      }

      .cell.removed {
        background: var(--diff-remove-bg);
        color: var(--accent-red);
        text-decoration: line-through;
      }

      .cell.modified {
        background: var(--diff-modify-bg);
      }

      .raw-json-section {
        margin-top: 32px;
      }

      .raw-json-toggle {
        display: flex;
        align-items: center;
        gap: 8px;
        background: none;
        border: 1px solid var(--border-primary);
        padding: 12px 20px;
        border-radius: 8px;
        color: var(--text-secondary);
        cursor: pointer;
        font-family: inherit;
        font-size: 14px;
        transition: all 0.2s ease;
      }

      .raw-json-toggle:hover {
        background: var(--bg-secondary);
        border-color: var(--text-muted);
      }

      .raw-json-content {
        display: none;
        margin-top: 16px;
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 20px;
        overflow-x: auto;
      }

      .raw-json-content.visible {
        display: block;
      }

      .raw-json-content pre {
        font-family: var(--font-mono);
        font-size: 12px;
        line-height: 1.6;
        color: var(--text-secondary);
        white-space: pre-wrap;
        word-break: break-word;
      }

      .warnings-section {
        background: rgba(248, 81, 73, 0.1);
        border: 1px solid rgba(248, 81, 73, 0.3);
        border-radius: 12px;
        padding: 20px;
        margin-bottom: 24px;
      }

      .warnings-title {
        display: flex;
        align-items: center;
        gap: 8px;
        font-weight: 600;
        color: var(--accent-red);
        margin-bottom: 12px;
      }

      .warnings-list {
        list-style: none;
      }

      .warnings-list li {
        padding: 8px 0;
        color: var(--text-secondary);
        border-bottom: 1px solid rgba(248, 81, 73, 0.2);
      }

      .warnings-list li:last-child {
        border-bottom: none;
        padding-bottom: 0;
      }

      .other-changes {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 16px;
        padding: 24px;
        margin-bottom: 24px;
      }

      .other-changes-title {
        font-size: 18px;
        font-weight: 600;
        margin-bottom: 16px;
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .other-changes-title .icon {
        font-size: 24px;
      }

      /* Structural Changes */
      .structural-changes {
        display: flex;
        flex-wrap: wrap;
        gap: 12px;
        margin-bottom: 20px;
      }

      .structural-change {
        display: flex;
        align-items: center;
        gap: 8px;
        padding: 12px 16px;
        border-radius: 8px;
        font-family: var(--font-mono);
        font-size: 13px;
        font-weight: 500;
      }

      .structural-change.added {
        background: var(--diff-add-bg);
        border: 1px solid var(--diff-add-border);
        color: var(--accent-green);
      }

      .structural-change.removed {
        background: var(--diff-remove-bg);
        border: 1px solid var(--diff-remove-border);
        color: var(--accent-red);
      }

      .structural-icon {
        font-size: 16px;
        font-weight: 700;
      }

      /* Visual Grid Styles */
      .sheet-grid-container {
        overflow-x: auto;
        margin: 16px 0;
        border-radius: 12px;
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
      }

      .sheet-grid {
        display: grid;
        min-width: fit-content;
      }

      .grid-cell {
        padding: 8px 12px;
        border-right: 1px solid var(--border-secondary);
        border-bottom: 1px solid var(--border-secondary);
        font-family: var(--font-mono);
        font-size: 12px;
        min-height: 36px;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: background 0.15s ease;
      }

      .grid-corner {
        background: var(--bg-tertiary);
        position: sticky;
        left: 0;
        z-index: 2;
      }

      .grid-col-header {
        background: var(--bg-tertiary);
        color: var(--text-secondary);
        font-weight: 600;
        text-align: center;
        position: sticky;
        top: 0;
        z-index: 1;
      }

      .grid-row-header {
        background: var(--bg-tertiary);
        color: var(--text-secondary);
        font-weight: 600;
        text-align: right;
        justify-content: flex-end;
        position: sticky;
        left: 0;
        z-index: 1;
      }

      .grid-col-header.col-added,
      .grid-row-header.row-added {
        background: var(--diff-add-bg);
        color: var(--accent-green);
      }

      .grid-col-header.col-removed,
      .grid-row-header.row-removed {
        background: var(--diff-remove-bg);
        color: var(--accent-red);
      }

      .grid-col-header.col-move-src,
      .grid-row-header.row-move-src,
      .grid-col-header.col-move-dst,
      .grid-row-header.row-move-dst {
        background: var(--diff-move-bg);
        color: var(--accent-purple);
      }

      .grid-col-header.col-move-src::after,
      .grid-row-header.row-move-src::after,
      .grid-col-header.col-move-dst::after,
      .grid-row-header.row-move-dst::after {
        content: " M";
        font-size: 10px;
        color: var(--accent-purple);
      }

      .cell-added {
        background: var(--diff-add-bg);
      }

      .cell-removed {
        background: var(--diff-remove-bg);
      }

      .cell-move-src {
        background: var(--diff-move-bg);
        color: var(--accent-purple);
        box-shadow: inset 0 0 0 1px var(--diff-move-border);
      }

      .cell-move-dst {
        background: rgba(163, 113, 247, 0.25);
        color: var(--accent-purple);
        box-shadow: inset 0 0 0 1px var(--diff-move-border);
      }

      .cell-edited {
        background: var(--diff-modify-bg);
        flex-direction: column;
        gap: 2px;
        padding: 4px 8px;
      }

      .cell-unchanged {
        background: var(--bg-primary);
      }

      .cell-unchanged:hover {
        background: var(--bg-secondary);
      }

      .cell-in-added-row,
      .cell-in-added-col {
        background: var(--diff-add-bg);
      }

      .cell-in-removed-row,
      .cell-in-removed-col {
        background: var(--diff-remove-bg);
      }

      .cell-change {
        display: flex;
        flex-direction: column;
        gap: 2px;
        width: 100%;
      }

      .cell-old {
        color: var(--accent-red);
        font-size: 11px;
        text-decoration: line-through;
        opacity: 0.7;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
      }

      .cell-new {
        color: var(--accent-green);
        font-size: 12px;
        font-weight: 500;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
      }

      .grid-legend {
        display: flex;
        gap: 20px;
        padding: 12px 16px;
        background: var(--bg-secondary);
        border-top: 1px solid var(--border-primary);
        border-radius: 0 0 12px 12px;
        font-size: 12px;
        color: var(--text-secondary);
        flex-wrap: wrap;
      }

      .legend-item {
        display: flex;
        align-items: center;
        gap: 6px;
      }

      .legend-box {
        width: 14px;
        height: 14px;
        border-radius: 3px;
      }

      .legend-added {
        background: var(--diff-add-bg);
        border: 1px solid var(--diff-add-border);
      }

      .legend-removed {
        background: var(--diff-remove-bg);
        border: 1px solid var(--diff-remove-border);
      }

      .legend-edited {
        background: var(--diff-modify-bg);
        border: 1px solid var(--diff-modify-border);
      }

      .legend-moved {
        background: var(--diff-move-bg);
        border: 1px solid var(--diff-move-border);
      }

      .legend-dot {
        font-size: 16px;
        color: var(--text-muted);
        line-height: 1;
      }

      .grid-skip-warning {
        padding: 12px 16px;
        border-radius: 10px;
        background: rgba(210, 153, 34, 0.12);
        border: 1px solid rgba(210, 153, 34, 0.4);
        color: var(--text-secondary);
        font-size: 13px;
      }

      .grid-skip-warning.flash {
        animation: changeFlash 1.2s ease-out;
        box-shadow: 0 0 0 2px rgba(88, 166, 255, 0.6);
      }

      .grid-partial-warning {
        padding: 12px 16px;
        border-radius: 10px;
        background: rgba(88, 166, 255, 0.12);
        border: 1px solid rgba(88, 166, 255, 0.4);
        color: var(--text-secondary);
        font-size: 13px;
        margin-bottom: 12px;
      }

      .grid-viewer {
        display: grid;
        gap: 12px;
      }

      .grid-toolbar {
        display: flex;
        align-items: center;
        justify-content: space-between;
        gap: 12px;
      }

      .grid-toolbar-group {
        display: flex;
        gap: 8px;
      }

      .grid-toolbar-hint {
        font-size: 12px;
        color: var(--text-muted);
      }

      .grid-mode-btn {
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        color: var(--text-secondary);
        padding: 6px 10px;
        border-radius: 8px;
        font-size: 12px;
        cursor: pointer;
        transition: border-color 0.2s ease, color 0.2s ease;
      }

      .grid-mode-btn:hover {
        border-color: var(--accent-blue);
        color: var(--text-primary);
      }

      .grid-mode-btn.active {
        background: var(--accent-blue);
        color: var(--bg-primary);
        border-color: var(--accent-blue);
      }

      .grid-viewer-body {
        display: grid;
        grid-template-columns: 1fr 320px;
        gap: 16px;
        align-items: start;
      }

      @media (max-width: 1100px) {
        .grid-viewer-body {
          grid-template-columns: 1fr;
        }
      }

      .grid-canvas-wrap {
        position: relative;
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        overflow: hidden;
        background: var(--bg-primary);
      }

      .grid-scroll {
        position: relative;
        overflow: auto;
        height: 520px;
      }

      .grid-scroll-spacer {
        width: 0;
        height: 0;
      }

      .grid-canvas {
        position: sticky;
        top: 0;
        left: 0;
        display: block;
      }

      .grid-tooltip {
        position: absolute;
        left: 0;
        top: 0;
        pointer-events: none;
        opacity: 0;
        transform: translate(-9999px, -9999px);
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 10px;
        padding: 10px 12px;
        font-size: 12px;
        color: var(--text-primary);
        max-width: 320px;
        box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
        transition: opacity 0.1s ease;
        z-index: 5;
      }

      .grid-tooltip.visible {
        opacity: 1;
      }

      .grid-tooltip-title {
        font-weight: 600;
        margin-bottom: 4px;
      }

      .grid-tooltip-meta {
        color: var(--text-secondary);
        font-size: 11px;
        margin-bottom: 6px;
      }

      .grid-tooltip-section {
        margin-bottom: 6px;
      }

      .grid-tooltip-section:last-child {
        margin-bottom: 0;
      }

      .grid-tooltip-label {
        font-size: 11px;
        color: var(--text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.4px;
        margin-bottom: 2px;
      }

      .grid-tooltip-value {
        font-family: var(--font-mono);
      }

      .grid-tooltip-formula {
        font-family: var(--font-mono);
        color: var(--text-muted);
      }

      .grid-inspector {
        background: var(--bg-secondary);
        border: 1px solid var(--border-primary);
        border-radius: 12px;
        padding: 12px;
        display: flex;
        flex-direction: column;
        gap: 8px;
        min-height: 120px;
      }

      .grid-inspector-title {
        font-size: 12px;
        text-transform: uppercase;
        letter-spacing: 0.4px;
        color: var(--text-secondary);
      }

      .grid-inspector-empty {
        font-size: 12px;
        color: var(--text-muted);
      }

      .grid-inspector-row {
        display: flex;
        justify-content: space-between;
        gap: 12px;
        font-size: 12px;
      }

      .grid-inspector-label {
        color: var(--text-muted);
        text-transform: uppercase;
        letter-spacing: 0.4px;
      }

      .grid-inspector-value {
        font-family: var(--font-mono);
        color: var(--text-primary);
        text-align: right;
        word-break: break-word;
      }

      .grid-inspector-jump {
        margin-top: 6px;
        align-self: flex-start;
        background: var(--bg-primary);
        border: 1px solid var(--border-primary);
        color: var(--text-secondary);
        padding: 6px 10px;
        border-radius: 8px;
        font-size: 12px;
        cursor: pointer;
        transition: border-color 0.2s ease, color 0.2s ease;
      }

      .grid-inspector-jump:hover {
        border-color: var(--accent-blue);
        color: var(--text-primary);
      }

      .grid-inspector.pinned {
        border-color: var(--accent-blue);
      }

      .details-section {
        margin-top: 20px;
        border: 1px solid var(--border-primary);
        border-radius: 10px;
        overflow: hidden;
      }

      .details-toggle {
        display: flex;
        align-items: center;
        gap: 8px;
        padding: 14px 18px;
        background: var(--bg-tertiary);
        cursor: pointer;
        font-size: 13px;
        font-weight: 500;
        color: var(--text-secondary);
        transition: all 0.2s ease;
        list-style: none;
      }

      .details-toggle::-webkit-details-marker {
        display: none;
      }

      .details-toggle::before {
        content: "";
        font-size: 10px;
        transition: transform 0.2s ease;
      }

      details[open] .details-toggle::before {
        transform: rotate(90deg);
      }

      .details-toggle:hover {
        background: var(--bg-hover);
        color: var(--text-primary);
      }

      .details-content {
        padding: 16px;
        background: var(--bg-primary);
      }
    </style>
  </head>
  <body>
    <div class="container">
      <header>
        <div class="logo">
          <div class="logo-icon"></div>
          <h1>Excel Diff</h1>
        </div>
        <div class="version">v<span id="version">...</span></div>
      </header>

      <section class="upload-section">
        <div class="upload-grid">
          <div class="upload-box">
            <label>Old File (Before)</label>
            <div class="file-drop" id="dropOld">
              <input id="fileOld" type="file" accept=".xlsx,.xlsm,.xltx,.xltm,.xlsb,.pbix,.pbit" />
              <div class="file-drop-icon"></div>
              <div class="file-drop-text">Drop file or <strong>browse</strong></div>
              <div class="file-name" id="nameOld"></div>
            </div>
          </div>
          <div class="upload-box">
            <label>New File (After)</label>
            <div class="file-drop" id="dropNew">
              <input id="fileNew" type="file" accept=".xlsx,.xlsm,.xltx,.xltm,.xlsb,.pbix,.pbit" />
              <div class="file-drop-icon"></div>
              <div class="file-drop-text">Drop file or <strong>browse</strong></div>
              <div class="file-name" id="nameNew"></div>
            </div>
          </div>
          <button class="diff-btn" id="run">
            <span>Compare</span>
            <span></span>
          </button>
        </div>
        <div class="action-bar">
          <div class="action-left">
            <button class="secondary-btn" id="cancel" disabled>Cancel</button>
            <div class="export-group">
              <button class="secondary-btn" id="exportJson" disabled>Download report JSON</button>
              <button class="secondary-btn" id="exportHtml" disabled>Download HTML report</button>
              <button class="secondary-btn" id="exportAudit" disabled>Export audit workbook</button>
            </div>
          </div>
          <div class="privacy-note">Runs locally in your browser. Files are not uploaded.</div>
        </div>
      </section>

      <section class="batch-section" id="batchSection">
        <div class="section-header">
          <div>
            <h2>Batch compare</h2>
            <p>Compare two folders and open any diff from the table.</p>
          </div>
        </div>
        <div class="batch-controls">
          <div class="batch-picker">
            <div class="batch-label">Old folder</div>
            <button class="secondary-btn" id="batchPickOld">Choose old folder</button>
            <div class="batch-path" id="batchOldLabel">No folder selected.</div>
          </div>
          <div class="batch-picker">
            <div class="batch-label">New folder</div>
            <button class="secondary-btn" id="batchPickNew">Choose new folder</button>
            <div class="batch-path" id="batchNewLabel">No folder selected.</div>
          </div>
          <button class="secondary-btn batch-run" id="batchRun" disabled>Run batch compare</button>
        </div>
        <div class="batch-results" id="batchResults"></div>
      </section>

      <section class="search-section" id="searchSection">
        <div class="section-header">
          <div>
            <h2>Deep search</h2>
            <p>Search changes or build an index to scan full workbooks.</p>
          </div>
        </div>
        <div class="search-controls">
          <input class="search-input" id="searchInput" type="text" placeholder="Search text, formula, or query" />
          <select class="search-select" id="searchScope">
            <option value="changes">Changes</option>
            <option value="old">Old workbook</option>
            <option value="new">New workbook</option>
          </select>
          <button class="secondary-btn" id="searchRun">Search</button>
          <button class="secondary-btn" id="searchIndex">Build index</button>
        </div>
        <div class="search-results" id="searchResults"></div>
      </section>

      <section class="recents-section" id="recentsSection">
        <div class="recents-header">
          <div class="recents-title">Recent comparisons</div>
        </div>
        <div class="recents-empty" id="recentsEmpty">No recent comparisons yet.</div>
        <div class="recents-list" id="recentsList"></div>
      </section>

      <div id="status"></div>

      <div class="large-mode-nav" id="largeModeNav"></div>

      <div id="results" class="results"></div>

      <div class="raw-json-section">
        <button class="raw-json-toggle" id="toggleJson">
          <span>{ }</span>
          <span>View Raw JSON</span>
        </button>
        <div class="raw-json-content" id="rawJsonContent">
          <pre id="raw"></pre>
        </div>
      </div>
    </div>

    <script type="module" src="./main.js"></script>
  </body>
</html>

```

---

### File: `web\main.js`

```javascript
import { renderWorkbookVm } from "./render.js";
import { buildWorkbookViewModel } from "./view_model.js";
import { mountSheetGridViewer } from "./grid_viewer.js";
import { downloadReportJson, downloadHtmlReport, downloadJsonl } from "./export.js";
import {
  createAppDiffClient,
  isDesktop,
  openFileDialog,
  openFolderDialog,
  loadRecents,
  saveRecent,
  loadDiffSummary,
  loadSheetPayload,
  exportAuditXlsx,
  runBatchCompare,
  loadBatchSummary,
  searchDiffOps,
  buildSearchIndex,
  searchWorkbookIndex
} from "./platform.js";

function byId(id) {
  const el = document.getElementById(id);
  if (!el) throw new Error("Missing element: " + id);
  return el;
}

function setStatus(msg, type = "") {
  const status = byId("status");
  status.textContent = msg;
  status.className = type;
}

function nextFrame() {
  return new Promise((resolve) => requestAnimationFrame(() => resolve()));
}

function baseName(path) {
  if (!path) return "";
  const parts = String(path).split(/[\\/]/);
  return parts[parts.length - 1] || "";
}

function fileDisplayName(file) {
  if (!file) return "";
  if (file.name) return file.name;
  if (file.path) return baseName(file.path);
  return "";
}

function buildDesktopSelection(path, name) {
  if (!path) return null;
  return {
    path,
    name: name || baseName(path)
  };
}

let diffClient = null;
let reviewController = null;
let activeViewerManager = null;
let engineVersion = "";
let isBusy = false;
let activeRunId = 0;
let lastReport = null;
let lastMeta = null;
let lastDiffId = null;
let lastSummary = null;
let lastMode = "payload";
let lastEngineOptions = null;
let isDesktopApp = false;
let selectedOld = null;
let selectedNew = null;
let recentComparisons = [];
let recentsSection = null;
let recentsList = null;
let recentsEmpty = null;
let largeModeNav = null;
let selectedOldFolder = null;
let selectedNewFolder = null;
let batchSection = null;
let batchResults = null;
let batchOldLabel = null;
let batchNewLabel = null;
let batchRunBtn = null;
let searchSection = null;
let searchResults = null;
let searchIndexCache = {
  old: { id: null, path: null },
  new: { id: null, path: null }
};
let largeSummaryCleanup = null;

const FILE_SIDES = {
  old: { dropId: "dropOld", inputId: "fileOld", nameId: "nameOld" },
  new: { dropId: "dropNew", inputId: "fileNew", nameId: "nameNew" }
};

function setBusy(state) {
  isBusy = state;
  byId("run").disabled = state;
  const cancelBtn = byId("cancel");
  if (cancelBtn) cancelBtn.disabled = !state;
}

function setExportsEnabled({ json = false, html = false, audit = false } = {}) {
  const jsonBtn = byId("exportJson");
  const htmlBtn = byId("exportHtml");
  const auditBtn = document.getElementById("exportAudit");
  if (jsonBtn) jsonBtn.disabled = !json;
  if (htmlBtn) htmlBtn.disabled = !html;
  if (auditBtn) auditBtn.disabled = !audit;
}

function clearResults() {
  byId("results").innerHTML = "";
  byId("results").classList.remove("visible");
  byId("raw").textContent = "";
  byId("rawJsonContent").classList.remove("visible");
  lastReport = null;
  lastMeta = null;
  lastDiffId = null;
  lastSummary = null;
  lastMode = "payload";
  if (largeModeNav) {
    largeModeNav.innerHTML = "";
    largeModeNav.classList.remove("visible");
  }
  if (largeSummaryCleanup) {
    largeSummaryCleanup();
    largeSummaryCleanup = null;
  }
}

function updateDropDisplay(side, file) {
  const config = FILE_SIDES[side];
  const drop = byId(config.dropId);
  const nameEl = byId(config.nameId);
  const label = fileDisplayName(file);
  if (label) {
    nameEl.textContent = label;
    drop.classList.add("has-file");
  } else {
    nameEl.textContent = "";
    drop.classList.remove("has-file");
  }
}

function setSelectedFile(side, file) {
  if (side === "old") {
    selectedOld = file;
  } else {
    selectedNew = file;
  }
  updateDropDisplay(side, file);
}

function toDesktopSelection(file) {
  if (!file) return null;
  const path = file.path || "";
  if (!path) return null;
  return buildDesktopSelection(path, file.name);
}

function setupFileDrop(side) {
  const config = FILE_SIDES[side];
  const drop = byId(config.dropId);
  const input = byId(config.inputId);

  function updateDisplay(file) {
    setSelectedFile(side, file);
  }

  if (isDesktopApp) {
    input.disabled = true;
    input.tabIndex = -1;
    input.style.display = "none";
    drop.addEventListener("click", async () => {
      try {
        const path = await openFileDialog();
        if (!path) return;
        updateDisplay(buildDesktopSelection(path));
      } catch (err) {
        setStatus(`Error: ${err.message || err}`, "error");
      }
    });
  } else {
    input.addEventListener("change", () => {
      updateDisplay(input.files[0]);
    });
  }

  drop.addEventListener("dragover", (e) => {
    e.preventDefault();
    drop.classList.add("dragover");
  });

  drop.addEventListener("dragleave", () => {
    drop.classList.remove("dragover");
  });

  drop.addEventListener("drop", (e) => {
    e.preventDefault();
    drop.classList.remove("dragover");
    const files = e.dataTransfer?.files || [];
    if (!files.length) return;

    if (isDesktopApp) {
      const selections = Array.from(files)
        .map(toDesktopSelection)
        .filter(Boolean);
      if (selections.length >= 2) {
        setSelectedFile("old", selections[0]);
        setSelectedFile("new", selections[1]);
        return;
      }
      if (selections[0]) {
        updateDisplay(selections[0]);
      }
      return;
    }

    input.files = files;
    updateDisplay(files[0]);
  });
}

const STAGE_LABELS = {
  init: "Initializing engine...",
  validate: "Validating inputs...",
  read: "Reading files...",
  transfer: "Transferring files to worker...",
  diff: "Diffing workbooks...",
  snapshot: "Building previews...",
  align: "Aligning sheets...",
  parse: "Parsing results...",
  render: "Rendering results...",
  hydrate: "Hydrating viewers..."
};

function showStage(stage, detail) {
  const text = detail || STAGE_LABELS[stage] || "";
  if (text) {
    setStatus(text, "loading");
  }
}

function handleWorkerStatus(status) {
  if (!isBusy) return;
  if (status && status.stage) {
    showStage(status.stage, status.detail);
  }
}

function setBatchFolder(side, path) {
  if (side === "old") {
    selectedOldFolder = path;
  } else {
    selectedNewFolder = path;
  }
  if (batchOldLabel) {
    batchOldLabel.textContent = selectedOldFolder || "";
  }
  if (batchNewLabel) {
    batchNewLabel.textContent = selectedNewFolder || "";
  }
  if (batchRunBtn) {
    batchRunBtn.disabled = !selectedOldFolder || !selectedNewFolder;
  }
}

function renderBatchResults(outcome) {
  if (!batchResults) return;
  const items = Array.isArray(outcome?.items) ? outcome.items : [];
  const rows = items
    .map(item => {
      const status = item.status || "";
      const diffId = item.diffId || "";
      const opCount = item.opCount != null ? item.opCount : "";
      const warnings = item.warningsCount != null ? item.warningsCount : "";
      const action = diffId
        ? `<button class="secondary-btn batch-open" data-diff-id="${diffId}">Open</button>`
        : "";
      return `
        <tr>
          <td>${status}</td>
          <td>${item.oldPath || ""}</td>
          <td>${item.newPath || ""}</td>
          <td>${opCount}</td>
          <td>${warnings}</td>
          <td>${action}</td>
        </tr>
      `;
    })
    .join("");

  batchResults.innerHTML = `
    <div class="batch-summary">
      <div><strong>${outcome.itemCount || items.length}</strong> items</div>
      <div><strong>${outcome.completedCount || 0}</strong> completed</div>
      <div>Status: ${outcome.status || ""}</div>
    </div>
    <div class="batch-table-wrap">
      <table class="batch-table">
        <thead>
          <tr>
            <th>Status</th>
            <th>Old path</th>
            <th>New path</th>
            <th>Ops</th>
            <th>Warnings</th>
            <th>Action</th>
          </tr>
        </thead>
        <tbody>
          ${rows || "<tr><td colspan=\"6\">No items.</td></tr>"}
        </tbody>
      </table>
    </div>
  `;
}

function setupBatchSection() {
  batchSection = document.getElementById("batchSection");
  batchResults = document.getElementById("batchResults");
  batchOldLabel = document.getElementById("batchOldLabel");
  batchNewLabel = document.getElementById("batchNewLabel");
  batchRunBtn = document.getElementById("batchRun");
  if (!batchSection) return;
  batchSection.classList.toggle("visible", isDesktopApp);

  const pickOld = document.getElementById("batchPickOld");
  const pickNew = document.getElementById("batchPickNew");

  if (pickOld) {
    pickOld.addEventListener("click", async () => {
      const path = await openFolderDialog();
      if (path) setBatchFolder("old", path);
    });
  }
  if (pickNew) {
    pickNew.addEventListener("click", async () => {
      const path = await openFolderDialog();
      if (path) setBatchFolder("new", path);
    });
  }

  if (batchRunBtn) {
    batchRunBtn.addEventListener("click", async () => {
      if (!selectedOldFolder || !selectedNewFolder) return;
      setStatus("Running batch compare...", "loading");
      try {
        const outcome = await runBatchCompare({
          oldRoot: selectedOldFolder,
          newRoot: selectedNewFolder,
          strategy: "relative",
          trusted: false
        });
        renderBatchResults(outcome);
        setStatus("Batch complete.", "");
      } catch (err) {
        handleError(err);
      }
    });
  }

  if (batchResults) {
    batchResults.addEventListener("click", event => {
      const btn = event.target.closest(".batch-open");
      if (!btn) return;
      const diffId = btn.dataset.diffId;
      if (diffId) {
        openStoredDiff({ diffId });
      }
    });
  }
}

function renderSearchResults(items, title) {
  if (!searchResults) return;
  const rows = items
    .map(item => {
      const sheet = item.sheet ? `<div class="search-meta">${item.sheet}</div>` : "";
      const addr = item.address ? `<div class="search-meta">${item.address}</div>` : "";
      const detail = item.detail ? `<div class="search-detail">${item.detail}</div>` : "";
      return `
        <div class="search-item">
          <div class="search-title">${item.label || item.text || ""}</div>
          ${sheet}
          ${addr}
          ${detail}
        </div>
      `;
    })
    .join("");

  searchResults.innerHTML = `
    <div class="search-summary">${title}</div>
    ${rows || "<div class=\"empty-state\">No results.</div>"}
  `;
}

function getSearchPath(side) {
  if (lastSummary) {
    return side === "old" ? lastSummary.oldPath : lastSummary.newPath;
  }
  if (side === "old" && selectedOld?.path) return selectedOld.path;
  if (side === "new" && selectedNew?.path) return selectedNew.path;
  return null;
}

async function ensureSearchIndex(side) {
  const path = getSearchPath(side);
  if (!path) throw new Error("Select files before building an index.");
  const cached = searchIndexCache[side];
  if (cached?.id && cached.path === path) return cached.id;
  const summary = await buildSearchIndex(path, side);
  searchIndexCache[side] = { id: summary.indexId, path: summary.path || path };
  return summary.indexId;
}

function setupSearchSection() {
  searchSection = document.getElementById("searchSection");
  searchResults = document.getElementById("searchResults");
  if (!searchSection) return;
  searchSection.classList.toggle("visible", isDesktopApp);

  const searchInput = document.getElementById("searchInput");
  const searchScope = document.getElementById("searchScope");
  const searchBtn = document.getElementById("searchRun");
  const indexBtn = document.getElementById("searchIndex");

  if (searchBtn) {
    searchBtn.addEventListener("click", async () => {
      const query = searchInput?.value || "";
      const scope = searchScope?.value || "changes";
      if (!query.trim()) return;

      try {
        if (scope === "changes") {
          if (!lastDiffId) throw new Error("Run a diff before searching.");
          const results = await searchDiffOps(lastDiffId, query, 100);
          renderSearchResults(results, `Matches for \"${query}\" in changes`);
        } else if (scope === "old" || scope === "new") {
          const indexId = await ensureSearchIndex(scope);
          const results = await searchWorkbookIndex(indexId, query, 100);
          const mapped = results.map(item => ({
            label: item.text,
            sheet: item.sheet,
            address: item.address,
            detail: item.kind
          }));
          renderSearchResults(mapped, `Matches for \"${query}\" in ${scope} workbook`);
        }
      } catch (err) {
        handleError(err);
      }
    });
  }

  if (indexBtn) {
    indexBtn.addEventListener("click", async () => {
      const scope = searchScope?.value || "changes";
      if (scope !== "old" && scope !== "new") return;
      try {
        await ensureSearchIndex(scope);
        setStatus(`Index ready for ${scope} workbook.`, "");
      } catch (err) {
        handleError(err);
      }
    });
  }
}

function normalizeDiffOutcome(result) {
  if (result && result.mode) {
    return {
      mode: result.mode || "payload",
      diffId: result.diffId || null,
      payload: result.payload || null,
      summary: result.summary || null,
      config: result.config || null
    };
  }
  return {
    mode: "payload",
    diffId: null,
    payload: result,
    summary: null,
    config: null
  };
}

function buildMeta(oldFile, newFile) {
  return {
    version: engineVersion,
    oldName: fileDisplayName(oldFile) || "",
    newName: fileDisplayName(newFile) || "",
    createdAtIso: new Date().toISOString()
  };
}

function buildMetaFromSummary(summary) {
  const oldPath = summary?.oldPath || "";
  const newPath = summary?.newPath || "";
  const oldName = summary?.oldName || (oldPath ? baseName(oldPath) : "");
  const newName = summary?.newName || (newPath ? baseName(newPath) : "");
  return {
    version: engineVersion,
    oldName,
    newName,
    createdAtIso: summary?.finishedAt || summary?.startedAt || new Date().toISOString()
  };
}

function collectGridPreviews() {
  const previews = {};
  if (!activeViewerManager || typeof activeViewerManager.getMountedViewers !== "function") {
    return previews;
  }
  for (const [sheetName, viewer] of activeViewerManager.getMountedViewers()) {
    if (!viewer || typeof viewer.capturePng !== "function") continue;
    const dataUrl = viewer.capturePng();
    if (dataUrl) previews[sheetName] = dataUrl;
  }
  return previews;
}

function handleError(err) {
  const message = err && err.message ? err.message : String(err);
  setStatus(`Error: ${message}`, "error");
  byId("results").innerHTML = `
    <div class="warnings-section">
      <div class="warnings-title">
        <span>!</span>
        <span>Error</span>
      </div>
      <p style="color: var(--text-secondary); margin-top: 8px;">${String(message)}</p>
    </div>
  `;
  byId("results").classList.add("visible");
}

function formatRecentTimestamp(iso) {
  if (!iso) return "";
  const dt = new Date(iso);
  if (Number.isNaN(dt.getTime())) return iso;
  return dt.toLocaleString();
}

function applyRecentSelection(entry) {
  if (!entry) return;
  setSelectedFile("old", buildDesktopSelection(entry.oldPath, entry.oldName));
  setSelectedFile("new", buildDesktopSelection(entry.newPath, entry.newName));
}

function renderRecents() {
  if (!recentsSection || !recentsList || !recentsEmpty) return;
  if (!isDesktopApp) {
    recentsSection.classList.remove("visible");
    return;
  }
  recentsSection.classList.add("visible");
  recentsList.innerHTML = "";
  if (!recentComparisons.length) {
    recentsEmpty.hidden = false;
    return;
  }
  recentsEmpty.hidden = true;

  recentComparisons.forEach((entry, index) => {
    const item = document.createElement("div");
    item.className = "recent-item";

    const main = document.createElement("div");
    main.className = "recent-main";

    const names = document.createElement("div");
    names.className = "recent-names";

    const oldSpan = document.createElement("span");
    oldSpan.className = "recent-name";
    oldSpan.textContent = entry.oldName || baseName(entry.oldPath);

    const arrow = document.createElement("span");
    arrow.className = "recent-arrow";
    arrow.textContent = "->";

    const newSpan = document.createElement("span");
    newSpan.className = "recent-name";
    newSpan.textContent = entry.newName || baseName(entry.newPath);

    names.appendChild(oldSpan);
    names.appendChild(arrow);
    names.appendChild(newSpan);

    const meta = document.createElement("div");
    meta.className = "recent-meta";
    meta.textContent = formatRecentTimestamp(entry.lastRunIso);

    main.appendChild(names);
    main.appendChild(meta);

    const actions = document.createElement("div");
    actions.className = "recent-actions";

    const loadBtn = document.createElement("button");
    loadBtn.type = "button";
    loadBtn.className = "secondary-btn recent-action";
    loadBtn.dataset.recentAction = "load";
    loadBtn.dataset.recentIndex = String(index);
    loadBtn.textContent = "Load";

    const openBtn = document.createElement("button");
    openBtn.type = "button";
    openBtn.className = "secondary-btn recent-action";
    openBtn.dataset.recentAction = "open";
    openBtn.dataset.recentIndex = String(index);
    openBtn.textContent = "Open";
    if (!entry.diffId) {
      openBtn.disabled = true;
    }

    const rerunBtn = document.createElement("button");
    rerunBtn.type = "button";
    rerunBtn.className = "secondary-btn recent-action";
    rerunBtn.dataset.recentAction = "rerun";
    rerunBtn.dataset.recentIndex = String(index);
    rerunBtn.textContent = "Re-run";

    const swapBtn = document.createElement("button");
    swapBtn.type = "button";
    swapBtn.className = "secondary-btn recent-action";
    swapBtn.dataset.recentAction = "swap";
    swapBtn.dataset.recentIndex = String(index);
    swapBtn.textContent = "Swap";

    actions.appendChild(loadBtn);
    actions.appendChild(openBtn);
    actions.appendChild(rerunBtn);
    actions.appendChild(swapBtn);

    item.appendChild(main);
    item.appendChild(actions);
    recentsList.appendChild(item);
  });
}

function handleRecentsClick(event) {
  const button = event.target.closest(".recent-action");
  if (!button) return;
  const index = Number(button.dataset.recentIndex);
  const action = button.dataset.recentAction;
  const entry = recentComparisons[index];
  if (!entry) return;

  if (action === "swap") {
    const swapped = {
      oldPath: entry.newPath,
      newPath: entry.oldPath,
      oldName: entry.newName,
      newName: entry.oldName,
      lastRunIso: entry.lastRunIso
    };
    applyRecentSelection(swapped);
    return;
  }

  if (action === "open") {
    openStoredDiff(entry);
    return;
  }

  applyRecentSelection(entry);
  if (action === "rerun") {
    runDiff();
  }
}

async function persistRecentComparison(oldFile, newFile, lastRunIso) {
  if (!isDesktopApp || !oldFile?.path || !newFile?.path) return;
  const entry = {
    oldPath: oldFile.path,
    newPath: newFile.path,
    oldName: fileDisplayName(oldFile),
    newName: fileDisplayName(newFile),
    lastRunIso: lastRunIso || new Date().toISOString(),
    diffId: lastDiffId || undefined,
    mode: lastMode || undefined
  };
  try {
    const updated = await saveRecent(entry);
    if (Array.isArray(updated)) {
      recentComparisons = updated;
      renderRecents();
    }
  } catch (err) {
    console.warn("Failed to save recent comparison:", err);
  }
}

async function loadRecentComparisons() {
  if (!isDesktopApp) return;
  try {
    const items = await loadRecents();
    if (Array.isArray(items)) {
      recentComparisons = items;
    }
  } catch (err) {
    console.warn("Failed to load recents:", err);
  }
  renderRecents();
}

async function openStoredDiff(entry) {
  if (!isDesktopApp || !entry?.diffId) return;
  cleanupViewers();
  clearResults();
  setExportsEnabled({ json: false, html: false, audit: false });
  setStatus("Loading stored diff...", "loading");
  try {
    const summary = await loadDiffSummary(entry.diffId);
    lastDiffId = entry.diffId;
    lastSummary = summary;
    lastMode = summary?.mode || "payload";
    lastReport = null;
    lastMeta = buildMetaFromSummary(summary);
    renderLargeSummary(summary);
    setExportsEnabled({ json: false, html: true, audit: isDesktopApp });
    const opCount = summary?.opCount || 0;
    setStatus(`Loaded ${opCount} change${opCount !== 1 ? "s" : ""}.`, "");
  } catch (err) {
    handleError(err);
  }
}

function cancelDiff() {
  if (!isBusy) return;
  activeRunId += 1;
  diffClient.cancel();
  clearResults();
  setExportsEnabled({ json: false, html: false, audit: false });
  setBusy(false);
  setStatus("Canceled.", "");
}

async function runDiff() {
  const oldFile = selectedOld;
  const newFile = selectedNew;

  if (!oldFile || !newFile) {
    setStatus("Please select both files to compare.", "error");
    return;
  }
  if (isDesktopApp && (!oldFile.path || !newFile.path)) {
    setStatus("Please select valid files to compare.", "error");
    return;
  }

  cleanupViewers();
  clearResults();
  setExportsEnabled({ json: false, html: false, audit: false });

  activeRunId += 1;
  const runId = activeRunId;
  setBusy(true);
  showStage("validate");
  showStage("read");

  try {
    const viewOptions = { ignoreBlankToBlank: true };
    const engineOptions = { preset: "balanced" };
    lastEngineOptions = { ...engineOptions };
    let payload;

    if (isDesktopApp) {
      payload = await diffClient.diff(
        {
          oldName: fileDisplayName(oldFile),
          newName: fileDisplayName(newFile),
          oldPath: oldFile.path,
          newPath: newFile.path
        },
        engineOptions
      );
    } else {
      const oldBuffer = await oldFile.arrayBuffer();
      const newBuffer = await newFile.arrayBuffer();
      if (runId !== activeRunId) return;

      showStage("transfer");

      payload = await diffClient.diff(
        {
          oldName: oldFile.name,
          newName: newFile.name,
          oldBuffer,
          newBuffer
        },
        engineOptions
      );
    }
    if (runId !== activeRunId) return;

    showStage("render");
    await nextFrame();

    const outcome = normalizeDiffOutcome(payload);
    lastDiffId = outcome.diffId || null;
    lastSummary = outcome.summary || null;
    lastMode = outcome.mode || "payload";
    if (outcome.config) {
      lastEngineOptions = {
        ...(outcome.config.preset ? { preset: outcome.config.preset } : {}),
        ...(outcome.config.limits ? { limits: outcome.config.limits } : {})
      };
    }

    if (outcome.mode === "payload" && outcome.payload) {
      const report = outcome.payload.report || outcome.payload;
      renderResults(outcome.payload, viewOptions);
      byId("raw").textContent = JSON.stringify(report, null, 2);

      const opCount = report.ops ? report.ops.length : 0;
      if (opCount === 0) {
        setStatus("Files are identical.", "");
      } else {
        setStatus(`Found ${opCount} difference${opCount !== 1 ? "s" : ""}.`, "");
      }

      lastReport = report;
      lastMeta = buildMeta(oldFile, newFile);
      setExportsEnabled({ json: true, html: true, audit: isDesktopApp });
    } else if (outcome.summary) {
      renderLargeSummary(outcome.summary);
      byId("raw").textContent = JSON.stringify(outcome.summary, null, 2);
      lastReport = null;
      lastMeta = buildMetaFromSummary(outcome.summary);

      const opCount = outcome.summary.opCount || 0;
      if (opCount === 0) {
        setStatus("Files are identical.", "");
      } else {
        setStatus(`Found ${opCount} difference${opCount !== 1 ? "s" : ""} (large mode).`, "");
      }

      setExportsEnabled({ json: false, html: true, audit: isDesktopApp });
    } else {
      throw new Error("Unexpected diff response.");
    }

    await persistRecentComparison(oldFile, newFile, lastMeta?.createdAtIso || "");
  } catch (e) {
    if (!isBusy && String(e).toLowerCase().includes("canceled")) {
      return;
    }
    handleError(e);
  } finally {
    if (runId === activeRunId) {
      setBusy(false);
    }
  }
}

function cleanupViewers() {
  if (reviewController) {
    reviewController.cleanup();
    reviewController = null;
  }
  activeViewerManager = null;
}

function renderResults(payload, options = {}, state = {}) {
  cleanupViewers();
  hideLargeModeNav();
  const workbookVm = buildWorkbookViewModel(payload, options);
  const resultsEl = byId("results");
  resultsEl.innerHTML = renderWorkbookVm(workbookVm);
  resultsEl.classList.add("visible");
  reviewController = setupReviewWorkflow(resultsEl, workbookVm, payload, options, state);
  activeViewerManager = reviewController.viewerManager || null;
  return workbookVm;
}

function renderSummaryCards(counts = {}) {
  const added = counts.added || 0;
  const removed = counts.removed || 0;
  const modified = counts.modified || 0;
  const moved = counts.moved || 0;
  return `
    <div class="summary-cards">
      <div class="summary-card added">
        <div class="count">${added}</div>
        <div class="label">Added</div>
      </div>
      <div class="summary-card removed">
        <div class="count">${removed}</div>
        <div class="label">Removed</div>
      </div>
      <div class="summary-card modified">
        <div class="count">${modified}</div>
        <div class="label">Modified</div>
      </div>
      <div class="summary-card moved">
        <div class="count">${moved}</div>
        <div class="label">Moved</div>
      </div>
    </div>
  `;
}

function renderLargeSummary(summary) {
  cleanupViewers();
  hideLargeModeNav();
  if (largeSummaryCleanup) {
    largeSummaryCleanup();
    largeSummaryCleanup = null;
  }

  const resultsEl = byId("results");
  const warnings = Array.isArray(summary?.warnings) ? summary.warnings : [];
  const warningsHtml = warnings.length
    ? `
      <div class="warnings-section">
        <div class="warnings-title">
          <span>!</span>
          <span>Warnings</span>
        </div>
        <ul class="warnings-list">
          ${warnings.map(w => `<li>${String(w)}</li>`).join("")}
        </ul>
      </div>
    `
    : "";

  const downloadButton = !isDesktopApp
    ? `<button class="secondary-btn" id="downloadJsonl">Download JSONL</button>`
    : "";

  const sheets = Array.isArray(summary?.sheets) ? summary.sheets : [];
  const sheetHtml = sheets
    .map(sheet => {
      const counts = sheet.counts || {};
      return `
        <div class="large-sheet-item" data-sheet="${sheet.sheetName}">
          <div class="large-sheet-main">
            <div class="large-sheet-name">${sheet.sheetName}</div>
            <div class="large-sheet-meta">${sheet.opCount || 0} changes</div>
          </div>
          <div class="large-sheet-counts">
            <span class="pill added">+${counts.added || 0}</span>
            <span class="pill removed">-${counts.removed || 0}</span>
            <span class="pill modified">~${counts.modified || 0}</span>
            <span class="pill moved">&gt;${counts.moved || 0}</span>
          </div>
          <button class="secondary-btn large-sheet-load" data-sheet="${sheet.sheetName}">Load details</button>
        </div>
      `;
    })
    .join("");

  resultsEl.innerHTML = `
    <div class="large-summary">
      <div class="large-summary-header">
        <div>
          <h2>Large Mode Summary</h2>
          <p>Sheet details load on demand to keep huge diffs responsive.</p>
        </div>
        <div class="large-summary-side">
          <div class="large-summary-meta">
            <span>${summary?.opCount || 0} ops</span>
            <span>${summary?.sheets?.length || 0} sheets</span>
          </div>
          ${downloadButton}
        </div>
      </div>
      ${warningsHtml}
      ${renderSummaryCards(summary?.counts || {})}
      <div class="large-sheet-list">
        ${sheetHtml || "<div class=\"empty-state\">No sheet-level changes recorded.</div>"}
      </div>
    </div>
  `;
  resultsEl.classList.add("visible");

  const onClick = event => {
    const download = event.target.closest("#downloadJsonl");
    if (download) {
      downloadLargeModeJsonl();
      return;
    }
    const button = event.target.closest(".large-sheet-load");
    if (!button) return;
    const sheetName = button.dataset.sheet;
    if (sheetName) {
      loadLargeSheet(sheetName);
    }
  };

  resultsEl.addEventListener("click", onClick);
  largeSummaryCleanup = () => resultsEl.removeEventListener("click", onClick);
}

async function downloadLargeModeJsonl() {
  if (isDesktopApp) return;
  if (isBusy) {
    setStatus("Wait for the current diff to finish before downloading JSONL.", "error");
    return;
  }
  if (!diffClient || typeof diffClient.downloadJsonl !== "function") {
    setStatus("JSONL download is unavailable.", "error");
    return;
  }
  const oldFile = selectedOld;
  const newFile = selectedNew;
  if (!oldFile || !newFile) {
    setStatus("Select files before downloading JSONL.", "error");
    return;
  }
  try {
    setStatus("Preparing JSONL download...", "loading");
    const oldBuffer = await oldFile.arrayBuffer();
    const newBuffer = await newFile.arrayBuffer();
    const blob = await diffClient.downloadJsonl(
      {
        oldName: fileDisplayName(oldFile),
        newName: fileDisplayName(newFile),
        oldBuffer,
        newBuffer
      },
      lastEngineOptions || { preset: "balanced" }
    );
    const meta = lastSummary ? buildMetaFromSummary(lastSummary) : buildMeta(oldFile, newFile);
    downloadJsonl({ blob, meta });
    setStatus("JSONL download ready.", "");
  } catch (err) {
    handleError(err);
  }
}

function showLargeModeNav(sheetName) {
  if (!largeModeNav) return;
  largeModeNav.innerHTML = `
    <button class="secondary-btn" id="largeBack">Back to summary</button>
    <span class="large-mode-title">${sheetName}</span>
  `;
  largeModeNav.classList.add("visible");
  const backBtn = largeModeNav.querySelector("#largeBack");
  if (backBtn) {
    backBtn.addEventListener("click", () => {
      if (lastSummary) {
        renderLargeSummary(lastSummary);
        setExportsEnabled({ json: false, html: true, audit: isDesktopApp });
      }
    });
  }
}

function hideLargeModeNav() {
  if (!largeModeNav) return;
  largeModeNav.classList.remove("visible");
  largeModeNav.innerHTML = "";
}

async function loadLargeSheet(sheetName) {
  if (!isDesktopApp || !lastDiffId) return;
  try {
    showStage("render", `Loading ${sheetName}...`);
    const payload = await loadSheetPayload(lastDiffId, sheetName);
    if (!payload) {
      throw new Error("No payload returned for sheet.");
    }
    renderResults(payload, { ignoreBlankToBlank: true });
    lastReport = payload.report || payload;
    lastMeta = buildMetaFromSummary(lastSummary || {});
    showLargeModeNav(sheetName);
    setExportsEnabled({ json: false, html: true, audit: isDesktopApp });
    setStatus(`Loaded ${sheetName}.`, "");
  } catch (err) {
    handleError(err);
  }
}

function buildReviewOrder(workbookVm) {
  const order = [];
  for (const sheet of workbookVm.sheets) {
    const anchors = sheet.changes?.anchors || [];
    for (const anchor of anchors) {
      order.push({ sheetName: sheet.name, anchorId: anchor.id });
    }
  }
  return order;
}

function setupReviewWorkflow(rootEl, workbookVm, payloadCache, options = {}, state = {}) {
  const anchorMap = new Map(
    workbookVm.sheets.map(sheet => [sheet.name, new Map((sheet.changes?.anchors || []).map(anchor => [anchor.id, anchor]))])
  );
  const displayOptions = {
    contentMode: state.contentMode || "values",
    focusRows: Boolean(state.focusRows),
    focusCols: Boolean(state.focusCols)
  };
  const viewerManager = hydrateGridViewers(rootEl, workbookVm, displayOptions, state.expandedSheets || null);
  const reviewOrder = buildReviewOrder(workbookVm);
  const reviewState = {
    activeSheetName: state.activeSheetName || null,
    activeAnchorId: state.activeAnchorId || null
  };

  const searchInput = rootEl.querySelector(".sheet-search");
  const focusRowsInput = rootEl.querySelector('input[data-filter="focus-rows"]');
  const focusColsInput = rootEl.querySelector('input[data-filter="focus-cols"]');
  const ignoreBlankInput = rootEl.querySelector('input[data-filter="ignore-blank"]');
  const contentModeSelect = rootEl.querySelector('select[data-filter="content-mode"]');

  if (focusRowsInput) focusRowsInput.checked = displayOptions.focusRows;
  if (focusColsInput) focusColsInput.checked = displayOptions.focusCols;
  if (ignoreBlankInput) ignoreBlankInput.checked = options.ignoreBlankToBlank !== false;
  if (contentModeSelect) contentModeSelect.value = displayOptions.contentMode;

  function setActiveSheet(sheetName) {
    const items = rootEl.querySelectorAll(".sheet-index-item");
    for (const item of items) {
      item.classList.toggle("active", item.dataset.sheet === sheetName);
    }
  }

  function applySheetFilter(value) {
    const term = String(value || "").trim().toLowerCase();
    const sections = rootEl.querySelectorAll(".sheet-section");
    const indexItems = rootEl.querySelectorAll(".sheet-index-item");
    for (const section of sections) {
      const name = (section.dataset.sheet || "").toLowerCase();
      section.hidden = term ? !name.includes(term) : false;
    }
    for (const item of indexItems) {
      const name = (item.dataset.sheet || "").toLowerCase();
      item.hidden = term ? !name.includes(term) : false;
    }
  }

  if (searchInput) {
    searchInput.value = state.sheetFilter || "";
    if (state.sheetFilter) applySheetFilter(state.sheetFilter);
  }

  function getSheetSection(sheetName) {
    const sections = rootEl.querySelectorAll(".sheet-section");
    for (const section of sections) {
      if (section.dataset.sheet === sheetName) return section;
    }
    return null;
  }

  function expandSheet(sheetName) {
    const section = getSheetSection(sheetName);
    if (!section) return null;
    section.classList.add("expanded");
    return section;
  }

  function ensureViewer(sheetName) {
    return viewerManager.ensureViewer(sheetName);
  }

  function flashElement(el) {
    if (!el) return;
    el.classList.add("flash");
    window.setTimeout(() => {
      el.classList.remove("flash");
    }, 1200);
  }

  function navigateToAnchor(sheetName, anchorId) {
    const sheetAnchors = anchorMap.get(sheetName);
    const anchor = sheetAnchors ? sheetAnchors.get(anchorId) : null;
    if (!anchor) return false;
    const section = expandSheet(sheetName);
    if (anchor.target.kind === "grid") {
      const viewer = ensureViewer(sheetName);
      if (viewer) {
        viewer.jumpToAnchor(anchorId);
        viewer.flashAnchor(anchorId);
        viewer.focus();
      }
    } else if (anchor.target.kind === "list") {
      const target = document.getElementById(anchor.target.elementId);
      if (target) {
        target.scrollIntoView({ behavior: "smooth", block: "center" });
        flashElement(target);
      }
    }
    if (section && anchor.target.kind === "grid") {
      section.scrollIntoView({ behavior: "smooth", block: "start" });
    }
    reviewState.activeSheetName = sheetName;
    reviewState.activeAnchorId = anchorId;
    setActiveSheet(sheetName);
    return true;
  }

  function findReviewIndex() {
    if (!reviewState.activeSheetName || !reviewState.activeAnchorId) return -1;
    return reviewOrder.findIndex(
      entry => entry.sheetName === reviewState.activeSheetName && entry.anchorId === reviewState.activeAnchorId
    );
  }

  function moveReview(delta) {
    if (!reviewOrder.length) return;
    let idx = findReviewIndex();
    if (idx === -1) {
      idx = delta > 0 ? 0 : reviewOrder.length - 1;
    } else {
      idx += delta;
    }
    if (idx < 0 || idx >= reviewOrder.length) return;
    const next = reviewOrder[idx];
    navigateToAnchor(next.sheetName, next.anchorId);
  }

  function captureState() {
    const expandedSheets = new Set();
    const sections = rootEl.querySelectorAll(".sheet-section.expanded");
    for (const section of sections) {
      if (section.dataset.sheet) expandedSheets.add(section.dataset.sheet);
    }
    return {
      expandedSheets,
      activeSheetName: reviewState.activeSheetName,
      activeAnchorId: reviewState.activeAnchorId,
      contentMode: displayOptions.contentMode,
      focusRows: displayOptions.focusRows,
      focusCols: displayOptions.focusCols,
      sheetFilter: searchInput ? searchInput.value : ""
    };
  }

  function rebuildResults(ignoreBlankToBlank) {
    const nextState = captureState();
    const nextOptions = { ...options, ignoreBlankToBlank };
    renderResults(payloadCache, nextOptions, nextState);
  }

  function onRootClick(event) {
    const navBtn = event.target.closest(".review-nav-btn");
    if (navBtn) {
      event.preventDefault();
      const direction = navBtn.dataset.reviewNav;
      moveReview(direction === "prev" ? -1 : 1);
      return;
    }

    const jumpBtn = event.target.closest(".change-jump");
    if (jumpBtn) {
      event.preventDefault();
      event.stopPropagation();
      const sheetName = jumpBtn.dataset.sheet;
      const anchorId = jumpBtn.dataset.anchor;
      if (sheetName && anchorId) {
        navigateToAnchor(sheetName, anchorId);
      }
      return;
    }

    const statusBtn = event.target.closest(".status-pill");
    if (statusBtn && statusBtn.tagName === "BUTTON") {
      event.preventDefault();
      event.stopPropagation();
      const sheetName = statusBtn.dataset.sheet;
      if (sheetName) {
        const section = expandSheet(sheetName);
        if (section) {
          const warning = section.querySelector(".grid-skip-warning");
          if (warning) {
            warning.scrollIntoView({ behavior: "smooth", block: "center" });
            flashElement(warning);
          } else {
            section.scrollIntoView({ behavior: "smooth", block: "start" });
          }
        }
      }
      return;
    }

    const indexBtn = event.target.closest(".sheet-index-item");
    if (indexBtn) {
      event.preventDefault();
      const sheetName = indexBtn.dataset.sheet;
      if (sheetName) {
        const section = expandSheet(sheetName);
        if (section) {
          ensureViewer(sheetName);
          section.scrollIntoView({ behavior: "smooth", block: "start" });
          reviewState.activeSheetName = sheetName;
          reviewState.activeAnchorId = null;
          setActiveSheet(sheetName);
        }
      }
    }
  }

  function onRootInput(event) {
    if (event.target.classList.contains("sheet-search")) {
      applySheetFilter(event.target.value);
    }
  }

  function onRootChange(event) {
    if (event.target === focusRowsInput) {
      displayOptions.focusRows = focusRowsInput.checked;
      viewerManager.setDisplayOptions(displayOptions);
    } else if (event.target === focusColsInput) {
      displayOptions.focusCols = focusColsInput.checked;
      viewerManager.setDisplayOptions(displayOptions);
    } else if (event.target === ignoreBlankInput) {
      rebuildResults(ignoreBlankInput.checked);
    } else if (event.target === contentModeSelect) {
      displayOptions.contentMode = contentModeSelect.value;
      viewerManager.setDisplayOptions(displayOptions);
    }
  }

  function onViewerFocus(event) {
    const mount = event.target.closest(".grid-viewer-mount");
    if (!mount) return;
    const sheetName = mount.dataset.sheet;
    if (!sheetName) return;
    reviewState.activeSheetName = sheetName;
    setActiveSheet(sheetName);
  }

  function onViewerAnchor(event) {
    const mount = event.target.closest(".grid-viewer-mount");
    if (!mount) return;
    const sheetName = mount.dataset.sheet;
    if (!sheetName) return;
    const anchorId = event.detail?.anchorId;
    if (!anchorId) return;
    reviewState.activeSheetName = sheetName;
    reviewState.activeAnchorId = anchorId;
    setActiveSheet(sheetName);
  }

  rootEl.addEventListener("click", onRootClick);
  rootEl.addEventListener("input", onRootInput);
  rootEl.addEventListener("change", onRootChange);
  rootEl.addEventListener("gridviewer:focus", onViewerFocus);
  rootEl.addEventListener("gridviewer:anchor", onViewerAnchor);

  viewerManager.setDisplayOptions(displayOptions);

  if (reviewState.activeSheetName && reviewState.activeAnchorId) {
    const moved = navigateToAnchor(reviewState.activeSheetName, reviewState.activeAnchorId);
    if (!moved) {
      expandSheet(reviewState.activeSheetName);
      setActiveSheet(reviewState.activeSheetName);
    }
  }

  return {
    viewerManager,
    cleanup() {
      rootEl.removeEventListener("click", onRootClick);
      rootEl.removeEventListener("input", onRootInput);
      rootEl.removeEventListener("change", onRootChange);
      rootEl.removeEventListener("gridviewer:focus", onViewerFocus);
      rootEl.removeEventListener("gridviewer:anchor", onViewerAnchor);
      viewerManager.cleanup();
    }
  };
}

function hydrateGridViewers(rootEl, workbookVm, displayOptions = {}, expandedSheets = null) {
  const sheetMap = new Map(workbookVm.sheets.map(sheet => [sheet.name, sheet]));
  const viewers = new Map();
  let currentOptions = { ...displayOptions };

  function getSectionByName(sheetName) {
    const sections = rootEl.querySelectorAll(".sheet-section");
    for (const section of sections) {
      if (section.dataset.sheet === sheetName) return section;
    }
    return null;
  }

  function mountForSection(section) {
    if (!section) return;
    const mount = section.querySelector(".grid-viewer-mount");
    if (!mount || mount.dataset.mounted) return;
    const sheetName = section.dataset.sheet || mount.dataset.sheet;
    const sheetVm = sheetMap.get(sheetName);
    if (!sheetVm) return;
    if (typeof sheetVm.ensureCellIndex === "function") {
      sheetVm.ensureCellIndex();
    }
    const initialMode = mount.dataset.initialMode || "side_by_side";
    const initialAnchor = mount.dataset.initialAnchor || "0";
    const viewer = mountSheetGridViewer({
      mountEl: mount,
      sheetVm,
      opts: { initialMode, initialAnchor, displayOptions: currentOptions }
    });
    mount.dataset.mounted = "true";
    viewers.set(sheetName, viewer);
  }

  function ensureViewer(sheetName) {
    const existing = viewers.get(sheetName);
    if (existing) return existing;
    const section = getSectionByName(sheetName);
    if (!section) return null;
    section.classList.add("expanded");
    mountForSection(section);
    return viewers.get(sheetName) || null;
  }

  function getViewer(sheetName) {
    return viewers.get(sheetName) || null;
  }

  function getMountedViewers() {
    return new Map(viewers);
  }

  function setDisplayOptions(nextOptions) {
    currentOptions = { ...currentOptions, ...nextOptions };
    for (const viewer of viewers.values()) {
      viewer.setDisplayOptions(currentOptions);
    }
  }

  function onHeaderClick(event) {
    const header = event.target.closest(".sheet-header");
    if (!header || !rootEl.contains(header)) return;
    if (event.target.closest("button")) return;
    const section = header.closest(".sheet-section");
    if (!section) return;
    section.classList.toggle("expanded");
    if (section.classList.contains("expanded")) {
      mountForSection(section);
    }
  }

  rootEl.addEventListener("click", onHeaderClick);

  const sections = rootEl.querySelectorAll(".sheet-section");
  if (expandedSheets && expandedSheets.size > 0) {
    for (const section of sections) {
      section.classList.toggle("expanded", expandedSheets.has(section.dataset.sheet));
    }
  } else {
    const anyExpanded = rootEl.querySelector(".sheet-section.expanded");
    if (!anyExpanded && sections.length > 0) {
      sections[0].classList.add("expanded");
    }
  }

  const expanded = rootEl.querySelectorAll(".sheet-section.expanded");
  for (const section of expanded) {
    mountForSection(section);
  }

  return {
    ensureViewer,
    getViewer,
    getMountedViewers,
    setDisplayOptions,
    cleanup() {
      rootEl.removeEventListener("click", onHeaderClick);
      for (const viewer of viewers.values()) {
        viewer.destroy();
      }
      viewers.clear();
    }
  };
}

async function main() {
  setStatus("Loading...", "loading");

  try {
    isDesktopApp = isDesktop();
    diffClient = createAppDiffClient({ onStatus: handleWorkerStatus });
    showStage("init");
    engineVersion = await diffClient.ready();
    byId("version").textContent = engineVersion;
    setStatus("");

    setupFileDrop("old");
    setupFileDrop("new");
    largeModeNav = byId("largeModeNav");
    setupBatchSection();
    setupSearchSection();

    recentsSection = byId("recentsSection");
    recentsList = byId("recentsList");
    recentsEmpty = byId("recentsEmpty");
    if (recentsList) {
      recentsList.addEventListener("click", handleRecentsClick);
    }
    await loadRecentComparisons();

    byId("run").addEventListener("click", runDiff);
    const cancelBtn = byId("cancel");
    if (cancelBtn) cancelBtn.addEventListener("click", cancelDiff);

    const exportJsonBtn = byId("exportJson");
    if (exportJsonBtn) {
      exportJsonBtn.addEventListener("click", () => {
        if (!lastReport || !lastMeta) return;
        downloadReportJson({ report: lastReport, meta: lastMeta });
      });
    }

    const exportHtmlBtn = byId("exportHtml");
    if (exportHtmlBtn) {
      exportHtmlBtn.addEventListener("click", () => {
        if (!lastMeta) return;
        const resultsHtml = byId("results").innerHTML;
        const cssText = document.querySelector("style")?.textContent || "";
        const reportJsonText = JSON.stringify(lastReport || lastSummary || {}, null, 2);
        const gridPreviews = collectGridPreviews();
        downloadHtmlReport({
          title: "Excel Diff Report",
          meta: lastMeta,
          renderedResultsHtml: resultsHtml,
          cssText,
          reportJsonText,
          gridPreviews
        });
      });
    }

    const exportAuditBtn = document.getElementById("exportAudit");
    if (exportAuditBtn) {
      exportAuditBtn.addEventListener("click", async () => {
        if (!lastDiffId) return;
        try {
          await exportAuditXlsx(lastDiffId);
        } catch (err) {
          handleError(err);
        }
      });
    }

    byId("toggleJson").addEventListener("click", () => {
      byId("rawJsonContent").classList.toggle("visible");
    });

    setBusy(false);
    setExportsEnabled({ json: false, html: false, audit: false });
  } catch (e) {
    setStatus("Failed to load: " + String(e), "error");
  }
}

main();

```

---

### File: `web\native_diff_client.js`

```javascript
function resolveTauri() {
  if (typeof window === "undefined") return null;
  return window.__TAURI__ || null;
}

function resolveInvoke(tauri) {
  if (tauri?.core?.invoke) return tauri.core.invoke;
  if (tauri?.invoke) return tauri.invoke;
  return null;
}

function resolveListen(tauri) {
  if (tauri?.event?.listen) return tauri.event.listen;
  return null;
}

export function getNativeBridge() {
  const tauri = resolveTauri();
  if (!tauri) return null;
  const invoke = resolveInvoke(tauri);
  if (typeof invoke !== "function") return null;
  return {
    invoke,
    listen: resolveListen(tauri)
  };
}

function notify(onStatus, payload) {
  if (typeof onStatus === "function") {
    onStatus(payload);
  }
}

export async function openNativeFileDialog() {
  const bridge = getNativeBridge();
  if (!bridge) {
    throw new Error("Native dialog unavailable.");
  }
  const result = await bridge.invoke("pick_file");
  if (!result) return null;
  return result;
}

export async function openNativeFolderDialog() {
  const bridge = getNativeBridge();
  if (!bridge) {
    throw new Error("Native dialog unavailable.");
  }
  const result = await bridge.invoke("pick_folder");
  if (!result) return null;
  return result;
}

export async function loadNativeRecents() {
  const bridge = getNativeBridge();
  if (!bridge) return [];
  return bridge.invoke("load_recents");
}

export async function saveNativeRecent(entry) {
  const bridge = getNativeBridge();
  if (!bridge) return [];
  return bridge.invoke("save_recent", { entry });
}

export async function loadNativeDiffSummary(diffId) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("load_diff_summary", { diff_id: diffId });
}

export async function loadNativeSheetPayload(diffId, sheetName) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("load_sheet_payload", { diff_id: diffId, sheet_name: sheetName });
}

export async function exportNativeAuditXlsx(diffId) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("export_audit_xlsx", { diff_id: diffId });
}

export async function runNativeBatchCompare(request) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("run_batch_compare", { request });
}

export async function loadNativeBatchSummary(batchId) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("load_batch_summary", { batch_id: batchId });
}

export async function loadNativeCapabilities() {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("get_capabilities");
}

export async function searchNativeDiffOps(diffId, query, limit) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("search_diff_ops", { diff_id: diffId, query, limit });
}

export async function buildNativeSearchIndex(path, side) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("build_search_index", { path, side });
}

export async function searchNativeWorkbookIndex(indexId, query, limit) {
  const bridge = getNativeBridge();
  if (!bridge) throw new Error("Native bridge unavailable.");
  return bridge.invoke("search_workbook_index", { index_id: indexId, query, limit });
}

export function createNativeDiffClient({ onStatus } = {}) {
  const bridge = getNativeBridge();
  if (!bridge) {
    throw new Error("Native bridge unavailable.");
  }

  let current = null;
  let requestCounter = 0;
  let cachedVersion = null;
  let readyPromise = null;
  let disposed = false;
  let unlisten = null;

  function nextRequestId() {
    requestCounter += 1;
    return requestCounter;
  }

  async function ensureListener() {
    if (unlisten || typeof bridge.listen !== "function") return;
    unlisten = await bridge.listen("diff-progress", event => {
      if (!current || !event || !event.payload) return;
      const payload = event.payload;
      if (payload.runId !== current.id) return;
      notify(onStatus, {
        stage: payload.stage,
        detail: payload.detail,
        source: "native"
      });
    });
  }

  async function ready() {
    if (cachedVersion) return cachedVersion;
    if (readyPromise) return readyPromise;
    readyPromise = bridge.invoke("get_version").then(version => {
      cachedVersion = version || "";
      return cachedVersion;
    });
    return readyPromise;
  }

  async function diff(files, options = {}) {
    if (disposed) {
      throw new Error("Native diff client disposed.");
    }
    if (current) {
      throw new Error("Diff already in progress.");
    }
    await ready();
    await ensureListener();
    const id = nextRequestId();
    return new Promise((resolve, reject) => {
      current = { id, resolve, reject };
      bridge
        .invoke("diff_paths_with_sheets", {
          old_path: files.oldPath,
          new_path: files.newPath,
          run_id: id,
          options
        })
        .then(payload => {
          if (!current || current.id !== id) return;
          current = null;
          resolve(payload);
        })
        .catch(err => {
          if (!current || current.id !== id) return;
          current = null;
          reject(err instanceof Error ? err : new Error(String(err)));
        });
    });
  }

  async function downloadJsonl() {
    throw new Error("JSONL download is only available in the web worker.");
  }

  function cancel() {
    if (!current) return false;
    const id = current.id;
    bridge.invoke("cancel_diff", { run_id: id }).catch(() => {});
    current.reject(new Error("Diff canceled."));
    current = null;
    return true;
  }

  function dispose() {
    disposed = true;
    if (unlisten) {
      unlisten();
      unlisten = null;
    }
    current = null;
  }

  return {
    ready,
    diff,
    cancel,
    dispose,
    downloadJsonl,
    loadSummary: loadNativeDiffSummary,
    loadSheetPayload: loadNativeSheetPayload,
    exportAuditXlsx: exportNativeAuditXlsx,
    runBatchCompare: runNativeBatchCompare,
    loadBatchSummary: loadNativeBatchSummary,
    getCapabilities: loadNativeCapabilities,
    searchDiffOps: searchNativeDiffOps,
    buildSearchIndex: buildNativeSearchIndex,
    searchWorkbookIndex: searchNativeWorkbookIndex
  };
}

```

---

### File: `web\platform.js`

```javascript
import { createDiffWorkerClient } from "./diff_worker_client.js";
import {
  createNativeDiffClient,
  getNativeBridge,
  loadNativeRecents,
  openNativeFileDialog,
  openNativeFolderDialog,
  saveNativeRecent,
  loadNativeDiffSummary,
  loadNativeSheetPayload,
  exportNativeAuditXlsx,
  runNativeBatchCompare,
  loadNativeBatchSummary,
  loadNativeCapabilities,
  searchNativeDiffOps,
  buildNativeSearchIndex,
  searchNativeWorkbookIndex
} from "./native_diff_client.js";

export function isDesktop() {
  return Boolean(getNativeBridge());
}

export function createAppDiffClient({ onStatus } = {}) {
  if (isDesktop()) {
    return createNativeDiffClient({ onStatus });
  }
  return createDiffWorkerClient({ onStatus });
}

export async function openFileDialog() {
  if (!isDesktop()) return null;
  return openNativeFileDialog();
}

export async function openFolderDialog() {
  if (!isDesktop()) return null;
  return openNativeFolderDialog();
}

export async function loadRecents() {
  if (!isDesktop()) return [];
  return loadNativeRecents();
}

export async function saveRecent(entry) {
  if (!isDesktop()) return [];
  return saveNativeRecent(entry);
}

export async function loadDiffSummary(diffId) {
  if (!isDesktop()) return null;
  return loadNativeDiffSummary(diffId);
}

export async function loadSheetPayload(diffId, sheetName) {
  if (!isDesktop()) return null;
  return loadNativeSheetPayload(diffId, sheetName);
}

export async function exportAuditXlsx(diffId) {
  if (!isDesktop()) return null;
  return exportNativeAuditXlsx(diffId);
}

export async function runBatchCompare(request) {
  if (!isDesktop()) return null;
  return runNativeBatchCompare(request);
}

export async function loadBatchSummary(batchId) {
  if (!isDesktop()) return null;
  return loadNativeBatchSummary(batchId);
}

export async function getCapabilities() {
  if (!isDesktop()) return null;
  return loadNativeCapabilities();
}

export async function searchDiffOps(diffId, query, limit) {
  if (!isDesktop()) return [];
  return searchNativeDiffOps(diffId, query, limit);
}

export async function buildSearchIndex(path, side) {
  if (!isDesktop()) return null;
  return buildNativeSearchIndex(path, side);
}

export async function searchWorkbookIndex(indexId, query, limit) {
  if (!isDesktop()) return [];
  return searchNativeWorkbookIndex(indexId, query, limit);
}

```

---

### File: `web\render.js`

```javascript
import { buildWorkbookViewModel } from "./view_model.js";

function esc(s) {
  return String(s)
    .replace(/&/g, "&amp;")
    .replace(/</g, "&lt;")
    .replace(/>/g, "&gt;")
    .replace(/"/g, "&quot;")
    .replace(/'/g, "&#39;");
}

function resolveString(report, id) {
  if (typeof id !== "number") return String(id);
  if (!report || !Array.isArray(report.strings)) return "<unknown>";
  return report.strings[id] != null ? report.strings[id] : "<unknown>";
}

function colToLetter(col) {
  let result = "";
  let c = col;
  while (c >= 0) {
    result = String.fromCharCode((c % 26) + 65) + result;
    c = Math.floor(c / 26) - 1;
  }
  return result;
}

function formatCellAddress(row, col) {
  return colToLetter(col) + (row + 1);
}

function parseCellAddress(addr) {
  if (!addr) return null;
  if (typeof addr === "object" && Number.isInteger(addr.row) && Number.isInteger(addr.col)) {
    return { row: addr.row, col: addr.col };
  }
  const match = /^([A-Z]+)(\d+)$/i.exec(String(addr).trim());
  if (!match) return null;
  const letters = match[1].toUpperCase();
  let col = 0;
  for (let i = 0; i < letters.length; i++) {
    col = col * 26 + (letters.charCodeAt(i) - 64);
  }
  const row = parseInt(match[2], 10) - 1;
  return { row, col: col - 1 };
}

function formatValue(report, val) {
  if (val === null || val === undefined) return "";
  if (val === "Blank") return "";
  if (typeof val === "object") {
    if (val.Number !== undefined) return String(val.Number);
    if (val.Text !== undefined) return resolveString(report, val.Text);
    if (val.Bool !== undefined) return val.Bool ? "TRUE" : "FALSE";
    if (val.Error !== undefined) return resolveString(report, val.Error);
    if (val.Formula !== undefined) return String(val.Formula);
    return JSON.stringify(val);
  }
  return String(val);
}

function truncateText(text, maxLen = 20) {
  const str = String(text ?? "");
  if (str.length > maxLen) return str.substring(0, maxLen - 3) + "...";
  return str;
}

function formatValueShort(report, val) {
  return truncateText(formatValue(report, val));
}

function resolveFormula(report, id) {
  if (id === null || id === undefined) return "";
  const text = resolveString(report, id);
  if (!text) return "";
  return text.startsWith("=") ? text : `=${text}`;
}

function buildCellMap(sheet) {
  const map = new Map();
  if (!sheet || !Array.isArray(sheet.cells)) return map;
  for (const cell of sheet.cells) {
    map.set(`${cell.row},${cell.col}`, cell);
  }
  return map;
}

function buildSheetLookup(sheets) {
  const map = new Map();
  if (!sheets) return map;
  const list = Array.isArray(sheets) ? sheets : sheets.sheets;
  if (!Array.isArray(list)) return map;
  for (const sheet of list) {
    map.set(sheet.name, sheet);
  }
  return map;
}

function buildAlignmentLookup(alignments) {
  const map = new Map();
  if (!Array.isArray(alignments)) return map;
  for (const alignment of alignments) {
    if (alignment && typeof alignment.sheet === "string") {
      map.set(alignment.sheet, alignment);
    }
  }
  return map;
}

function buildIndexMap(entries, key) {
  const map = new Map();
  if (!Array.isArray(entries)) return map;
  for (let i = 0; i < entries.length; i++) {
    const entry = entries[i];
    const value = entry ? entry[key] : null;
    if (value !== null && value !== undefined) {
      map.set(value, i);
    }
  }
  return map;
}

function formatAxisTitle(entry, axis) {
  if (!entry) return "";
  const hasOld = entry.old !== null && entry.old !== undefined;
  const hasNew = entry.new !== null && entry.new !== undefined;
  let oldLabel = "";
  let newLabel = "";
  if (axis === "row") {
    if (hasOld) oldLabel = `Old row ${entry.old + 1}`;
    if (hasNew) newLabel = `New row ${entry.new + 1}`;
  } else {
    if (hasOld) oldLabel = `Old col ${colToLetter(entry.old)}`;
    if (hasNew) newLabel = `New col ${colToLetter(entry.new)}`;
  }
  if (oldLabel && newLabel) return `${oldLabel} | ${newLabel}`;
  return oldLabel || newLabel || "";
}

function buildSheetGridData(report, ops, oldSheet, newSheet, alignment) {
  if (alignment) {
    return buildSheetGridDataAligned(report, ops, oldSheet, newSheet, alignment);
  }
  return buildSheetGridDataLegacy(report, ops, oldSheet, newSheet);
}

function buildSheetGridDataAligned(report, ops, oldSheet, newSheet, alignment) {
  const skipped = Boolean(alignment && alignment.skipped);
  if (skipped) {
    return { hasData: false, skipped: true, alignment };
  }

  const rows = Array.isArray(alignment?.rows) ? alignment.rows : [];
  const cols = Array.isArray(alignment?.cols) ? alignment.cols : [];

  const oldCells = buildCellMap(oldSheet);
  const newCells = buildCellMap(newSheet);

  const newRowToView = buildIndexMap(rows, "new");
  const newColToView = buildIndexMap(cols, "new");
  const cellEdits = new Map();

  for (const op of ops) {
    if (op.kind !== "CellEdited") continue;
    const addr = parseCellAddress(op.addr);
    if (!addr) continue;
    const viewRow = newRowToView.get(addr.row);
    const viewCol = newColToView.get(addr.col);
    if (viewRow === undefined || viewCol === undefined) continue;
    cellEdits.set(`${viewRow},${viewCol}`, {
      fromValue: op.from ? formatValue(report, op.from.value) : "",
      toValue: op.to ? formatValue(report, op.to.value) : "",
      fromFormula: resolveFormula(report, op.from?.formula),
      toFormula: resolveFormula(report, op.to?.formula)
    });
  }

  const hasData = rows.length > 0 && cols.length > 0;
  return {
    hasData,
    skipped: false,
    alignment,
    rows,
    cols,
    cellEdits,
    oldCells,
    newCells
  };
}

function buildSheetGridDataLegacy(report, ops, oldSheet, newSheet) {
  const cellEdits = new Map();
  const addedRows = new Set();
  const removedRows = new Set();
  const addedCols = new Set();
  const removedCols = new Set();

  let minRow = Infinity, maxRow = -1;
  let minCol = Infinity, maxCol = -1;
  let hasSheetOp = false;

  for (const op of ops) {
    if (op.kind === "CellEdited") {
      const addr = parseCellAddress(op.addr);
      if (!addr) continue;
      const r = addr.row;
      const c = addr.col;
      cellEdits.set(`${r},${c}`, {
        fromValue: op.from ? formatValue(report, op.from.value) : "",
        toValue: op.to ? formatValue(report, op.to.value) : "",
        fromFormula: resolveFormula(report, op.from?.formula),
        toFormula: resolveFormula(report, op.to?.formula)
      });
      minRow = Math.min(minRow, r);
      maxRow = Math.max(maxRow, r);
      minCol = Math.min(minCol, c);
      maxCol = Math.max(maxCol, c);
    } else if (op.kind === "RowAdded") {
      addedRows.add(op.row_idx);
      minRow = Math.min(minRow, op.row_idx);
      maxRow = Math.max(maxRow, op.row_idx);
    } else if (op.kind === "RowRemoved") {
      removedRows.add(op.row_idx);
      minRow = Math.min(minRow, op.row_idx);
      maxRow = Math.max(maxRow, op.row_idx);
    } else if (op.kind === "ColumnAdded") {
      addedCols.add(op.col_idx);
      minCol = Math.min(minCol, op.col_idx);
      maxCol = Math.max(maxCol, op.col_idx);
    } else if (op.kind === "ColumnRemoved") {
      removedCols.add(op.col_idx);
      minCol = Math.min(minCol, op.col_idx);
      maxCol = Math.max(maxCol, op.col_idx);
    } else if (op.kind === "RectReplaced") {
      minRow = Math.min(minRow, op.start_row);
      maxRow = Math.max(maxRow, op.start_row + op.row_count - 1);
      minCol = Math.min(minCol, op.start_col);
      maxCol = Math.max(maxCol, op.start_col + op.col_count - 1);
    } else if (op.kind === "BlockMovedRows") {
      minRow = Math.min(minRow, op.src_start_row, op.dst_start_row);
      maxRow = Math.max(maxRow, op.src_start_row + op.row_count - 1, op.dst_start_row + op.row_count - 1);
    } else if (op.kind === "BlockMovedColumns") {
      minCol = Math.min(minCol, op.src_start_col, op.dst_start_col);
      maxCol = Math.max(maxCol, op.src_start_col + op.col_count - 1, op.dst_start_col + op.col_count - 1);
    } else if (op.kind === "BlockMovedRect") {
      minRow = Math.min(minRow, op.src_start_row, op.dst_start_row);
      maxRow = Math.max(maxRow, op.src_start_row + op.src_row_count - 1, op.dst_start_row + op.src_row_count - 1);
      minCol = Math.min(minCol, op.src_start_col, op.dst_start_col);
      maxCol = Math.max(maxCol, op.src_start_col + op.src_col_count - 1, op.dst_start_col + op.src_col_count - 1);
    } else if (op.kind === "SheetAdded" || op.kind === "SheetRemoved" || op.kind === "SheetRenamed") {
      hasSheetOp = true;
    }
  }

  const hasChanges = hasSheetOp || cellEdits.size > 0 || addedRows.size > 0 || removedRows.size > 0 || addedCols.size > 0 || removedCols.size > 0;
  if (!hasChanges) {
    return { hasData: false };
  }

  const oldCells = buildCellMap(oldSheet);
  const newCells = buildCellMap(newSheet);

  const sheetRows = Math.max(oldSheet?.nrows || 0, newSheet?.nrows || 0);
  const sheetCols = Math.max(oldSheet?.ncols || 0, newSheet?.ncols || 0);
  if (sheetRows > 0 && sheetCols > 0) {
    return {
      cellEdits,
      addedRows,
      removedRows,
      addedCols,
      removedCols,
      oldCells,
      newCells,
      startRow: 0,
      endRow: sheetRows - 1,
      startCol: 0,
      endCol: sheetCols - 1,
      hasData: true
    };
  }

  if (minRow === Infinity) minRow = 0;
  if (maxRow === -1) maxRow = 0;
  if (minCol === Infinity) minCol = 0;
  if (maxCol === -1) maxCol = 0;

  const contextRows = 1;
  const contextCols = 1;
  const startRow = Math.max(0, minRow - contextRows);
  const endRow = maxRow + contextRows;
  const startCol = Math.max(0, minCol - contextCols);
  const endCol = maxCol + contextCols;

  return {
    cellEdits,
    addedRows,
    removedRows,
    addedCols,
    removedCols,
    oldCells,
    newCells,
    startRow,
    endRow,
    startCol,
    endCol,
    hasData: true
  };
}

function renderSheetGrid(report, gridData) {
  if (gridData.skipped) {
    return `
      <div class="grid-skip-warning">
        Grid preview skipped because the aligned view is too large or inconsistent.
      </div>
    `;
  }
  if (!gridData.hasData) return "";
  if (gridData.alignment) {
    return renderAlignedGrid(report, gridData);
  }
  return renderSheetGridLegacy(report, gridData);
}

function renderAlignedGrid(report, gridData) {
  const { rows, cols, cellEdits, oldCells, newCells } = gridData;
  if (!rows || !cols || rows.length === 0 || cols.length === 0) return "";

  const numCols = cols.length;

  function cellText(cell) {
    if (!cell) return "";
    const value = cell.value ?? "";
    const formula = cell.formula ?? "";
    return value || formula || "";
  }

  function cellTitle(label, value, formula) {
    const parts = [];
    if (value) parts.push(value);
    if (formula && formula != value) parts.push(formula);
    if (!parts.length) return "";
    return label ? `${label}: ${parts.join(" | ")}` : parts.join(" | ");
  }

  let html = `<div class="sheet-grid-container">
    <div class="sheet-grid" style="grid-template-columns: 50px repeat(${numCols}, minmax(100px, 1fr));">`;

  html += `<div class="grid-cell grid-corner"></div>`;
  for (let c = 0; c < cols.length; c++) {
    const colEntry = cols[c];
    const kind = colEntry?.kind;
    let cls = "grid-cell grid-col-header";
    if (kind === "insert") cls += " col-added";
    if (kind === "delete") cls += " col-removed";
    if (kind === "move_src") cls += " col-move-src";
    if (kind === "move_dst") cls += " col-move-dst";
    const title = formatAxisTitle(colEntry, "col");
    const moveAttr = colEntry?.move_id ? ` data-move-id="${esc(colEntry.move_id)}"` : "";
    html += `<div class="${cls}"${moveAttr} title="${esc(title)}">${colToLetter(c)}</div>`;
  }

  for (let r = 0; r < rows.length; r++) {
    const rowEntry = rows[r];
    const rowKind = rowEntry?.kind;
    let rowHeaderCls = "grid-cell grid-row-header";
    if (rowKind === "insert") rowHeaderCls += " row-added";
    if (rowKind === "delete") rowHeaderCls += " row-removed";
    if (rowKind === "move_src") rowHeaderCls += " row-move-src";
    if (rowKind === "move_dst") rowHeaderCls += " row-move-dst";
    const rowTitle = formatAxisTitle(rowEntry, "row");
    const rowMoveAttr = rowEntry?.move_id ? ` data-move-id="${esc(rowEntry.move_id)}"` : "";
    html += `<div class="${rowHeaderCls}"${rowMoveAttr} title="${esc(rowTitle)}">${r + 1}</div>`;

    for (let c = 0; c < cols.length; c++) {
      const colEntry = cols[c];
      const rowKind = rowEntry?.kind;
      const colKind = colEntry?.kind;
      const key = `${r},${c}`;
      const edit = cellEdits.get(key);
      const oldRow = rowEntry?.old;
      const newRow = rowEntry?.new;
      const oldCol = colEntry?.old;
      const newCol = colEntry?.new;

      const oldCell =
        oldRow !== null && oldRow !== undefined && oldCol !== null && oldCol !== undefined
          ? oldCells.get(`${oldRow},${oldCol}`)
          : null;
      const newCell =
        newRow !== null && newRow !== undefined && newCol !== null && newCol !== undefined
          ? newCells.get(`${newRow},${newCol}`)
          : null;

      const isMoveSrc = rowKind === "move_src" || colKind === "move_src";
      const isMoveDst = rowKind === "move_dst" || colKind === "move_dst";
      const isDelete = (rowKind === "delete" || colKind === "delete") && !isMoveSrc;
      const isInsert = (rowKind === "insert" || colKind === "insert") && !isMoveDst;

      let cls = "grid-cell";
      let content = "";
      let title = "";

      if (
        edit &&
        oldRow !== null && oldRow !== undefined &&
        newRow !== null && newRow !== undefined &&
        oldCol !== null && oldCol !== undefined &&
        newCol !== null && newCol !== undefined
      ) {
        cls += " cell-edited";
        const fromText = edit.fromValue || edit.fromFormula || "(empty)";
        const toText = edit.toValue || edit.toFormula || "(empty)";
        content = `<div class="cell-change"><span class="cell-old">${esc(truncateText(fromText))}</span><span class="cell-new">${esc(truncateText(toText))}</span></div>`;
        title = `Changed: ${fromText} -> ${toText}`;
      } else if (isMoveSrc && (cellText(oldCell) || oldCell?.formula)) {
        cls += " cell-move-src";
        const oldValue = oldCell?.value ?? "";
        const oldFormula = oldCell?.formula ?? "";
        const display = cellText(oldCell);
        content = esc(truncateText(display));
        title = cellTitle("Moved (from)", oldValue, oldFormula);
      } else if (isMoveDst && (cellText(newCell) || newCell?.formula)) {
        cls += " cell-move-dst";
        const newValue = newCell?.value ?? "";
        const newFormula = newCell?.formula ?? "";
        const display = cellText(newCell);
        content = esc(truncateText(display));
        title = cellTitle("Moved (to)", newValue, newFormula);
      } else if (isDelete && (cellText(oldCell) || oldCell?.formula)) {
        cls += " cell-removed";
        const oldValue = oldCell?.value ?? "";
        const oldFormula = oldCell?.formula ?? "";
        const display = cellText(oldCell);
        content = esc(truncateText(display));
        title = cellTitle("Removed", oldValue, oldFormula);
      } else if (isInsert && (cellText(newCell) || newCell?.formula)) {
        cls += " cell-added";
        const newValue = newCell?.value ?? "";
        const newFormula = newCell?.formula ?? "";
        const display = cellText(newCell);
        content = esc(truncateText(display));
        title = cellTitle("Added", newValue, newFormula);
      } else if (cellText(newCell) || cellText(oldCell)) {
        cls += " cell-unchanged";
        const newValue = newCell?.value ?? "";
        const newFormula = newCell?.formula ?? "";
        const oldValue = oldCell?.value ?? "";
        const oldFormula = oldCell?.formula ?? "";
        const display = cellText(newCell) || cellText(oldCell);
        const titleValue = newValue || oldValue;
        const titleFormula = newFormula || oldFormula;
        content = esc(truncateText(display));
        title = cellTitle("Value", titleValue, titleFormula);
      } else {
        cls += " cell-empty";
      }

      html += `<div class="${cls}" title="${esc(title)}">${content}</div>`;
    }
  }

  html += `</div></div>`;

  html += `<div class="grid-legend">
    <span class="legend-item"><span class="legend-box legend-edited"></span> Modified</span>
    <span class="legend-item"><span class="legend-box legend-added"></span> Added row/col</span>
    <span class="legend-item"><span class="legend-box legend-removed"></span> Removed row/col</span>
    <span class="legend-item"><span class="legend-box legend-moved"></span> Moved row/col</span>
  </div>`;

  return html;
}

function renderSheetGridLegacy(report, gridData) {
  if (!gridData.hasData) return "";

  const { cellEdits, addedRows, removedRows, addedCols, removedCols, startRow, endRow, startCol, endCol, oldCells, newCells } = gridData;

  const numCols = endCol - startCol + 1;

  function cellText(cell) {
    if (!cell) return "";
    const value = cell.value ?? "";
    const formula = cell.formula ?? "";
    return value || formula || "";
  }

  function cellTitle(label, value, formula) {
    const parts = [];
    if (value) parts.push(value);
    if (formula && formula != value) parts.push(formula);
    if (!parts.length) return "";
    return label ? `${label}: ${parts.join(" | ")}` : parts.join(" | ");
  }

  let html = `<div class="sheet-grid-container">
    <div class="sheet-grid" style="grid-template-columns: 50px repeat(${numCols}, minmax(100px, 1fr));">`;

  html += `<div class="grid-cell grid-corner"></div>`;
  for (let c = startCol; c <= endCol; c++) {
    const isAdded = addedCols.has(c);
    const isRemoved = removedCols.has(c);
    let cls = "grid-cell grid-col-header";
    if (isAdded) cls += " col-added";
    if (isRemoved) cls += " col-removed";
    html += `<div class="${cls}">${colToLetter(c)}${isAdded ? " ?os" : ""}${isRemoved ? " ?o" : ""}</div>`;
  }

  for (let r = startRow; r <= endRow; r++) {
    const rowAdded = addedRows.has(r);
    const rowRemoved = removedRows.has(r);

    let rowHeaderCls = "grid-cell grid-row-header";
    if (rowAdded) rowHeaderCls += " row-added";
    if (rowRemoved) rowHeaderCls += " row-removed";
    html += `<div class="${rowHeaderCls}">${r + 1}${rowAdded ? " ?os" : ""}${rowRemoved ? " ?o" : ""}</div>`;

    for (let c = startCol; c <= endCol; c++) {
      const key = `${r},${c}`;
      const edit = cellEdits.get(key);
      const colAdded = addedCols.has(c);
      const colRemoved = removedCols.has(c);
      const oldCell = oldCells.get(key);
      const newCell = newCells.get(key);

      let cls = "grid-cell";
      let content = "";
      let title = "";

      if (edit) {
        cls += " cell-edited";
        const fromText = edit.fromValue || edit.fromFormula || "(empty)";
        const toText = edit.toValue || edit.toFormula || "(empty)";
        content = `<div class="cell-change"><span class="cell-old">${esc(truncateText(fromText))}</span><span class="cell-new">${esc(truncateText(toText))}</span></div>`;
        title = `Changed: ${fromText} ?+' ${toText}`;
      } else if ((rowRemoved || colRemoved) && (cellText(oldCell) || oldCell?.formula)) {
        cls += " cell-removed";
        const oldValue = oldCell?.value ?? "";
        const oldFormula = oldCell?.formula ?? "";
        const display = cellText(oldCell);
        content = esc(truncateText(display));
        title = cellTitle("Removed", oldValue, oldFormula);
      } else if ((rowAdded || colAdded) && (cellText(newCell) || newCell?.formula)) {
        cls += " cell-added";
        const newValue = newCell?.value ?? "";
        const newFormula = newCell?.formula ?? "";
        const display = cellText(newCell);
        content = esc(truncateText(display));
        title = cellTitle("Added", newValue, newFormula);
      } else if (cellText(newCell) || cellText(oldCell)) {
        cls += " cell-unchanged";
        const newValue = newCell?.value ?? "";
        const newFormula = newCell?.formula ?? "";
        const oldValue = oldCell?.value ?? "";
        const oldFormula = oldCell?.formula ?? "";
        const display = cellText(newCell) || cellText(oldCell);
        const titleValue = newValue || oldValue;
        const titleFormula = newFormula || oldFormula;
        content = esc(truncateText(display));
        title = cellTitle("Value", titleValue, titleFormula);
      } else {
        cls += " cell-empty";
      }

      html += `<div class="${cls}" title="${esc(title)}">${content}</div>`;
    }
  }

  html += `</div></div>`;

  html += `<div class="grid-legend">
    <span class="legend-item"><span class="legend-box legend-edited"></span> Modified</span>
    <span class="legend-item"><span class="legend-box legend-added"></span> Added row/col</span>
    <span class="legend-item"><span class="legend-box legend-removed"></span> Removed row/col</span>
  </div>`;

  return html;
}

function categorizeOps(report) {
  const ops = Array.isArray(report.ops) ? report.ops : [];
  
  const sheetOps = new Map();
  const vbaOps = [];
  const namedRangeOps = [];
  const chartOps = [];
  const queryOps = [];
  const modelOps = [];
  
  let addedCount = 0;
  let removedCount = 0;
  let modifiedCount = 0;
  let movedCount = 0;
  
  for (const op of ops) {
    const kind = op.kind;
    
    if (kind === "SheetAdded" || kind === "SheetRemoved") {
      const sheetName = resolveString(report, op.sheet);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      if (kind === "SheetAdded") addedCount++;
      else removedCount++;
    } else if (kind === "SheetRenamed") {
      const sheetName = resolveString(report, op.sheet ?? op.to);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      modifiedCount++;
    } else if (kind.startsWith("Row") || kind.startsWith("Column") || kind.startsWith("Cell") || kind.startsWith("Block") || kind.startsWith("Rect")) {
      const sheetName = resolveString(report, op.sheet);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else if (kind.includes("Moved")) movedCount++;
      else if (kind.includes("Edited") || kind.includes("Changed") || kind.includes("Replaced")) modifiedCount++;
    } else if (kind.startsWith("Vba")) {
      vbaOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("NamedRange")) {
      namedRangeOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("Chart")) {
      chartOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("Query")) {
      queryOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (
      kind === "CalculatedColumnDefinitionChanged" ||
      kind.startsWith("Table") ||
      kind.startsWith("ModelColumn") ||
      kind.startsWith("Relationship") ||
      kind.startsWith("Measure")
    ) {
      modelOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    }
  }
  
  return {
    sheetOps,
    vbaOps,
    namedRangeOps,
    chartOps,
    queryOps,
    modelOps,
    counts: { added: addedCount, removed: removedCount, modified: modifiedCount, moved: movedCount }
  };
}

function renderSummaryCards(counts) {
  const total = counts.added + counts.removed + counts.modified + counts.moved;
  if (total === 0) {
    return `
      <div class="no-changes">
        <div class="no-changes-icon"></div>
        <h2>No Differences Found</h2>
        <p>The two files are identical.</p>
      </div>
    `;
  }
  
  let html = '<div class="summary-cards">';
  
  if (counts.added > 0) {
    html += `
      <div class="summary-card added">
        <div class="count">${counts.added}</div>
        <div class="label">Added</div>
      </div>
    `;
  }
  
  if (counts.removed > 0) {
    html += `
      <div class="summary-card removed">
        <div class="count">${counts.removed}</div>
        <div class="label">Removed</div>
      </div>
    `;
  }
  
  if (counts.modified > 0) {
    html += `
      <div class="summary-card modified">
        <div class="count">${counts.modified}</div>
        <div class="label">Modified</div>
      </div>
    `;
  }
  
  if (counts.moved > 0) {
    html += `
      <div class="summary-card moved">
        <div class="count">${counts.moved}</div>
        <div class="label">Moved</div>
      </div>
    `;
  }
  
  html += '</div>';
  return html;
}

function renderSheetOp(report, op) {
  const kind = op.kind;
  
  if (kind === "SheetAdded") {
    return `
      <div class="change-item added">
        <div class="change-icon">+</div>
        <span>Sheet added</span>
      </div>
    `;
  }
  
  if (kind === "SheetRenamed") {
    const fromName = resolveString(report, op.from);
    const toName = resolveString(report, op.to ?? op.sheet);
    return `
      <div class="change-item modified">
        <div class="change-icon">~</div>
        <span>Sheet renamed: ${esc(fromName)} &rarr; ${esc(toName)}</span>
      </div>
    `;
  }

  if (kind === "SheetRemoved") {
    return `
      <div class="change-item removed">
        <div class="change-icon"></div>
        <span>Sheet removed</span>
      </div>
    `;
  }
  
  if (kind === "RowAdded") {
    return `
      <div class="change-item added">
        <div class="change-icon">+</div>
        <span class="change-location">Row ${op.row_idx + 1}</span>
        <span class="change-detail">Row added</span>
      </div>
    `;
  }
  
  if (kind === "RowRemoved") {
    return `
      <div class="change-item removed">
        <div class="change-icon"></div>
        <span class="change-location">Row ${op.row_idx + 1}</span>
        <span class="change-detail">Row removed</span>
      </div>
    `;
  }
  
  if (kind === "RowReplaced") {
    return `
      <div class="change-item modified">
        <div class="change-icon">~</div>
        <span class="change-location">Row ${op.row_idx + 1}</span>
        <span class="change-detail">Row replaced</span>
      </div>
    `;
  }
  
  if (kind === "ColumnAdded") {
    return `
      <div class="change-item added">
        <div class="change-icon">+</div>
        <span class="change-location">Column ${colToLetter(op.col_idx)}</span>
        <span class="change-detail">Column added</span>
      </div>
    `;
  }
  
  if (kind === "ColumnRemoved") {
    return `
      <div class="change-item removed">
        <div class="change-icon"></div>
        <span class="change-location">Column ${colToLetter(op.col_idx)}</span>
        <span class="change-detail">Column removed</span>
      </div>
    `;
  }
  
  if (kind === "CellEdited") {
    const addr = typeof op.addr === "string" ? op.addr : formatCellAddress(op.addr.row, op.addr.col);
    const fromVal = op.from ? formatValue(report, op.from.value) : "<empty>";
    const toVal = op.to ? formatValue(report, op.to.value) : "<empty>";
    const fromFormula = op.from?.formula;
    const toFormula = op.to?.formula;
    
    let detail = "";
    if (fromFormula || toFormula) {
      const oldF = fromFormula ? esc(resolveFormula(report, fromFormula)) : "";
      const newF = toFormula ? esc(resolveFormula(report, toFormula)) : "";
      if (oldF && newF && oldF !== newF) {
        detail = `
          <span class="change-value old">${oldF}</span>
          <span class="change-arrow"></span>
          <span class="change-value">${newF}</span>
        `;
      } else if (oldF && !newF) {
        detail = `<span class="change-value old">${oldF}</span> <span class="change-arrow"></span> <span class="change-value">${esc(toVal)}</span>`;
      } else if (!oldF && newF) {
        detail = `<span class="change-value old">${esc(fromVal)}</span> <span class="change-arrow"></span> <span class="change-value">${newF}</span>`;
      } else {
        detail = `<span class="change-value">${newF || esc(toVal)}</span>`;
      }
    } else {
      detail = `
        <span class="change-value old">${esc(fromVal)}</span>
        <span class="change-arrow"></span>
        <span class="change-value">${esc(toVal)}</span>
      `;
    }
    
    return `
      <div class="change-item modified">
        <div class="change-icon">~</div>
        <span class="change-location">${addr}</span>
        <div class="change-detail">${detail}</div>
      </div>
    `;
  }
  
  if (kind === "BlockMovedRows") {
    const count = op.row_count;
    const from = op.src_start_row + 1;
    const to = op.dst_start_row + 1;
    return `
      <div class="change-item moved">
        <div class="change-icon"></div>
        <span class="change-location">Rows ${from}${from + count - 1}</span>
        <div class="change-detail">
          Moved to row ${to}
        </div>
      </div>
    `;
  }
  
  if (kind === "BlockMovedColumns") {
    const count = op.col_count;
    const from = colToLetter(op.src_start_col);
    const to = colToLetter(op.dst_start_col);
    const fromEnd = colToLetter(op.src_start_col + count - 1);
    return `
      <div class="change-item moved">
        <div class="change-icon"></div>
        <span class="change-location">Columns ${from}${fromEnd}</span>
        <div class="change-detail">
          Moved to column ${to}
        </div>
      </div>
    `;
  }
  
  if (kind === "RectReplaced") {
    const startAddr = formatCellAddress(op.start_row, op.start_col);
    const endAddr = formatCellAddress(op.start_row + op.row_count - 1, op.start_col + op.col_count - 1);
    return `
      <div class="change-item modified">
        <div class="change-icon">~</div>
        <span class="change-location">${startAddr}:${endAddr}</span>
        <div class="change-detail">Region replaced</div>
      </div>
    `;
  }
  
  return `
    <div class="change-item modified">
      <div class="change-icon">?</div>
      <span>${esc(kind)}</span>
    </div>
  `;
}

function renderSheetSection(report, sheetName, ops, sheetLookup, alignmentLookup) {
  const adds = ops.filter(o => o.kind.includes("Added")).length;
  const removes = ops.filter(o => o.kind.includes("Removed")).length;
  const mods = ops.filter(o => o.kind.includes("Edited") || o.kind.includes("Changed") || o.kind.includes("Replaced")).length;
  const moves = ops.filter(o => o.kind.includes("Moved")).length;
  
  let badge = `${ops.length} change${ops.length !== 1 ? "s" : ""}`;
  
  const rowOps = ops.filter(o => o.kind.startsWith("Row"));
  const colOps = ops.filter(o => o.kind.startsWith("Column"));
  const cellOps = ops.filter(o => o.kind === "CellEdited");
  const moveOps = ops.filter(o => o.kind.startsWith("Block"));
  const otherOps = ops.filter(o => !o.kind.startsWith("Row") && !o.kind.startsWith("Column") && o.kind !== "CellEdited" && !o.kind.startsWith("Block") && o.kind !== "SheetAdded" && o.kind !== "SheetRemoved" && o.kind !== "SheetRenamed");
  
  const oldSheet = sheetLookup ? sheetLookup.old.get(sheetName) : null;
  const newSheet = sheetLookup ? sheetLookup.new.get(sheetName) : null;
  const alignment = alignmentLookup ? alignmentLookup.get(sheetName) : null;
  const gridData = buildSheetGridData(report, ops, oldSheet, newSheet, alignment);
  const gridHtml = renderSheetGrid(report, gridData);
  
  let contentHtml = "";
  
  if (gridHtml) {
    contentHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Visual Diff</span>
        </div>
        ${gridHtml}
      </div>
    `;
  }
  
  let detailsHtml = "";
  
  if (rowOps.length > 0) {
    detailsHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Row Changes (${rowOps.length})</span>
        </div>
        <div class="change-list">
          ${rowOps.map(op => renderSheetOp(report, op)).join("")}
        </div>
      </div>
    `;
  }
  
  if (colOps.length > 0) {
    detailsHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Column Changes (${colOps.length})</span>
        </div>
        <div class="change-list">
          ${colOps.map(op => renderSheetOp(report, op)).join("")}
        </div>
      </div>
    `;
  }
  
  if (cellOps.length > 0) {
    detailsHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Cell Changes (${cellOps.length})</span>
        </div>
        <div class="change-list">
          ${cellOps.map(op => renderSheetOp(report, op)).join("")}
        </div>
      </div>
    `;
  }
  
  if (moveOps.length > 0) {
    detailsHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Moved Blocks (${moveOps.length})</span>
        </div>
        <div class="change-list">
          ${moveOps.map(op => renderSheetOp(report, op)).join("")}
        </div>
      </div>
    `;
  }
  
  if (otherOps.length > 0) {
    detailsHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span></span>
          <span>Other Changes (${otherOps.length})</span>
        </div>
        <div class="change-list">
          ${otherOps.map(op => renderSheetOp(report, op)).join("")}
        </div>
      </div>
    `;
  }
  
  if (detailsHtml) {
    contentHtml += `
      <details class="details-section" open>
        <summary class="details-toggle">Detailed Changes</summary>
        <div class="details-content">
          ${detailsHtml}
        </div>
      </details>
    `;
  }
  
  return `
    <section class="sheet-section" data-sheet="${esc(sheetName)}">
      <div class="sheet-header">
        <div class="sheet-title">
          <div class="sheet-icon"></div>
          <span class="sheet-name">${esc(sheetName)}</span>
          <span class="sheet-badge">${badge}</span>
        </div>
        <svg class="expand-icon" width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
          <path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z" />
        </svg>
      </div>
      <div class="sheet-content">
        ${contentHtml}
      </div>
    </section>
  `;
}

function renderOtherOp(report, op) {
  const kind = op.kind;
  const name = op.name !== undefined ? resolveString(report, op.name) : "";
  
  if (kind.includes("Added")) {
    return `
      <div class="change-item added">
        <div class="change-icon">+</div>
        <span>${esc(kind.replace("Added", ""))}: ${esc(name)}</span>
      </div>
    `;
  }
  
  if (kind.includes("Removed")) {
    return `
      <div class="change-item removed">
        <div class="change-icon"></div>
        <span>${esc(kind.replace("Removed", ""))}: ${esc(name)}</span>
      </div>
    `;
  }
  
  if (kind.includes("Changed") || kind.includes("Renamed")) {
    return `
      <div class="change-item modified">
        <div class="change-icon">~</div>
        <span>${esc(kind.replace("Changed", "").replace("Renamed", ""))}: ${esc(name)}</span>
      </div>
    `;
  }
  
  return `
    <div class="change-item modified">
      <div class="change-icon">?</div>
      <span>${esc(kind)}: ${esc(name)}</span>
    </div>
  `;
}

function renderOtherChangesSection(report, title, icon, ops) {
  if (ops.length === 0) return "";
  
  return `
    <div class="other-changes">
      <div class="other-changes-title">
        <span class="icon">${icon}</span>
        <span>${esc(title)} (${ops.length})</span>
      </div>
      <div class="change-list">
        ${ops.map(op => renderOtherOp(report, op)).join("")}
      </div>
    </div>
  `;
}

function renderWarnings(warnings) {
  if (!warnings || warnings.length === 0) return "";
  
  return `
    <div class="warnings-section">
      <div class="warnings-title">
        <span></span>
        <span>Warnings</span>
      </div>
      <ul class="warnings-list">
        ${warnings.map(w => `<li>${esc(w)}</li>`).join("")}
      </ul>
    </div>
  `;
}

function renderPreviewLimitations(vm) {
  const hasLimitations = vm.sheets?.some(sheet => sheet.renderPlan?.status?.kind && sheet.renderPlan.status.kind !== "ok");
  if (!hasLimitations) return "";
  return `
    <div class="preview-limitations">
      <div class="preview-limitations-title">Preview limitations</div>
      <p><strong>Partial</strong> means the preview is limited for performance; edited cells remain exact.</p>
      <p><strong>Skipped</strong> means the aligned view was too large or inconsistent to render. The change list is still valid.</p>
      <p><strong>Missing</strong> means snapshots or alignment data were not available for that sheet.</p>
    </div>
  `;
}

function renderReviewToolbar(vm) {
  if (!vm.sheets || vm.sheets.length === 0) return "";
  return `
    <div class="review-toolbar">
      <div class="review-toolbar-left">
        <div class="toolbar-field">
          <label class="toolbar-label" for="sheetSearch">Sheet search</label>
          <input type="search" id="sheetSearch" class="sheet-search" placeholder="Search sheets" />
        </div>
        <div class="toolbar-actions">
          <button type="button" class="review-nav-btn" data-review-nav="prev">Prev change</button>
          <button type="button" class="review-nav-btn" data-review-nav="next">Next change</button>
        </div>
      </div>
      <div class="review-toolbar-right">
        <label class="toolbar-toggle">
          <input type="checkbox" data-filter="focus-rows" />
          Show only changed rows
        </label>
        <label class="toolbar-toggle">
          <input type="checkbox" data-filter="focus-cols" />
          Show only changed columns
        </label>
        <label class="toolbar-toggle">
          <input type="checkbox" data-filter="ignore-blank" checked />
          Ignore blank-to-blank
        </label>
        <label class="toolbar-select">
          <span>Display</span>
          <select data-filter="content-mode">
            <option value="values">Values</option>
            <option value="formulas">Formulas</option>
            <option value="both">Both</option>
          </select>
        </label>
      </div>
    </div>
  `;
}

function renderSheetIndex(vm) {
  if (!vm.sheets || vm.sheets.length === 0) return "";
  const maxOps = Math.max(1, ...vm.sheets.map(sheet => sheet.opCount || 0));
  const maxAnchors = Math.max(1, ...vm.sheets.map(sheet => (sheet.changes?.anchors || []).length));
  const maxDensity = Math.max(maxOps, maxAnchors);

  let html = `
    <div class="sheet-index">
      <div class="sheet-index-title">Sheet Index</div>
      <div class="sheet-index-list">
  `;

  for (const sheet of vm.sheets) {
    const anchorCount = sheet.changes?.anchors ? sheet.changes.anchors.length : 0;
    const densityValue = Math.max(anchorCount, sheet.opCount || 0);
    const densityPct = Math.round((densityValue / maxDensity) * 100);
    const statusKind = sheet.renderPlan?.status?.kind || "ok";
    const statusLabel = statusKind === "ok" ? "OK" : statusKind.toUpperCase();
    const statusTitle = sheet.renderPlan?.status?.message ? ` title="${esc(sheet.renderPlan.status.message)}"` : "";

    html += `
      <button type="button" class="sheet-index-item" data-sheet="${esc(sheet.name)}">
        <div class="sheet-index-main">
          <span class="sheet-index-name">${esc(sheet.name)}</span>
          <span class="sheet-index-badges">
            <span class="sheet-index-badge">${sheet.opCount}</span>
            <span class="sheet-index-badge">${anchorCount}</span>
          </span>
        </div>
        <div class="sheet-index-meta">
          <div class="density-bar"><span style="width: ${densityPct}%"></span></div>
          <span class="status-pill ${statusKind}"${statusTitle}>${statusLabel}</span>
        </div>
      </button>
    `;
  }

  html += `
      </div>
    </div>
  `;
  return html;
}


function renderChangeItemVm(item, sheetName) {
  const changeType = item.changeType || "modified";
  const cls = `change-item ${changeType}`;
  const icon = changeType === "added" ? "+" : changeType === "removed" ? "-" : changeType === "moved" ? ">" : "~";
  const detail = item.detail ? `<span class="change-detail">${esc(item.detail)}</span>` : "";
  const navTargets = Array.isArray(item.navTargets) ? item.navTargets : [];
  const actions = navTargets.length
    ? `<div class="change-actions">${navTargets
        .map(target => `<button type="button" class="change-jump" data-sheet="${esc(sheetName)}" data-anchor="${esc(target.anchorId)}">${esc(target.label || "Jump")}</button>`)
        .join("")}</div>`
    : "";
  const itemId = `change-${sheetName}-${item.id}`;
  return `
    <div class="${cls}" id="${esc(itemId)}">
      <div class="change-icon">${icon}</div>
      <span class="change-location">${esc(item.label || "")}</span>
      ${detail}
      ${actions}
    </div>
  `;
}

function renderChangeGroupVm(title, icon, items, sheetName) {
  if (!items || items.length === 0) return "";
  return `
    <div class="change-group">
      <div class="change-group-title">
        <span>${icon}</span>
        <span>${esc(title)} (${items.length})</span>
      </div>
      <div class="change-list">
        ${items.map(item => renderChangeItemVm(item, sheetName)).join("")}
      </div>
    </div>
  `;
}

function renderGridLegend() {
  return `
    <div class="grid-legend">
      <span class="legend-item"><span class="legend-box legend-edited"></span> Modified</span>
      <span class="legend-item"><span class="legend-box legend-added"></span> Added row/col</span>
      <span class="legend-item"><span class="legend-box legend-removed"></span> Removed row/col</span>
      <span class="legend-item"><span class="legend-box legend-moved"></span> Moved row/col</span>
    </div>
  `;
}

function renderRegionGrid(sheetVm, region) {
  const rows = sheetVm.axis.rows.entries;
  const cols = sheetVm.axis.cols.entries;
  const bounds = region.renderBounds || region;
  if (!bounds || rows.length === 0 || cols.length === 0) return "";

  const numCols = bounds.right - bounds.left + 1;

  let html = `<div class="sheet-grid-container">
    <div class="sheet-grid" style="grid-template-columns: 50px repeat(${numCols}, minmax(100px, 1fr));">`;

  html += `<div class="grid-cell grid-corner"></div>`;
  for (let c = bounds.left; c <= bounds.right; c++) {
    const colEntry = cols[c];
    const kind = colEntry?.kind;
    let cls = "grid-cell grid-col-header";
    if (kind === "insert") cls += " col-added";
    if (kind === "delete") cls += " col-removed";
    if (kind === "move_src") cls += " col-move-src";
    if (kind === "move_dst") cls += " col-move-dst";
    const title = formatAxisTitle(colEntry, "col");
    const moveAttr = colEntry?.move_id ? ` data-move-id="${esc(colEntry.move_id)}"` : "";
    html += `<div class="${cls}"${moveAttr} title="${esc(title)}">${colToLetter(c)}</div>`;
  }

  for (let r = bounds.top; r <= bounds.bottom; r++) {
    const rowEntry = rows[r];
    const rowKind = rowEntry?.kind;
    let rowHeaderCls = "grid-cell grid-row-header";
    if (rowKind === "insert") rowHeaderCls += " row-added";
    if (rowKind === "delete") rowHeaderCls += " row-removed";
    if (rowKind === "move_src") rowHeaderCls += " row-move-src";
    if (rowKind === "move_dst") rowHeaderCls += " row-move-dst";
    const rowTitle = formatAxisTitle(rowEntry, "row");
    const rowMoveAttr = rowEntry?.move_id ? ` data-move-id="${esc(rowEntry.move_id)}"` : "";
    html += `<div class="${rowHeaderCls}"${rowMoveAttr} title="${esc(rowTitle)}">${r + 1}</div>`;

    for (let c = bounds.left; c <= bounds.right; c++) {
      const cell = sheetVm.cellAt(r, c);
      let cls = "grid-cell";
      let content = "";
      let title = cell.display.tooltip || "";

      if (cell.diffKind === "edited") {
        cls += " cell-edited";
        const fromText = cell.edit?.fromValue || cell.edit?.fromFormula || "(empty)";
        const toText = cell.edit?.toValue || cell.edit?.toFormula || "(empty)";
        content = `<div class="cell-change"><span class="cell-old">${esc(truncateText(fromText))}</span><span class="cell-new">${esc(truncateText(toText))}</span></div>`;
        title = `Changed: ${fromText} -> ${toText}`;
      } else if (cell.diffKind === "added") {
        cls += " cell-added";
        content = esc(truncateText(cell.display.text || ""));
      } else if (cell.diffKind === "removed") {
        cls += " cell-removed";
        content = esc(truncateText(cell.display.text || ""));
      } else if (cell.diffKind === "moved") {
        cls += cell.moveRole === "src" ? " cell-move-src" : " cell-move-dst";
        content = esc(truncateText(cell.display.text || ""));
      } else if (cell.diffKind === "unchanged") {
        cls += " cell-unchanged";
        content = esc(truncateText(cell.display.text || ""));
      } else {
        cls += " cell-empty";
      }

      html += `<div class="${cls}" title="${esc(title)}">${content}</div>`;
    }
  }

  html += `</div></div>`;
  return html;
}

function renderSheetGridVm(sheetVm) {
  const status = sheetVm.renderPlan.status;
  if (status.kind === "missing" && sheetVm.changes.regions.length === 0) {
    return "";
  }
  if (status.kind === "skipped" || status.kind === "missing") {
    return `
      <div class="grid-skip-warning">
        ${esc(status.message || "Grid preview unavailable.")}
      </div>
    `;
  }

  const warningHtml =
    status.kind === "partial"
      ? `
        <div class="grid-partial-warning">
          ${esc(status.message || "Preview limited for performance; edited cells remain exact.")}
        </div>
      `
      : "";

  const regionIds = sheetVm.renderPlan.regionsToRender || [];
  const hasGridAnchors = Array.isArray(sheetVm.changes?.anchors)
    ? sheetVm.changes.anchors.some(anchor => anchor.target?.kind === "grid")
    : false;
  if (regionIds.length === 0 && !hasGridAnchors) return "";

  const initialAnchor = Array.isArray(sheetVm.changes?.anchors)
    ? sheetVm.changes.anchors.find(anchor => anchor.target?.kind === "grid")
    : null;
  const initialAnchorId = initialAnchor ? initialAnchor.id : "0";

  return `
    ${warningHtml}
    <div class="grid-viewer-mount" data-sheet="${esc(sheetVm.name)}" data-initial-mode="side_by_side" data-initial-anchor="${esc(initialAnchorId)}"></div>
    ${renderGridLegend()}
  `;
}

function renderSheetVm(sheetVm) {
  const badge = `${sheetVm.opCount} change${sheetVm.opCount !== 1 ? "s" : ""}`;
  const anchorCount = sheetVm.changes?.anchors ? sheetVm.changes.anchors.length : 0;
  const anchorBadge = anchorCount > 0
    ? `<span class="sheet-badge anchor-badge">${anchorCount} anchor${anchorCount !== 1 ? "s" : ""}</span>`
    : "";
  const status = sheetVm.renderPlan.status || { kind: "ok" };
  const statusLabel = status.kind === "ok" ? "OK" : status.kind.toUpperCase();
  const statusTitle = status.message ? ` title="${esc(status.message)}"` : "";
  const statusPill = `<button type="button" class="status-pill ${status.kind}" data-sheet="${esc(sheetVm.name)}"${statusTitle}>${statusLabel}</button>`;
  const gridHtml = renderSheetGridVm(sheetVm);

  let contentHtml = "";
  if (gridHtml) {
    contentHtml += `
      <div class="change-group">
        <div class="change-group-title">
          <span>*</span>
          <span>Visual Diff</span>
        </div>
        ${gridHtml}
      </div>
    `;
  }

  const rowItems = sheetVm.changes.items.filter(item => item.group === "rows");
  const colItems = sheetVm.changes.items.filter(item => item.group === "cols");
  const cellItems = sheetVm.changes.items.filter(item => item.group === "cells");
  const moveItems = sheetVm.changes.items.filter(item => item.group === "moves");
  const otherItems = sheetVm.changes.items.filter(item => item.group === "other");

  let detailsHtml = "";
  detailsHtml += renderChangeGroupVm("Row Changes", "R", rowItems, sheetVm.name);
  detailsHtml += renderChangeGroupVm("Column Changes", "C", colItems, sheetVm.name);
  detailsHtml += renderChangeGroupVm("Cell Changes", "*", cellItems, sheetVm.name);
  detailsHtml += renderChangeGroupVm("Moved Blocks", ">", moveItems, sheetVm.name);
  detailsHtml += renderChangeGroupVm("Other Changes", "?", otherItems, sheetVm.name);

  if (detailsHtml) {
    contentHtml += `
      <details class="details-section" open>
        <summary class="details-toggle">Detailed Changes</summary>
        <div class="details-content">
          ${detailsHtml}
        </div>
      </details>
    `;
  }

  return `
    <section class="sheet-section" data-sheet="${esc(sheetVm.name)}">
      <div class="sheet-header">
        <div class="sheet-title">
          <div class="sheet-icon">#</div>
          <span class="sheet-name">${esc(sheetVm.name)}</span>
          <span class="sheet-badge">${badge}</span>
          ${anchorBadge}
          ${statusPill}
        </div>
        <svg class="expand-icon" width="20" height="20" viewBox="0 0 20 20" fill="currentColor">
          <path fill-rule="evenodd" d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z" />
        </svg>
      </div>
      <div class="sheet-content">
        ${contentHtml}
      </div>
    </section>
  `;
}

function renderOtherChangesVm(title, icon, items) {
  if (!items || items.length === 0) return "";
  return `
    <div class="other-changes">
      <div class="other-changes-title">
        <span class="icon">${icon}</span>
        <span>${esc(title)} (${items.length})</span>
      </div>
      <div class="change-list">
        ${items.map(item => renderChangeItemVm(item, "workbook")).join("")}
      </div>
    </div>
  `;
}

export function renderWorkbookVm(vm) {
  let html = "";
  html += renderWarnings(vm.warnings);
  html += renderPreviewLimitations(vm);
  html += renderSummaryCards(vm.counts);

  const total = vm.counts.added + vm.counts.removed + vm.counts.modified + vm.counts.moved;
  if (total === 0) {
    return html;
  }

  html += renderReviewToolbar(vm);
  html += renderSheetIndex(vm);

  for (const sheetVm of vm.sheets) {
    html += renderSheetVm(sheetVm);
  }

  html += renderOtherChangesVm("VBA Modules", "V", vm.other.vba);
  html += renderOtherChangesVm("Named Ranges", "N", vm.other.namedRanges);
  html += renderOtherChangesVm("Charts", "C", vm.other.charts);
  html += renderOtherChangesVm("Power Query", "Q", vm.other.queries);
  html += renderOtherChangesVm("Model", "M", vm.other.model);

  return html;
}

export function renderReportHtml(payloadOrReport) {
  const vm = buildWorkbookViewModel(payloadOrReport);
  return renderWorkbookVm(vm);
}

```

---

### File: `web\test_outcome_payload.js`

```javascript
import assert from "node:assert/strict";
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { buildWorkbookViewModel } from "./view_model.js";
import { renderReportHtml } from "./render.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const payloadPath =
  process.env.WEB_PAYLOAD_PATH || path.join(__dirname, "testdata", "sample_payload.json");
const outcomePath =
  process.env.WEB_OUTCOME_PATH || path.join(__dirname, "testdata", "sample_outcome.json");

const payload = JSON.parse(fs.readFileSync(payloadPath, "utf8"));
const outcome = JSON.parse(fs.readFileSync(outcomePath, "utf8"));

const payloadVm = buildWorkbookViewModel(payload);
assert.ok(payloadVm.sheets.length > 0, "payload should include at least one sheet");
const payloadHtml = renderReportHtml(payload);
assert.ok(payloadHtml.includes("summary-cards"), "payload render should include summary cards");

assert.equal(outcome.mode, "payload", "outcome fixture should be payload mode");
assert.ok(outcome.payload, "outcome fixture should include payload");
const outcomeVm = buildWorkbookViewModel(outcome.payload);
assert.ok(outcomeVm.sheets.length > 0, "outcome payload should include sheets");
const outcomeHtml = renderReportHtml(outcome.payload);
assert.ok(outcomeHtml.includes("summary-cards"), "outcome render should include summary cards");

console.log("ok");

```

---

### File: `web\test_render.js`

```javascript
import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { renderReportHtml } from "./render.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const p = path.join(__dirname, "testdata", "sample_report.json");
const report = JSON.parse(fs.readFileSync(p, "utf8"));
const html = renderReportHtml(report);

function mustInclude(s) {
  if (!html.includes(s)) {
    console.error("Missing:", s);
    process.exit(1);
  }
}

mustInclude("Power Query");
mustInclude("Query: Query1");
mustInclude("Step diffs");
mustInclude("Model");
mustInclude("Column: Sales.Amount");
mustInclude("Relationship: Sales[CustomerId] -&gt; Customers[Id]");
mustInclude("Calculated Column: Sales.Calc");
mustInclude("Measure: Measure1");

console.log("ok");

```

---

### File: `web\test_view_model.js`

```javascript
import assert from "node:assert/strict";
import { buildWorkbookViewModel } from "./view_model.js";

function findSheet(vm, name) {
  const sheet = vm.sheets.find(item => item.name === name);
  assert.ok(sheet, `Missing sheet ${name}`);
  return sheet;
}

function makeCellEdited(sheetId, row, col) {
  return {
    kind: "CellEdited",
    sheet: sheetId,
    addr: { row, col },
    from: { value: { Number: row } },
    to: { value: { Number: row + 1 } }
  };
}

function testRowInsertionMapping() {
  const report = {
    strings: ["Sheet1"],
    ops: [{ kind: "RowAdded", sheet: 0, row_idx: 0 }],
    warnings: []
  };
  const oldSheet = {
    name: "Sheet1",
    nrows: 2,
    ncols: 1,
    cells: [
      { row: 0, col: 0, value: "A", formula: null },
      { row: 1, col: 0, value: "B", formula: null }
    ]
  };
  const newSheet = {
    name: "Sheet1",
    nrows: 3,
    ncols: 1,
    cells: [
      { row: 0, col: 0, value: "X", formula: null },
      { row: 1, col: 0, value: "A", formula: null },
      { row: 2, col: 0, value: "B", formula: null }
    ]
  };
  const alignment = {
    sheet: "Sheet1",
    rows: [
      { old: null, new: 0, kind: "insert" },
      { old: 0, new: 1, kind: "match" },
      { old: 1, new: 2, kind: "match" }
    ],
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload);
  const sheetVm = findSheet(vm, "Sheet1");
  const cell = sheetVm.cellAt(1, 0);
  assert.equal(cell.diffKind, "unchanged");
  assert.equal(cell.old.row, 0);
  assert.equal(cell.new.row, 1);
}

function testMoveIdentity() {
  const report = {
    strings: ["MoveSheet"],
    ops: [
      { kind: "BlockMovedRows", sheet: 0, src_start_row: 0, dst_start_row: 1, row_count: 1 }
    ],
    warnings: []
  };
  const oldSheet = {
    name: "MoveSheet",
    nrows: 2,
    ncols: 1,
    cells: [
      { row: 0, col: 0, value: "A", formula: null },
      { row: 1, col: 0, value: "B", formula: null }
    ]
  };
  const newSheet = {
    name: "MoveSheet",
    nrows: 2,
    ncols: 1,
    cells: [
      { row: 0, col: 0, value: "B", formula: null },
      { row: 1, col: 0, value: "A", formula: null }
    ]
  };
  const alignment = {
    sheet: "MoveSheet",
    rows: [
      { old: 0, new: null, kind: "move_src", move_id: "r:0+1->1" },
      { old: 1, new: 0, kind: "match" },
      { old: null, new: 1, kind: "move_dst", move_id: "r:0+1->1" }
    ],
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [{ id: "r:0+1->1", axis: "row", src_start: 0, dst_start: 1, count: 1 }],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload);
  const sheetVm = findSheet(vm, "MoveSheet");

  const srcCell = sheetVm.cellAt(0, 0);
  assert.equal(srcCell.diffKind, "moved");
  assert.equal(srcCell.moveRole, "src");
  assert.equal(srcCell.moveId, "r:0+1->1");

  const dstCell = sheetVm.cellAt(2, 0);
  assert.equal(dstCell.diffKind, "moved");
  assert.equal(dstCell.moveRole, "dst");
  assert.equal(dstCell.moveId, "r:0+1->1");
}

function testRowGrouping() {
  const report = { strings: ["GroupSheet"], ops: [], warnings: [] };
  const ops = [];
  for (let i = 0; i < 10; i++) {
    ops.push({ kind: "RowAdded", sheet: 0, row_idx: i });
  }
  report.ops = ops;

  const oldSheet = { name: "GroupSheet", nrows: 0, ncols: 1, cells: [] };
  const newSheet = { name: "GroupSheet", nrows: 10, ncols: 1, cells: [] };
  const rows = [];
  for (let i = 0; i < 10; i++) {
    rows.push({ old: null, new: i, kind: "insert" });
  }
  const alignment = {
    sheet: "GroupSheet",
    rows,
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };

  const vm = buildWorkbookViewModel(payload);
  const sheetVm = findSheet(vm, "GroupSheet");
  const rowAdds = sheetVm.changes.items.filter(item => item.group === "rows" && item.changeType === "added");
  assert.equal(rowAdds.length, 1);
  assert.ok(/Rows 1-10 added/.test(rowAdds[0].label));
}

function testRegionCompaction() {
  const report = { strings: ["RegionSheet"], ops: [], warnings: [] };
  const ops = [
    makeCellEdited(0, 0, 0),
    makeCellEdited(0, 0, 1),
    makeCellEdited(0, 1, 0),
    makeCellEdited(0, 1, 1),
    makeCellEdited(0, 100, 0),
    makeCellEdited(0, 100, 1),
    makeCellEdited(0, 101, 0),
    makeCellEdited(0, 101, 1)
  ];
  report.ops = ops;

  const oldSheet = { name: "RegionSheet", nrows: 120, ncols: 2, cells: [] };
  const newSheet = { name: "RegionSheet", nrows: 120, ncols: 2, cells: [] };
  const rows = [];
  for (let i = 0; i < 120; i++) {
    rows.push({ old: i, new: i, kind: "match" });
  }
  const cols = [
    { old: 0, new: 0, kind: "match" },
    { old: 1, new: 1, kind: "match" }
  ];
  const alignment = {
    sheet: "RegionSheet",
    rows,
    cols,
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload, { maxCellsPerRegion: 100 });
  const sheetVm = findSheet(vm, "RegionSheet");
  const regions = sheetVm.changes.regions.filter(region => region.kind === "cell");
  assert.equal(regions.length, 2);
}

function testRegionMaxCells() {
  const report = { strings: ["CapSheet"], ops: [], warnings: [] };
  const ops = [];
  for (let col = 0; col < 120; col++) {
    ops.push(makeCellEdited(0, 0, col));
  }
  report.ops = ops;

  const oldSheet = { name: "CapSheet", nrows: 1, ncols: 120, cells: [] };
  const newSheet = { name: "CapSheet", nrows: 1, ncols: 120, cells: [] };
  const rows = [{ old: 0, new: 0, kind: "match" }];
  const cols = [];
  for (let col = 0; col < 120; col++) {
    cols.push({ old: col, new: col, kind: "match" });
  }
  const alignment = {
    sheet: "CapSheet",
    rows,
    cols,
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload, { maxCellsPerRegion: 50 });
  const sheetVm = findSheet(vm, "CapSheet");
  const regions = sheetVm.changes.regions.filter(region => region.kind === "cell");
  assert.ok(regions.length >= 3);
  for (const region of regions) {
    assert.ok(region.cellCount <= 50);
  }
}

function testAnchorsForRowChanges() {
  const report = {
    strings: ["AnchorSheet"],
    ops: [{ kind: "RowAdded", sheet: 0, row_idx: 0 }],
    warnings: []
  };
  const oldSheet = { name: "AnchorSheet", nrows: 1, ncols: 1, cells: [] };
  const newSheet = { name: "AnchorSheet", nrows: 2, ncols: 1, cells: [] };
  const alignment = {
    sheet: "AnchorSheet",
    rows: [
      { old: null, new: 0, kind: "insert" },
      { old: 0, new: 1, kind: "match" }
    ],
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload);
  const sheetVm = findSheet(vm, "AnchorSheet");
  const anchor = sheetVm.changes.anchors.find(item => item.id === "row:added:0-0");
  assert.ok(anchor, "Missing row add anchor");
  assert.equal(anchor.target.kind, "grid");
  assert.equal(anchor.target.viewRow, 0);
}

function testIgnoreBlankToBlank() {
  const report = {
    strings: ["BlankSheet"],
    ops: [
      {
        kind: "CellEdited",
        sheet: 0,
        addr: { row: 0, col: 0 },
        from: { value: "Blank" },
        to: { value: "Blank" }
      }
    ],
    warnings: []
  };
  const oldSheet = { name: "BlankSheet", nrows: 1, ncols: 1, cells: [] };
  const newSheet = { name: "BlankSheet", nrows: 1, ncols: 1, cells: [] };
  const alignment = {
    sheet: "BlankSheet",
    rows: [{ old: 0, new: 0, kind: "match" }],
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };

  const vmDefault = buildWorkbookViewModel(payload);
  const sheetDefault = findSheet(vmDefault, "BlankSheet");
  assert.equal(sheetDefault.changes.items.length, 0);
  assert.equal(sheetDefault.changes.regions.length, 0);
  assert.equal(sheetDefault.changes.anchors.length, 0);

  const vmInclude = buildWorkbookViewModel(payload, { ignoreBlankToBlank: false });
  const sheetInclude = findSheet(vmInclude, "BlankSheet");
  assert.ok(sheetInclude.changes.regions.length > 0);
  assert.ok(sheetInclude.changes.anchors.length > 0);
}

function testSheetRenameMapping() {
  const report = {
    strings: ["OldSheet", "NewSheet"],
    ops: [
      { kind: "SheetRenamed", sheet: 1, from: 0, to: 1 }
    ],
    warnings: []
  };
  const oldSheet = {
    name: "OldSheet",
    nrows: 1,
    ncols: 1,
    cells: [{ row: 0, col: 0, value: "A", formula: null }]
  };
  const newSheet = {
    name: "NewSheet",
    nrows: 1,
    ncols: 1,
    cells: [{ row: 0, col: 0, value: "B", formula: null }]
  };
  const alignment = {
    sheet: "NewSheet",
    rows: [{ old: 0, new: 0, kind: "match" }],
    cols: [{ old: 0, new: 0, kind: "match" }],
    moves: [],
    skipped: false
  };
  const payload = {
    report,
    sheets: { old: { sheets: [oldSheet] }, new: { sheets: [newSheet] } },
    alignments: [alignment]
  };
  const vm = buildWorkbookViewModel(payload);
  const sheetVm = findSheet(vm, "NewSheet");
  const cell = sheetVm.cellAt(0, 0);
  assert.equal(cell.old.cell.value, "A");
  assert.equal(cell.new.cell.value, "B");
}

testRowInsertionMapping();
testMoveIdentity();
testRowGrouping();
testRegionCompaction();
testRegionMaxCells();
testAnchorsForRowChanges();
testIgnoreBlankToBlank();
testSheetRenameMapping();

console.log("ok");

```

---

### File: `web\view_model.js`

```javascript
const CELL_KEY_STRIDE = 16384;

const DEFAULT_OPTS = {
  contextRows: 1,
  contextCols: 1,
  maxCellsPerRegion: 200,
  maxVisualCells: 5000,
  mergeGap: 1,
  ignoreBlankToBlank: true
};

function resolveString(report, id) {
  if (typeof id !== "number") return String(id);
  if (!report || !Array.isArray(report.strings)) return "<unknown>";
  return report.strings[id] != null ? report.strings[id] : "<unknown>";
}

function formatChangeKind(kind) {
  if (!kind) return "";
  return String(kind).replace(/_/g, " ");
}

function formatFieldLabel(field) {
  if (!field) return "";
  return String(field).replace(/_/g, " ");
}

function formatColumnRef(report, tableId, columnId) {
  const table = resolveString(report, tableId);
  const column = resolveString(report, columnId);
  return `${table}.${column}`;
}

function formatRelationshipRef(report, op) {
  const fromTable = resolveString(report, op.from_table);
  const fromColumn = resolveString(report, op.from_column);
  const toTable = resolveString(report, op.to_table);
  const toColumn = resolveString(report, op.to_column);
  return `${fromTable}[${fromColumn}] -> ${toTable}[${toColumn}]`;
}

function isModelKind(kind) {
  return (
    kind === "CalculatedColumnDefinitionChanged" ||
    kind.startsWith("Table") ||
    kind.startsWith("ModelColumn") ||
    kind.startsWith("Relationship") ||
    kind.startsWith("Measure")
  );
}

function colToLetter(col) {
  let result = "";
  let c = col;
  while (c >= 0) {
    result = String.fromCharCode((c % 26) + 65) + result;
    c = Math.floor(c / 26) - 1;
  }
  return result;
}

function formatCellAddress(row, col) {
  return colToLetter(col) + (row + 1);
}

function parseCellAddress(addr) {
  if (!addr) return null;
  if (typeof addr === "object" && Number.isInteger(addr.row) && Number.isInteger(addr.col)) {
    return { row: addr.row, col: addr.col };
  }
  const match = /^([A-Z]+)(\d+)$/i.exec(String(addr).trim());
  if (!match) return null;
  const letters = match[1].toUpperCase();
  let col = 0;
  for (let i = 0; i < letters.length; i++) {
    col = col * 26 + (letters.charCodeAt(i) - 64);
  }
  const row = parseInt(match[2], 10) - 1;
  return { row, col: col - 1 };
}

function formatValue(report, val) {
  if (val === null || val === undefined) return "";
  if (val === "Blank") return "";
  if (typeof val === "object") {
    if (val.Number !== undefined) return String(val.Number);
    if (val.Text !== undefined) return resolveString(report, val.Text);
    if (val.Bool !== undefined) return val.Bool ? "TRUE" : "FALSE";
    if (val.Error !== undefined) return resolveString(report, val.Error);
    if (val.Formula !== undefined) return String(val.Formula);
    return JSON.stringify(val);
  }
  return String(val);
}

function resolveFormula(report, id) {
  if (id === null || id === undefined) return "";
  const text = resolveString(report, id);
  if (!text) return "";
  return text.startsWith("=") ? text : `=${text}`;
}

function normalizeSheetList(list) {
  if (!list) return [];
  if (Array.isArray(list)) return list;
  if (Array.isArray(list.sheets)) return list.sheets;
  return [];
}

function normalizePayload(payloadOrReport) {
  const payload = payloadOrReport && payloadOrReport.report ? payloadOrReport : { report: payloadOrReport };
  const report = payload.report || payloadOrReport || {};
  const rawSheets = payload.sheets || null;
  const alignments = Array.isArray(payload.alignments) ? payload.alignments : [];
  const sheets = {
    oldSheets: normalizeSheetList(rawSheets?.old),
    newSheets: normalizeSheetList(rawSheets?.new)
  };
  return { report, sheets, alignments };
}

function buildSheetLookup(sheets) {
  const map = new Map();
  for (const sheet of sheets || []) {
    if (sheet && typeof sheet.name === "string") {
      map.set(sheet.name, sheet);
    }
  }
  return map;
}

function buildAlignmentLookup(alignments) {
  const map = new Map();
  if (!Array.isArray(alignments)) return map;
  for (const alignment of alignments) {
    if (alignment && typeof alignment.sheet === "string") {
      map.set(alignment.sheet, alignment);
    }
  }
  return map;
}

function categorizeOps(report) {
  const ops = Array.isArray(report?.ops) ? report.ops : [];
  const sheetOps = new Map();
  const renameMap = new Map();
  const vbaOps = [];
  const namedRangeOps = [];
  const chartOps = [];
  const queryOps = [];
  const modelOps = [];

  let addedCount = 0;
  let removedCount = 0;
  let modifiedCount = 0;
  let movedCount = 0;

  for (const op of ops) {
    const kind = op.kind;
    if (kind === "SheetAdded" || kind === "SheetRemoved") {
      const sheetName = resolveString(report, op.sheet);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      if (kind === "SheetAdded") addedCount++;
      else removedCount++;
    } else if (kind === "SheetRenamed") {
      const sheetName = resolveString(report, op.sheet ?? op.to);
      const fromName = resolveString(report, op.from);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      renameMap.set(sheetName, fromName);
      modifiedCount++;
    } else if (kind.startsWith("Row") || kind.startsWith("Column") || kind.startsWith("Cell") || kind.startsWith("Block") || kind.startsWith("Rect")) {
      const sheetName = resolveString(report, op.sheet);
      if (!sheetOps.has(sheetName)) sheetOps.set(sheetName, []);
      sheetOps.get(sheetName).push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else if (kind.includes("Moved")) movedCount++;
      else if (kind.includes("Edited") || kind.includes("Changed") || kind.includes("Replaced")) modifiedCount++;
    } else if (kind.startsWith("Vba")) {
      vbaOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("NamedRange")) {
      namedRangeOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("Chart")) {
      chartOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (kind.startsWith("Query")) {
      queryOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    } else if (isModelKind(kind)) {
      modelOps.push(op);
      if (kind.includes("Added")) addedCount++;
      else if (kind.includes("Removed")) removedCount++;
      else modifiedCount++;
    }
  }

  return {
    sheetOps,
    renameMap,
    vbaOps,
    namedRangeOps,
    chartOps,
    queryOps,
    modelOps,
    counts: { added: addedCount, removed: removedCount, modified: modifiedCount, moved: movedCount }
  };
}

function makeAxisVm(entries, oldLen, newLen) {
  const list = Array.isArray(entries) ? entries : [];
  const oldToView = new Array(oldLen || 0).fill(null);
  const newToView = new Array(newLen || 0).fill(null);
  for (let i = 0; i < list.length; i++) {
    const entry = list[i];
    if (!entry) continue;
    if (entry.old !== null && entry.old !== undefined && oldToView.length) {
      oldToView[entry.old] = i;
    }
    if (entry.new !== null && entry.new !== undefined && newToView.length) {
      newToView[entry.new] = i;
    }
  }
  return {
    entries: list,
    oldToView,
    newToView,
    oldLen: oldLen || 0,
    newLen: newLen || 0,
    count: list.length
  };
}

function makeCellMap(sheet) {
  const map = new Map();
  if (!sheet || !Array.isArray(sheet.cells)) return map;
  for (const cell of sheet.cells) {
    const key = cell.row * CELL_KEY_STRIDE + cell.col;
    map.set(key, cell);
  }
  return map;
}

function mapIndexToView(index, map) {
  if (index === null || index === undefined) return null;
  if (Array.isArray(map) && map.length > 0) {
    const mapped = map[index];
    if (mapped !== null && mapped !== undefined) return mapped;
  }
  return index;
}

function makeEditMap(report, sheetOps, rowsVm, colsVm, opts) {
  const editMap = new Map();
  const ignoreBlank = opts?.ignoreBlankToBlank !== false;
  for (const op of sheetOps) {
    if (op.kind !== "CellEdited") continue;
    const addr = parseCellAddress(op.addr);
    if (!addr) continue;
    const viewRow = mapIndexToView(addr.row, rowsVm.newToView);
    const viewCol = mapIndexToView(addr.col, colsVm.newToView);
    if (viewRow === null || viewCol === null) continue;
    const key = viewRow * CELL_KEY_STRIDE + viewCol;
    const fromValue = op.from ? formatValue(report, op.from.value) : "";
    const toValue = op.to ? formatValue(report, op.to.value) : "";
    const fromFormula = resolveFormula(report, op.from?.formula);
    const toFormula = resolveFormula(report, op.to?.formula);
    if (ignoreBlank && !fromValue && !toValue && !fromFormula && !toFormula) {
      continue;
    }
    editMap.set(key, { fromValue, toValue, fromFormula, toFormula });
  }
  return editMap;
}

function cellDisplayText(cell) {
  if (!cell) return "";
  return cell.value || cell.formula || "";
}

function cellTooltip(label, cell) {
  if (!cell) return "";
  const value = cell.value ?? "";
  const formula = cell.formula ?? "";
  if (!value && !formula) return "";
  if (value && formula && value !== formula) {
    return label ? `${label}: ${value} | ${formula}` : `${value} | ${formula}`;
  }
  const text = value || formula;
  return label ? `${label}: ${text}` : text;
}

function buildCellVm(viewRow, viewCol, rowsVm, colsVm, oldCells, newCells, editMap) {
  const rowEntry = rowsVm.entries[viewRow];
  const colEntry = colsVm.entries[viewCol];
  if (!rowEntry || !colEntry) {
    return {
      viewRow,
      viewCol,
      old: null,
      new: null,
      diffKind: "empty",
      display: { text: "", tooltip: "" }
    };
  }

  const oldRow = rowEntry.old;
  const oldCol = colEntry.old;
  const newRow = rowEntry.new;
  const newCol = colEntry.new;

  const oldCell =
    oldRow !== null && oldRow !== undefined && oldCol !== null && oldCol !== undefined
      ? oldCells.get(oldRow * CELL_KEY_STRIDE + oldCol) || null
      : null;
  const newCell =
    newRow !== null && newRow !== undefined && newCol !== null && newCol !== undefined
      ? newCells.get(newRow * CELL_KEY_STRIDE + newCol) || null
      : null;

  const viewKey = viewRow * CELL_KEY_STRIDE + viewCol;
  const edit = editMap.get(viewKey);

  const rowKind = rowEntry.kind;
  const colKind = colEntry.kind;
  const isInsert = rowKind === "insert" || colKind === "insert";
  const isDelete = rowKind === "delete" || colKind === "delete";
  const isMoveSrc = rowKind === "move_src" || colKind === "move_src";
  const isMoveDst = rowKind === "move_dst" || colKind === "move_dst";
  const moveRole = isMoveSrc ? "src" : isMoveDst ? "dst" : undefined;
  const moveId = rowEntry.move_id || colEntry.move_id || undefined;

  let diffKind = "empty";
  if (edit) diffKind = "edited";
  else if (isInsert) diffKind = "added";
  else if (isDelete) diffKind = "removed";
  else if (isMoveSrc || isMoveDst) diffKind = "moved";
  else if (oldCell || newCell) diffKind = "unchanged";

  let displayText = "";
  let tooltip = "";
  if (diffKind === "edited") {
    const fromText = edit.fromValue || edit.fromFormula || "";
    const toText = edit.toValue || edit.toFormula || "";
    displayText = toText || fromText;
    tooltip = fromText || toText ? `Changed: ${fromText || "(empty)"} -> ${toText || "(empty)"}` : "";
  } else if (diffKind === "added") {
    displayText = cellDisplayText(newCell);
    tooltip = cellTooltip("Added", newCell);
  } else if (diffKind === "removed") {
    displayText = cellDisplayText(oldCell);
    tooltip = cellTooltip("Removed", oldCell);
  } else if (diffKind === "moved") {
    const cell = moveRole === "src" ? oldCell : newCell;
    displayText = cellDisplayText(cell);
    tooltip = cellTooltip("Moved", cell);
  } else if (diffKind === "unchanged") {
    displayText = cellDisplayText(newCell || oldCell);
    tooltip = cellTooltip("Value", newCell || oldCell);
  }

  return {
    viewRow,
    viewCol,
    old: oldRow !== null && oldRow !== undefined && oldCol !== null && oldCol !== undefined ? { row: oldRow, col: oldCol, cell: oldCell } : null,
    new: newRow !== null && newRow !== undefined && newCol !== null && newCol !== undefined ? { row: newRow, col: newCol, cell: newCell } : null,
    diffKind,
    moveId,
    moveRole,
    edit: edit || undefined,
    display: { text: displayText, tooltip }
  };
}

function mapRangeToView(start, count, map) {
  if (start === null || start === undefined || count === null || count === undefined) return null;
  if (count <= 0) return null;
  let minView = null;
  let maxView = null;
  const end = start + count - 1;
  for (let idx = start; idx <= end; idx++) {
    const view = mapIndexToView(idx, map);
    if (view === null || view === undefined) continue;
    if (minView === null || view < minView) minView = view;
    if (maxView === null || view > maxView) maxView = view;
  }
  if (minView === null || maxView === null) return null;
  return { start: minView, end: maxView };
}

function viewStartForRange(axisVm, start, count, side) {
  if (!axisVm) return null;
  const map = side === "old" ? axisVm.oldToView : axisVm.newToView;
  const range = mapRangeToView(start, count, map);
  return range ? range.start : null;
}

function viewIndexForSide(axisVm, viewIndex, side) {
  if (axisVm?.entries?.length) {
    const entry = axisVm.entries[viewIndex];
    const value = entry ? entry[side] : null;
    if (value !== null && value !== undefined) return value;
  }
  return viewIndex;
}

function formatRowRange(axisVm, startView, endView, side, action) {
  const start = viewIndexForSide(axisVm, startView, side);
  const end = viewIndexForSide(axisVm, endView, side);
  if (start === end) return `Row ${start + 1} ${action}`;
  return `Rows ${start + 1}-${end + 1} ${action}`;
}

function formatColRange(axisVm, startView, endView, side, action) {
  const start = viewIndexForSide(axisVm, startView, side);
  const end = viewIndexForSide(axisVm, endView, side);
  if (start === end) return `Column ${colToLetter(start)} ${action}`;
  return `Columns ${colToLetter(start)}-${colToLetter(end)} ${action}`;
}

function groupConsecutive(indices) {
  const sorted = [...indices].sort((a, b) => a - b);
  const groups = [];
  let start = null;
  let prev = null;
  for (const idx of sorted) {
    if (start === null) {
      start = idx;
      prev = idx;
      continue;
    }
    if (idx === prev + 1) {
      prev = idx;
      continue;
    }
    groups.push({ start, end: prev, count: prev - start + 1 });
    start = idx;
    prev = idx;
  }
  if (start !== null) {
    groups.push({ start, end: prev, count: prev - start + 1 });
  }
  return groups;
}

function clusterEditsToRegions(editKeys, opts) {
  if (!editKeys || editKeys.length === 0) return [];
  const byRow = new Map();
  for (const key of editKeys) {
    const row = Math.floor(key / CELL_KEY_STRIDE);
    const col = key % CELL_KEY_STRIDE;
    if (!byRow.has(row)) byRow.set(row, []);
    byRow.get(row).push(col);
  }
  const rows = Array.from(byRow.keys()).sort((a, b) => a - b);
  const regions = [];
  const mergeGap = opts.mergeGap ?? 0;
  const maxCells = opts.maxCellsPerRegion ?? 200;
  let active = [];

  for (const row of rows) {
    const cols = byRow.get(row).sort((a, b) => a - b);
    const runs = [];
    let runStart = null;
    let runEnd = null;
    for (const col of cols) {
      if (runStart === null) {
        runStart = col;
        runEnd = col;
        continue;
      }
      if (col === runEnd + 1) {
        runEnd = col;
        continue;
      }
      runs.push({ start: runStart, end: runEnd, count: runEnd - runStart + 1 });
      runStart = col;
      runEnd = col;
    }
    if (runStart !== null) {
      runs.push({ start: runStart, end: runEnd, count: runEnd - runStart + 1 });
    }

    const expandedRuns = [];
    if (maxCells > 0) {
      for (const run of runs) {
        if (run.count <= maxCells) {
          expandedRuns.push(run);
          continue;
        }
        let segStart = run.start;
        while (segStart <= run.end) {
          const segEnd = Math.min(run.end, segStart + maxCells - 1);
          expandedRuns.push({ start: segStart, end: segEnd, count: segEnd - segStart + 1 });
          segStart = segEnd + 1;
        }
      }
    } else {
      expandedRuns.push(...runs);
    }

    const nextActive = [];
    const used = new Set();

    for (const run of expandedRuns) {
      let matched = null;
      let matchedIndex = -1;
      for (let i = 0; i < active.length; i++) {
        const region = active[i];
        if (region.rowEnd !== row - 1) continue;
        const overlaps =
          run.start <= region.colEnd + mergeGap &&
          run.end >= region.colStart - mergeGap;
        if (overlaps) {
          matched = region;
          matchedIndex = i;
          break;
        }
      }

      if (matched) {
        const nextCount = matched.cellCount + run.count;
        if (nextCount > maxCells) {
          regions.push(matched);
        } else {
          matched.rowEnd = row;
          matched.colStart = Math.min(matched.colStart, run.start);
          matched.colEnd = Math.max(matched.colEnd, run.end);
          matched.cellCount = nextCount;
          nextActive.push(matched);
          used.add(matchedIndex);
          continue;
        }
      }

      nextActive.push({
        rowStart: row,
        rowEnd: row,
        colStart: run.start,
        colEnd: run.end,
        cellCount: run.count
      });
    }

    for (let i = 0; i < active.length; i++) {
      if (!used.has(i)) {
        regions.push(active[i]);
      }
    }
    active = nextActive;
  }

  regions.push(...active);

  return regions.map((region, idx) => ({
    id: `cells-${idx + 1}`,
    kind: "cell",
    top: region.rowStart,
    bottom: region.rowEnd,
    left: region.colStart,
    right: region.colEnd,
    cellCount: region.cellCount
  }));
}

function capBounds(bounds, maxVisualCells) {
  if (!maxVisualCells || maxVisualCells <= 0) return bounds;
  let rows = bounds.bottom - bounds.top + 1;
  let cols = bounds.right - bounds.left + 1;
  if (rows * cols <= maxVisualCells) return bounds;
  const maxRows = Math.max(1, Math.floor(maxVisualCells / cols));
  if (maxRows < rows) {
    return { ...bounds, bottom: bounds.top + maxRows - 1 };
  }
  const maxCols = Math.max(1, Math.floor(maxVisualCells / rows));
  if (maxCols < cols) {
    return { ...bounds, right: bounds.left + maxCols - 1 };
  }
  return bounds;
}

function expandBounds(region, rowsCount, colsCount, opts) {
  const contextRows = opts.contextRows ?? 0;
  const contextCols = opts.contextCols ?? 0;
  const top = Math.max(0, region.top - contextRows);
  const left = Math.max(0, region.left - contextCols);
  const bottom = Math.min(rowsCount - 1, region.bottom + contextRows);
  const right = Math.min(colsCount - 1, region.right + contextCols);
  return capBounds({ top, left, bottom, right }, opts.maxVisualCells);
}

function buildChangeItems({ report, ops, rowsVm, colsVm, alignment, regions }) {
  const items = [];

  const rowAdds = [];
  const rowRemoves = [];
  const colAdds = [];
  const colRemoves = [];

  for (const op of ops) {
    if (op.kind === "RowAdded") {
      const viewIdx = mapIndexToView(op.row_idx, rowsVm.newToView);
      if (viewIdx !== null && viewIdx !== undefined) {
        rowAdds.push(viewIdx);
      }
    } else if (op.kind === "RowRemoved") {
      const viewIdx = mapIndexToView(op.row_idx, rowsVm.oldToView);
      if (viewIdx !== null && viewIdx !== undefined) {
        rowRemoves.push(viewIdx);
      }
    } else if (op.kind === "ColumnAdded") {
      const viewIdx = mapIndexToView(op.col_idx, colsVm.newToView);
      if (viewIdx !== null && viewIdx !== undefined) {
        colAdds.push(viewIdx);
      }
    } else if (op.kind === "ColumnRemoved") {
      const viewIdx = mapIndexToView(op.col_idx, colsVm.oldToView);
      if (viewIdx !== null && viewIdx !== undefined) {
        colRemoves.push(viewIdx);
      }
    } else if (op.kind === "RowReplaced") {
      const viewIdx = mapIndexToView(op.row_idx, rowsVm.newToView);
      items.push({
        id: `row-replaced-${op.row_idx}`,
        group: "rows",
        changeType: "modified",
        label: `Row ${viewIndexForSide(rowsVm, viewIdx, "new") + 1} replaced`,
        axis: "row",
        viewStart: viewIdx,
        viewEnd: viewIdx
      });
    }
  }

  for (const group of groupConsecutive(rowAdds)) {
    items.push({
      id: `row-added-${group.start}`,
      group: "rows",
      changeType: "added",
      label: formatRowRange(rowsVm, group.start, group.end, "new", "added"),
      axis: "row",
      viewStart: group.start,
      viewEnd: group.end
    });
  }
  for (const group of groupConsecutive(rowRemoves)) {
    items.push({
      id: `row-removed-${group.start}`,
      group: "rows",
      changeType: "removed",
      label: formatRowRange(rowsVm, group.start, group.end, "old", "removed"),
      axis: "row",
      viewStart: group.start,
      viewEnd: group.end
    });
  }
  for (const group of groupConsecutive(colAdds)) {
    items.push({
      id: `col-added-${group.start}`,
      group: "cols",
      changeType: "added",
      label: formatColRange(colsVm, group.start, group.end, "new", "added"),
      axis: "col",
      viewStart: group.start,
      viewEnd: group.end
    });
  }
  for (const group of groupConsecutive(colRemoves)) {
    items.push({
      id: `col-removed-${group.start}`,
      group: "cols",
      changeType: "removed",
      label: formatColRange(colsVm, group.start, group.end, "old", "removed"),
      axis: "col",
      viewStart: group.start,
      viewEnd: group.end
    });
  }

  const alignmentMoves = Array.isArray(alignment?.moves) ? alignment.moves : [];
  if (alignmentMoves.length > 0) {
    for (const move of alignmentMoves) {
      if (move.axis === "row") {
        const start = move.src_start + 1;
        const end = move.src_start + move.count;
        const label = start === end ? `Row ${start} moved` : `Rows ${start}-${end} moved`;
        items.push({
          id: `move-${move.id}`,
          group: "moves",
          changeType: "moved",
          label,
          detail: `to row ${move.dst_start + 1}`,
          moveId: move.id,
          axis: "row",
          srcStart: move.src_start,
          dstStart: move.dst_start,
          count: move.count
        });
      } else if (move.axis === "col") {
        const start = colToLetter(move.src_start);
        const end = colToLetter(move.src_start + move.count - 1);
        const label = start === end ? `Column ${start} moved` : `Columns ${start}-${end} moved`;
        items.push({
          id: `move-${move.id}`,
          group: "moves",
          changeType: "moved",
          label,
          detail: `to column ${colToLetter(move.dst_start)}`,
          moveId: move.id,
          axis: "col",
          srcStart: move.src_start,
          dstStart: move.dst_start,
          count: move.count
        });
      }
    }
  } else {
    for (const op of ops) {
      if (op.kind === "BlockMovedRows") {
        const start = op.src_start_row + 1;
        const end = op.src_start_row + op.row_count;
        const label = start === end ? `Row ${start} moved` : `Rows ${start}-${end} moved`;
        const moveId = `r:${op.src_start_row}+${op.row_count}->${op.dst_start_row}`;
        items.push({
          id: `move-rows-${op.src_start_row}-${op.dst_start_row}`,
          group: "moves",
          changeType: "moved",
          label,
          detail: `to row ${op.dst_start_row + 1}`,
          moveId,
          axis: "row",
          srcStart: op.src_start_row,
          dstStart: op.dst_start_row,
          count: op.row_count
        });
      } else if (op.kind === "BlockMovedColumns") {
        const start = colToLetter(op.src_start_col);
        const end = colToLetter(op.src_start_col + op.col_count - 1);
        const label = start === end ? `Column ${start} moved` : `Columns ${start}-${end} moved`;
        const moveId = `c:${op.src_start_col}+${op.col_count}->${op.dst_start_col}`;
        items.push({
          id: `move-cols-${op.src_start_col}-${op.dst_start_col}`,
          group: "moves",
          changeType: "moved",
          label,
          detail: `to column ${colToLetter(op.dst_start_col)}`,
          moveId,
          axis: "col",
          srcStart: op.src_start_col,
          dstStart: op.dst_start_col,
          count: op.col_count
        });
      }
    }
  }

  for (const op of ops) {
    if (op.kind === "BlockMovedRect") {
      const srcStart = formatCellAddress(op.src_start_row, op.src_start_col);
      const srcEnd = formatCellAddress(op.src_start_row + op.src_row_count - 1, op.src_start_col + op.src_col_count - 1);
      const dstStart = formatCellAddress(op.dst_start_row, op.dst_start_col);
      const dstEnd = formatCellAddress(op.dst_start_row + op.src_row_count - 1, op.dst_start_col + op.src_col_count - 1);
      const moveId = `rect:${op.src_start_row},${op.src_start_col}+${op.src_row_count}x${op.src_col_count}->${op.dst_start_row},${op.dst_start_col}`;
      items.push({
        id: `move-rect-${srcStart}-${dstStart}`,
        group: "moves",
        changeType: "moved",
        label: `Range ${srcStart}:${srcEnd} moved`,
        detail: `to ${dstStart}:${dstEnd}`,
        moveId,
        moveKind: "rect"
      });
    }
  }

  for (const region of regions) {
    if (region.kind === "cell") {
      const cellCount = region.cellCount || ((region.bottom - region.top + 1) * (region.right - region.left + 1));
      const detail = cellCount > 1 ? `${cellCount} cells` : "";
      items.push({
        id: `cell-region-${region.id}`,
        group: "cells",
        changeType: "modified",
        label: `${region.label} modified`,
        detail,
        regionId: region.id
      });
    } else if (region.kind === "rect") {
      items.push({
        id: `rect-region-${region.id}`,
        group: "cells",
        changeType: "modified",
        label: `${region.label} replaced`,
        regionId: region.id
      });
    }
  }

  for (const op of ops) {
    if (op.kind === "SheetRenamed") {
      const fromName = resolveString(report, op.from);
      const toName = resolveString(report, op.to ?? op.sheet);
      const fromId = op.from ?? "unknown";
      const toId = op.to ?? op.sheet ?? "unknown";
      items.push({
        id: `sheet-renamed-${fromId}-${toId}`,
        group: "other",
        changeType: "modified",
        label: "Sheet renamed",
        detail: `${fromName} -> ${toName}`
      });
    }
  }

  const handledKinds = new Set([
    "RowAdded",
    "RowRemoved",
    "RowReplaced",
    "ColumnAdded",
    "ColumnRemoved",
    "CellEdited",
    "RectReplaced",
    "BlockMovedRows",
    "BlockMovedColumns",
    "BlockMovedRect",
    "SheetAdded",
    "SheetRemoved",
    "SheetRenamed"
  ]);

  for (const op of ops) {
    if (handledKinds.has(op.kind)) continue;
    items.push({
      id: `other-${op.kind}`,
      group: "other",
      changeType: "modified",
      label: op.kind
    });
  }

  return items;
}

function anchorPriority(group, regionKind) {
  if (group === "moves") return 0;
  if (group === "rows" || group === "cols") return 1;
  if (regionKind === "rect") return 2;
  if (group === "cells") return 3;
  return 4;
}

function buildChangeAnchors({ sheetName, status, items, regions, rowsVm, colsVm }) {
  const entries = [];
  const regionById = new Map();
  const moveRegions = new Map();

  for (const region of regions || []) {
    regionById.set(region.id, region);
    if (region.moveId) {
      if (!moveRegions.has(region.moveId)) {
        moveRegions.set(region.moveId, {});
      }
      const record = moveRegions.get(region.moveId);
      if (region.kind === "move_src") record.src = region;
      if (region.kind === "move_dst") record.dst = region;
    }
  }

  const canGrid =
    (status?.kind === "ok" || status?.kind === "partial") &&
    Array.isArray(rowsVm?.entries) &&
    Array.isArray(colsVm?.entries) &&
    rowsVm.entries.length > 0 &&
    colsVm.entries.length > 0;

  function addAnchor({ id, group, changeType, label, detail, viewRow, viewCol, regionId, moveId, listElementId, regionKind }) {
    if (!id || !listElementId) return;
    const hasGridTarget = canGrid && Number.isFinite(viewRow) && Number.isFinite(viewCol);
    const target = hasGridTarget
      ? {
          kind: "grid",
          viewRow,
          viewCol,
          ...(regionId ? { regionId } : {}),
          ...(moveId ? { moveId } : {})
        }
      : { kind: "list", elementId: listElementId };
    const anchor = {
      id,
      group: group || "cells",
      changeType: changeType || "modified",
      label: label || id,
      target
    };
    if (detail) anchor.detail = detail;
    entries.push({
      anchor,
      sortRow: Number.isFinite(viewRow) ? viewRow : Number.MAX_SAFE_INTEGER,
      sortCol: Number.isFinite(viewCol) ? viewCol : Number.MAX_SAFE_INTEGER,
      priority: anchorPriority(group, regionKind)
    });
  }

  for (const item of items || []) {
    const listElementId = `change-${sheetName}-${item.id}`;

    if (item.regionId) {
      const region = regionById.get(item.regionId);
      if (region) {
        addAnchor({
          id: `region:${region.id}`,
          group: item.group,
          changeType: item.changeType,
          label: item.label,
          detail: item.detail,
          viewRow: region.top,
          viewCol: region.left,
          regionId: region.id,
          moveId: region.moveId,
          listElementId,
          regionKind: region.kind
        });
      }
      continue;
    }

    if (item.group === "rows") {
      addAnchor({
        id: `row:${item.changeType}:${item.viewStart}-${item.viewEnd}`,
        group: "rows",
        changeType: item.changeType,
        label: item.label,
        detail: item.detail,
        viewRow: item.viewStart,
        viewCol: 0,
        listElementId
      });
      continue;
    }

    if (item.group === "cols") {
      addAnchor({
        id: `col:${item.changeType}:${item.viewStart}-${item.viewEnd}`,
        group: "cols",
        changeType: item.changeType,
        label: item.label,
        detail: item.detail,
        viewRow: 0,
        viewCol: item.viewStart,
        listElementId
      });
      continue;
    }

    if (item.group === "moves" && item.moveId) {
      if (item.axis === "row" || item.axis === "col") {
        const axisVm = item.axis === "row" ? rowsVm : colsVm;
        const srcStart = viewStartForRange(axisVm, item.srcStart, item.count, "old");
        const dstStart = viewStartForRange(axisVm, item.dstStart, item.count, "new");
        if (srcStart !== null && srcStart !== undefined) {
          addAnchor({
            id: `move:${item.moveId}:src`,
            group: "moves",
            changeType: "moved",
            label: item.label,
            detail: item.detail,
            viewRow: item.axis === "row" ? srcStart : 0,
            viewCol: item.axis === "col" ? srcStart : 0,
            moveId: item.moveId,
            listElementId
          });
        }
        if (dstStart !== null && dstStart !== undefined) {
          addAnchor({
            id: `move:${item.moveId}:dst`,
            group: "moves",
            changeType: "moved",
            label: item.label,
            detail: item.detail,
            viewRow: item.axis === "row" ? dstStart : 0,
            viewCol: item.axis === "col" ? dstStart : 0,
            moveId: item.moveId,
            listElementId
          });
        }
        continue;
      }

      if (item.moveKind === "rect") {
        const moveRegion = moveRegions.get(item.moveId);
        if (moveRegion?.src) {
          addAnchor({
            id: `region:${moveRegion.src.id}`,
            group: "moves",
            changeType: "moved",
            label: item.label,
            detail: "From",
            viewRow: moveRegion.src.top,
            viewCol: moveRegion.src.left,
            regionId: moveRegion.src.id,
            moveId: item.moveId,
            listElementId,
            regionKind: moveRegion.src.kind
          });
        }
        if (moveRegion?.dst) {
          addAnchor({
            id: `region:${moveRegion.dst.id}`,
            group: "moves",
            changeType: "moved",
            label: item.label,
            detail: "To",
            viewRow: moveRegion.dst.top,
            viewCol: moveRegion.dst.left,
            regionId: moveRegion.dst.id,
            moveId: item.moveId,
            listElementId,
            regionKind: moveRegion.dst.kind
          });
        }
      }
    }
  }

  entries.sort((a, b) => {
    if (a.sortRow !== b.sortRow) return a.sortRow - b.sortRow;
    if (a.sortCol !== b.sortCol) return a.sortCol - b.sortCol;
    if (a.priority !== b.priority) return a.priority - b.priority;
    return String(a.anchor.id).localeCompare(String(b.anchor.id));
  });

  return entries.map(entry => entry.anchor);
}

function attachNavTargets(items, anchors) {
  const anchorIds = new Set((anchors || []).map(anchor => anchor.id));
  for (const item of items || []) {
    const targets = [];
    if (item.regionId) {
      const anchorId = `region:${item.regionId}`;
      if (anchorIds.has(anchorId)) targets.push({ anchorId });
    } else if (item.group === "rows") {
      const anchorId = `row:${item.changeType}:${item.viewStart}-${item.viewEnd}`;
      if (anchorIds.has(anchorId)) targets.push({ anchorId });
    } else if (item.group === "cols") {
      const anchorId = `col:${item.changeType}:${item.viewStart}-${item.viewEnd}`;
      if (anchorIds.has(anchorId)) targets.push({ anchorId });
    } else if (item.group === "moves" && item.moveId) {
      if (item.axis === "row" || item.axis === "col") {
        const srcId = `move:${item.moveId}:src`;
        const dstId = `move:${item.moveId}:dst`;
        if (anchorIds.has(srcId)) targets.push({ anchorId: srcId, label: "From" });
        if (anchorIds.has(dstId)) targets.push({ anchorId: dstId, label: "To" });
      } else if (item.moveKind === "rect") {
        const srcId = `region:move-src-${item.moveId}`;
        const dstId = `region:move-dst-${item.moveId}`;
        if (anchorIds.has(srcId)) targets.push({ anchorId: srcId, label: "From" });
        if (anchorIds.has(dstId)) targets.push({ anchorId: dstId, label: "To" });
      }
    }
    if (targets.length > 0) {
      item.navTargets = targets;
    }
  }
}

function buildRegions({ ops, rowsVm, colsVm, editMap, opts }) {
  const regions = [];
  const editKeys = Array.from(editMap.keys());
  regions.push(...clusterEditsToRegions(editKeys, opts));

  for (const op of ops) {
    if (op.kind === "RectReplaced") {
      const rowRange = mapRangeToView(op.start_row, op.row_count, rowsVm.newToView);
      const colRange = mapRangeToView(op.start_col, op.col_count, colsVm.newToView);
      if (!rowRange || !colRange) continue;
      regions.push({
        id: `rect-${op.start_row}-${op.start_col}`,
        kind: "rect",
        top: rowRange.start,
        bottom: rowRange.end,
        left: colRange.start,
        right: colRange.end,
        cellCount: (rowRange.end - rowRange.start + 1) * (colRange.end - colRange.start + 1)
      });
    } else if (op.kind === "BlockMovedRect") {
      const moveId = `rect:${op.src_start_row},${op.src_start_col}+${op.src_row_count}x${op.src_col_count}->${op.dst_start_row},${op.dst_start_col}`;
      const srcRows = mapRangeToView(op.src_start_row, op.src_row_count, rowsVm.oldToView);
      const srcCols = mapRangeToView(op.src_start_col, op.src_col_count, colsVm.oldToView);
      const dstRows = mapRangeToView(op.dst_start_row, op.src_row_count, rowsVm.newToView);
      const dstCols = mapRangeToView(op.dst_start_col, op.src_col_count, colsVm.newToView);
      if (srcRows && srcCols) {
        regions.push({
          id: `move-src-${moveId}`,
          kind: "move_src",
          moveId,
          top: srcRows.start,
          bottom: srcRows.end,
          left: srcCols.start,
          right: srcCols.end,
          cellCount: (srcRows.end - srcRows.start + 1) * (srcCols.end - srcCols.start + 1)
        });
      }
      if (dstRows && dstCols) {
        regions.push({
          id: `move-dst-${moveId}`,
          kind: "move_dst",
          moveId,
          top: dstRows.start,
          bottom: dstRows.end,
          left: dstCols.start,
          right: dstCols.end,
          cellCount: (dstRows.end - dstRows.start + 1) * (dstCols.end - dstCols.start + 1)
        });
      }
    }
  }

  return regions;
}

function labelRegion(region, rowsVm, colsVm) {
  const startRow = viewIndexForSide(rowsVm, region.top, "new");
  const endRow = viewIndexForSide(rowsVm, region.bottom, "new");
  const startCol = viewIndexForSide(colsVm, region.left, "new");
  const endCol = viewIndexForSide(colsVm, region.right, "new");
  const startAddr = formatCellAddress(startRow, startCol);
  const endAddr = formatCellAddress(endRow, endCol);
  if (region.kind === "rect") {
    return `Region ${startAddr}:${endAddr}`;
  }
  if (startAddr === endAddr) return `Cell ${startAddr}`;
  return `Cells ${startAddr}:${endAddr}`;
}

function buildSheetViewModel({ report, sheetName, ops, oldSheet, newSheet, alignment, opts }) {
  const oldRows = oldSheet?.nrows || 0;
  const oldCols = oldSheet?.ncols || 0;
  const newRows = newSheet?.nrows || 0;
  const newCols = newSheet?.ncols || 0;

  const rowEntries = Array.isArray(alignment?.rows) ? alignment.rows : [];
  const colEntries = Array.isArray(alignment?.cols) ? alignment.cols : [];
  const rowsVm = makeAxisVm(rowEntries, oldRows, newRows);
  const colsVm = makeAxisVm(colEntries, oldCols, newCols);

  let oldCells = null;
  let newCells = null;

  function ensureCellMaps() {
    if (!oldCells) oldCells = makeCellMap(oldSheet);
    if (!newCells) newCells = makeCellMap(newSheet);
  }
  const editMap = makeEditMap(report, ops, rowsVm, colsVm, opts);

  const baseRegions = buildRegions({ ops, rowsVm, colsVm, editMap, opts });
  for (const region of baseRegions) {
    region.label = labelRegion(region, rowsVm, colsVm);
  }

  const items = buildChangeItems({ report, ops, rowsVm, colsVm, alignment, regions: baseRegions });

  const kindOrder = { move_src: 0, move_dst: 0, rect: 1, cell: 2 };
  baseRegions.sort((a, b) => {
    if (a.top !== b.top) return a.top - b.top;
    if (a.left !== b.left) return a.left - b.left;
    const ak = kindOrder[a.kind] ?? 99;
    const bk = kindOrder[b.kind] ?? 99;
    if (ak !== bk) return ak - bk;
    return String(a.id).localeCompare(String(b.id));
  });

  const rowsCount = rowsVm.entries.length;
  const colsCount = colsVm.entries.length;
  for (const region of baseRegions) {
    if (rowsCount > 0 && colsCount > 0) {
      region.renderBounds = expandBounds(region, rowsCount, colsCount, opts);
    }
  }

  const preview = {
    truncatedOld: Boolean(oldSheet?.truncated),
    truncatedNew: Boolean(newSheet?.truncated)
  };
  if (oldSheet?.note || newSheet?.note) {
    preview.note = oldSheet?.note || newSheet?.note;
  }

  let status = { kind: "ok" };
  if (!alignment || alignment.skipped) {
    const message = alignment?.skip_reason
      ? alignment.skip_reason
      : "Grid preview skipped because the aligned view is too large or inconsistent.";
    status = alignment?.skipped
      ? { kind: "skipped", message }
      : { kind: "missing", message: "Alignment data is missing for this sheet." };
  } else if (rowsCount === 0 || colsCount === 0) {
    status = { kind: "missing", message: "Sheet snapshots are missing for this sheet." };
  } else if (preview.truncatedOld || preview.truncatedNew) {
    status = {
      kind: "partial",
      message: preview.note || "Preview limited for performance; edited cells remain exact."
    };
  }

  const anchors = buildChangeAnchors({ sheetName, status, items, regions: baseRegions, rowsVm, colsVm });
  attachNavTargets(items, anchors);

  const regionsToRender =
    status.kind === "ok" || status.kind === "partial"
      ? baseRegions.map(region => region.id)
      : [];

  return {
    name: sheetName,
    axis: { rows: rowsVm, cols: colsVm },
    preview,
    ensureCellIndex: () => ensureCellMaps(),
    cellAt: (viewRow, viewCol) => {
      ensureCellMaps();
      return buildCellVm(viewRow, viewCol, rowsVm, colsVm, oldCells, newCells, editMap);
    },
    changes: { items, regions: baseRegions, anchors },
    renderPlan: {
      regionsToRender,
      status,
      contextRows: opts.contextRows,
      contextCols: opts.contextCols,
      maxVisualCells: opts.maxVisualCells
    },
    opCount: ops.length
  };
}

function buildOtherItems(report, ops, prefix) {
  const items = [];
  for (const op of ops) {
    const kind = op.kind || prefix;
    const name = op.name !== undefined ? resolveString(report, op.name) : "";
    let label = "";
    let detail = "";
    if (kind.startsWith("Query")) {
      label = `Query: ${name}`;
      if (op.semantic_detail?.step_diffs?.length) {
        detail = "Step diffs";
      }
    } else if (kind.startsWith("Table")) {
      label = `Table: ${name}`;
    } else if (kind.startsWith("ModelColumn") || kind === "CalculatedColumnDefinitionChanged") {
      const columnLabel = formatColumnRef(report, op.table, op.name);
      label =
        kind === "CalculatedColumnDefinitionChanged"
          ? `Calculated Column: ${columnLabel}`
          : `Column: ${columnLabel}`;
      if (kind === "ModelColumnTypeChanged") {
        const oldType = op.old_type != null ? resolveString(report, op.old_type) : "<none>";
        const newType = op.new_type != null ? resolveString(report, op.new_type) : "<none>";
        detail = `Type: ${oldType} -> ${newType}`;
      } else if (kind === "ModelColumnPropertyChanged") {
        const oldVal = op.old != null ? resolveString(report, op.old) : "<none>";
        const newVal = op.new != null ? resolveString(report, op.new) : "<none>";
        detail = `${formatFieldLabel(op.field)}: ${oldVal} -> ${newVal}`;
      } else if (kind === "ModelColumnAdded" && op.data_type != null) {
        detail = `Type: ${resolveString(report, op.data_type)}`;
      } else if (kind === "CalculatedColumnDefinitionChanged") {
        const kindLabel = formatChangeKind(op.change_kind);
        detail = kindLabel ? `Definition changed (${kindLabel})` : "Definition changed";
      }
    } else if (kind.startsWith("Relationship")) {
      label = `Relationship: ${formatRelationshipRef(report, op)}`;
      if (kind === "RelationshipPropertyChanged") {
        const oldVal = op.old != null ? resolveString(report, op.old) : "<none>";
        const newVal = op.new != null ? resolveString(report, op.new) : "<none>";
        detail = `${formatFieldLabel(op.field)}: ${oldVal} -> ${newVal}`;
      }
    } else if (kind.startsWith("Measure")) {
      label = `Measure: ${name}`;
      if (kind === "MeasureDefinitionChanged") {
        const kindLabel = formatChangeKind(op.change_kind);
        detail = kindLabel ? `Definition changed (${kindLabel})` : "Definition changed";
      }
    } else if (kind.startsWith("NamedRange")) {
      label = `Named Range: ${name}`;
    } else if (kind.startsWith("Chart")) {
      label = `Chart: ${name}`;
    } else if (kind.startsWith("Vba")) {
      label = `VBA Module: ${name}`;
    } else {
      label = name ? `${kind}: ${name}` : String(kind);
    }
    const changeType = kind.includes("Added") ? "added" : kind.includes("Removed") ? "removed" : "modified";
    items.push({
      id: `${kind}-${name}`,
      changeType,
      label,
      detail
    });
  }
  return items;
}

export function buildWorkbookViewModel(payloadOrReport, opts = {}) {
  const { report, sheets, alignments } = normalizePayload(payloadOrReport);
  const options = { ...DEFAULT_OPTS, ...opts };
  const { sheetOps, renameMap, vbaOps, namedRangeOps, chartOps, queryOps, modelOps, counts } = categorizeOps(report);

  const oldLookup = buildSheetLookup(sheets.oldSheets);
  const newLookup = buildSheetLookup(sheets.newSheets);
  const alignmentLookup = buildAlignmentLookup(alignments);

  const sheetVms = [];
  for (const [sheetName, ops] of sheetOps.entries()) {
    const sheetVm = buildSheetViewModel({
      report,
      sheetName,
      ops,
      oldSheet: oldLookup.get(sheetName) || oldLookup.get(renameMap.get(sheetName)) || null,
      newSheet: newLookup.get(sheetName) || null,
      alignment: alignmentLookup.get(sheetName) || alignmentLookup.get(renameMap.get(sheetName)) || null,
      opts: options
    });
    sheetVms.push(sheetVm);
  }

  sheetVms.sort((a, b) => a.name.toLowerCase().localeCompare(b.name.toLowerCase()));

  return {
    report,
    warnings: Array.isArray(report?.warnings) ? report.warnings : [],
    counts,
    sheets: sheetVms,
    other: {
      vba: buildOtherItems(report, vbaOps, "Vba"),
      namedRanges: buildOtherItems(report, namedRangeOps, "NamedRange"),
      charts: buildOtherItems(report, chartOps, "Chart"),
      queries: buildOtherItems(report, queryOps, "Query"),
      model: buildOtherItems(report, modelOps, "Model")
    }
  };
}

```

---
