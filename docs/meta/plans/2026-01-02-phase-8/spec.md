## Phase 8 deliverables

Phase 8 has two concrete goals:

1. **Harden fixtures** so they are reproducible *or* validated (manifest + checksums) in CI, eliminating “missing-file” style failures. 
2. **Add maintainer docs**: (a) “entry‑point maps”, and (b) a short conceptual narrative of **parse → IR → diff → output** with key types, living in `core/src/lib.rs` and/or `/docs`.

What follows is an implementation plan that is explicitly anchored in how this repository works today (fixture generator, manifests, CI, and the Rust entry points).

---

## Current reality to design against

### Fixtures are required at runtime for tests and some workflows

* Rust tests locate fixtures via `../fixtures/generated/<filename>` (from `core/tests/common/mod.rs`).
* Many tests expect specific files like `pbix_legacy_one_query_a.pbix` and will fail if missing.
* The directory is treated as generated output (not “source of truth”): `.gitignore` ignores `fixtures/generated/*.xlsx`, `*.pbix`, etc. 

### CI already generates fixtures, but validation/strictness is weak today

* CI installs the Python generator and runs: `generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --force` before `cargo test --workspace`. 
* The generator script currently **prints errors but does not fail the process**. It catches exceptions per scenario and continues, meaning CI can “successfully generate fixtures” while still missing outputs, leading to later “missing file” test failures.

### Release smoke workflow references generated fixtures directly

* The release workflow’s smoke job runs diffs against `fixtures/generated/minimal.xlsx` and `fixtures/generated/col_append_right_*.xlsx` (and does so from a checkout).
  If those files aren’t present (and they’re generally not committed, per `.gitignore`), the workflow is fragile unless it also generates fixtures.

### Manifests and generators already exist, including PBIX-with-dependencies

* PBIX fixtures are generated by `fixtures/src/generators/pbix.py`.
* Some PBIX scenarios depend on **other generated outputs** via `base_file: "generated/m_change_literal_a.xlsx"` etc. That means manifest ordering and dependency presence matter.

These realities drive the Phase 8 plan: **make fixture generation strict + verify outputs** and **document how code flows through the system**.

---

## Workstream A: Harden fixtures

### A1) Make fixture generation “strict by default” (no silent partial success)

**Objective:** If any scenario fails (missing template, unknown generator, runtime error), the `generate-fixtures` step must exit non‑zero. This prevents missing-file failures from leaking into unrelated test phases. This is directly aligned with the maintainability warning in the design evaluation.

**Concrete changes (Python generator):**

1. **Change failure semantics** in `fixtures/src/generate.py`:

   * Track whether *any* scenario failed or was skipped due to unknown generator.
   * At end, `sys.exit(1)` if any failures occurred.
   * Treat “unknown generator” as an error, not a warning.
   * Keep an opt-in flag like `--continue-on-error` for local exploratory runs (but CI should not use it).

   Why this matters: today, the script prints stack traces but still finishes successfully.

2. **Make `--force` real** (or remove it):

   * CI passes `--force` today. 
   * Implement consistent semantics:

     * If `--force` is set: overwrite outputs.
     * If not set: error if an output exists and would be overwritten *or* skip deterministically (pick one; error is safer for CI correctness).

3. Add a `--clean` option:

   * Delete only within the chosen output dir (default `fixtures/generated`) to avoid foot‑guns.
   * Use it in CI to guarantee a clean slate even on cached runners.

**Acceptance criteria:**

* If any generator raises (e.g., missing template in `CopyTemplateGenerator`), the fixture step fails immediately with a clear scenario ID and reason.
* CI never reaches `cargo test` with incomplete fixtures caused by partial generation.

---

### A2) Add manifest “preflight” validation (catch missing dependencies early)

**Objective:** Fail *before* generation when the manifest is internally inconsistent (duplicate outputs, missing template paths, or `base_file` dependencies that can’t possibly be satisfied).

This is especially important because PBIX scenarios may depend on earlier generated XLSX outputs.

**Preflight checks to implement in `generate-fixtures`:**

1. **Output name uniqueness**

   * Build a set of all outputs across scenarios; fail if duplicates exist (duplicates silently overwrite otherwise).

2. **Required arg file existence**

   * For generator args known to point to files (e.g., `template`, `base_file`, `model_schema_file`), verify resolvability with the same lookup rules used by generators (current generators try `fixtures/<path>` as fallback).

3. **Generated dependency ordering**

   * When `base_file` points into `generated/…`, validate that:

     * The referenced file appears as an output in this manifest **earlier** than the dependent scenario, or
     * It already exists in output_dir (for incremental local runs).
   * Provide actionable errors: “Scenario X depends on generated/Y which is not produced earlier in the manifest.”

**Acceptance criteria:**

* Mis-ordered or missing PBIX base dependencies fail in the fixture step, not later via missing files in tests.

---

### A3) Add checksum/lock validation (reproducible-or-validated fixtures)

Phase 8 explicitly calls for reproducibility or checksum validation in CI.

You have two viable “grounded” options; I recommend doing both in a layered way:

#### Option 1 (fastest to ship): **Validate presence + structural sanity**

* Add `generate-fixtures --verify` that:

  * Confirms every expected output exists.
  * Confirms key file types are structurally parseable:

    * For `.xlsx`/`.xlsm`/`.pbit`/`.pbix`: open as ZIP successfully.
    * For `.xlsx`: verify `[Content_Types].xml` exists (or the manifest’s corrupt-container scenarios intentionally omit it).
* This alone eliminates most “missing file” and “empty output” failures.

#### Option 2 (recommended): **Lock file with content checksums**

* Introduce a lock file per manifest, e.g.:

  * `fixtures/manifest_cli_tests.lock.json`
  * (optionally later) `fixtures/manifest_perf_e2e.lock.json` 

* Add generator modes:

  * `--write-lock <path>`: generate outputs then compute checksums and write lock.
  * `--verify-lock <path>`: compute checksums and compare to lock; fail with a diff-like report (missing, extra, mismatch).

**Important detail for “grounded reality”: ZIP-based artifacts aren’t trivially byte-stable**
Many OpenXML writers embed timestamps (for example in `docProps/core.xml`) which can change even if workbook semantics don’t. So the checksum strategy should be “content-normalized” for ZIP containers:

* For ZIP-like formats (`.xlsx`, `.xlsm`, `.pbix`, `.pbit`, `.zip`):

  * Hash each entry’s *contents* (sorted by filename).
  * Optionally normalize known volatile fields (e.g., strip/normalize created/modified timestamps in `docProps/core.xml`) before hashing.
  * Then hash the concatenation of `(entry_name, entry_content_hash)` pairs into one final digest.

* For non-ZIP files (e.g., `.txt`), hash raw bytes.

This gives a stable checksum signal without requiring the generator to rewrite internal ZIP metadata.

**Acceptance criteria:**

* A clean CI run can:

  * Generate fixtures from `fixtures/manifest_cli_tests.yaml`
  * Verify them against `fixtures/manifest_cli_tests.lock.json` with deterministic results.
* Any fixture drift becomes a *fixture step* failure with a clear report, not a downstream surprise.

---

### A4) Add a “fixture references” guard (tests + workflows must match manifests)

The maintainability report explicitly calls out fixture hygiene and missing PBIX fixture paths as a hazard, and recommends treating fixtures as first-class artifacts with CI validation.

A lock file validates what you generated; it does *not* guarantee you generated everything tests/workflows reference unless those references are checked too.

**Add a small script (Python) that:**

1. Scans Rust test sources for `fixture_path("...")` usage:

   * `core/tests/**` and `cli/tests/**` are the minimum.
2. Scans workflows for `fixtures/generated/...` references (release smoke is the key one).
3. Compares the referenced filenames to the outputs listed in the manifest(s) used by that workflow:

   * For CI unit/integration tests: `fixtures/manifest_cli_tests.yaml`
   * For release smoke: either reuse `manifest_cli_tests.yaml` or a new dedicated smoke manifest (see A5).

**Output should be actionable:**

* “These tests reference fixtures not present in fixtures/manifest_cli_tests.yaml: …”
* “These workflow steps reference fixtures not produced by fixtures/manifest_release_smoke.yaml: …”

**Acceptance criteria:**

* Adding a new `fixture_path("new.xlsx")` test without updating the manifest fails fast in CI with a clear message, before generation or tests run.

---

### A5) Fix release smoke workflow so it never depends on pre-existing generated files

Right now release smoke runs diffs against files in `fixtures/generated/...`.
Phase 8 should make this robust from a clean checkout.

**Two grounded implementation choices:**

#### Choice A (minimal change): Generate the full CLI test fixture set in release smoke

* Reuse the same steps as CI:

  * Setup Python
  * Install requirements
  * Install `excel-fixtures`
  * `generate-fixtures --manifest fixtures/manifest_cli_tests.yaml --force`
* Then run smoke diff commands.

Pros: simplest; no new manifest.
Cons: may generate more fixtures than necessary for a smoke check.

#### Choice B (recommended): Create a tiny “release smoke” manifest

Create `fixtures/manifest_release_smoke.yaml` containing only what the smoke job needs:

* `minimal.xlsx`
* `col_append_right_a.xlsx`
* `col_append_right_b.xlsx`

`col_append_right_*` already exists as a scenario in the CLI tests manifest excerpt.

Then update release smoke job to:

* Generate from `manifest_release_smoke.yaml`
* Optionally verify with `manifest_release_smoke.lock.json`

Pros: faster, clearer contract.
Cons: one more manifest to maintain (mitigated by the reference-check script in A4).

**Acceptance criteria:**

* Release smoke passes from a fresh checkout without relying on committed generated artifacts.

---

### A6) Document the fixture system for maintainers (ties into Workstream B)

Even though Phase 8’s doc requirement is broader, fixture hardening will introduce new “how to update fixtures” workflows. Document it so maintainers don’t fight the system.

Include:

* What `fixtures/generated` is (generated output dir).
* What each manifest is for (`manifest_cli_tests.yaml`, perf manifests, release smoke manifest).
* The new “lock/verify/update” loop.
* The rule: **tests and workflows must only reference fixtures produced by their manifest**.

This aligns with the “first-class artifacts” recommendation.

---

## Workstream B: Maintainer docs (entry-point maps + conceptual narrative)

This workstream is explicitly called out in Phase 8 and reinforced by the design evaluation recommendations.

### B1) Create `docs/maintainers/entrypoints.md` (entry‑point maps)

**Goal:** A maintainer should be able to answer “where do I start?” in under a minute, for each host and each subsystem.

**Proposed contents (grounded in actual entry points):**

1. **Core public entry points (library consumers)**

   * `WorkbookPackage::open(...)` and `WorkbookPackage::diff(...)` / streaming variants in `core/src/package.rs`.
   * Mention `DiffConfig` and `DiffSession`/`StringPool` relationship; `with_default_session` exists and is used by legacy wrappers and helpers.
   * Streaming APIs revolve around `DiffSink` and the `begin → emit → finish` lifecycle.

2. **CLI entry points**

   * `cli/src/commands/diff.rs` is the main driver; it imports `WorkbookPackage`, `DiffConfig`, `JsonLinesSink`, `with_default_session`, etc.
   * Host selection and opening are in `cli/src/commands/host.rs` (HostKind → open package).

3. **WASM entry points**

   * `wasm/src/lib.rs` exports the diff function(s), does host-kind selection via `ui_payload::host_kind_from_name`, and constrains memory via `config.max_wasm_memory_bytes = Some(256 * 1024 * 1024)`.

4. **Desktop (Tauri) entry points**

   * The Tauri command handler orchestrates diff, store usage, and UI payload generation.

5. **Web entry points (JS)**

   * `web/diff_worker.js` and its client wrapper coordinate calling WASM and returning results; include a pointer map to `web/main.js` and `web/native_diff_client.js` for platform selection. 

6. “If you’re changing X, start here”

   * Example guidance (aligned with design evaluation):

     * Alignment/moves: `engine/grid_diff.rs` → `move_mask` → `alignment/*` (doc-level pointer, not necessarily exhaustive). 
     * Streaming behavior: `core/src/sink.rs` + engine streaming entry points.
     * Parsing: `excel_open_xml.rs`, `grid_parser.rs`, and DataMashup framing/building modules.

**Acceptance criteria:**

* A new maintainer can trace “CLI diff request” → “core diff engine” in < 5 minutes using only this doc.

---

### B2) Create `docs/maintainers/architecture.md` (parse → IR → diff → output narrative)

The design evaluation describes the architectural spine clearly (“bytes → meaning → IR → operations”) and explicitly recommends capturing this as a narrative doc.

**Doc should teach the mental model using the actual types:**

1. **Parse (bytes → structured meaning)**

   * Container safety + bounded reads (ZIP/OPC + limits). 
   * OpenXML parsing (`excel_open_xml`, `grid_parser`) produces a `Workbook` with `Sheet`/`Grid`.
   * DataMashup parsing split: framing vs semantic build.

2. **IR (intermediate representation)**

   * Define IR plainly: the internal, minimal model the diff engine uses.
   * Key types to call out:

     * `WorkbookPackage` (top-level: workbook + optional DataMashup/VBA/etc).
     * `Workbook`, `Sheet`, `Grid`, `CellValue`, `Cell`.
     * `StringPool` / `StringId` and why string interning exists (compact reports, determinism, shared naming).

3. **Diff (IR → operations)**

   * The engine’s role: orchestrate grid/object/M diffs and emit a stream of `DiffOp` operations.
   * Explain the two output modes:

     * **Collected**: `DiffReport` holds ops in memory.
     * **Streaming**: `DiffSink` receives ops incrementally; determinism relies on the sink lifecycle rules.
   * Mention hardening controls conceptually (timeouts/memory/ops); keep it high-level but point to config.

4. **Output (operations → serialized/UX)**

   * JSON report serialization (`output/json.rs`) and JSONL streaming (`JsonLinesSink`).
   * Host-specific formatting:

     * CLI renders to text/JSON/JSONL.
     * Desktop/Web build UI payloads.

**Acceptance criteria:**

* The doc names the actual types and files so it remains “codebase-real,” not aspirational.

---

### B3) Add a short version of the narrative to `core/src/lib.rs`, with links

Phase 8 allows the narrative to live in `core/src/lib.rs` or `/docs`. Best practice here: put a **short** overview in `lib.rs` and link to the detailed `/docs` pages.

Concrete update:

* Add a crate-level section:

  * “Architecture overview: Parse → IR → Diff → Output”
  * Links to:

    * `docs/maintainers/architecture.md`
    * `docs/maintainers/entrypoints.md`

This is consistent with how the crate already centralizes exported APIs and advanced entry points.

---

### B4) Create `docs/streaming_contract.md` (because the code references it)

`core/src/sink.rs` explicitly references `docs/streaming_contract.md` for determinism/lifecycle/string table rules. 

If that file is missing or incomplete, Phase 8 should add it and align it with:

* The documented lifecycle (`begin → emit → finish`) in `DiffSink`.
* The determinism/streaming tests that enforce consistent ordering and summary correctness.

**Acceptance criteria:**

* A maintainer can read the contract doc and understand:

  * When `begin` happens
  * What “string table rules” mean
  * How partial/incomplete diffs are represented (warnings, `complete=false`)
  * What errors guarantee `finish()` still runs

---

## Recommended sequencing (so Phase 8 lands cleanly)

1. **A1 Strict generator behavior** (fail-fast)
2. **A2 Manifest preflight validation** (dependency/order sanity)
3. **A5 Release smoke workflow uses fixture generation** (eliminate external fragility)
4. **A4 Reference-check script** (prevents drift)
5. **A3 Lock/checksum verification** (strongest long-term guarantee)
6. **B1/B2/B3/B4 Docs** (entrypoints + architecture + streaming contract + lib.rs links)

This ordering ensures the “missing-file failures” are eliminated early, and the docs then describe a stable reality.

---

## Phase 8 definition of done

You can treat Phase 8 as complete when all of the following are true:

### Fixtures

* CI fixture generation fails if any scenario fails (no silent partial success).
* CI validates fixture presence (and ideally checksums) before running Rust tests.
* Release smoke workflow generates the fixtures it uses from a manifest before running the smoke diffs.
* A reference-check script prevents tests/workflows from referencing fixtures that aren’t produced by their manifest.

### Docs

* `docs/maintainers/entrypoints.md` exists and maps CLI/WASM/Desktop/Web → core APIs (with file paths).
* `docs/maintainers/architecture.md` exists and explains parse → IR → diff → output using real types (`WorkbookPackage`, `Workbook`, `Grid`, `DiffOp`, `DiffReport`, `DiffSink`).
* `core/src/lib.rs` links to the maintainer docs (short overview + pointers).
* `docs/streaming_contract.md` exists and matches the streaming contract described in `DiffSink`. 

