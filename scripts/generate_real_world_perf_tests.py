#!/usr/bin/env python3
"""
Generate core/tests/e2e_perf_real_world.rs from datasets/real_world/cases.yaml.

We keep the Rust test list deterministic and data-driven so adding a new case is a YAML-only change.
"""

from __future__ import annotations

import argparse
import re
from pathlib import Path

from real_world_lib import load_cases


def rust_ident(s: str) -> str:
    # Convert to a safe Rust identifier.
    s2 = re.sub(r"[^A-Za-z0-9_]+", "_", s)
    if not s2:
        s2 = "case"
    if s2[0].isdigit():
        s2 = "case_" + s2
    return s2


def derived_b_dataset_id(case_id: str) -> str:
    return f"derived_b__{case_id}"


HEADER = """// AUTO-GENERATED by scripts/generate_real_world_perf_tests.py
// Do not edit by hand.

#![cfg(feature = "perf-metrics")]
#![allow(dead_code, non_snake_case)]

use excel_diff::perf::DiffMetrics;
use excel_diff::{CallbackSink, ContainerLimits, DiffConfig, DiffSink, JsonLinesSink, PbixPackage, WorkbookPackage};
use serde::Deserialize;
use std::fs::File;
use std::io::Read;
use std::path::{Path, PathBuf};

#[derive(Debug, Deserialize)]
struct CorpusIndex {
    version: u32,
    files: Vec<CorpusFileEntry>,
}

#[derive(Debug, Deserialize)]
struct CorpusFileEntry {
    sha256: Option<String>,
    extension: Option<String>,
    filename: Option<String>,
    dataset_id: Option<String>,
    size_bytes: Option<u64>,
}

fn corpus_dir() -> PathBuf {
    std::env::var_os("TABULENSIS_REAL_WORLD_CORPUS_DIR")
        .map(PathBuf::from)
        .unwrap_or_else(|| PathBuf::from(env!("CARGO_MANIFEST_DIR")).join("..").join("corpus_public"))
}

fn load_corpus_index() -> CorpusIndex {
    let index_path = corpus_dir().join("index.json");
    let bytes = std::fs::read(&index_path).unwrap_or_else(|e| {
        panic!("failed to read corpus index {}: {e}", index_path.display());
    });
    serde_json::from_slice(&bytes).unwrap_or_else(|e| {
        panic!("failed to parse corpus index {}: {e}", index_path.display());
    })
}

fn resolve_dataset_path(dataset_id: &str) -> PathBuf {
    let index = load_corpus_index();
    for entry in &index.files {
        if entry.dataset_id.as_deref() == Some(dataset_id) {
            let filename = entry.filename.as_deref().unwrap_or_else(|| {
                panic!("corpus entry for {dataset_id} missing filename in index.json");
            });
            return corpus_dir().join(filename);
        }
    }
    panic!("dataset_id not found in corpus index: {dataset_id}");
}

fn read_file_with_size(path: &Path) -> (Vec<u8>, u64) {
    let bytes = std::fs::metadata(path).map(|m| m.len()).unwrap_or(0);
    let mut file = File::open(path).unwrap_or_else(|e| panic!("failed to open {}: {e}", path.display()));
    let mut data = Vec::with_capacity(bytes.min(usize::MAX as u64) as usize);
    file.read_to_end(&mut data).unwrap_or_else(|e| panic!("failed to read {}: {e}", path.display()));
    (data, bytes)
}

fn log_perf_metric(case_id: &str, metrics: &DiffMetrics, old_bytes: u64, new_bytes: u64, op_count: usize, workload_id: u32) {
    let total_input_bytes = old_bytes.saturating_add(new_bytes);
    println!(
        "PERF_METRIC {case_id} total_time_ms={} parse_time_ms={} diff_time_ms={} signature_build_time_ms={} move_detection_time_ms={} alignment_time_ms={} cell_diff_time_ms={} op_emit_time_ms={} report_serialize_time_ms={} peak_memory_bytes={} grid_storage_bytes={} string_pool_bytes={} op_buffer_bytes={} alignment_buffer_bytes={} rows_processed={} cells_compared={} anchors_found={} moves_detected={} hash_lookups_est={} allocations_est={} old_bytes={} new_bytes={} total_input_bytes={} op_count={} workload_id={}",
        metrics.total_time_ms,
        metrics.parse_time_ms,
        metrics.diff_time_ms,
        metrics.signature_build_time_ms,
        metrics.move_detection_time_ms,
        metrics.alignment_time_ms,
        metrics.cell_diff_time_ms,
        metrics.op_emit_time_ms,
        metrics.report_serialize_time_ms,
        metrics.peak_memory_bytes,
        metrics.grid_storage_bytes,
        metrics.string_pool_bytes,
        metrics.op_buffer_bytes,
        metrics.alignment_buffer_bytes,
        metrics.rows_processed,
        metrics.cells_compared,
        metrics.anchors_found,
        metrics.moves_detected,
        metrics.hash_lookups_est,
        metrics.allocations_est,
        old_bytes,
        new_bytes,
        total_input_bytes,
        op_count,
        workload_id
    );
}

struct CountingSink {
    inner: Box<dyn DiffSink>,
    count: usize,
}

impl CountingSink {
    fn new(inner: Box<dyn DiffSink>) -> Self {
        Self { inner, count: 0 }
    }
}

impl DiffSink for CountingSink {
    fn begin(&mut self, pool: &excel_diff::StringPool) -> Result<(), excel_diff::DiffError> {
        self.inner.begin(pool)
    }

    fn emit(&mut self, op: excel_diff::DiffOp) -> Result<(), excel_diff::DiffError> {
        self.count += 1;
        self.inner.emit(op)
    }

    fn finish(&mut self) -> Result<(), excel_diff::DiffError> {
        self.inner.finish()
    }
}

#[derive(Clone, Copy)]
enum Workload {
    StreamingFast = 0,
    StreamingJsonl = 1,
}

fn run_case(case_id: &str, dataset_a: &str, dataset_b: &str, workload: Workload, expect_ops: bool) {
    let path_a = resolve_dataset_path(dataset_a);
    let path_b = resolve_dataset_path(dataset_b);
    let (old_data, old_bytes) = read_file_with_size(&path_a);
    let (new_data, new_bytes) = read_file_with_size(&path_b);

    let config = DiffConfig::default();

    let ext_a = path_a.extension().and_then(|s| s.to_str()).unwrap_or("").to_lowercase();
    let ext_b = path_b.extension().and_then(|s| s.to_str()).unwrap_or("").to_lowercase();
    assert_eq!(ext_a, ext_b, "dataset A/B extensions should match for a case");

    if ext_a == "pbix" || ext_a == "pbit" {
        let pkg_a = PbixPackage::open(std::io::Cursor::new(old_data)).expect("pbix A should parse");
        let pkg_b = PbixPackage::open(std::io::Cursor::new(new_data)).expect("pbix B should parse");

        let mut sink = match workload {
            Workload::StreamingFast => CountingSink::new(Box::new(CallbackSink::new(|_op| {}))),
            Workload::StreamingJsonl => CountingSink::new(Box::new(JsonLinesSink::new(std::io::sink()))),
        };

        let summary = pkg_a
            .diff_streaming(&pkg_b, &config, &mut sink)
            .expect("pbix diff_streaming should succeed");

        assert!(summary.complete, "expected pbix diff to complete");
        assert_eq!(summary.op_count, sink.count, "op_count should match sink-emitted ops");
        if expect_ops {
            assert!(summary.op_count > 0, "expected ops for case");
        } else {
            assert_eq!(summary.op_count, 0, "expected no ops for case");
        }

        let metrics = summary.metrics.expect("expected perf metrics");
        log_perf_metric(case_id, &metrics, old_bytes, new_bytes, summary.op_count, workload as u32);
        return;
    }

    // Default: OpenXML workbooks.
    let limits = ContainerLimits {
        max_entries: 10_000,
        max_part_uncompressed_bytes: 512 * 1024 * 1024,
        max_total_uncompressed_bytes: 1024 * 1024 * 1024,
    };

    let mut sink = match workload {
        Workload::StreamingFast => CountingSink::new(Box::new(CallbackSink::new(|_op| {}))),
        Workload::StreamingJsonl => CountingSink::new(Box::new(JsonLinesSink::new(std::io::sink()))),
    };

    let summary = WorkbookPackage::diff_openxml_streaming_fast_with_limits(
        std::io::Cursor::new(old_data),
        std::io::Cursor::new(new_data),
        limits,
        &config,
        &mut sink,
    )
    .expect("diff_openxml_streaming_fast_with_limits should succeed");

    assert!(summary.complete, "expected streaming diff to complete");
    assert_eq!(summary.op_count, sink.count, "op_count should match sink-emitted ops");
    if expect_ops {
        assert!(summary.op_count > 0, "expected at least one op");
    } else {
        assert_eq!(summary.op_count, 0, "expected no ops");
    }

    let metrics = summary.metrics.expect("expected perf metrics");
    log_perf_metric(case_id, &metrics, old_bytes, new_bytes, summary.op_count, workload as u32);
}

"""


def main() -> int:
    parser = argparse.ArgumentParser(description="Generate real-world perf tests from cases.yaml")
    parser.add_argument(
        "--cases",
        type=Path,
        default=Path("datasets/real_world/cases.yaml"),
        help="Path to cases.yaml",
    )
    parser.add_argument(
        "--out",
        type=Path,
        default=Path("core/tests/e2e_perf_real_world.rs"),
        help="Output Rust test file path",
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="Check that the output is up to date (exit non-zero if it would change)",
    )
    args = parser.parse_args()

    data = load_cases(args.cases)
    cases = data.get("cases", [])

    lines: list[str] = [HEADER]

    for case in sorted(cases, key=lambda c: str(c.get("case_id") if isinstance(c, dict) else "")):
        if not isinstance(case, dict):
            continue
        case_id = str(case.get("case_id") or "").strip()
        if not case_id:
            continue
        dataset_a = str(case.get("dataset_a") or "").strip()
        dataset_b = case.get("dataset_b")
        workload = str(case.get("workload") or "diff_streaming_fast").strip()
        expect_ops = case.get("expect_ops", None)

        if not dataset_a or not dataset_b:
            continue

        if isinstance(dataset_b, str):
            dataset_b_id = dataset_b.strip()
        elif isinstance(dataset_b, dict):
            # Derived dataset; export script is expected to materialize it into the corpus.
            dataset_b_id = derived_b_dataset_id(case_id)
        else:
            continue

        wl = "Workload::StreamingFast"
        if workload == "diff_streaming_jsonl":
            wl = "Workload::StreamingJsonl"
        elif workload == "diff_streaming_fast":
            wl = "Workload::StreamingFast"
        else:
            raise SystemExit(f"Unsupported workload in cases.yaml: {workload} (case_id={case_id})")

        # Best-effort expectation: treat identical workloads as no-ops unless explicitly set.
        if expect_ops is None:
            expect_ops_bool = True
        else:
            expect_ops_bool = bool(expect_ops)

        fn_name = rust_ident(case_id)
        lines.append("#[test]\n")
        lines.append(
            '#[ignore = "Real-world perf case: run via scripts/export_real_world_metrics.py (or cargo test --features perf-metrics -- --ignored)"]\n'
        )
        lines.append(f"fn {fn_name}() {{\n")
        lines.append(
            f'    run_case("{case_id}", "{dataset_a}", "{dataset_b_id}", {wl}, {str(expect_ops_bool).lower()});\n'
        )
        lines.append("}\n\n")

    content = "".join(lines)

    out_path: Path = args.out
    if args.check and out_path.exists():
        current = out_path.read_text(encoding="utf-8")
        if current != content:
            print(f"ERROR: {out_path} is out of date. Re-run generator.")
            return 2
        print(f"{out_path} is up to date.")
        return 0

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(content, encoding="utf-8")
    print(f"Wrote: {out_path} ({len(cases)} case(s))")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
